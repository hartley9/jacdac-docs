{"version":3,"file":"8e0cf18a-05a7603ac6435f24db56.js","mappings":";;;;;AAAA,wBAAwB,mBAAO,CAAC,KAAgG;;AAEhI,mBAAO,CAAC,KAAkC;;AAE1C;AACA;AACA,oBAAoB,sBAAsB;AAC1C;;AAEA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,YAAY;AACZ;;AAEA,cAAc,cAAc;AAC5B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,0FAA0F,cAAc;AACxG;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA,mBAAmB,eAAe;AAClC,+BAA+B;AAC/B;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,gBAAgB;AACpC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,4DAA4D,8CAA8C;AAC1G;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,4DAA4D,8CAA8C,6CAA6C;AACvJ;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,cAAc;AAChC;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,QAAQ;AAC9B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,SAAS;AAC/B;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,sBAAsB,UAAU;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,mBAAmB,eAAe;AAClC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,oBAAoB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,oBAAoB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,oCAAoC,wBAAwB,oBAAoB;AAChF;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,iCAAiC;AACjC;;AAEA;AACA,mBAAmB,oBAAoB;AACvC;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA,wFAAwF,aAAa;AACrG;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,iDAAiD,+BAA+B,qBAAM,MAAM,qBAAM,CAAC,oDAAoD;AACvJ;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS;AACT;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,8sCAA8sC;AAC9sC,EAAE;;AAEF;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD,oBAAoB;AACpB;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,iEAAiE,cAAc;AAC/E;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,+EAA+E;AAC/E;AACA;AACA;;AAEA,WAAW,aAAa;AACxB;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;;AAEA,8DAA8D,4BAA4B;AAC1F;AACA;;AAEA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA,iBAAiB,SAAS;AAC1B;;AAEA,+HAA+H,0BAA0B;AACzJ;AACA;;AAEA;AACA;;AAEA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB,gBAAgB;AACpC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0EAA0E;AAC1E;;AAEA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA,wBAAwB,gBAAgB;AACxC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA,YAAY;AACZ;;AAEA,oBAAoB,gBAAgB;AACpC;AACA;;AAEA,oBAAoB,gBAAgB;AACpC;AACA;;AAEA;AACA;;AAEA;;AAEA,wBAAwB,gBAAgB;AACxC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;AACA;;AAEA,sBAAsB,0BAA0B;AAChD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,qBAAqB,iBAAiB;AACtC;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,iCAAiC;;AAEjC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,uCAAuC;AACvC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,6BAA6B,QAAQ;AACrC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,gCAAgC,cAAc;AAC9C;;AAEA,uBAAuB,UAAU;AACjC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,uBAAuB,qCAAqC;AAC5D;;AAEA;AACA;;AAEA;;AAEA,sDAAsD,UAAU;AAChE;;AAEA;AACA;AACA,IAAI,wBAAwB,UAAU;AACtC;;AAEA;AACA;;AAEA;AACA;;AAEA,qBAAqB,qBAAqB;AAC1C;AACA;;AAEA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,4FAA4F,eAAe;AAC3G;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wEAAwE,eAAe;AACvF;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,uBAAuB,qBAAqB;AAC5C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,kBAAkB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,kBAAkB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC,uBAAuB;AACxB;AACA,CAAC,iDAAiD;AAClD;AACA,CAAC,+CAA+C;AAChD;AACA,CAAC,qDAAqD;AACtD;AACA,CAAC,yDAAyD;AAC1D;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,oCAAoC,2BAA2B;AAC/D;;AAEA;AACA;;AAEA;AACA,2FAA2F;AAC3F;;AAEA;;AAEA,sBAAsB,cAAc;AACpC;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP,KAAK,SAAS;AACd;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY;AACZ;AACA,SAAS;;AAET;AACA;;AAEA;AACA;;AAEA;AACA;AACA,MAAM;AACN;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,uBAAuB,4CAA4C;AACnE;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0EAA0E,eAAe;AACzF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;AACT;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,SAAS,wDAAwD;AACjE;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA,uBAAuB,iBAAiB;AACxC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,iCAAiC,mBAAmB;AACpD;AACA;;AAEA;AACA;;AAEA;AACA,WAAW;;AAEX;AACA;AACA;AACA,SAAS;;AAET;AACA,QAAQ;;AAER;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,6BAA6B,kBAAkB;AAC/C;AACA;AACA,UAAU;AACV;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA,2BAA2B,kBAAkB;AAC7C;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;;AAEA,yBAAyB,aAAa;AACtC;AACA;;AAEA;;AAEA;AACA;AACA,MAAM;AACN;AACA;;AAEA,qDAAqD,gDAAgD,+CAA+C;AACpJ;AACA;AACA;;AAEA;AACA;;AAEA,6BAA6B,oBAAoB;AACjD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,mCAAmC,UAAU;AAC7C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,WAAW,mBAAmB;AAC9B;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;;AAEA,kBAAkB,UAAU;AAC5B;AACA;;AAEA,wBAAwB,aAAa;AACrC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,QAAQ;AAC1B;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,QAAQ;AAC1B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW;AACX,UAAU;AACV;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,WAAW;AACX,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,UAAU;AACV;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,gBAAgB;AAChB;AACA;;AAEA,sBAAsB,uBAAuB;AAC7C;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA,qBAAqB,mBAAO,CAAC,KAAY;AACzC;AACA;;AAEA;AACA;AACA,gBAAgB,mBAAO,CAAC,KAAM;AAC9B;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA,wBAAwB;AACxB;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0DAA0D;AAC1D;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oGAAoG;AACpG;AACA;AACA;AACA;;AAEA,qBAAqB,qBAAqB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA,qBAAqB,mBAAmB;AACxC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,4BAA4B,iBAAiB;AAC7C;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI,wBAAwB,UAAU;AACtC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,gBAAgB;AACpC,wCAAwC;AACxC;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,gBAAgB;AACpC,8DAA8D;AAC9D;;AAEA;AACA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,kBAAkB;AACpC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mDAAmD;AACnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mEAAmE;AACnE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,6CAA6C;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,GAAG,iEAAiE;AACpE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,WAAW,uBAAuB;AAClC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,gCAAgC,WAAW;AAC3C,yCAAyC;AACzC;;AAEA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;AACA,uDAAuD,kCAAkC;AACzF;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA,sCAAsC;AACtC;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,OAAO;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,qBAAqB,eAAe;AACpC;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;AACD,mIAAmI,qBAAM,GAAG,qBAAM;;AAElJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,cAAc;AACtC;AACA;AACA;;AAEA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP,sBAAsB,mBAAmB;AACzC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP,sBAAsB,mBAAmB;AACzC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,oCAAoC,6BAA6B,cAAc;AAC/E;AACA;;AAEA,eAAe,aAAa;AAC5B;AACA;;AAEA,oBAAoB,qBAAqB;AACzC;AACA;;AAEA,+DAA+D,OAAO;AACtE;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gHAAgH,OAAO;AACvH;AACA;;AAEA,iFAAiF,OAAO;AACxF;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP,sBAAsB,mBAAmB;AACzC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD,2BAA2B;AAC3B;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ,SAAS;AACjB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,2CAA2C,MAAM;AACjD;AACA;;AAEA,eAAe,OAAO;AACtB;AACA;;AAEA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA,wBAAwB;AACxB,SAAS;AACT,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,6BAA6B,MAAM;AACnC;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA,sDAAsD,IAAI;AAC1D;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA;AACA,qCAAqC,aAAa;AAClD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;AACA,GAAG;AACH,CAAC;AACD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,WAAW,GAAG;AACd;AACA;AACA;;AAEA;AACA;AACA,QAAQ;;AAER;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,wBAAwB;AAC7C;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,wBAAwB;AAC7C;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,uCAAuC;AACvC;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sLAAsL;AACtL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,4EAA4E,OAAO;AACnF;AACA;;AAEA,qBAAqB,wBAAwB;AAC7C;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB,oBAAoB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA,SAAS,gBAAgB;AACzB;AACA;;AAEA,gBAAgB,WAAW;AAC3B;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,SAAS,6BAA6B;AACtC;AACA;AACA;AACA;AACA;AACA,MAAM;;AAEN;;AAEA;;AAEA,kCAAkC,aAAa;AAC/C;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,kBAAkB;AACtC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,yBAAyB,iBAAiB;AAC1C;AACA;AACA,KAAK;AACL,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;AACA;AACA,uCAAuC,aAAa;AACpD;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kEAAkE;AAClE;;AAEA;AACA;AACA;AACA;AACA,kEAAkE;AAClE;;AAEA;AACA;AACA;AACA,OAAO;AACP;;AAEA,uBAAuB,UAAU;AACjC;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC,iCAAiC;AAClC;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;;AAEN;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,oBAAoB,OAAO;AAC3B;AACA;AACA,GAAG;AACH;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,8EAA8E;AAC9E;AACA;;AAEA,uBAAuB,aAAa;AACpC;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,2BAA2B,UAAU;AACrC;AACA;AACA,IAAI;AACJ;AACA;;AAEA,uBAAuB,UAAU;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,sBAAsB;AACxC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;;AAEA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA,uBAAuB,uBAAuB;AAC9C;AACA;AACA;;AAEA;AACA;;AAEA,4BAA4B,UAAU;AACtC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,uBAAuB,uBAAuB;AAC9C;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;;AAEA,qBAAqB,iBAAiB;AACtC;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,wJAAwJ;AACxJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,2GAA2G,GAAG;AAC9G;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA,yBAAyB,UAAU;AACnC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA,kCAAkC,WAAW;AAC7C,2CAA2C;AAC3C;;AAEA;;AAEA,uBAAuB,iBAAiB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA,yBAAyB,2BAA2B;AACpD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC,oBAAoB,iBAAiB;AACr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uBAAuB,eAAe;AACtC;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,6CAA6C,aAAa;AAC1D;AACA,UAAU,0CAA0C,aAAa;AACjE,0BAA0B,UAAU;AACpC;AACA;AACA,UAAU,0CAA0C,aAAa;AACjE,4BAA4B,YAAY;AACxC,4BAA4B,UAAU;AACtC;AACA;AACA;AACA,UAAU;AACV;;AAEA,6BAA6B,aAAa;AAC1C,+BAA+B,aAAa;AAC5C,gCAAgC,YAAY;AAC5C,gCAAgC,UAAU;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,kHAAkH;AAClH;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,+BAA+B,8EAA8E;AAC7G;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,iCAAiC;;AAEjC;AACA;AACA;;AAEA;;AAEA;;AAEA,wDAAwD;AACxD;;AAEA;AACA,iCAAiC;;AAEjC;AACA;AACA;;AAEA;;AAEA,wDAAwD;AACxD;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACz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iDAAiD;AACjD;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,gFAAgF;AAChF;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;AACA,sDAAsD;AACtD;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8CAA8C,uBAAuB;AACrE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,eAAe;AACf;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,iTAAiT;AACjT;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,yBAAyB,mCAAmC;AAC5D;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;AACA;AACA,KAAK;AACL,+EAA+E;AAC/E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC,yDAAyD;AAC1D;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;AAC1B;AACA;;AAEA;AACA;AACA,6JAA6J;AAC7J;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,cAAc;AACtC;AACA;AACA;;AAEA;;AAEA,0BAA0B,kBAAk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sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;;AAEA;AACA,oFAAoF,mCAAmC;AACvH;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,wEAAwE;AACxE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA,0BAA0B,mBAAmB,mBAAmB;AAChE;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,uBAAuB;AACvB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,+BAA+B;AAC/B;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB;AACtB;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,QAAQ;;AAER;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,aAAa;AAC/B;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,wGAAwG;AACxG;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,wEAAwE;AACxE;;AAEA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,gBAAgB;AAChB;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA,wBAAwB,iCAAiC;AACzD;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,cAAc;AACd,cAAc;AACd,cAAc;AACd,cAAc;AACd,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA,yBAAyB,mCAAmC;AAC5D;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,gDAAgD;AAChD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,4BAA4B,oCAAoC;AAChE;AACA;;AAEA;AACA;;AAEA,4BAA4B,qBAAqB;AACjD;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kCAAkC;AAC1D;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,uBAAuB,2BAA2B;AAClD;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,YAAY;;AAEZ;;AAEA,8BAA8B,qBAAqB;AACnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,+BAA+B;AACrD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,wBAAwB,2BAA2B;AACnD;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA,yBAAyB,kCAAkC;AAC3D;AACA;;AAEA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;;AAEA;AACA;;AAEA,+BAA+B,kCAAkC;AACjE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,iCAAiC;AACzD;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kCAAkC;AAC1D;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,qCAAqC;AACrC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,6EAA6E;AAC7E;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,WAAW,oBAAoB;AAC/B;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,gJAAgJ,mBAAmB;AACnK;AACA;AACA;AACA;;AAEA;AACA,4IAA4I;AAC5I;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA,8IAA8I;AAC9I;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,oVAAoV;AACpV;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;;AAER;AACA;AACA;;AAEA,aAAa,aAAa;AAC1B;AACA;AACA;AACA;;AAEA,4CAA4C,+BAA+B;AAC3E;;AAEA;AACA,sKAAsK;AACtK;AACA;;AAEA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;AACA;;AAEA;AACA;;AAEA,mCAAmC,uBAAuB;AAC1D;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA,iCAAiC,mBAAmB;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,eAAe;;AAEf,kCAAkC,+BAA+B;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,gCAAgC;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;AACA,4DAA4D,mBAAmB;AAC/E;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,iCAAiC,mBAAmB;AACpD;AACA;;AAEA;AACA,WAAW;AACX;;AAEA;AACA,OAAO;AACP;AACA;AACA;AACA;;AAEA,WAAW,oBAAoB;AAC/B;;AAEA;AACA;;AAEA,yBAAyB,mBAAmB;AAC5C;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mCAAmC,mBAAmB;AACtD;AACA;AACA;;AAEA;AACA;;AAEA,qCAAqC,mBAAmB;AACxD;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;;AAEA,6BAA6B,iBAAiB;AAC9C;;AAEA;AACA;;AAEA;AACA,SAAS;AACT;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,+SAA+S;AAC/S;;AAEA;AACA;;AAEA;AACA,QAAQ;AACR;;AAEA;AACA;;AAEA;AACA,QAAQ;;AAER;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA,iDAAiD;AACjD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,0CAA0C,wDAAwD;AAClG;AACA;AACA;AACA,iEAAiE;AACjE;AACA,GAAG,EAAE;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,uDAAuD;AACvD;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,0EAA0E,kBAAkB;AAC5F;AACA;AACA;;AAEA,uBAAuB,wBAAwB;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,2BAA2B,qBAAqB;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,qCAAqC,kBAAkB;AACvD;AACA;AACA;;AAEA,uBAAuB,wBAAwB;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,uKAAuK;AACvK;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;AACA,SAAS;AACT;AACA,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,6BAA6B;AACrD;AACA;AACA;;AAEA;AACA;AACA,0BAA0B,6BAA6B;AACvD;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA,cAAc;AACd;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA,aAAa;AACb;AACA,SAAS;AACT;;AAEA,0BAA0B,6BAA6B;AACvD;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,yBAAyB,iBAAiB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,oDAAoD,kBAAkB;AACtE;AACA;AACA;AACA,aAAa;AACb,YAAY;AACZ;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,sCAAsC;AAC9D;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA,4BAA4B,sBAAsB;AAClD;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,2BAA2B,oBAAoB;AAC/C;AACA;AACA;AACA;AACA;AACA,8CAA8C,kBAAkB;AAChE;AACA;;AAEA,8BAA8B,kBAAkB;AAChD;AACA;AACA;;AAEA,4BAA4B,kBAAkB;AAC9C;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,4BAA4B;AACxD;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA,4BAA4B,mCAAmC;AAC/D;;AAEA;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,oCAAoC;AAChE;;AAEA,sFAAsF;AACtF;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;;AAEA;AACA;;AAEA,0BAA0B,oCAAoC;AAC9D;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,mEAAmE;AACnE;AACA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,6DAA6D,sEAAsE;AACnI;;AAEA;AACA;AACA;AACA;AACA,2EAA2E;AAC3E;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,4BAA4B;AAC5B;AACA,6DAA6D;AAC7D;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,4BAA4B;AAC5B;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK,cAAc;AACnB;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;;AAEA,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,wHAAwH;AACxH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,6BAA6B;AAC7B;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,yCAAyC;AACzC;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,yBAAyB;AACzB;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB,mDAAmD;AAC5E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,6BAA6B,oEAAoE,oSAAoS,qEAAqE;AAC1c;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qDAAqD,kBAAkB;AACvE;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,6BAA6B,kDAAkD;AAC/E;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB,kDAAkD;AAC3E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB,iDAAiD;AAC1E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mDAAmD;AACnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,2GAA2G;AAC3G,MAAM,kJAAkJ;AACxJ;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oKAAoK;AACpK,kJAAkJ,mJAAmJ;AACrS;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,sGAAsG;AACtG;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,sGAAsG;AACtG;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,mBAAmB;AAC3C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wCAAwC;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA,iOAAiO;AACjO,MAAM;AACN;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,wKAAwK,oRAAoR;AAC5b;AACA;;AAEA,4BAA4B,6BAA6B;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK,mBAAmB;AACxB;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B;AAC3B;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B;AAC3B;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,SAAS;AACT,QAAQ;;AAER;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B;AAC3B;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,uBAAuB;AAC7C;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,2BAA2B;AAC3B;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,0BAA0B,0BAA0B;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,8CAA8C,cAAc;AAC5D;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iJAAiJ,yPAAyP;AAC1Y;AACA;;AAEA,4BAA4B,6BAA6B;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,0BAA0B;AAC1B;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAAS;AACT,QAAQ;;AAER;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,2BAA2B;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,0BAA0B;AAC1B;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,gCAAgC;AACxD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,qBAAqB;AACrB;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,4BAA4B,gBAAgB;AAC5C;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA,wBAAwB,iCAAiC;AACzD;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB;AACzB;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA,iEAAiE,4BAA4B,4BAA4B;AACzH;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,gCAAgC,mBAAmB;AACnD;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,cAAc;AACd;;AAEA;AACA,cAAc;AACd;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,YAAY;AACZ;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,uBAAuB,iBAAiB;AACxC;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA,0DAA0D;AAC1D;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA,yBAAyB,iBAAiB;AAC1C;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA,MAAM;AACN;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;;AAEA,6BAA6B,kBAAkB;AAC/C;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA,wDAAwD,2DAA2D;AACnH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,wBAAwB;AACxB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,4BAA4B;AAC5B;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,0BAA0B;AAClD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA,4BAA4B,6IAA6I,uGAAuG;AAChR;AACA;AACA,8GAA8G;AAC9G;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA,sHAAsH;AACtH;AACA;AACA;AACA,4GAA4G,kEAAkE;AAC9K;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,kMAAkM;AAClM;AACA;AACA,MAAM;AACN;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,iNAAiN;AACjN;AACA;AACA,MAAM;AACN;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,yFAAyF;AACzF;AACA;;AAEA;AACA;AACA,SAAS;AACT;AACA,SAAS;AACT;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC,+BAA+B;AAChC;AACA,mEAAmE;AACnE,CAAC,+BAA+B;AAChC;AACA;AACA;AACA;AACA,CAAC,6CAA6C;AAC9C;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,iZAAiZ;AACjZ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uDAAuD,kBAAkB;AACzE;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uDAAuD,kBAAkB;AACzE;AACA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB,uBAAuB,UAAU;AACjC;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,uBAAuB,iBAAiB;AACxC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI;AACJ;AACA;AACA;AACA;;AAEA,yBAAyB,mBAAmB;AAC5C;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,qBAAqB,eAAe;AACpC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;;AAEA;;AAEA,uBAAuB,UAAU;AACjC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,qBAAqB;AAC7C;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA;;AAEA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;;AAEA,yBAAyB,UAAU;AACnC;AACA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;;AAEA,4BAA4B,WAAW;AACvC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,4BAA4B,YAAY;AACxC;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,YAAY;AACxC;AACA;AACA;;AAEA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,uBAAuB,UAAU;AACjC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,+BAA+B,WAAW;AAC1C;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA,0BAA0B,eAAe;AACzC;AACA;;AAEA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA,QAAQ;AACR,4BAA4B,eAAe;AAC3C;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,yBAAyB,UAAU;AACnC;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA,wBAAwB,YAAY;AACpC;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;;AAEA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,WAAW,SAAS;AACpB;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,qBAAqB,qBAAqB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,SAAS;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,qBAAqB,iBAAiB;AACtC;AACA;;AAEA;;AAEA,sBAAsB,yBAAyB;AAC/C;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,yEAAyE,MAAM;AAC/E,sCAAsC,2BAA2B;AACjE;AACA;;AAEA,aAAa,2BAA2B;AACxC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA,YAAY;AACZ;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;;AAEA,uCAAuC;AACvC;;AAEA,0BAA0B,cAAc;AACxC,2BAA2B,aAAa;AACxC;AACA;AACA;;AAEA;AACA;AACA,iDAAiD;AACjD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,wBAAwB,cAAc;AACtC,yBAAyB,aAAa;AACtC;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC,2BAA2B,UAAU;AACrC;AACA;AACA;;AAEA,gCAAgC,YAAY;AAC5C,oCAAoC,cAAc;AAClD;;AAEA,oCAAoC,aAAa;AACjD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,sBAAsB;AAC9C,uBAAuB,kBAAkB;AACzC;AACA;AACA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,kCAAkC,aAAa;AAC/C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,qBAAqB;AAC/C;AACA;;AAEA,eAAe,MAAM;AACrB;AACA;;AAEA;;AAEA,0BAA0B,kBAAkB;AAC5C;;AAEA;;AAEA,iBAAiB,QAAQ;AACzB;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,gCAAgC,SAAS;AACzC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,sBAAsB;AAC9C,uBAAuB,iBAAiB;AACxC;AACA;;AAEA,eAAe,MAAM;AACrB;AACA;;AAEA;AACA;;AAEA,4BAA4B,qBAAqB;AACjD;;AAEA;;AAEA,iBAAiB,QAAQ;AACzB;AACA;;AAEA;AACA;;AAEA,8BAA8B,oBAAoB;AAClD;;AAEA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,oCAAoC,aAAa;AACjD;;AAEA,uCAAuC,aAAa;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,iBAAiB;AACnC,oBAAoB,kBAAkB;AACtC,sBAAsB,gBAAgB;AACtC;AACA;;AAEA,eAAe,MAAM;AACrB;AACA;;AAEA;;AAEA,2BAA2B,oBAAoB;AAC/C;;AAEA;;AAEA,iBAAiB,QAAQ;AACzB;AACA;;AAEA;;AAEA,6BAA6B,mBAAmB;AAChD;;AAEA;;AAEA,mBAAmB,MAAM;AACzB;AACA;;AAEA;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,mCAAmC,UAAU;AAC7C;;AAEA,mCAAmC,UAAU;AAC7C;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG,EAAE;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,mBAAmB;AAC7C,2BAA2B,mBAAmB;AAC9C,6BAA6B,kBAAkB;AAC/C;AACA;AACA;;AAEA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,sGAAsG,WAAW;AACjH;;AAEA,yGAAyG,WAAW;AACpH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,oBAAoB;AAC9C,2BAA2B,kBAAkB;AAC7C;AACA;;AAEA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,qGAAqG,WAAW;AAChH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,qBAAqB;AAC7C;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;;AAEA;;AAEA;;AAEA,gCAAgC,sBAAsB;AACtD;;AAEA,kCAAkC,uBAAuB;AACzD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,uBAAuB,UAAU;AACjC;AACA;;AAEA,yBAAyB,qBAAqB;AAC9C,2BAA2B,sBAAsB;AACjD;;AAEA,4BAA4B,mBAAmB;AAC/C,kCAAkC,aAAa;AAC/C;;AAEA,qCAAqC,cAAc;AACnD;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,2BAA2B,UAAU;AACrC;AACA;AACA;;AAEA;;AAEA,iCAAiC,cAAc;AAC/C;;AAEA,kCAAkC,YAAY;AAC9C;AACA;;AAEA,kCAAkC,WAAW;AAC7C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,oBAAoB;AAC5C;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,qBAAqB;AACjD;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;;AAEA;AACA;;AAEA,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,kCAAkC,WAAW;AAC7C;;AAEA;;AAEA;;AAEA;;AAEA,oCAAoC,sBAAsB;AAC1D;;AAEA,sCAAsC,uBAAuB;AAC7D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;;AAEA,uBAAuB,UAAU;AACjC;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,2BAA2B,qBAAqB;AAChD;;AAEA,4BAA4B,qBAAqB;AACjD;;AAEA,8BAA8B,mBAAmB;AACjD;AACA;;AAEA,oCAAoC,aAAa;AACjD;AACA;;AAEA,uCAAuC,cAAc;AACrD;AACA;;AAEA,yCAAyC,cAAc;AACvD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,2BAA2B,UAAU;AACrC;AACA;AACA;;AAEA,4BAA4B,SAAS;AACrC;AACA;AACA;;AAEA;;AAEA,mCAAmC,cAAc;AACjD;;AAEA,qCAAqC,cAAc;AACnD;;AAEA,qCAAqC,WAAW;AAChD;AACA;;AAEA,sCAAsC,WAAW;AACjD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA,wDAAwD,WAAW;AACnE,4BAA4B,WAAW;AACvC;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA,2BAA2B,UAAU;AACrC;;AAEA;AACA,gCAAgC,WAAW;AAC3C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,QAAQ,yBAAyB,WAAW;AAC5C;;AAEA;AACA,8BAA8B,WAAW;AACzC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC,wBAAwB,WAAW;AACnC;;AAEA,mDAAmD;AACnD;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,qBAAqB;AAC7C;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;;AAEA;;AAEA;AACA;;AAEA,gCAAgC,sBAAsB;AACtD;;AAEA,kCAAkC,WAAW;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA,yBAAyB,sBAAsB;AAC/C;AACA;;AAEA;;AAEA,2BAA2B,oBAAoB;AAC/C,iCAAiC,cAAc;AAC/C;;AAEA,mCAAmC,cAAc;AACjD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,2BAA2B,UAAU;AACrC;AACA;AACA;;AAEA;;AAEA,iCAAiC,cAAc;AAC/C;;AAEA,mCAAmC,cAAc;AACjD;AACA;;AAEA,kCAAkC,WAAW;AAC7C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;;AAEA,2BAA2B,UAAU;AACrC;;AAEA,6BAA6B,UAAU;AACvC;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,6DAA6D,WAAW;AACxE;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,6BAA6B,UAAU;AACvC;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,+DAA+D,WAAW;AAC1E;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,6BAA6B,UAAU;AACvC;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,+DAA+D,WAAW;AAC1E;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA,QAAQ;;AAER;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;;AAEP;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,MAAM;AACN;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,6BAA6B,UAAU;AACvC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,WAAW,QAAQ;AACnB;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;;AAEA;;AAEA,4BAA4B,eAAe;AAC3C;AACA;;AAEA;;AAEA,4BAA4B,eAAe;AAC3C;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,sBAAsB;AAC9C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG,EAAE;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,mBAAmB;AAC7C,4BAA4B,oBAAoB;AAChD,6BAA6B,kBAAkB;AAC/C;AACA;AACA;;AAEA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,yGAAyG,UAAU;AACnH;;AAEA,wGAAwG,UAAU;AAClH;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,oBAAoB;AAC9C,4BAA4B,mBAAmB;AAC/C;AACA;;AAEA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,wGAAwG,UAAU;AAClH;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;;AAEA,wBAAwB,sBAAsB;AAC9C;AACA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;;AAEA;;AAEA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;;AAEA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,6BAA6B,UAAU;AACvC;;AAEA;;AAEA;AACA;;AAEA,sGAAsG,WAAW;AACjH;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,6BAA6B,UAAU;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,iCAAiC,wBAAwB;AACzD;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI,oEAAoE;AACxE;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC,4BAA4B,WAAW;AACvC;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,sBAAsB;AACtB;;AAEA;AACA;AACA,IAAI,sCAAsC;AAC1C;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA,mBAAmB;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC,yCAAyC;AAC1C;AACA,CAAC,uCAAuC;AACxC;AACA,CAAC,qDAAqD;AACtD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA,kIAAkI,0BAA0B;AAC5J;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA,uKAAuK,8CAA8C,+DAA+D,SAAS,wCAAwC,sHAAsH,SAAS,uJAAuJ,yCAAyC,SAAS,sCAAsC,iDAAiD,SAAS,2MAA2M,oEAAoE,SAAS,sCAAsC,+EAA+E,SAAS,6CAA6C,iCAAiC,sCAAsC,SAAS,+BAA+B,iDAAiD,SAAS,8CAA8C,yCAAyC,SAAS,mCAAmC,iDAAiD,SAAS;AACthD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kFAAkF,4JAA4J;AAC9O;;AAEA;AACA;AACA;AACA,mGAAmG,sMAAsM;AACzS;;AAEA;AACA;AACA,8CAA8C,mFAAmF,KAAK;AACtI;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD,wEAAwE,2CAA2C,6CAA6C,qBAAqB,wCAAwC,OAAO,gCAAgC,4BAA4B,wCAAwC,QAAQ,wBAAwB,oDAAoD,QAAQ,yBAAyB,qDAAqD,OAAO,qCAAqC,wCAAwC,kDAAkD,gCAAgC,wBAAwB,gCAAgC,0BAA0B,kCAAkC,sCAAsC,gCAAgC,0BAA0B,mCAAmC,sCAAsC,yBAAyB,KAAK;;AAEn9B;AACA;AACA;AACA;AACA;AACA,4FAA4F,8GAA8G,SAAS,uBAAuB,sGAAsG,wEAAwE,mCAAmC,0BAA0B,KAAK,MAAM,sCAAsC,yDAAyD,+CAA+C,WAAW,2CAA2C,SAAS;AAC7qB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,4FAA4F,8GAA8G,SAAS,uBAAuB,sGAAsG,wEAAwE,mCAAmC,0BAA0B,KAAK,MAAM,sCAAsC,yDAAyD,6EAA6E,WAAW,2CAA2C,SAAS;AAC3sB;;AAEA;;AAEA;AACA;AACA;AACA;AACA,4GAA4G,sCAAsC,kDAAkD,SAAS;AAC7M;;AAEA;;AAEA;AACA;AACA;AACA;AACA,4GAA4G,2CAA2C,4EAA4E,kDAAkD,SAAS;AAC9R;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gIAAgI,2CAA2C,iDAAiD,0CAA0C,+CAA+C,+CAA+C,kDAAkD,qFAAqF,wDAAwD,yBAAyB,6BAA6B,+BAA+B,YAAY,sBAAsB,+BAA+B,YAAY,sBAAsB,+BAA+B,YAAY,MAAM,+BAA+B,WAAW,qEAAqE,SAAS;AACp4B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,YAAY;AACpC,0BAA0B,YAAY;AACtC;;AAEA,+CAA+C,wEAAwE,mDAAmD,2EAA2E,qDAAqD,wDAAwD,4CAA4C,qDAAqD,iDAAiD,oDAAoD,sFAAsF,yDAAyD,mCAAmC,wDAAwD,kBAAkB,sBAAsB,wDAAwD,kBAAkB,sBAAsB,wDAAwD,kBAAkB,MAAM,wDAAwD,iBAAiB,eAAe,aAAa;AAC/kC;AACA;;AAEA,uFAAuF,2CAA2C,mCAAmC,sCAAsC,4BAA4B,kBAAkB,sBAAsB,6EAA6E,SAAS;AACrW;;AAEA;;AAEA;AACA;AACA,mFAAmF,iDAAiD,uCAAuC,6CAA6C,qBAAqB,4CAA4C,sBAAsB,OAAO;AACtT;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mMAAmM;AACnM,uKAAuK;AACvK,MAAM,4HAA4H;AAClI;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,MAAM;;AAEN;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA,oBAAoB,QAAQ;AAC5B;AACA;AACA,QAAQ;;AAER;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,SAAS,wBAAwB;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;AACA,EAAE;;AAEF;AACA;;AAEA;AACA;;AAEA,iHAAiH,oDAAoD,kDAAkD;AACvN;AACA;AACA,QAAQ;;AAER;AACA;AACA,sDAAsD;AACtD;;AAEA;AACA,wDAAwD;AACxD;;AAEA;AACA,wDAAwD;AACxD;;AAEA;AACA,wDAAwD;AACxD;;AAEA,uDAAuD;AACvD;AACA,GAAG;AACH;AACA;AACA,qCAAqC;AACrC;;AAEA;AACA,uCAAuC,wCAAwC;AAC/E;;AAEA;AACA,uCAAuC,0CAA0C;AACjF;;AAEA;AACA,uCAAuC,0CAA0C;AACjF;;AAEA,sCAAsC;AACtC;;AAEA;AACA,qHAAqH;AACrH,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wEAAwE,6DAA6D,OAAO;AAC5I;;AAEA;AACA,2CAA2C,kDAAkD,OAAO;AACpG;;AAEA;AACA,0CAA0C,mCAAmC,OAAO;AACpF;;AAEA;AACA,2DAA2D,0BAA0B,gCAAgC,6CAA6C,0EAA0E,2BAA2B,cAAc,cAAc,cAAc,cAAc,cAAc,QAAQ,2BAA2B,cAAc,cAAc,cAAc,cAAc,cAAc,cAAc,QAAQ,0BAA0B,0IAA0I,+BAA+B,OAAO,4CAA4C,wBAAwB,6BAA6B,oCAAoC,mBAAmB,SAAS,mBAAmB,OAAO,wJAAwJ,iCAAiC,mDAAmD,sCAAsC,2CAA2C,OAAO;AACrpC;;AAEA,mFAAmF,+BAA+B,sCAAsC,gEAAgE,GAAG,4DAA4D,+BAA+B,oCAAoC,2CAA2C,gEAAgE,GAAG;AACxc,yHAAyH,gEAAgE,oCAAoC,2CAA2C,gEAAgE,GAAG;AAC3U,0JAA0J,+EAA+E,+BAA+B,sCAAsC,gEAAgE,GAAG;AACjX,+EAA+E,yCAAyC,0HAA0H,KAAK,0CAA0C,2CAA2C,8CAA8C,KAAK;;AAE/X;AACA,uCAAuC,iBAAiB,OAAO;AAC/D;;AAEA;AACA;AACA,0DAA0D,yEAAyE,SAAS,0CAA0C,0DAA0D,SAAS,4DAA4D,yEAAyE,SAAS,0CAA0C,0DAA0D,SAAS,6CAA6C,2GAA2G,uHAAuH,iEAAiE,OAAO,sCAAsC,sHAAsH,iEAAiE,OAAO;AAC/iC;;AAEA;AACA,0DAA0D,yDAAyD,SAAS,0CAA0C,sDAAsD,SAAS,4DAA4D,yDAAyD,SAAS,0CAA0C,sDAAsD,SAAS,6CAA6C,iHAAiH,wDAAwD,OAAO,sCAAsC,sHAAsH,2DAA2D,OAAO;AACv4B;;AAEA;AACA,gDAAgD,2GAA2G,qEAAqE,qFAAqF,uHAAuH,gEAAgE,wCAAwC,mCAAmC,mDAAmD,oDAAoD,gCAAgC,OAAO;AACrsB;AACA;AACA;AACA,yCAAyC,sHAAsH,gEAAgE,yCAAyC,oCAAoC,+CAA+C,gDAAgD,gCAAgC,OAAO;AAClb;;AAEA;AACA,8CAA8C,6GAA6G,2DAA2D,+GAA+G,KAAK;AAC1U;AACA,yCAAyC,sHAAsH,gEAAgE,oDAAoD,OAAO;AAC1R;;AAEA;AACA,gDAAgD,2GAA2G,uHAAuH,gEAAgE,uEAAuE,qFAAqF,yDAAyD,0CAA0C,qCAAqC,wCAAwC,mCAAmC,mDAAmD,oDAAoD,oCAAoC,OAAO;AACn1B;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,sBAAsB;AAC5C,+FAA+F,sDAAsD;AACrJ;;AAEA,4DAA4D,uHAAuH,gEAAgE,iEAAiE,oCAAoC,+CAA+C,gDAAgD,2DAA2D,OAAO;AACzf;;AAEA;AACA,gDAAgD,4FAA4F,6DAA6D,6HAA6H,OAAO;AAC7U;AACA,yCAAyC,iGAAiG,gEAAgE,wDAAwD,OAAO;AACzQ;;AAEA;AACA;AACA,yCAAyC,sHAAsH,kEAAkE,0EAA0E,wBAAwB,OAAO;AAC1U;;AAEA;AACA;AACA,yCAAyC,iGAAiG,gEAAgE,4EAA4E,sBAAsB,OAAO;AACnT;;AAEA;AACA;AACA,wEAAwE,6GAA6G,qFAAqF,SAAS,4CAA4C,oFAAoF,SAAS;AAC5Z;AACA,6CAA6C,2GAA2G,qEAAqE,uHAAuH,kEAAkE,iDAAiD,oDAAoD,6BAA6B,OAAO,wCAAwC,sHAAsH,kEAAkE,6CAA6C,gDAAgD,6BAA6B,OAAO;AACh4B;;AAEA;AACA,qEAAqE,2EAA2E,SAAS,4CAA4C,gFAAgF,SAAS,8DAA8D,qHAAqH,+DAA+D,iCAAiC,SAAS,4CAA4C,0HAA0H,kEAAkE,iCAAiC,SAAS,8DAA8D,qHAAqH,+DAA+D,iCAAiC,SAAS,4CAA4C,0HAA0H,kEAAkE,iCAAiC,SAAS,+CAA+C,iHAAiH,6DAA6D,oCAAoC,wCAAwC,2BAA2B,OAAO,wCAAwC,sHAAsH,gEAAgE,0CAA0C,8CAA8C,2BAA2B,OAAO;AAC7hE;;AAEA;AACA;AACA;;AAEA;AACA;AACA,mFAAmF,qFAAqF,OAAO;AAC/K;;AAEA;AACA;AACA;AACA,4DAA4D,uBAAuB;AACnF;AACA,iEAAiE,uDAAuD,SAAS;AACjI;AACA,8CAA8C,mGAAmG,iDAAiD,OAAO;AACzM;AACA,uCAAuC,6EAA6E,iDAAiD,OAAO;AAC5K;;AAEA;AACA;AACA;AACA;AACA;AACA,sDAAsD,iIAAiI,wFAAwF,4DAA4D,OAAO;AAClV;AACA,+CAA+C,wFAAwF,4DAA4D,OAAO;AAC1M;;AAEA;AACA;AACA;AACA,6EAA6E,sDAAsD;AACnI;AACA;AACA;AACA,0EAA0E,uDAAuD,SAAS;AAC1I;AACA,gEAAgE,wGAAwG,mDAAmD,SAAS,oDAAoD,wFAAwF,mDAAmD,SAAS,kEAAkE,wGAAwG,mDAAmD,SAAS,oDAAoD,wFAAwF,mDAAmD,SAAS,sDAAsD,2GAA2G,iDAAiD,OAAO,gDAAgD,qFAAqF,iDAAiD,OAAO;AAChvC;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kGAAkG,2GAA2G,gEAAgE,SAAS,0DAA0D,yFAAyF,gEAAgE,SAAS;AAClf,6DAA6D,iIAAiI,0EAA0E,+FAA+F,4DAA4D,OAAO;AAC1a;AACA,sDAAsD,mHAAmH,4DAA4D,OAAO;AAC5O;;AAEA;AACA;AACA;AACA;AACA;AACA,mGAAmG,2GAA2G,mDAAmD,SAAS,yDAAyD,6FAA6F,iDAAiD,OAAO;AACxd;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,mIAAmI,4EAA4E,SAAS;AACxN;;AAEA,oFAAoF,6EAA6E,sDAAsD;AACvN;AACA;AACA;AACA,uEAAuE,gGAAgG,gFAAgF,mDAAmD,SAAS,yDAAyD,yFAAyF,8DAA8D,iDAAiD,OAAO,uEAAuE,gGAAgG,gFAAgF,mDAAmD,SAAS,yDAAyD,yFAAyF,8DAA8D,iDAAiD,OAAO,6DAA6D,uJAAuJ,4FAA4F,mDAAmD,SAAS,uDAAuD,0IAA0I,kEAAkE,+CAA+C,KAAK;AACzxD;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,4JAA4J,iFAAiF,WAAW;AACxP;;AAEA;AACA,oEAAoE,iIAAiI,0EAA0E,0FAA0F,2HAA2H,4DAA4D,OAAO;AACviB;AACA;AACA;AACA,6DAA6D,8IAA8I,4DAA4D,OAAO;AAC9Q;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,kJAAkJ,8EAA8E,WAAW;AAC3O;;AAEA,+FAA+F,4HAA4H,sDAAsD;AACjR;AACA;AACA;AACA;AACA,kGAAkG,+CAA+C,kCAAkC,+DAA+D,iIAAiI,mDAAmD,SAAS,wEAAwE,oCAAoC,wEAAwE,mHAAmH,qDAAqD,WAAW;AACtxB,kGAAkG,4EAA4E,oCAAoC,6GAA6G,mDAAmD,SAAS,oEAAoE,qEAAqE,kCAAkC,yFAAyF,iDAAiD,OAAO;AACvrB;AACA,sEAAsE,iJAAiJ,6CAA6C,yFAAyF,0FAA0F,iDAAiD,OAAO,sEAAsE,wKAAwK,sEAAsE,mDAAmD,SAAS;AAC/1B;;AAEA;AACA;AACA;AACA;AACA,4EAA4E,0EAA0E,0FAA0F,6EAA6E,gDAAgD,2CAA2C,iIAAiI,6CAA6C,oDAAoD,4FAA4F,oDAAoD,OAAO;AACjxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,eAAe;AACrC;AACA;;AAEA,oDAAoD,mCAAmC,0CAA0C,iDAAiD,qFAAqF,4DAA4D,OAAO;AAC1U;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,0JAA0J,4EAA4E,SAAS;AAC/O;;AAEA,2GAA2G,oJAAoJ,sDAAsD;AACrT;AACA;AACA;AACA;AACA,+CAA+C;AAC/C,yDAAyD;AACzD,yDAAyD;AACzD,8GAA8G,kFAAkF,8GAA8G,iIAAiI,mDAAmD,SAAS,kFAAkF,kCAAkC,4HAA4H,+GAA+G,mDAAmD,SAAS;AACt4B,8GAA8G,8JAA8J,qCAAqC,gIAAgI,mDAAmD,SAAS,kFAAkF,kIAAkI,qCAAqC,8GAA8G,mDAAmD,SAAS;AACh5B;AACA,kFAAkF,qOAAqO,2GAA2G,iDAAiD,OAAO,8EAA8E,wLAAwL,qFAAqF,iDAAiD,OAAO;AAC72B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,mKAAmK,4EAA4E,SAAS;AACxP;;AAEA,uHAAuH,4KAA4K,sDAAsD;AACzV;AACA;AACA;AACA;AACA,mHAAmH,yBAAyB,gJAAgJ,+GAA+G,mDAAmD,SAAS,qHAAqH,wLAAwL,4BAA4B,8GAA8G,mDAAmD,SAAS,4FAA4F,+PAA+P,oEAAoE,iDAAiD,OAAO;AACj5C;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,oMAAoM,4EAA4E,SAAS;AACzR;;AAEA;AACA;AACA;AACA;AACA;AACA,sJAAsJ,wPAAwP,sDAAsD;AACpc;AACA;AACA;AACA;AACA,oJAAoJ,yBAAyB,8KAA8K,+GAA+G,mDAAmD,SAAS,sJAAsJ,iQAAiQ,4BAA4B,8GAA8G,mDAAmD,SAAS,2HAA2H,yRAAyR,oEAAoE,iDAAiD,OAAO;AACnnD;;AAEA;AACA;AACA;AACA,uCAAuC,4BAA4B,mBAAmB,MAAM,yBAAyB,mCAAmC,SAAS,OAAO;AACxK;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0DAA0D,gDAAgD;AAC1G;AACA;AACA,8BAA8B;AAC9B;AACA;;AAEA;AACA,4FAA4F,kDAAkD,UAAU;AACxJ;AACA;;AAEA,uFAAuF,yGAAyG,+EAA+E;AAC/Q;AACA,IAAI,8DAA8D;;AAElE,sCAAsC,kDAAkD,mFAAmF,6BAA6B;AACxM;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,qKAAqK,yDAAyD,SAAS;AACvO;AACA;AACA;AACA;AACA;AACA,0DAA0D,gDAAgD;AAC1G;AACA,sJAAsJ,kDAAkD,uEAAuE,OAAO;AACtR;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,YAAY;AACZ,YAAY;AACZ;;AAEA,sBAAsB,gCAAgC;AACtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,QAAQ;;AAER;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,8EAA8E;AAC9E;AACA;AACA;AACA,MAAM;AACN,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kDAAkD,kDAAkD,kDAAkD,kDAAkD,iDAAiD,mDAAmD,mDAAmD;AAC/V;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;AACA,QAAQ;;AAER;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;;AAEF;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,yDAAyD,gDAAgD,WAAW,UAAU;AAC9H;AACA;AACA;AACA;AACA;AACA,8CAA8C,iDAAiD,kCAAkC,iCAAiC,cAAc,MAAM,4EAA4E,aAAa,WAAW;AAC1R;AACA;;AAEA;;AAEA;AACA;;AAEA,kBAAkB,QAAQ;AAC1B,oBAAoB,QAAQ;AAC5B;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,OAAO;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,yCAAyC,gCAAgC,sBAAsB,sBAAsB,2CAA2C,yCAAyC;AACzM;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,+BAA+B;AAC/B,iDAAiD,0CAA0C,6GAA6G,0DAA0D,0EAA0E,4EAA4E,4HAA4H,kCAAkC;AACtjB;;AAEA,uIAAuI,uCAAuC,mCAAmC,yBAAyB,uCAAuC,uCAAuC,wDAAwD,SAAS;AACzX;;AAEA;;AAEA;AACA,mEAAmE,0GAA0G,OAAO;AACpL;;AAEA;AACA;AACA,mJAAmJ;AACnJ;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,sLAAsL,gCAAgC,uBAAuB,sCAAsC,sCAAsC,yBAAyB,SAAS;AAC3V;;AAEA;;AAEA,kDAAkD;AAClD,yBAAyB;AACzB,2BAA2B;AAC3B,oDAAoD;AACpD,mEAAmE;AACnE,8EAA8E;AAC9E,wBAAwB;AACxB,oDAAoD;AACpD,yBAAyB;AACzB,6BAA6B,uDAAuD,qDAAqD,qDAAqD,qDAAqD,oBAAoB;AACvQ,yEAAyE,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AAC1R,yFAAyF,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AAC1S,oDAAoD;;AAEpD;AACA;AACA,oOAAoO,gCAAgC,uBAAuB,qCAAqC,qCAAqC,yBAAyB,SAAS;AACvY;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0CAA0C,+CAA+C,kDAAkD,+DAA+D,SAAS;AACnN;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,uDAAuD;AACvD;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM,0CAA0C;;AAEhD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;;AAER;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,QAAQ,wDAAwD;AAChE;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA,mCAAmC,cAAc;AACjD;;AAEA;AACA,6OAA6O;AAC7O;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ,iJAAiJ;AACzJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,sDAAsD,2BAA2B;;AAEjF;AACA;AACA,sOAAsO,gCAAgC,uBAAuB,sCAAsC,sCAAsC,2CAA2C,SAAS;AAC7Z;;AAEA;;AAEA,wEAAwE,6CAA6C,6CAA6C,6CAA6C;;AAE/M;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kGAAkG,0BAA0B,0BAA0B,YAAY,wFAAwF,0HAA0H,4BAA4B,4BAA4B,0GAA0G,4BAA4B,4BAA4B,eAAe;AAC7lB;;AAEA,6JAA6J,uHAAuH,4DAA4D,4DAA4D,kFAAkF,gJAAgJ,+HAA+H,4DAA4D,4DAA4D,kFAAkF;AACv7B;AACA,mEAAmE,gCAAgC,uBAAuB,qCAAqC,qCAAqC,gDAAgD,sDAAsD,SAAS;AACnT;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,+CAA+C;AAC/C,+EAA+E,yEAAyE;;AAExJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,2CAA2C;AAC3C,2EAA2E,yEAAyE;;AAEpJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,wDAAwD;AACxD,6DAA6D,2BAA2B;AACxF,sFAAsF,6CAA6C,6CAA6C,6CAA6C;;AAE7N;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;;AAEP;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,4DAA4D,oCAAoC,mCAAmC,oDAAoD,oCAAoC,+BAA+B,oCAAoC,qCAAqC;AAC/W,gDAAgD;AAChD;AACA;AACA;AACA,6OAA6O,yCAAyC,gCAAgC,0BAA0B,oBAAoB,MAAM,wCAAwC,wCAAwC,wDAAwD,wDAAwD,qMAAqM,8DAA8D,WAAW,wBAAwB,SAAS,uBAAuB,uCAAuC,2CAA2C,kFAAkF,SAAS;AAC7hC;;AAEA;;AAEA;AACA,8CAA8C;AAC9C,8CAA8C;AAC9C;;AAEA;AACA;AACA,sOAAsO,gCAAgC,uBAAuB,8CAA8C,8CAA8C,8CAA8C,8CAA8C,iEAAiE,SAAS;AAC/hB;;AAEA;;AAEA,0BAA0B;;AAE1B;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,2CAA2C;;AAE3C;AACA;;AAEA,qGAAqG;AACrG;;AAEA;AACA,2EAA2E,uBAAuB,WAAW,iFAAiF,gDAAgD,2DAA2D,SAAS,uBAAuB,2CAA2C,gCAAgC,iCAAiC,iDAAiD,iCAAiC,4BAA4B,oBAAoB,SAAS,qCAAqC,4MAA4M,uCAAuC,kDAAkD,qCAAqC,sEAAsE,wCAAwC,gCAAgC,wHAAwH,wCAAwC,gCAAgC,4JAA4J,uCAAuC,8BAA8B,SAAS;AACv+C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mDAAmD,wCAAwC,UAAU,qCAAqC,6EAA6E,uCAAuC,UAAU,MAAM,4DAA4D,qEAAqE,8DAA8D,wCAAwC,2DAA2D,sCAAsC,aAAa,WAAW,SAAS;AACvnB;AACA,gFAAgF,gEAAgE,6EAA6E,uGAAuG,gEAAgE,6EAA6E;AACjd;AACA,2EAA2E,uCAAuC,WAAW,sFAAsF,mDAAmD,gDAAgD,4DAA4D,SAAS,uBAAuB,2CAA2C,gCAAgC,iCAAiC,iDAAiD,oDAAoD,gCAAgC,+BAA+B,+BAA+B,+BAA+B,4BAA4B,oBAAoB,SAAS,qCAAqC,gOAAgO,uCAAuC,kDAAkD,qCAAqC,2MAA2M,wCAAwC,gCAAgC,kNAAkN,wCAAwC,gCAAgC,yNAAyN,uCAAuC,oCAAoC,SAAS;AACt/D;;AAEA;;AAEA;AACA;;AAEA,SAAS,gDAAgD;AACzD;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,gDAAgD,wCAAwC,OAAO;AACvI;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,6CAA6C,+BAA+B,mCAAmC,4BAA4B,qCAAqC,SAAS,wCAAwC,6EAA6E,qCAAqC,8BAA8B,uCAAuC,WAAW,SAAS,0BAA0B,OAAO;AACrf;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI;AACJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,2BAA2B;;AAE3B;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,0DAA0D,iBAAiB,KAAK,mBAAmB;AACnG;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iEAAiE,oCAAoC;AACrG;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,oEAAoE;AACpE,KAAK;AACL;AACA,0CAA0C,oFAAoF,4BAA4B,SAAS;AACnK;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,mEAAmE;AACnE,KAAK;AACL;AACA,0CAA0C,mFAAmF,4BAA4B,SAAS;AAClK;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,mHAAmH,2CAA2C,gCAAgC,iCAAiC,gDAAgD,qCAAqC,mDAAmD,4BAA4B,oBAAoB,MAAM,oDAAoD,iDAAiD,KAAK,iDAAiD,2EAA2E,oCAAoC,gCAAgC,aAAa,WAAW,sCAAsC,SAAS;AAC9wB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA,iGAAiG,kCAAkC,uFAAuF,kCAAkC,uFAAuF,kCAAkC,uFAAuF,kCAAkC;AAC9e,MAAM,6DAA6D,kCAAkC,6CAA6C,kCAAkC,6CAA6C,kCAAkC,6CAA6C,kCAAkC;;AAElV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2UAA2U;AAC3U;AACA,kFAAkF,sJAAsJ,SAAS;AACjP,sEAAsE,gIAAgI,SAAS,2CAA2C,oDAAoD,8EAA8E,8EAA8E,2LAA2L,+BAA+B,uCAAuC,0CAA0C,4BAA4B,oBAAoB,MAAM,2BAA2B,sEAAsE,yCAAyC,sHAAsH,mRAAmR,mEAAmE,qBAAqB,WAAW,+BAA+B,SAAS;AACz8C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,0DAA0D,iBAAiB,KAAK,mBAAmB;AACnG;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uEAAuE;AACvE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uDAAuD;AACvD;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,kEAAkE;AAClE,mDAAmD,iEAAiE,+DAA+D;AACnL;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iFAAiF,6CAA6C;AAC9H;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yIAAyI,mEAAmE,yBAAyB,6CAA6C,kCAAkC,8BAA8B,2DAA2D,uCAAuC,uCAAuC,+HAA+H,yCAAyC,mCAAmC,iCAAiC,+BAA+B,oBAAoB,uCAAuC,qCAAqC,8DAA8D,yBAAyB,eAAe,iCAAiC,oBAAoB,yCAAyC,uCAAuC,+DAA+D,2BAA2B,iBAAiB,uDAAuD,gOAAgO,+CAA+C,sCAAsC,yCAAyC,sQAAsQ,iBAAiB,eAAe,aAAa,6CAA6C,WAAW;AAC55D;AACA;AACA;AACA;AACA,yCAAyC,wCAAwC,UAAU,MAAM,iDAAiD,SAAS;AAC3J,wFAAwF,iEAAiE,yDAAyD,mDAAmD,4BAA4B,4DAA4D,uDAAuD,uCAAuC,WAAW,uBAAuB,wCAAwC,SAAS,uBAAuB,2CAA2C,gCAAgC,4BAA4B,yDAAyD,qCAAqC,qCAAqC,yIAAyI,+BAA+B,sBAAsB,6BAA6B,oBAAoB,qCAAqC,mCAAmC,4DAA4D,uBAAuB,aAAa,+BAA+B,qBAAqB,UAAU,sDAAsD,yRAAyR,2CAA2C,iDAAiD,uCAAuC,uMAAuM,4CAA4C,gCAAgC,+NAA+N,4CAA4C,gCAAgC,2PAA2P,2CAA2C,WAAW,oCAAoC,SAAS;AACllF;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sKAAsK,mFAAmF,yBAAyB,6CAA6C,iCAAiC,8BAA8B,mFAAmF,qCAAqC,qCAAqC,qCAAqC,wIAAwI,yCAAyC,mCAAmC,+BAA+B,oBAAoB,uCAAuC,qCAAqC,6DAA6D,yBAAyB,eAAe,iCAAiC,oBAAoB,yCAAyC,uCAAuC,gEAAgE,2BAA2B,iBAAiB,mCAAmC,oBAAoB,2CAA2C,yCAAyC,iEAAiE,6BAA6B,mBAAmB,8DAA8D,wOAAwO,iDAAiD,wCAAwC,2CAA2C,wYAAwY,mBAAmB,iBAAiB,eAAe,aAAa,6CAA6C,WAAW;AAC/4E;AACA;AACA;AACA;AACA,yCAAyC,wCAAwC,UAAU,MAAM,iDAAiD,SAAS;AAC3J,iHAAiH,iFAAiF,yDAAyD,mDAAmD,4BAA4B,qEAAqE,uDAAuD,uCAAuC,WAAW,uBAAuB,6CAA6C,SAAS,uBAAuB,2CAA2C,+BAA+B,4BAA4B,iFAAiF,mCAAmC,mCAAmC,mCAAmC,iJAAiJ,+BAA+B,sBAAsB,6BAA6B,oBAAoB,qCAAqC,mCAAmC,2DAA2D,uBAAuB,aAAa,+BAA+B,oBAAoB,qCAAqC,qCAAqC,8DAA8D,yBAAyB,eAAe,iCAAiC,qBAAqB,UAAU,wDAAwD,yTAAyT,+CAA+C,mDAAmD,yCAAyC,wNAAwN,gDAAgD,gCAAgC,qPAAqP,gDAAgD,gCAAgC,sRAAsR,+CAA+C,aAAa,sCAAsC,WAAW,SAAS;AACljG;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,8HAA8H,6FAA6F,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gDAAgD,uCAAuC,uCAAuC,oLAAoL,2BAA2B,oBAAoB,oDAAoD,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,+BAA+B,oBAAoB,kDAAkD,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,wDAAwD,mDAAmD,aAAa,WAAW,6BAA6B,SAAS;AACn+C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oKAAoK,6GAA6G,uBAAuB,2CAA2C,+BAA+B,4BAA4B,wEAAwE,qCAAqC,qCAAqC,qCAAqC,4MAA4M,6BAA6B,oBAAoB,mDAAmD,6EAA6E,sFAAsF,uBAAuB,aAAa,gCAAgC,+BAA+B,oBAAoB,sDAAsD,gFAAgF,0GAA0G,yBAAyB,eAAe,kCAAkC,iCAAiC,oBAAoB,uDAAuD,iFAAiF,6GAA6G,2BAA2B,iBAAiB,oCAAoC,qEAAqE,qDAAqD,eAAe,aAAa,WAAW,6BAA6B,SAAS;AAC1gE;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,kLAAkL,sCAAsC,4CAA4C,oDAAoD,uCAAuC,uCAAuC,4EAA4E,oEAAoE,SAAS;AAC/hB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,kLAAkL,sCAAsC,sCAAsC,uCAAuC,2CAA2C,mDAAmD,4EAA4E,iDAAiD,SAAS;AACzgB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,2CAA2C,oDAAoD,iIAAiI,mEAAmE,sEAAsE,SAAS;AAClX;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,8CAA8C,8EAA8E,0CAA0C,oCAAoC,0CAA0C,SAAS;AAC7P,2EAA2E,8EAA8E,0CAA0C,oCAAoC,gFAAgF,4CAA4C,sCAAsC,WAAW,SAAS;AAC7Z,4IAA4I,mFAAmF;AAC/N,0CAA0C,mDAAmD,mCAAmC,yDAAyD,4EAA4E,SAAS;AAC9Q;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,wCAAwC;AACxC;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,qCAAqC;;AAErC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,6BAA6B;AAC7B;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,iEAAiE,0CAA0C,6BAA6B,6BAA6B,mBAAmB,WAAW,oDAAoD,SAAS;AACrQ;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,+DAA+D,yCAAyC,oCAAoC,6BAA6B,mBAAmB,WAAW,gEAAgE,SAAS;AACrR;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,uGAAuG,+CAA+C,+CAA+C,iCAAiC,+RAA+R,SAAS;AAC9gB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA,kEAAkE;;AAElE,wBAAwB,kBAAkB;AAC1C,sHAAsH;AACtH;;AAEA,0FAA0F,2CAA2C,2CAA2C,4BAA4B,4BAA4B,qDAAqD;AAC7R;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA,qDAAqD,8FAA8F,WAAW;;AAE9J,wBAAwB,kBAAkB;AAC1C;AACA,kHAAkH,iLAAiL,WAAW;AAC9S;;AAEA;AACA,+KAA+K,iFAAiF,iCAAiC,uBAAuB,oDAAoD,mEAAmE,8DAA8D,8DAA8D,gDAAgD,WAAW,8DAA8D,8DAA8D,gDAAgD,WAAW,8DAA8D,4HAA4H,gDAAgD,WAAW,4BAA4B,SAAS;AACvjC;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8CAA8C,6DAA6D,oCAAoC,qCAAqC,qDAAqD,oCAAoC,6CAA6C,wCAAwC,+CAA+C;AACjZ,gDAAgD;AAChD,8PAA8P,iEAAiE,uBAAuB,2CAA2C,gCAAgC,0CAA0C,kHAAkH,qCAAqC,qCAAqC,kLAAkL,2BAA2B,qBAAqB,OAAO,oDAAoD,4DAA4D,uBAAuB,aAAa,+BAA+B,qBAAqB,OAAO,sDAAsD,6DAA6D,yBAAyB,eAAe,iCAAiC,qBAAqB,UAAU,6NAA6N,uCAAuC,qPAAqP,mDAAmD,kBAAkB,MAAM,qPAAqP,mDAAmD,iBAAiB,eAAe,2CAA2C,uCAAuC,8IAA8I,kBAAkB,MAAM,8IAA8I,iBAAiB,kBAAkB,gCAAgC,+JAA+J,uCAAuC,6KAA6K,mDAAmD,kBAAkB,MAAM,6KAA6K,mDAAmD,iBAAiB,kBAAkB,gCAAgC,sNAAsN,uCAAuC,yOAAyO,mDAAmD,kBAAkB,MAAM,yOAAyO,mDAAmD,iBAAiB,iBAAiB,aAAa,WAAW,mCAAmC,4EAA4E,SAAS;AAC/0I;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wGAAwG,iFAAiF,uBAAuB,2CAA2C,+BAA+B,4BAA4B,oFAAoF,sCAAsC,sCAAsC,sCAAsC,wMAAwM,2BAA2B,qBAAqB,OAAO,oDAAoD,2DAA2D,uBAAuB,aAAa,+BAA+B,qBAAqB,OAAO,sDAAsD,8DAA8D,yBAAyB,eAAe,iCAAiC,qBAAqB,OAAO,wDAAwD,+DAA+D,2BAA2B,iBAAiB,mCAAmC,qBAAqB,UAAU,qQAAqQ,yPAAyP,qDAAqD,iBAAiB,6CAA6C,kJAAkJ,kBAAkB,gCAAgC,qLAAqL,+KAA+K,mDAAmD,kBAAkB,gCAAgC,qPAAqP,4OAA4O,mDAAmD,iBAAiB,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACvvG;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,YAAY;AACpC,0BAA0B,YAAY;AACtC,gEAAgE,4CAA4C,6EAA6E,6FAA6F,qEAAqE,uDAAuD,6GAA6G,0GAA0G,yDAAyD,+DAA+D,yCAAyC,6CAA6C,4KAA4K,oBAAoB,MAAM,6CAA6C,4KAA4K,mBAAmB,iBAAiB,eAAe,aAAa;AACtxC;AACA;;AAEA,0CAA0C,uCAAuC,kCAAkC,8DAA8D,yBAAyB,oEAAoE,SAAS;AACvR;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL,IAAI;AACJ;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK,EAAE;AACP;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,8GAA8G,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,oLAAoL,4BAA4B,6BAA6B,MAAM,6BAA6B,+BAA+B,OAAO,0FAA0F,8DAA8D,yBAAyB,eAAe,iCAAiC,8BAA8B,OAAO,4FAA4F,+DAA+D,2BAA2B,iBAAiB,qEAAqE,uDAAuD,qDAAqD,gDAAgD,kBAAkB,MAAM,uDAAuD,qDAAqD,gDAAgD,iBAAiB,iBAAiB,aAAa,WAAW,6BAA6B,SAAS;AACjgD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,8HAA8H,uBAAuB,2CAA2C,gCAAgC,kDAAkD,0GAA0G,qCAAqC,qCAAqC,wLAAwL,2BAA2B,qBAAqB,OAAO,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,mDAAmD,+BAA+B,qBAAqB,OAAO,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,qDAAqD,iCAAiC,iCAAiC,OAAO,uCAAuC,8DAA8D,8DAA8D,6CAA6C,kBAAkB,MAAM,8DAA8D,8DAA8D,6CAA6C,iBAAiB,iBAAiB,aAAa,WAAW,6BAA6B,SAAS;AAC73D;;AAEA;;AAEA;AACA;AACA,8GAA8G,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,4BAA4B,gCAAgC,4BAA4B,6BAA6B,MAAM,6BAA6B,8BAA8B,OAAO,2FAA2F,6DAA6D,yBAAyB,eAAe,iCAAiC,+BAA+B,OAAO,4FAA4F,gEAAgE,2BAA2B,iBAAiB,mCAAmC,8BAA8B,OAAO,8FAA8F,iEAAiE,6BAA6B,mBAAmB,6DAA6D,yDAAyD,gDAAgD,iBAAiB,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACh7C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oKAAoK,uBAAuB,2CAA2C,+BAA+B,4BAA4B,0EAA0E,qCAAqC,qCAAqC,qCAAqC,gCAAgC,2BAA2B,qBAAqB,OAAO,6EAA6E,sFAAsF,uBAAuB,aAAa,gCAAgC,mDAAmD,+BAA+B,qBAAqB,OAAO,gFAAgF,wGAAwG,yBAAyB,eAAe,kCAAkC,qDAAqD,iCAAiC,qBAAqB,OAAO,iFAAiF,6GAA6G,2BAA2B,iBAAiB,oCAAoC,uDAAuD,mCAAmC,iCAAiC,OAAO,oEAAoE,sEAAsE,6CAA6C,iBAAiB,eAAe,aAAa,WAAW,6BAA6B,SAAS;AAC/5D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,2DAA2D;AAC3D;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,sCAAsC,mCAAmC;AACzE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6EAA6E,wDAAwD,qBAAqB,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,8DAA8D,mCAAmC,mCAAmC,mCAAmC,gFAAgF,kDAAkD,mBAAmB,WAAW,gDAAgD,6CAA6C,wCAAwC,qDAAqD,6CAA6C,mBAAmB,WAAW,sCAAsC,qDAAqD,6CAA6C,mBAAmB,WAAW,qDAAqD,mCAAmC,2GAA2G,gEAAgE,+EAA+E,+EAA+E,6EAA6E,+EAA+E,oEAAoE,oEAAoE,8EAA8E,6DAA6D,gCAAgC,YAAY,MAAM,oKAAoK,kFAAkF,gCAAgC,WAAW,SAAS;AACntE;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,oNAAoN,wEAAwE,8DAA8D,qCAAqC,0CAA0C,+BAA+B,qCAAqC,4DAA4D,+DAA+D,WAAW,yBAAyB,SAAS;AACrpB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH,sBAAsB,sCAAsC;AAC5D;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,0JAA0J,yCAAyC,0BAA0B,wDAAwD,wDAAwD,wDAAwD,wCAAwC,+CAA+C,sCAAsC,+CAA+C,gHAAgH,gCAAgC,oEAAoE,0BAA0B,OAAO;AACtyB;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8CAA8C,6DAA6D,oCAAoC,qCAAqC,qDAAqD,oCAAoC,6CAA6C,wCAAwC,+CAA+C;AACjZ,gDAAgD;AAChD,8PAA8P,iEAAiE,uBAAuB,2CAA2C,+BAA+B,uDAAuD,4BAA4B,uCAAuC,2CAA2C,uCAAuC,qCAAqC,iLAAiL,2GAA2G,qBAAqB,OAAO,oDAAoD,mDAAmD,uBAAuB,aAAa,+BAA+B,qBAAqB,OAAO,sDAAsD,qDAAqD,yBAAyB,eAAe,qDAAqD,+CAA+C,qCAAqC,aAAa,WAAW,mCAAmC,4EAA4E,SAAS;AAC3jD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,QAAQ,aAAa,qBAAqB,eAAe,WAAW;;AAEjG,wBAAwB,WAAW;AACnC,0DAA0D,mDAAmD,mDAAmD,uDAAuD,sCAAsC;AAC7P;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC,mEAAmE,mDAAmD,0DAA0D,uDAAuD,6CAA6C;AACpR;;AAEA,4DAA4D,8CAA8C;;AAE1G,0BAA0B,qBAAqB;AAC/C;AACA;;AAEA,gEAAgE;AAChE,oFAAoF,yGAAyG,8EAA8E,2LAA2L,+DAA+D,qBAAqB,uDAAuD,mBAAmB,8JAA8J,kEAAkE,sEAAsE,+DAA+D,iMAAiM,gDAAgD,uBAAuB,+FAA+F,sBAAsB,MAAM,0FAA0F,qBAAqB,6HAA6H,wEAAwE,mDAAmD,+DAA+D,qBAAqB,uDAAuD,mBAAmB,qEAAqE;AACt5D;;AAEA,qJAAqJ,kHAAkH,oFAAoF,iMAAiM,qEAAqE,uBAAuB,6DAA6D,qBAAqB,4EAA4E,6GAA6G,kFAAkF,2DAA2D,uBAAuB,oJAAoJ,wHAAwH,oFAAoF,oHAAoH,sFAAsF,6DAA6D,uEAAuE,yBAAyB,+DAA+D,uBAAuB,iFAAiF;AACr5D;AACA,UAAU,2FAA2F,yGAAyG,8EAA8E,yLAAyL,+DAA+D,qBAAqB,uDAAuD,mBAAmB,2GAA2G,gFAAgF,mLAAmL,mEAAmE,qBAAqB,2DAA2D,mBAAmB,gHAAgH,kFAAkF,sDAAsD,oEAAoE,4DAA4D,qBAAqB,gGAAgG,4HAA4H,wEAAwE,mDAAmD,+DAA+D,qBAAqB,uDAAuD,mBAAmB,mDAAmD,6GAA6G,kFAAkF,yDAAyD,kEAAkE,qBAAqB,2DAA2D,mBAAmB,oIAAoI,+JAA+J;;AAE5vF,sGAAsG,2EAA2E,wHAAwH,iFAAiF;AAC1X;;AAEA,uBAAuB;AACvB;;AAEA;AACA;AACA,4CAA4C,4DAA4D,oCAAoC,mCAAmC,oDAAoD,oCAAoC,+BAA+B,oCAAoC,qCAAqC;AAC/W,gDAAgD;AAChD,8PAA8P,iEAAiE,uBAAuB,6CAA6C,+BAA+B,uDAAuD,4BAA4B,uCAAuC,2CAA2C,qCAAqC,qCAAqC,wIAAwI,sFAAsF,4EAA4E,SAAS;AACp8B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,8GAA8G,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,sEAAsE,gCAAgC,wEAAwE,8BAA8B,MAAM,6BAA6B,+BAA+B,OAAO,0FAA0F,8DAA8D,yBAAyB,eAAe,iCAAiC,8BAA8B,OAAO,4FAA4F,+DAA+D,2BAA2B,iBAAiB,uDAAuD,mDAAmD,8CAA8C,eAAe,aAAa,WAAW,6BAA6B,SAAS;AAChtC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,8HAA8H,uBAAuB,2CAA2C,gCAAgC,6BAA6B,4CAA4C,qCAAqC,qCAAqC,gCAAgC,6BAA6B,qBAAqB,OAAO,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,mDAAmD,+BAA+B,qBAAqB,OAAO,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,qDAAqD,iFAAiF,qBAAqB,OAAO,kDAAkD,4DAA4D,4DAA4D,2CAA2C,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACz+C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,iGAAiG,6CAA6C,uEAAuE,2BAA2B,SAAS;AACzP;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN,wFAAwF,iEAAiE,2CAA2C,uBAAuB,2CAA2C,+BAA+B,4BAA4B,2EAA2E,wCAAwC,wCAAwC,wCAAwC,0BAA0B,oBAAoB,MAAM,gDAAgD,qDAAqD,8BAA8B,oBAAoB,MAAM,oDAAoD,yDAAyD,yDAAyD,8CAA8C,4CAA4C,qCAAqC,iCAAiC,mBAAmB,iBAAiB,eAAe,aAAa,WAAW,kCAAkC,4BAA4B,SAAS;AAC7qC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA,QAAQ;;AAER;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;;AAEP;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,oDAAoD;AACpD,oCAAoC,uDAAuD,qDAAqD,qDAAqD,qDAAqD,oBAAoB;AAC9Q;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uDAAuD;AACvD,gFAAgF,6EAA6E;AAC7J;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,mDAAmD;AACnD,oCAAoC;AACpC;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,wPAAwP,qCAAqC,qCAAqC,qCAAqC,qCAAqC,qCAAqC,2BAA2B,eAAe,kCAAkC,+EAA+E;AAC5kB;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,2BAA2B;AAC3B;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC;AACnC;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4DAA4D,EAAE;AAC9D;AACA,4CAA4C;AAC5C;AACA,4EAA4E,gFAAgF,iCAAiC,iDAAiD,kEAAkE,iGAAiG,+BAA+B,4BAA4B,oBAAoB,MAAM,+CAA+C,mEAAmE,gCAAgC,gCAAgC,2CAA2C,2CAA2C,+FAA+F,WAAW,0BAA0B,SAAS,uBAAuB,2CAA2C,qDAAqD,SAAS;AAC3/B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK,2FAA2F,iFAAiF,SAAS;AAC1L;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,kEAAkE,6CAA6C,8BAA8B,iDAAiD,8BAA8B,wDAAwD,8EAA8E,cAAc,MAAM,iFAAiF,aAAa,mCAAmC,WAAW;AAClgB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD,+BAA+B;AAC/B;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iDAAiD,sBAAsB,sBAAsB,kBAAkB,kHAAkH,MAAM,MAAM,iBAAiB,KAAK;AACnQ,gDAAgD,wBAAwB,wCAAwC,4BAA4B,+BAA+B,gGAAgG,2CAA2C,KAAK,kBAAkB,2CAA2C,KAAK,kBAAkB,2CAA2C,KAAK,kBAAkB,2CAA2C,KAAK,wBAAwB;AACzhB;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,gEAAgE,2CAA2C,+BAA+B,+BAA+B,gCAAgC,0FAA0F,0DAA0D,sBAAsB,2BAA2B,6BAA6B,YAAY,sBAAsB,6BAA6B,YAAY,sBAAsB,6BAA6B,YAAY,sBAAsB,6BAA6B,WAAW,kDAAkD,SAAS;AAC9qB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,gEAAgE,2CAA2C,+BAA+B,+BAA+B,gCAAgC,mCAAmC,2BAA2B,QAAQ,QAAQ,2BAA2B,QAAQ,QAAQ,qCAAqC,sCAAsC,wHAAwH,4DAA4D,0BAA0B,+BAA+B,iCAAiC,gBAAgB,sBAAsB,iCAAiC,gBAAgB,sBAAsB,iCAAiC,gBAAgB,sBAAsB,iCAAiC,eAAe,mEAAmE,aAAa,WAAW,2CAA2C,SAAS;AAC9gC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK,EAAE;AACP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,iGAAiG,wBAAwB,sDAAsD,iCAAiC,4BAA4B,gCAAgC,MAAM,0DAA0D,gGAAgG,aAAa,qDAAqD,WAAW;AACzf;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,0CAA0C,kDAAkD,0CAA0C,SAAS;AAC/I;;AAEA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,qCAAqC;AACrC,2DAA2D;AAC3D;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,4CAA4C;AAC5C,sEAAsE;AACtE;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,wDAAwD;AACxD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uCAAuC;AACvC;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uCAAuC;AACvC;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,kCAAkC;AAClC,qDAAqD;AACrD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,yCAAyC;AACzC,gEAAgE;AAChE;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,qCAAqC,kBAAkB;AACvD,6CAA6C,8CAA8C,+CAA+C,+CAA+C,+CAA+C,+CAA+C,oBAAoB;AAC3S;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC;AACnC;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,wDAAwD;AACxD,oIAAoI;AACpI;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,+CAA+C;AAC/C;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uDAAuD;AACvD,kJAAkJ;AAClJ;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wIAAwI,0CAA0C,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,qCAAqC,0BAA0B,uCAAuC,qBAAqB,MAAM,4BAA4B,qDAAqD,2CAA2C,2BAA2B,aAAa,WAAW,yCAAyC,yBAAyB,SAAS;AACxsB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wIAAwI,0CAA0C,2CAA2C,2BAA2B,2BAA2B,2BAA2B,2BAA2B,kEAAkE,iEAAiE,gCAAgC,sDAAsD,oZAAoZ,kDAAkD,gCAAgC,gCAAgC,gEAAgE,0EAA0E,6BAA6B,kFAAkF,eAAe,WAAW,0CAA0C,yCAAyC,qBAAqB,MAAM,kCAAkC,oEAAoE,8EAA8E,yEAAyE,8EAA8E,sDAAsD,gCAAgC,uCAAuC,8BAA8B,oDAAoD,+GAA+G,sEAAsE,+BAA+B,4EAA4E,iBAAiB,eAAe,8BAA8B,2BAA2B,aAAa,WAAW,yDAAyD,4BAA4B,SAAS;AACl7E;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,wOAAwO,2CAA2C,4BAA4B,4BAA4B,4BAA4B,+BAA+B,0BAA0B,4BAA4B,MAAM,sEAAsE,kHAAkH,4CAA4C,8DAA8D,+BAA+B,0CAA0C,mBAAmB,MAAM,kCAAkC,yBAAyB,eAAe,yDAAyD,8EAA8E,eAAe,oBAAoB,sBAAsB,eAAe,aAAa,0EAA0E,2CAA2C,mBAAmB,KAAK,kCAAkC,yBAAyB,eAAe,wDAAwD,oMAAoM,6BAA6B,0DAA0D,iBAAiB,qCAAqC,2CAA2C,gCAAgC,iBAAiB,eAAe,oBAAoB,sBAAsB,eAAe,aAAa,SAAS,0BAA0B,SAAS;AACz6D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;;AAEA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,4DAA4D;AAC5D,0DAA0D,iEAAiE,iDAAiD;AAC5K;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,8HAA8H,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gDAAgD,uCAAuC,uCAAuC,oLAAoL,2BAA2B,oBAAoB,kDAAkD,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,+BAA+B,qBAAqB,OAAO,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,wDAAwD,0FAA0F,uKAAuK,yEAAyE,0CAA0C,aAAa,WAAW,6BAA6B,SAAS;AAC3pD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oKAAoK,uBAAuB,2CAA2C,+BAA+B,4BAA4B,wEAAwE,qCAAqC,qCAAqC,qCAAqC,4MAA4M,6BAA6B,oBAAoB,kDAAkD,6EAA6E,sFAAsF,uBAAuB,aAAa,gCAAgC,+BAA+B,oBAAoB,sDAAsD,gFAAgF,0GAA0G,yBAAyB,eAAe,kCAAkC,iCAAiC,oBAAoB,uDAAuD,iFAAiF,6GAA6G,2BAA2B,iBAAiB,oCAAoC,qEAAqE,8HAA8H,0PAA0P,2EAA2E,4CAA4C,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACt1E;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,4BAA4B,sBAAsB;AAClD;AACA;;AAEA;;AAEA;AACA,QAAQ;;AAER;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,4DAA4D;AAC5D,0DAA0D,iEAAiE,iDAAiD;AAC5K;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4FAA4F,4DAA4D,uBAAuB,kDAAkD,0BAA0B,oBAAoB,MAAM,qCAAqC,gEAAgE,cAAc,4BAA4B,oEAAoE,aAAa,WAAW,+CAA+C,0CAA0C,SAAS,gDAAgD,mCAAmC,yBAAyB,yCAAyC,+BAA+B,uDAAuD,cAAc,sBAAsB,2DAA2D,aAAa,0CAA0C,WAAW;AAC9+B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,uDAAuD,+BAA+B,yDAAyD,YAAY,yBAAyB,6DAA6D,WAAW,0BAA0B;;AAEtR,kDAAkD,0GAA0G,qCAAqC,8BAA8B,8GAA8G,WAAW;AACxV,MAAM;AACN,uDAAuD,sEAAsE,6EAA6E,+CAA+C,kKAAkK,0BAA0B;;AAErb,kDAAkD,0GAA0G,qCAAqC,8BAA8B,8GAA8G,WAAW,yBAAyB,qCAAqC,4EAA4E,8GAA8G,uCAAuC,gCAAgC,kHAAkH,aAAa,WAAW;AACjyB;;AAEA,wFAAwF,kEAAkE,uBAAuB,uDAAuD,iCAAiC,oDAAoD,SAAS;AACtU;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,sCAAsC,qBAAqB;AAC3D,gDAAgD,2CAA2C,iDAAiD;AAC5I;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK,oEAAoE,2CAA2C,gCAAgC,mCAAmC,0BAA0B,4BAA4B,uBAAuB,MAAM,sCAAsC,4BAA4B,kCAAkC,qBAAqB,aAAa,WAAW,8GAA8G,SAAS;AAClhB;;AAEA;;AAEA,4BAA4B,eAAe,IAAI,eAAe;AAC9D,wDAAwD,8CAA8C,wBAAwB,oBAAoB,oBAAoB,KAAK,oBAAoB,oBAAoB,KAAK,oBAAoB,oBAAoB,KAAK,oBAAoB,oBAAoB,KAAK,oBAAoB;AACtU;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,0BAA0B;AAC1B;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,uBAAuB;;AAEvB;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,uGAAuG,2CAA2C,kDAAkD,uHAAuH,SAAS;AACpU;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,4FAA4F,4DAA4D,uBAAuB,kDAAkD,+EAA+E,6BAA6B,YAAY,MAAM,iDAAiD,4CAA4C,WAAW,SAAS,gDAAgD,mCAAmC,yBAAyB,yCAAyC,8CAA8C,+BAA+B,cAAc,MAAM,4CAA4C,aAAa,WAAW;AAC1wB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2C,+BAA+B,6BAA6B,6BAA6B,wBAAwB,mCAAmC,2EAA2E,gDAAgD,+BAA+B;AACzV;AACA;;AAEA,iDAAiD,eAAe;AAChE,yEAAyE,sDAAsD,YAAY,MAAM,+CAA+C,iGAAiG,WAAW;AAC5S;;AAEA,sBAAsB,OAAO,uFAAuF,kEAAkE,uBAAuB,uDAAuD,iCAAiC,oDAAoD,SAAS;AAClW;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,6CAA6C,iBAAiB,KAAK,mBAAmB,iBAAiB,KAAK,wFAAwF;AACpM,uKAAuK,wEAAwE,8CAA8C,yHAAyH,4CAA4C,4CAA4C,4CAA4C,4CAA4C,8EAA8E,iDAAiD;AACrsB;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC;AACnC;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,mEAAmE;AACnE,gFAAgF,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AACjS;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,8EAA8E;AAC9E,gGAAgG,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AACjT;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0RAA0R,2EAA2E,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gCAAgC,yFAAyF,yHAAyH,mGAAmG,yEAAyE,yEAAyE,uEAAuE,yEAAyE,kEAAkE,kEAAkE,4EAA4E,2DAA2D,gCAAgC,SAAS;AAC14C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+TAA+T,mIAAmI,uDAAuD,0DAA0D,SAAS,uBAAuB,2CAA2C,4BAA4B,4BAA4B,yGAAyG,yFAAyF,yHAAyH,mGAAmG,sIAAsI,0DAA0D,2gBAA2gB,kaAAka,gaAAga,+ZAA+Z,kEAAkE,2DAA2D,kEAAkE,qDAAqD,gCAAgC,SAAS;AAC7xG;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0CAA0C,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,oCAAoC,2DAA2D,yDAAyD,+DAA+D,4DAA4D,sDAAsD,mDAAmD,wHAAwH,gEAAgE,+DAA+D,+DAA+D,6DAA6D,uBAAuB,cAAc,2CAA2C,uHAAuH,uBAAuB,aAAa,sCAAsC,sBAAsB,cAAc,6CAA6C,2HAA2H,yBAAyB,eAAe,qDAAqD,gDAAgD,6EAA6E,uDAAuD,mDAAmD,oDAAoD,iDAAiD,4EAA4E,wDAAwD,mDAAmD,4DAA4D,kIAAkI,eAAe,6DAA6D,2GAA2G,eAAe,+DAA+D,6GAA6G,eAAe,gEAAgE,uGAAuG,eAAe,aAAa,WAAW,gEAAgE,SAAS;AACv0F;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8RAA8R,2EAA2E,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gCAAgC,yFAAyF,0MAA0M,4EAA4E,gCAAgC,SAAS;AAC35B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mUAAmU,mIAAmI,uDAAuD,0DAA0D,SAAS,uBAAuB,2CAA2C,4BAA4B,4BAA4B,yGAAyG,yFAAyF,0MAA0M,sIAAsI,0DAA0D,obAAob,gCAAgC,SAAS;AACnuD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0CAA0C,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,oCAAoC,2DAA2D,yDAAyD,+DAA+D,4DAA4D,sDAAsD,mDAAmD,wHAAwH,uEAAuE,+DAA+D,sEAAsE,6DAA6D,uBAAuB,cAAc,2CAA2C,uHAAuH,uBAAuB,aAAa,sCAAsC,sBAAsB,cAAc,6CAA6C,2HAA2H,yBAAyB,eAAe,yIAAyI,6IAA6I,kOAAkO,kOAAkO,qEAAqE,qDAAqD,eAAe,aAAa,WAAW,gEAAgE,SAAS;AAC34E;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,4FAA4F,0CAA0C,0DAA0D,WAAW;AAC3M;AACA;AACA,0CAA0C,mDAAmD,0CAA0C,SAAS;AAChJ;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA,qDAAqD,uCAAuC,mCAAmC,4GAA4G,+BAA+B,mIAAmI,aAAa,8BAA8B,WAAW,qCAAqC,iDAAiD,mCAAmC;AAC5jB;AACA,KAAK,eAAe,+BAA+B;AACnD;AACA,KAAK,eAAe,aAAa,gCAAgC;AACjE;AACA,KAAK,eAAe,kCAAkC;AACtD;AACA,KAAK,eAAe,eAAe,aAAa,8BAA8B,WAAW;AACzF;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,6EAA6E,0DAA0D,8CAA8C,4CAA4C,6CAA6C,8BAA8B,8BAA8B,qHAAqH,qHAAqH,6DAA6D,6DAA6D,2HAA2H,2EAA2E,aAAa,mCAAmC,WAAW;AAC/6B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,6IAA6I,2BAA2B,sBAAsB,MAAM,4BAA4B,qBAAqB,MAAM,MAAM,kCAAkC,oBAAoB,QAAQ,MAAM,0BAA0B,OAAO,KAAK;AAC3W;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qCAAqC;AACrC;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2IAA2I,yBAAyB,sDAAsD,4BAA4B,+BAA+B,4BAA4B,oBAAoB,MAAM,qCAAqC,8BAA8B,oBAAoB,MAAM,8DAA8D,wFAAwF,eAAe,gDAAgD,kDAAkD,6BAA6B,eAAe,aAAa,iEAAiE,WAAW;AACpzB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,0CAA0C;AAC1C;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA;AACA;AACA;AACA,0CAA0C,kDAAkD,4CAA4C,4BAA4B,4CAA4C,YAAY,MAAM,4CAA4C,WAAW,SAAS;AAClS;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,sLAAsL,4CAA4C,gEAAgE;AAClS;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,oDAAoD;AACpD;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,kCAAkC,aAAa,mBAAmB;AAClE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,2DAA2D;AAC3D;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qCAAqC,mCAAmC;AACxE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,4DAA4D,yCAAyC,sCAAsC,mCAAmC,mBAAmB,yBAAyB,qBAAqB,iBAAiB,KAAK,wBAAwB,qBAAqB,KAAK,SAAS,gCAAgC,KAAK,kBAAkB;AACvX;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,iCAAiC,wBAAwB;AACzD;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,6BAA6B;AAC7B;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,6BAA6B;AAC7B;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qDAAqD;AACrD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ,uHAAuH;AACvH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA,kFAAkF,gEAAgE,uBAAuB,oDAAoD,0CAA0C,SAAS;AAChR;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI,oEAAoE;AACxE;AACA;AACA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,2BAA2B;AAC3B;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iDAAiD,+CAA+C;AAChG;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA,0CAA0C,kDAAkD,0CAA0C,SAAS;AAC/I;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,gEAAgE,4CAA4C,iCAAiC,mCAAmC,o0BAAo0B,2DAA2D,qEAAqE,+EAA+E,6DAA6D,6DAA6D,yIAAyI,6DAA6D,uCAAuC,sEAAsE,qBAAqB,wBAAwB,YAAY,+BAA+B,mCAAmC,aAAa,MAAM,mCAAmC,YAAY,UAAU;AAC5zD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,6DAA6D,iHAAiH,iCAAiC,mCAAmC,2wCAA2wC,mEAAmE,2EAA2E,wCAAwC,oDAAoD,yDAAyD,UAAU;AAC/yD;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,4BAA4B,YAAY;AACxC;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,2BAA2B,YAAY;AACvC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8EAA8E,yCAAyC,wCAAwC,sCAAsC,qCAAqC,oCAAoC,sBAAsB,MAAM,4CAA4C,0CAA0C,2GAA2G,uBAAuB,gFAAgF,qBAAqB,oBAAoB,+BAA+B,qCAAqC,oCAAoC,sBAAsB,MAAM,4CAA4C,wEAAwE,2CAA2C,sDAAsD,uBAAuB,qBAAqB,mBAAmB,wDAAwD,kBAAkB,+BAA+B,sCAAsC,qCAAqC,oCAAoC,sBAAsB,MAAM,2CAA2C,gFAAgF,qBAAqB,oBAAoB,+BAA+B,qCAAqC,oCAAoC,sBAAsB,MAAM,2CAA2C,uEAAuE,qBAAqB,mBAAmB,wDAAwD,kBAAkB,+BAA+B,yDAAyD,kBAAkB,MAAM,kCAAkC,iBAAiB,eAAe,wGAAwG,kCAAkC,uGAAuG,2EAA2E,kBAAkB,MAAM,sDAAsD,iBAAiB,mCAAmC,eAAe,6BAA6B,iDAAiD,kCAAkC,sCAAsC,kCAAkC,kCAAkC,wCAAwC,oCAAoC,oCAAoC,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,2DAA2D,wCAAwC,sDAAsD,kBAAkB,MAAM,oEAAoE,oEAAoE,oEAAoE,oEAAoE,8CAA8C,kDAAkD,kDAAkD,yGAAyG,oBAAoB,MAAM,+CAA+C,+CAA+C,+CAA+C,+CAA+C,6PAA6P,0PAA0P,iHAAiH,mBAAmB,iBAAiB,uCAAuC,eAAe;AACxsJ;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0DAA0D;AAC1D;AACA,2EAA2E,uCAAuC,WAAW;AAC7H;AACA,2EAA2E,wBAAwB,WAAW,4EAA4E,gDAAgD,2DAA2D,SAAS,gDAAgD,8DAA8D,SAAS,uBAAuB,2CAA2C,gCAAgC,iCAAiC,+GAA+G,0EAA0E,iCAAiC,4BAA4B,oBAAoB,SAAS,qCAAqC,4MAA4M,6UAA6U,uCAAuC,kDAAkD,qCAAqC,uLAAuL,6DAA6D,qKAAqK,wCAAwC,gCAAgC,8LAA8L,iOAAiO,wCAAwC,gCAAgC,qMAAqM,qRAAqR,uCAAuC,8BAA8B,SAAS;AACv+F;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;;AAEH;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,wFAAwF,cAAc;AACtG;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC,qCAAqC;AACrC;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,4DAA4D,8CAA8C;AAC1G;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,4DAA4D,8CAA8C,6CAA6C;AACvJ;AACA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,cAAc;AAChC;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,QAAQ;AAC9B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,wBAAwB,cAAc;AACtC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,sBAAsB;AAC5C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,sBAAsB;AAC5C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,oCAAoC,wBAAwB,oBAAoB;AAChF;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,iCAAiC;AACjC;;AAEA;AACA,mBAAmB,oBAAoB;AACvC;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA,4FAA4F,eAAe;AAC3G;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,qDAAqD,+BAA+B,qBAAM,UAAU,qBAAM,CAAC,wDAAwD;AACnK;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS;AACT;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC;AACnC;AACA,KAAK;AACL,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,4sCAA4sC;AAC5sC,EAAE;;AAEF;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD,kBAAkB;AAClB;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,2DAA2D,cAAc;AACzE;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,2EAA2E;AAC3E;AACA;AACA;;AAEA,WAAW,aAAa;AACxB;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;;AAEA,8DAA8D,4BAA4B;AAC1F;AACA;;AAEA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA,iBAAiB,SAAS;AAC1B;;AAEA,2HAA2H,0BAA0B;AACrJ;AACA;;AAEA;AACA;;AAEA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB,kBAAkB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0EAA0E;AAC1E;;AAEA;AACA;AACA;;AAEA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA,YAAY;AACZ;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,iCAAiC,YAAY;AAC7C;AACA;;AAEA,wBAAwB,8BAA8B;AACtD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,kCAAkC;;AAElC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,uCAAuC;AACvC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,6BAA6B,QAAQ;AACrC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,iCAAiC,eAAe;AAChD;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wBAAwB,oCAAoC;AAC5D;;AAEA;AACA;;AAEA;;AAEA,qDAAqD,WAAW;AAChE;;AAEA;AACA;AACA,IAAI,yBAAyB,WAAW;AACxC;;AAEA;AACA;;AAEA;AACA;;AAEA,sBAAsB,sBAAsB;AAC5C;AACA;;AAEA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,4FAA4F,eAAe;AAC3G;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wEAAwE,eAAe;AACvF;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,wBAAwB,sBAAsB;AAC9C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,kBAAkB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,kBAAkB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC,mBAAmB;AACpB;AACA,CAAC,6CAA6C;AAC9C;AACA,CAAC,2CAA2C;AAC5C;AACA,CAAC,iDAAiD;AAClD;AACA,CAAC,qDAAqD;AACtD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,iCAAiC;AACjC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,oCAAoC,2BAA2B;AAC/D;;AAEA;AACA;;AAEA;AACA,6FAA6F;AAC7F;;AAEA;;AAEA,sBAAsB,cAAc;AACpC;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP,KAAK,SAAS;AACd;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY;AACZ;AACA,SAAS;;AAET;AACA;;AAEA;AACA;;AAEA;AACA;AACA,MAAM;AACN;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,6CAA6C;AACrE;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0EAA0E,eAAe;AACzF;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX,SAAS;AACT;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,SAAS,sDAAsD;AAC/D;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,iCAAiC,mBAAmB;AACpD;AACA;;AAEA;AACA;;AAEA;AACA,WAAW;;AAEX;AACA;AACA;AACA,SAAS;;AAET;AACA,QAAQ;;AAER;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B,qBAAqB;AACnD;AACA;AACA,UAAU;AACV;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA,4BAA4B,qBAAqB;AACjD;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;;AAEA,0BAA0B,eAAe;AACzC;AACA;;AAEA;;AAEA;AACA;AACA,MAAM;AACN;AACA;;AAEA,sDAAsD,iDAAiD,gDAAgD;AACvJ;AACA;AACA;;AAEA;AACA;;AAEA,8BAA8B,sBAAsB;AACpD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oCAAoC,WAAW;AAC/C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA,qBAAqB,gBAAgB;AACrC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,WAAW,mBAAmB;AAC9B;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;;AAEA,kBAAkB,UAAU;AAC5B;AACA;;AAEA,yBAAyB,cAAc;AACvC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,QAAQ;AAC1B;AACA;;AAEA,uBAAuB,YAAY;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,QAAQ;AAC1B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,WAAW;AACX,UAAU;AACV;AACA;;AAEA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;AACA;AACA,aAAa;AACb;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,WAAW;AACX,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,UAAU;AACV;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,gBAAgB;AAChB;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,gBAAgB;AAChB;AACA;;AAEA,sBAAsB,uBAAuB;AAC7C;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA,qBAAqB,mBAAO,CAAC,KAAY;AACzC;AACA;;AAEA;AACA;AACA,gBAAgB,mBAAO,CAAC,KAAM;AAC9B;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA,wBAAwB;AACxB;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,gBAAgB;AAChB;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;;AAEA;AACA;AACA,WAAW;AACX,SAAS;AACT,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;;AAEA;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;;AAEL;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA,0DAA0D;AAC1D;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2C,2EAA2E,yFAAyF,yFAAyF,sCAAsC;AAC9U,kRAAkR,iDAAiD;AACnU;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;AACA,ibAAib;AACjb;;AAEA;;AAEA,wBAAwB,eAAe;AACvC,0BAA0B,WAAW;AACrC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR;AACA;;AAEA;AACA,MAAM;;AAEN;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,yBAAyB,gBAAgB;AACzC;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;AACA;AACA,UAAU;;AAEV;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oGAAoG;AACpG;AACA;AACA;AACA;;AAEA,sBAAsB,sBAAsB;AAC5C;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,sBAAsB,oBAAoB;AAC1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,6BAA6B,kBAAkB;AAC/C;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI,yBAAyB,WAAW;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC,4CAA4C;AAC5C;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC,kEAAkE;AAClE;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,kBAAkB;AACpC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mDAAmD;AACnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,mEAAmE;AACnE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,6CAA6C;AAC7C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,GAAG,iEAAiE;AACpE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,WAAW,wBAAwB;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,iCAAiC,YAAY;AAC7C,4CAA4C;AAC5C;;AAEA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;AACA,wDAAwD,mCAAmC;AAC3F;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA,0FAA0F,eAAe;AACzG;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA,kCAAkC;AAClC;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA,sCAAsC;AACtC;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,OAAO;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,4DAA4D,8CAA8C;AAC1G;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,cAAc;AACtC;AACA;AACA;;AAEA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP,sBAAsB,mBAAmB;AACzC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP,sBAAsB,mBAAmB;AACzC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,oCAAoC,6BAA6B,cAAc;AAC/E;AACA;;AAEA,eAAe,aAAa;AAC5B;AACA;;AAEA,oBAAoB,qBAAqB;AACzC;AACA;;AAEA,+DAA+D,OAAO;AACtE;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,gHAAgH,OAAO;AACvH;AACA;;AAEA,iFAAiF,OAAO;AACxF;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP,sBAAsB,mBAAmB;AACzC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;;AAEV;AACA,OAAO;AACP,sBAAsB;AACtB,OAAO;AACP;;AAEA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ,SAAS;AACjB;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,2CAA2C,MAAM;AACjD;AACA;;AAEA,eAAe,OAAO;AACtB;AACA;;AAEA;AACA;;AAEA;AACA;AACA,OAAO;AACP;AACA,OAAO;AACP;AACA,wBAAwB;AACxB,SAAS;AACT,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,6BAA6B,MAAM;AACnC;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA,sDAAsD,IAAI;AAC1D;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;;AAEA;AACA,qCAAqC,aAAa;AAClD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;AACA,GAAG;AACH,CAAC;AACD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,WAAW,GAAG;AACd;AACA;AACA;;AAEA;AACA;AACA,QAAQ;;AAER;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,WAAW;AACX;AACA;AACA,QAAQ;;AAER;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,yBAAyB;AAC/C;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,yBAAyB;AAC/C;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,yBAAyB;AAC/C;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,mCAAmC;AACnC;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sLAAsL;AACtL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,yBAAyB,mBAAmB;AAC5C;AACA;;AAEA;AACA;;AAEA,oCAAoC,mBAAmB;AACvD;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,4EAA4E,OAAO;AACnF;AACA;;AAEA,sBAAsB,yBAAyB;AAC/C;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,yBAAyB,gBAAgB;AACzC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB,oBAAoB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,yBAAyB,YAAY;AACrC;AACA;AACA;;AAEA,2BAA2B,uBAAuB;AAClD;AACA;AACA;AACA,SAAS;AACT;;AAEA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;;AAEA,SAAS,gBAAgB;AACzB;AACA;;AAEA,gBAAgB,WAAW;AAC3B;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA,GAAG,IAAI;AACP;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,SAAS,6BAA6B;AACtC;AACA;AACA;AACA;AACA;AACA,MAAM;;AAEN;;AAEA;;AAEA,mCAAmC,eAAe;AAClD;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,mBAAmB;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA,KAAK;AACL,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;AACA;AACA,yCAAyC,eAAe;AACxD;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA,CAAC;;AAED;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mEAAmE;AACnE;;AAEA;AACA;AACA;AACA;AACA,mEAAmE;AACnE;;AAEA;AACA;AACA;AACA,OAAO;AACP;;AAEA,wBAAwB,WAAW;AACn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oBAAoB,OAAO;AAC3B;AACA;AACA,GAAG;AACH;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,8EAA8E;AAC9E;AACA;;AAEA,wBAAwB,eAAe;AACvC;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA,IAAI;AACJ;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,sBAAsB;AACxC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,yBAAyB;AACjD;AACA;AACA;;AAEA;AACA;;AAEA,6BAA6B,WAAW;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,yBAAyB;AACjD;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,wJAAwJ;AACxJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,uGAAuG,GAAG;AAC1G;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,0BAA0B,WAAW;AACr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mCAAmC,YAAY;AAC/C,8CAA8C;AAC9C;;AAEA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA,0BAA0B,4BAA4B;AACtD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC,oBAAoB,iBAAiB;AACr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wBAAwB,gBAAgB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,8CAA8C,cAAc;AAC5D;AACA,UAAU,2CAA2C,cAAc;AACnE,0BAA0B,UAAU;AACpC;AACA;AACA,UAAU,2CAA2C,cAAc;AACnE,6BAA6B,aAAa;AAC1C,4BAA4B,UAAU;AACtC;AACA;AACA;AACA,UAAU;AACV;;AAEA,8BAA8B,cAAc;AAC5C,+BAA+B,aAAa;AAC5C,iCAAiC,aAAa;AAC9C,gCAAgC,UAAU;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,gHAAgH;AAChH;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,iCAAiC,4EAA4E;AAC7G;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,kCAAkC;;AAElC;AACA;AACA;;AAEA;;AAEA;;AAEA,sDAAsD;AACtD;;AAEA;AACA,kCAAkC;;AAElC;AACA;AACA;;AAEA;;AAEA,sDAAsD;AACtD;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACz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mDAAmD;AACnD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,8EAA8E;AAC9E;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;AACA,oDAAoD;AACpD;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,+DAA+D;AAC/D;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,+CAA+C,wBAAwB;AACvE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,eAAe;AACf;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,iTAAiT;AACjT;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,oCAAoC;AAC9D;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;AACA;AACA,KAAK;AACL,6EAA6E;AAC7E;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC,qDAAqD;AACtD;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;AACA,4CAA4C;AAC5C;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;AAC1B;AACA;;AAEA;AACA;AACA,iKAAiK;AACjK;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,0BAA0B;;AAE1B;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,cAAc;AACtC;AACA;AACA;;AAEA;;AAEA,0BAA0B,kBAAk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sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;;AAEA;AACA,qFAAqF,oCAAoC;AACzH;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,6EAA6E;AAC7E;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA,0BAA0B,mBAAmB,mBAAmB;AAChE;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,qBAAqB;AACrB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,6BAA6B;AAC7B;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB;AACtB;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,QAAQ;;AAER;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,aAAa;AAC/B;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA,wGAAwG;AACxG;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,wEAAwE;AACxE;;AAEA,wBAAwB,2CAA2C;AACnE;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,gBAAgB;AAChB;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA,wBAAwB,iCAAiC;AACzD;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,cAAc;AACd,cAAc;AACd,cAAc;AACd,cAAc;AACd,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,oCAAoC;AAC9D;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,iDAAiD;AACjD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,4BAA4B,oCAAoC;AAChE;AACA;;AAEA;AACA;;AAEA,4BAA4B,sBAAsB;AAClD;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kCAAkC;AAC1D;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,4BAA4B;AACpD;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,YAAY;;AAEZ;;AAEA,8BAA8B,sBAAsB;AACpD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,+BAA+B;AACrD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,wBAAwB,2BAA2B;AACnD;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;AACA;;AAEA;;AAEA;AACA;AACA;AACA,YAAY;AACZ;AACA;;AAEA;AACA;;AAEA,gCAAgC,oCAAoC;AACpE;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,iCAAiC;AACzD;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kCAAkC;AAC1D;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,cAAc;AACd;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,sCAAsC;AACtC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,2EAA2E;AAC3E;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,WAAW,kBAAkB;AAC7B;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,gJAAgJ,mBAAmB;AACnK;AACA;AACA;AACA;;AAEA;AACA,4IAA4I;AAC5I;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA,8IAA8I;AAC9I;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kVAAkV;AAClV;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;;AAER;AACA;AACA;;AAEA,aAAa,aAAa;AAC1B;AACA;AACA;AACA;;AAEA,4CAA4C,+BAA+B;AAC3E;;AAEA;AACA,sKAAsK;AACtK;AACA;;AAEA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;AACA;;AAEA;AACA;;AAEA,mCAAmC,uBAAuB;AAC1D;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA,iCAAiC,mBAAmB;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,eAAe;AACf;AACA;AACA,eAAe;;AAEf,kCAAkC,+BAA+B;AACjE;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,gCAAgC;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;AACA,0DAA0D,mBAAmB;AAC7E;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,iCAAiC,mBAAmB;AACpD;AACA;;AAEA;AACA,WAAW;AACX;;AAEA;AACA,OAAO;AACP;AACA;AACA;AACA;;AAEA,WAAW,oBAAoB;AAC/B;;AAEA;AACA;;AAEA,yBAAyB,mBAAmB;AAC5C;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,mCAAmC,mBAAmB;AACtD;AACA;AACA;;AAEA;AACA;;AAEA,qCAAqC,mBAAmB;AACxD;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;;AAEA,6BAA6B,iBAAiB;AAC9C;;AAEA;AACA;;AAEA;AACA,SAAS;AACT;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,2SAA2S;AAC3S;;AAEA;AACA;;AAEA;AACA,QAAQ;AACR;;AAEA;AACA;;AAEA;AACA,QAAQ;;AAER;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA,iDAAiD;AACjD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,wDAAwD;AAChG;AACA;AACA;AACA,+DAA+D;AAC/D;AACA,GAAG,EAAE;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,qDAAqD;AACrD;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,wEAAwE,kBAAkB;AAC1F;AACA;AACA;;AAEA,wBAAwB,yBAAyB;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,4BAA4B,sBAAsB;AAClD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,qCAAqC,kBAAkB;AACvD;AACA;AACA;;AAEA,wBAAwB,yBAAyB;AACjD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,qKAAqK;AACrK;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;AACA,SAAS;AACT;AACA,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,6BAA6B;AACrD;AACA;AACA;;AAEA;AACA;AACA,0BAA0B,6BAA6B;AACvD;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA,cAAc;AACd;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA,aAAa;AACb;AACA,SAAS;AACT;;AAEA,0BAA0B,6BAA6B;AACvD;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,oDAAoD,kBAAkB;AACtE;AACA;AACA;AACA,aAAa;AACb,YAAY;AACZ;AACA;AACA,WAAW;AACX;AACA;AACA,SAAS;AACT;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,sCAAsC;AAC9D;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA,4BAA4B,sBAAsB;AAClD;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,sBAAsB;AAClD;AACA;AACA;AACA;AACA;AACA,+CAA+C,kBAAkB;AACjE;AACA;;AAEA,8BAA8B,kBAAkB;AAChD;AACA;AACA;;AAEA,4BAA4B,kBAAkB;AAC9C;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,4BAA4B;AACxD;AACA;AACA;AACA,WAAW;AACX;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA,4BAA4B,mCAAmC;AAC/D;;AAEA;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,oCAAoC;AAChE;;AAEA,uFAAuF;AACvF;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,SAAS;AACT,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,4BAA4B;AACtD;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA,0BAA0B,mCAAmC;AAC7D;;AAEA;AACA;;AAEA,0BAA0B,oCAAoC;AAC9D;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,iEAAiE;AACjE;AACA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,2DAA2D,oEAAoE;AAC/H;;AAEA;AACA;AACA;AACA;AACA,yEAAyE;AACzE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,4BAA4B;AAC5B;AACA,2DAA2D;AAC3D;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,4BAA4B;AAC5B;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA,YAAY;AACZ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK,cAAc;AACnB;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;;AAEA,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,wHAAwH;AACxH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,6BAA6B;AAC7B;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA,yCAAyC;AACzC;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA,yBAAyB;AACzB;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB,mDAAmD;AAC5E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,6BAA6B,oEAAoE,8RAA8R,qEAAqE;AACpc;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qDAAqD,kBAAkB;AACvE;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,6BAA6B,kDAAkD;AAC/E;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB,kDAAkD;AAC3E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB,iDAAiD;AAC1E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,mDAAmD;AACnD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,2GAA2G;AAC3G,MAAM,kJAAkJ;AACxJ;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,kKAAkK;AAClK,gJAAgJ,iJAAiJ;AACjS;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,kGAAkG;AAClG;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,kGAAkG;AAClG;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,mBAAmB;AAC3C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wCAAwC;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;;AAET;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA,6NAA6N;AAC7N,MAAM;AACN;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,wKAAwK,gRAAgR;AACxb;AACA;;AAEA,4BAA4B,6BAA6B;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK,mBAAmB;AACxB;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B;AAC3B;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B;AAC3B;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,SAAS;AACT,QAAQ;;AAER;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B;AAC3B;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,uBAAuB;AAC7C;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,2BAA2B;AAC3B;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,0BAA0B,0BAA0B;AACpD;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,8CAA8C,cAAc;AAC5D;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iJAAiJ,qPAAqP;AACtY;AACA;;AAEA,4BAA4B,6BAA6B;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,0BAA0B;AAC1B;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAAS;AACT,QAAQ;;AAER;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,2BAA2B;AAC3B;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,0BAA0B;AAC1B;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,gCAAgC;AACxD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,qBAAqB;AACrB;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,4BAA4B,gBAAgB;AAC5C;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA,wBAAwB,iCAAiC;AACzD;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,yBAAyB;AACzB;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA,iEAAiE,4BAA4B,4BAA4B;AACzH;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,gCAAgC,mBAAmB;AACnD;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,cAAc;AACd;;AAEA;AACA,cAAc;AACd;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,YAAY;AACZ;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA,0DAA0D;AAC1D;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA,MAAM;AACN;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;;AAEA,8BAA8B,mBAAmB;AACjD;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA,sDAAsD,yDAAyD;AAC/G;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,wBAAwB;AACxB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,4BAA4B;AAC5B;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,0BAA0B;AAClD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA,4BAA4B,2IAA2I,uGAAuG;AAC9Q;AACA;AACA,8GAA8G;AAC9G;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA,sHAAsH;AACtH;AACA;AACA;AACA,0GAA0G,kEAAkE;AAC5K;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,kMAAkM;AAClM;AACA;AACA,MAAM;AACN;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA,iNAAiN;AACjN;AACA;AACA,MAAM;AACN;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA,iBAAiB;AACjB;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,yFAAyF;AACzF;AACA;;AAEA;AACA;AACA,SAAS;AACT;AACA,SAAS;AACT;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,qCAAqC;AACrC;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA,wBAAwB;AACxB;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,2BAA2B;AAC5B;AACA,mEAAmE;AACnE,CAAC,2BAA2B;AAC5B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,kEAAkE;AAClE;;AAEA;AACA;AACA;AACA;AACA;AACA,kOAAkO;AAClO;AACA;AACA,cAAc;AACd;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA,kJAAkJ;AAClJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,8BAA8B,4CAA4C;AAC1E;;AAEA;AACA;AACA,kCAAkC;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qBAAqB;AACrB,oBAAoB;AACpB;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK,QAAQ;AACb;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP,KAAK,IAAI;AACT;;AAEA;AACA;AACA;AACA;AACA,oHAAoH;AACpH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB;AACvB;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,mGAAmG;AACnG,4GAA4G;AAC5G;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,oBAAoB;AACpB;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,0FAA0F,qKAAqK;AAC/P;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,uCAAuC;AACvC;;AAEA,0BAA0B,qBAAqB;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,qBAAqB;AAC7C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA;AACA,KAAK;AACL;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,8DAA8D;AAC9D;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA,sEAAsE;AACtE;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,sEAAsE;AACtE;AACA;AACA;AACA;AACA;;AAEA;AACA,4EAA4E;AAC5E;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,sEAAsE;AACtE;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,4EAA4E;AAC5E;AACA;AACA;;AAEA;AACA,sEAAsE;AACtE;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA,0DAA0D;AAC1D;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA,GAAG;AACH;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAa;AACb;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,eAAe;AACf;AACA;AACA;AACA,eAAe;AACf;;AAEA,mBAAmB,KAAK;AACx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kBAAkB;AAC5C;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;;AAEA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,cAAc;AACtC;;AAEA;AACA;;AAEA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,4BAA4B,sBAAsB;AAClD;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;;AAEP;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,GAAG;;AAEH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,oBAAoB,8BAA8B;AAClD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,kCAAkC;;AAElC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAAS,aAAa;AACtB;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;;AAEA,SAAS,aAAa;AACtB;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,uFAAuF,4CAA4C,gCAAgC;AACnK;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,qGAAqG;AACrG;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc;AACd;AACA;AACA;AACA,8BAA8B;AAC9B;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,2EAA2E;AAC3E;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA,OAAO;AACP,8BAA8B;AAC9B;AACA;AACA;AACA;AACA,OAAO;;AAEP,gBAAgB;AAChB;AACA;;AAEA,aAAa,aAAa;AAC1B;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,QAAQ;AACR;;AAEA,WAAW,aAAa;AACxB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA,gKAAgK;AAChK;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,sGAAsG;AACtG;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,yCAAyC,sGAAsG;AAC/I;AACA,0EAA0E;AAC1E;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kMAAkM;AAClM;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,mEAAmE;AACnE;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA,iEAAiE;AACjE;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,wBAAwB;AACxB;AACA;AACA,GAAG;AACH;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,WAAW,MAAM;AACjB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,aAAa,QAAQ;AACrB;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,aAAa,QAAQ;AACrB;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA,aAAa,QAAQ;AACrB;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,aAAa,aAAa;AAC1B;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,aAAa,qCAAqC;AAClD;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA,aAAa,8BAA8B;AAC3C;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,aAAa;AACb;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,aAAa;AACb;AACA;AACA,UAAU;AACV;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA,aAAa,oCAAoC;AACjD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA,CAAC,yCAAyC;;AAE1C;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,yCAAyC;AACtD;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,WAAW,sBAAsB;AACjC;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,4DAA4D,2BAA2B;AACvF;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;;AAEA,GAAG;AACH;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA,wCAAwC,cAAc;AACtD;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,4QAA4Q;AAC5Q;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,qFAAqF;AACrF;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA,cAAc;AACd;;AAEA,oBAAoB,iCAAiC;AACrD;AACA;;AAEA;AACA;AACA;AACA;AACA,uDAAuD;AACvD;AACA;AACA;AACA,UAAU;AACV;;AAEA,iFAAiF;AACjF;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,YAAY;AACZ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR;AACA;;AAEA;AACA;;AAEA;AACA,iKAAiK;AACjK,QAAQ;;AAER;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,WAAW;AACX;AACA;AACA,WAAW;AACX,SAAS;AACT,OAAO;AACP,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR;AACA;;AAEA;;AAEA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA,QAAQ;AACR;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA,qGAAqG;AACrG;AACA;AACA,QAAQ,EAAE,mBAAO,CAAC,KAAgB;;AAElC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4GAA4G;AAC5G;AACA;AACA;AACA;AACA;AACA,WAAW;;AAEX;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;;AAED;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,oBAAoB,mBAAO,CAAC,KAAI;;AAEhC;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,6YAA6Y;AAC7Y;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uDAAuD,kBAAkB;AACzE;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;;AAEP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uDAAuD,kBAAkB;AACzE;AACA;AACA;;AAEA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB,wBAAwB,WAAW;AACnC;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI;AACJ;AACA;AACA;AACA;;AAEA,0BAA0B,oBAAoB;AAC9C;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,sBAAsB;AAC9C;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA;;AAEA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;;AAEA,4BAA4B,WAAW;AACvC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,4BAA4B,YAAY;AACxC;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,YAAY;AACxC;AACA;AACA;;AAEA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,+BAA+B,WAAW;AAC1C;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA,0BAA0B,eAAe;AACzC;AACA;;AAEA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA,QAAQ;AACR,4BAA4B,eAAe;AAC3C;AACA;;AAEA;AACA;AACA;;AAEA,oBAAoB,OAAO;AAC3B;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,0BAA0B,YAAY;AACtC;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA,wBAAwB,YAAY;AACpC;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,OAAO;AACP;;AAEA;;AAEA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,WAAW,SAAS;AACpB;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,sBAAsB,sBAAsB;AAC5C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,qBAAqB,UAAU;AAC/B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;;AAEA,sBAAsB,yBAAyB;AAC/C;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,mEAAmE,MAAM;AACzE,oCAAoC,yBAAyB;AAC7D;AACA;;AAEA,aAAa,yBAAyB;AACtC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA,YAAY;AACZ;AACA;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;;AAEA,uCAAuC;AACvC;;AAEA,0BAA0B,cAAc;AACxC,4BAA4B,cAAc;AAC1C;AACA;AACA;;AAEA;AACA;AACA,iDAAiD;AACjD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,wBAAwB,cAAc;AACtC,0BAA0B,cAAc;AACxC;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC,4BAA4B,WAAW;AACvC;AACA;AACA;;AAEA,iCAAiC,cAAc;AAC/C,oCAAoC,eAAe;AACnD;;AAEA,sCAAsC,eAAe;AACrD;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,sBAAsB;AAC9C,wBAAwB,mBAAmB;AAC3C;AACA;AACA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,mCAAmC,cAAc;AACjD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,qBAAqB;AAC/C;AACA;;AAEA,eAAe,MAAM;AACrB;AACA;;AAEA;;AAEA,2BAA2B,mBAAmB;AAC9C;;AAEA;;AAEA,iBAAiB,SAAS;AAC1B;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,kCAAkC,UAAU;AAC5C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,sBAAsB;AAC9C,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA,eAAe,MAAM;AACrB;AACA;;AAEA;AACA;;AAEA,4BAA4B,qBAAqB;AACjD;;AAEA;;AAEA,iBAAiB,SAAS;AAC1B;AACA;;AAEA;AACA;;AAEA,8BAA8B,oBAAoB;AAClD;;AAEA;;AAEA,mBAAmB,SAAS;AAC5B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,qCAAqC,cAAc;AACnD;;AAEA,uCAAuC,cAAc;AACrD;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,kBAAkB,iBAAiB;AACnC,oBAAoB,kBAAkB;AACtC,sBAAsB,gBAAgB;AACtC;AACA;;AAEA,eAAe,MAAM;AACrB;AACA;;AAEA;;AAEA,4BAA4B,qBAAqB;AACjD;;AAEA;;AAEA,iBAAiB,SAAS;AAC1B;AACA;;AAEA;;AAEA,8BAA8B,oBAAoB;AAClD;;AAEA;;AAEA,mBAAmB,MAAM;AACzB;AACA;;AAEA;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,qCAAqC,WAAW;AAChD;;AAEA,oCAAoC,WAAW;AAC/C;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG,EAAE;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,mBAAmB;AAC7C,4BAA4B,oBAAoB;AAChD,8BAA8B,mBAAmB;AACjD;AACA;AACA;;AAEA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,0GAA0G,WAAW;AACrH;;AAEA,6GAA6G,WAAW;AACxH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,oBAAoB;AAC9C,4BAA4B,mBAAmB;AAC/C;AACA;;AAEA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,yGAAyG,WAAW;AACpH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,qBAAqB;AAC7C;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;;AAEA;;AAEA;;AAEA,gCAAgC,sBAAsB;AACtD;;AAEA,kCAAkC,uBAAuB;AACzD;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA,0BAA0B,sBAAsB;AAChD,2BAA2B,sBAAsB;AACjD;;AAEA,6BAA6B,oBAAoB;AACjD,mCAAmC,cAAc;AACjD;;AAEA,sCAAsC,eAAe;AACrD;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;;AAEA;;AAEA,kCAAkC,eAAe;AACjD;;AAEA,mCAAmC,cAAc;AACjD;AACA;;AAEA,kCAAkC,WAAW;AAC7C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,oBAAoB;AAC5C;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,qBAAqB;AACjD;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;;AAEA;AACA;;AAEA,gCAAgC,oBAAoB;AACpD;AACA;;AAEA,kCAAkC,WAAW;AAC7C;;AAEA;;AAEA;;AAEA;;AAEA,oCAAoC,sBAAsB;AAC1D;;AAEA,sCAAsC,uBAAuB;AAC7D;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,2BAA2B,qBAAqB;AAChD;;AAEA,6BAA6B,sBAAsB;AACnD;;AAEA,+BAA+B,oBAAoB;AACnD;AACA;;AAEA,qCAAqC,cAAc;AACnD;AACA;;AAEA,wCAAwC,eAAe;AACvD;AACA;;AAEA,yCAAyC,cAAc;AACvD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;;AAEA,6BAA6B,UAAU;AACvC;AACA;AACA;;AAEA;;AAEA,oCAAoC,eAAe;AACnD;;AAEA,qCAAqC,cAAc;AACnD;;AAEA,uCAAuC,aAAa;AACpD;AACA;;AAEA,sCAAsC,WAAW;AACjD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA,wDAAwD,WAAW;AACnE,4BAA4B,WAAW;AACvC;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA,2BAA2B,UAAU;AACrC;;AAEA;AACA,gCAAgC,WAAW;AAC3C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,QAAQ,yBAAyB,WAAW;AAC5C;;AAEA;AACA,8BAA8B,WAAW;AACzC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC,wBAAwB,WAAW;AACnC;;AAEA,mDAAmD;AACnD;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C;AACA;;AAEA,wBAAwB,qBAAqB;AAC7C;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;;AAEA,4BAA4B,oBAAoB;AAChD;AACA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;;AAEA;;AAEA;AACA;;AAEA,gCAAgC,sBAAsB;AACtD;;AAEA,kCAAkC,WAAW;AAC7C;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA,0BAA0B,uBAAuB;AACjD;AACA;;AAEA;;AAEA,2BAA2B,oBAAoB;AAC/C,iCAAiC,cAAc;AAC/C;;AAEA,oCAAoC,eAAe;AACnD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;;AAEA;;AAEA,kCAAkC,eAAe;AACjD;;AAEA,mCAAmC,cAAc;AACjD;AACA;;AAEA,kCAAkC,WAAW;AAC7C;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,6BAA6B,UAAU;AACvC;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,+DAA+D,WAAW;AAC1E;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,8BAA8B,WAAW;AACzC;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,+DAA+D,WAAW;AAC1E;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,8BAA8B,WAAW;AACzC;AACA;AACA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,+DAA+D,WAAW;AAC1E;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA,QAAQ;;AAER;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;;AAEP;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,MAAM;AACN;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,8BAA8B,WAAW;AACzC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,WAAW,QAAQ;AACnB;AACA;AACA;;AAEA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;;AAEA;;AAEA,4BAA4B,eAAe;AAC3C;AACA;;AAEA;;AAEA,4BAA4B,eAAe;AAC3C;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,wBAAwB,sBAAsB;AAC9C;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG,EAAE;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,mBAAmB;AAC7C,4BAA4B,oBAAoB;AAChD,8BAA8B,mBAAmB;AACjD;AACA;AACA;;AAEA;;AAEA,gCAAgC,WAAW;AAC3C;;AAEA,0GAA0G,WAAW;AACrH;;AAEA,6GAA6G,WAAW;AACxH;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,qBAAqB;AAC3C,wBAAwB,sBAAsB;AAC9C,0BAA0B,oBAAoB;AAC9C,4BAA4B,mBAAmB;AAC/C;AACA;;AAEA;;AAEA,8BAA8B,WAAW;AACzC;;AAEA,yGAAyG,WAAW;AACpH;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;;AAEA,wBAAwB,sBAAsB;AAC9C;AACA;;AAEA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;;AAEA,0BAA0B,sBAAsB;AAChD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC,wBAAwB,WAAW;AACnC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;;AAEA;;AAEA;;AAEA;;AAEA,0BAA0B,WAAW;AACrC;;AAEA;;AAEA;;AAEA;;AAEA,4BAA4B,WAAW;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,8BAA8B,WAAW;AACzC;;AAEA;;AAEA;AACA;;AAEA,wGAAwG,WAAW;AACnH;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,gBAAgB;AACtC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC;;AAEA,0BAA0B,WAAW;AACrC;;AAEA,4BAA4B,WAAW;AACvC;;AAEA,8BAA8B,WAAW;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;;AAEA,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC,wBAAwB,WAAW;AACnC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,iCAAiC,wBAAwB;AACzD;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI,oEAAoE;AACxE;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC,4BAA4B,WAAW;AACvC;;AAEA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,sBAAsB;AACtB;;AAEA;AACA;AACA,IAAI,sCAAsC;AAC1C;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;;AAEA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA,sBAAsB,WAAW;AACjC;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;;AAEA,iBAAiB;AACjB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA,CAAC,qCAAqC;AACtC;AACA,CAAC,mCAAmC;AACpC;AACA,CAAC,iDAAiD;AAClD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;AACA;AACA,gIAAgI,0BAA0B;AAC1J;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;;AAEA;;AAEA;AACA;AACA,qKAAqK,8CAA8C,+DAA+D,SAAS,wCAAwC,sHAAsH,SAAS,uJAAuJ,yCAAyC,SAAS,sCAAsC,iDAAiD,SAAS,2MAA2M,oEAAoE,SAAS,sCAAsC,+EAA+E,SAAS,6CAA6C,iCAAiC,sCAAsC,SAAS,+BAA+B,iDAAiD,SAAS,8CAA8C,yCAAyC,SAAS,mCAAmC,iDAAiD,SAAS;AACphD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kFAAkF,4JAA4J;AAC9O;;AAEA;AACA;AACA;AACA,mGAAmG,sMAAsM;AACzS;;AAEA;AACA;AACA,8CAA8C,mFAAmF,KAAK;AACtI;;AAEA;AACA;AACA;AACA,CAAC;AACD;AACA,CAAC;AACD;AACA,CAAC;AACD,sEAAsE,2CAA2C,6CAA6C,qBAAqB,wCAAwC,OAAO,gCAAgC,4BAA4B,wCAAwC,QAAQ,wBAAwB,oDAAoD,QAAQ,yBAAyB,qDAAqD,OAAO,qCAAqC,wCAAwC,kDAAkD,gCAAgC,wBAAwB,gCAAgC,0BAA0B,kCAAkC,sCAAsC,gCAAgC,0BAA0B,mCAAmC,sCAAsC,yBAAyB,KAAK;;AAEj9B;AACA;AACA;AACA;AACA;AACA,4FAA4F,4GAA4G,SAAS,uBAAuB,sGAAsG,wEAAwE,mCAAmC,0BAA0B,KAAK,MAAM,sCAAsC,yDAAyD,+CAA+C,WAAW,2CAA2C,SAAS;AAC3qB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,4FAA4F,4GAA4G,SAAS,uBAAuB,sGAAsG,wEAAwE,mCAAmC,0BAA0B,KAAK,MAAM,sCAAsC,yDAAyD,6EAA6E,WAAW,2CAA2C,SAAS;AACzsB;;AAEA;;AAEA;AACA;AACA;AACA;AACA,0GAA0G,sCAAsC,kDAAkD,SAAS;AAC3M;;AAEA;;AAEA;AACA;AACA;AACA;AACA,0GAA0G,2CAA2C,4EAA4E,kDAAkD,SAAS;AAC5R;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8HAA8H,2CAA2C,iDAAiD,0CAA0C,+CAA+C,+CAA+C,kDAAkD,qFAAqF,wDAAwD,yBAAyB,6BAA6B,+BAA+B,YAAY,sBAAsB,+BAA+B,YAAY,sBAAsB,+BAA+B,YAAY,MAAM,+BAA+B,WAAW,qEAAqE,SAAS;AACl4B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,YAAY;AACpC,0BAA0B,YAAY;AACtC;;AAEA,+CAA+C,wEAAwE,mDAAmD,2EAA2E,qDAAqD,wDAAwD,4CAA4C,qDAAqD,iDAAiD,oDAAoD,sFAAsF,yDAAyD,mCAAmC,wDAAwD,kBAAkB,sBAAsB,wDAAwD,kBAAkB,sBAAsB,wDAAwD,kBAAkB,MAAM,wDAAwD,iBAAiB,eAAe,aAAa;AAC/kC;AACA;;AAEA,qFAAqF,2CAA2C,mCAAmC,sCAAsC,4BAA4B,kBAAkB,sBAAsB,6EAA6E,SAAS;AACnW;;AAEA;;AAEA;AACA;AACA,mFAAmF,iDAAiD,uCAAuC,6CAA6C,qBAAqB,4CAA4C,sBAAsB,OAAO;AACtT;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,6LAA6L;AAC7L,mKAAmK;AACnK,MAAM,0HAA0H;AAChI;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,MAAM;;AAEN;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA,oBAAoB,QAAQ;AAC5B;AACA;AACA,QAAQ;;AAER;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,SAAS,wBAAwB;AACjC;AACA;;AAEA;AACA;;AAEA;AACA;AACA,EAAE;;AAEF;AACA;;AAEA;AACA;;AAEA,iHAAiH,oDAAoD,kDAAkD;AACvN;AACA;AACA,QAAQ;;AAER;AACA;AACA,sDAAsD;AACtD;;AAEA;AACA,wDAAwD;AACxD;;AAEA;AACA,wDAAwD;AACxD;;AAEA;AACA,wDAAwD;AACxD;;AAEA,uDAAuD;AACvD;AACA,GAAG;AACH;AACA;AACA,qCAAqC;AACrC;;AAEA;AACA,uCAAuC,wCAAwC;AAC/E;;AAEA;AACA,uCAAuC,0CAA0C;AACjF;;AAEA;AACA,uCAAuC,0CAA0C;AACjF;;AAEA,sCAAsC;AACtC;;AAEA;AACA,qHAAqH;AACrH,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,wEAAwE,6DAA6D,OAAO;AAC5I;;AAEA;AACA,2CAA2C,kDAAkD,OAAO;AACpG;;AAEA;AACA,0CAA0C,mCAAmC,OAAO;AACpF;;AAEA;AACA,2DAA2D,0BAA0B,gCAAgC,6CAA6C,0EAA0E,2BAA2B,cAAc,cAAc,cAAc,cAAc,cAAc,QAAQ,2BAA2B,cAAc,cAAc,cAAc,cAAc,cAAc,cAAc,QAAQ,0BAA0B,0IAA0I,+BAA+B,OAAO,4CAA4C,wBAAwB,6BAA6B,oCAAoC,mBAAmB,SAAS,mBAAmB,OAAO,wJAAwJ,iCAAiC,mDAAmD,sCAAsC,2CAA2C,OAAO;AACrpC;;AAEA,iFAAiF,+BAA+B,sCAAsC,gEAAgE,GAAG,4DAA4D,+BAA+B,oCAAoC,2CAA2C,gEAAgE,GAAG;AACtc,uHAAuH,gEAAgE,oCAAoC,2CAA2C,gEAAgE,GAAG;AACzU,wJAAwJ,+EAA+E,+BAA+B,sCAAsC,gEAAgE,GAAG;AAC/W,6EAA6E,yCAAyC,0HAA0H,KAAK,0CAA0C,2CAA2C,8CAA8C,KAAK;;AAE7X;AACA,uCAAuC,iBAAiB,OAAO;AAC/D;;AAEA;AACA;AACA,0DAA0D,yEAAyE,SAAS,0CAA0C,0DAA0D,SAAS,4DAA4D,yEAAyE,SAAS,0CAA0C,0DAA0D,SAAS,6CAA6C,2GAA2G,uHAAuH,iEAAiE,OAAO,sCAAsC,sHAAsH,iEAAiE,OAAO;AAC/iC;;AAEA;AACA,0DAA0D,yDAAyD,SAAS,0CAA0C,sDAAsD,SAAS,4DAA4D,yDAAyD,SAAS,0CAA0C,sDAAsD,SAAS,6CAA6C,iHAAiH,wDAAwD,OAAO,sCAAsC,sHAAsH,2DAA2D,OAAO;AACv4B;;AAEA;AACA,gDAAgD,2GAA2G,qEAAqE,qFAAqF,uHAAuH,gEAAgE,wCAAwC,mCAAmC,mDAAmD,oDAAoD,gCAAgC,OAAO;AACrsB;AACA;AACA;AACA,yCAAyC,sHAAsH,gEAAgE,yCAAyC,oCAAoC,+CAA+C,gDAAgD,gCAAgC,OAAO;AAClb;;AAEA;AACA,8CAA8C,6GAA6G,2DAA2D,6GAA6G,KAAK;AACxU;AACA,yCAAyC,sHAAsH,gEAAgE,oDAAoD,OAAO;AAC1R;;AAEA;AACA,gDAAgD,2GAA2G,uHAAuH,gEAAgE,uEAAuE,qFAAqF,yDAAyD,0CAA0C,qCAAqC,wCAAwC,mCAAmC,mDAAmD,oDAAoD,oCAAoC,OAAO;AACn1B;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,sBAAsB;AAC5C,+FAA+F,sDAAsD;AACrJ;;AAEA,4DAA4D,uHAAuH,gEAAgE,iEAAiE,oCAAoC,+CAA+C,gDAAgD,2DAA2D,OAAO;AACzf;;AAEA;AACA,gDAAgD,4FAA4F,6DAA6D,2HAA2H,OAAO;AAC3U;AACA,yCAAyC,iGAAiG,gEAAgE,wDAAwD,OAAO;AACzQ;;AAEA;AACA;AACA,yCAAyC,sHAAsH,kEAAkE,0EAA0E,wBAAwB,OAAO;AAC1U;;AAEA;AACA;AACA,yCAAyC,iGAAiG,gEAAgE,4EAA4E,sBAAsB,OAAO;AACnT;;AAEA;AACA;AACA,sEAAsE,6GAA6G,qFAAqF,SAAS,4CAA4C,oFAAoF,SAAS;AAC1Z;AACA,6CAA6C,2GAA2G,qEAAqE,uHAAuH,kEAAkE,iDAAiD,oDAAoD,6BAA6B,OAAO,wCAAwC,sHAAsH,kEAAkE,6CAA6C,gDAAgD,6BAA6B,OAAO;AACh4B;;AAEA;AACA,mEAAmE,2EAA2E,SAAS,4CAA4C,gFAAgF,SAAS,8DAA8D,qHAAqH,+DAA+D,iCAAiC,SAAS,4CAA4C,0HAA0H,kEAAkE,iCAAiC,SAAS,8DAA8D,qHAAqH,+DAA+D,iCAAiC,SAAS,4CAA4C,0HAA0H,kEAAkE,iCAAiC,SAAS,+CAA+C,iHAAiH,6DAA6D,oCAAoC,wCAAwC,2BAA2B,OAAO,wCAAwC,sHAAsH,gEAAgE,0CAA0C,8CAA8C,2BAA2B,OAAO;AAC3hE;;AAEA;AACA;AACA;;AAEA;AACA;AACA,mFAAmF,mFAAmF,OAAO;AAC7K;;AAEA;AACA;AACA;AACA,4DAA4D,uBAAuB;AACnF;AACA,iEAAiE,uDAAuD,SAAS;AACjI;AACA,8CAA8C,mGAAmG,iDAAiD,OAAO;AACzM;AACA,uCAAuC,6EAA6E,iDAAiD,OAAO;AAC5K;;AAEA;AACA;AACA;AACA;AACA;AACA,sDAAsD,iIAAiI,wFAAwF,4DAA4D,OAAO;AAClV;AACA,+CAA+C,wFAAwF,4DAA4D,OAAO;AAC1M;;AAEA;AACA;AACA;AACA,6EAA6E,oDAAoD;AACjI;AACA;AACA;AACA,0EAA0E,uDAAuD,SAAS;AAC1I;AACA,gEAAgE,wGAAwG,mDAAmD,SAAS,oDAAoD,wFAAwF,mDAAmD,SAAS,kEAAkE,wGAAwG,mDAAmD,SAAS,oDAAoD,wFAAwF,mDAAmD,SAAS,sDAAsD,2GAA2G,iDAAiD,OAAO,gDAAgD,qFAAqF,iDAAiD,OAAO;AAChvC;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gGAAgG,2GAA2G,gEAAgE,SAAS,0DAA0D,yFAAyF,gEAAgE,SAAS;AAChf,6DAA6D,iIAAiI,0EAA0E,+FAA+F,4DAA4D,OAAO;AAC1a;AACA,sDAAsD,mHAAmH,4DAA4D,OAAO;AAC5O;;AAEA;AACA;AACA;AACA;AACA;AACA,iGAAiG,2GAA2G,mDAAmD,SAAS,yDAAyD,6FAA6F,iDAAiD,OAAO;AACtd;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,+HAA+H,0EAA0E,SAAS;AAClN;;AAEA,oFAAoF,6EAA6E,oDAAoD;AACrN;AACA;AACA;AACA,uEAAuE,gGAAgG,gFAAgF,mDAAmD,SAAS,yDAAyD,yFAAyF,8DAA8D,iDAAiD,OAAO,uEAAuE,gGAAgG,gFAAgF,mDAAmD,SAAS,yDAAyD,yFAAyF,8DAA8D,iDAAiD,OAAO,6DAA6D,uJAAuJ,4FAA4F,mDAAmD,SAAS,uDAAuD,0IAA0I,kEAAkE,+CAA+C,KAAK;AACzxD;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,wJAAwJ,gFAAgF,WAAW;AACnP;;AAEA;AACA,oEAAoE,iIAAiI,0EAA0E,0FAA0F,2HAA2H,4DAA4D,OAAO;AACviB;AACA;AACA;AACA,6DAA6D,8IAA8I,4DAA4D,OAAO;AAC9Q;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,8IAA8I,4EAA4E,WAAW;AACrO;;AAEA,+FAA+F,4HAA4H,oDAAoD;AAC/Q;AACA;AACA;AACA;AACA,kGAAkG,+CAA+C,kCAAkC,+DAA+D,iIAAiI,mDAAmD,SAAS,wEAAwE,oCAAoC,wEAAwE,mHAAmH,qDAAqD,WAAW;AACtxB,kGAAkG,4EAA4E,oCAAoC,6GAA6G,mDAAmD,SAAS,oEAAoE,qEAAqE,kCAAkC,yFAAyF,iDAAiD,OAAO;AACvrB;AACA,sEAAsE,iJAAiJ,6CAA6C,yFAAyF,0FAA0F,iDAAiD,OAAO,sEAAsE,wKAAwK,sEAAsE,mDAAmD,SAAS;AAC/1B;;AAEA;AACA;AACA;AACA;AACA,4EAA4E,0EAA0E,0FAA0F,6EAA6E,gDAAgD,2CAA2C,iIAAiI,6CAA6C,oDAAoD,4FAA4F,oDAAoD,OAAO;AACjxB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,eAAe;AACrC;AACA;;AAEA,oDAAoD,mCAAmC,0CAA0C,iDAAiD,qFAAqF,4DAA4D,OAAO;AAC1U;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,sJAAsJ,0EAA0E,SAAS;AACzO;;AAEA,2GAA2G,oJAAoJ,oDAAoD;AACnT;AACA;AACA;AACA;AACA,+CAA+C;AAC/C,yDAAyD;AACzD,yDAAyD;AACzD,8GAA8G,kFAAkF,8GAA8G,iIAAiI,mDAAmD,SAAS,kFAAkF,kCAAkC,4HAA4H,+GAA+G,mDAAmD,SAAS;AACt4B,8GAA8G,8JAA8J,qCAAqC,gIAAgI,mDAAmD,SAAS,kFAAkF,kIAAkI,qCAAqC,8GAA8G,mDAAmD,SAAS;AACh5B;AACA,kFAAkF,qOAAqO,2GAA2G,iDAAiD,OAAO,8EAA8E,wLAAwL,qFAAqF,iDAAiD,OAAO;AAC72B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,+JAA+J,0EAA0E,SAAS;AAClP;;AAEA,uHAAuH,4KAA4K,oDAAoD;AACvV;AACA;AACA;AACA;AACA,mHAAmH,yBAAyB,gJAAgJ,+GAA+G,mDAAmD,SAAS,qHAAqH,wLAAwL,4BAA4B,8GAA8G,mDAAmD,SAAS,4FAA4F,6PAA6P,oEAAoE,iDAAiD,OAAO;AAC/4C;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA,gMAAgM,0EAA0E,SAAS;AACnR;;AAEA;AACA;AACA;AACA;AACA;AACA,sJAAsJ,wPAAwP,oDAAoD;AAClc;AACA;AACA;AACA;AACA,oJAAoJ,yBAAyB,8KAA8K,+GAA+G,mDAAmD,SAAS,sJAAsJ,iQAAiQ,4BAA4B,8GAA8G,mDAAmD,SAAS,2HAA2H,uRAAuR,oEAAoE,iDAAiD,OAAO;AACjnD;;AAEA;AACA;AACA;AACA,uCAAuC,4BAA4B,mBAAmB,MAAM,yBAAyB,mCAAmC,SAAS,OAAO;AACxK;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0DAA0D,gDAAgD;AAC1G;AACA;AACA,8BAA8B;AAC9B;AACA;;AAEA;AACA,4FAA4F,kDAAkD,UAAU;AACxJ;AACA;;AAEA,uFAAuF,yGAAyG,+EAA+E;AAC/Q;AACA,IAAI,8DAA8D;;AAElE,sCAAsC,kDAAkD,mFAAmF,6BAA6B;AACxM;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,mKAAmK,yDAAyD,SAAS;AACrO;AACA;AACA;AACA;AACA;AACA,0DAA0D,gDAAgD;AAC1G;AACA,sJAAsJ,kDAAkD,uEAAuE,OAAO;AACtR;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,YAAY;AACZ,YAAY;AACZ;;AAEA,sBAAsB,gCAAgC;AACtD;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,QAAQ;;AAER;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,4EAA4E;AAC5E;AACA;AACA;AACA,MAAM;AACN,GAAG;AACH;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,kDAAkD,kDAAkD,kDAAkD,kDAAkD,iDAAiD,mDAAmD,mDAAmD;AAC/V;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;;AAEA;AACA;AACA;AACA,QAAQ;AACR;AACA;;AAEA;AACA;AACA,QAAQ;;AAER;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;;AAEF;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,yDAAyD,gDAAgD,WAAW,UAAU;AAC9H;AACA;AACA;AACA;AACA;AACA,8CAA8C,iDAAiD,kCAAkC,iCAAiC,cAAc,MAAM,4EAA4E,aAAa,WAAW;AAC1R;AACA;;AAEA;;AAEA;AACA;;AAEA,kBAAkB,QAAQ;AAC1B,oBAAoB,QAAQ;AAC5B;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,OAAO;AAC7B;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,yCAAyC,gCAAgC,sBAAsB,sBAAsB,2CAA2C,yCAAyC;AACzM;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,WAAW;AACnC,+BAA+B;AAC/B,iDAAiD,0CAA0C,6GAA6G,0DAA0D,0EAA0E,4EAA4E,4HAA4H,kCAAkC;AACtjB;;AAEA,mIAAmI,uCAAuC,mCAAmC,yBAAyB,uCAAuC,uCAAuC,wDAAwD,SAAS;AACrX;;AAEA;;AAEA;AACA,mEAAmE,wGAAwG,OAAO;AAClL;;AAEA;AACA;AACA,mJAAmJ;AACnJ;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,oLAAoL,gCAAgC,uBAAuB,sCAAsC,sCAAsC,yBAAyB,SAAS;AACzV;;AAEA;;AAEA,kDAAkD;AAClD,yBAAyB;AACzB,2BAA2B;AAC3B,oDAAoD;AACpD,mEAAmE;AACnE,8EAA8E;AAC9E,sBAAsB;AACtB,oDAAoD;AACpD,uBAAuB;AACvB,6BAA6B,uDAAuD,qDAAqD,qDAAqD,qDAAqD,oBAAoB;AACvQ,yEAAyE,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AAC1R,yFAAyF,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AAC1S,oDAAoD;;AAEpD;AACA;AACA,kOAAkO,gCAAgC,uBAAuB,qCAAqC,qCAAqC,yBAAyB,SAAS;AACrY;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0CAA0C,+CAA+C,kDAAkD,+DAA+D,SAAS;AACnN;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,mDAAmD;AACnD;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,MAAM,0CAA0C;;AAEhD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;;AAER;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,QAAQ,yDAAyD;AACjE;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;;AAEA;AACA,mCAAmC,cAAc;AACjD;;AAEA;AACA,2OAA2O;AAC3O;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,MAAM;;AAEN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ,iJAAiJ;AACzJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,sDAAsD,2BAA2B;;AAEjF;AACA;AACA,kOAAkO,gCAAgC,uBAAuB,sCAAsC,sCAAsC,2CAA2C,SAAS;AACzZ;;AAEA;;AAEA,sEAAsE,6CAA6C,6CAA6C,6CAA6C;;AAE7M;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gGAAgG,0BAA0B,0BAA0B,YAAY,sFAAsF,0HAA0H,4BAA4B,4BAA4B,0GAA0G,4BAA4B,4BAA4B,eAAe;AACzlB;;AAEA,8JAA8J,wHAAwH,4DAA4D,4DAA4D,kFAAkF,iJAAiJ,gIAAgI,4DAA4D,4DAA4D,kFAAkF;AAC37B;AACA,mEAAmE,gCAAgC,uBAAuB,qCAAqC,qCAAqC,gDAAgD,sDAAsD,SAAS;AACnT;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,6CAA6C;AAC7C,6EAA6E,yEAAyE;;AAEtJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,yCAAyC;AACzC,yEAAyE,yEAAyE;;AAElJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,sDAAsD;AACtD,2DAA2D,2BAA2B;AACtF,oFAAoF,6CAA6C,6CAA6C,6CAA6C;;AAE3N;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;;AAEP;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4CAA4C,4DAA4D,oCAAoC,mCAAmC,oDAAoD,oCAAoC,+BAA+B,oCAAoC,qCAAqC;AAC/W,gDAAgD;AAChD;AACA;AACA;AACA,6OAA6O,yCAAyC,gCAAgC,0BAA0B,oBAAoB,MAAM,wCAAwC,wCAAwC,wDAAwD,wDAAwD,qMAAqM,8DAA8D,WAAW,wBAAwB,SAAS,uBAAuB,uCAAuC,2CAA2C,kFAAkF,SAAS;AAC7hC;;AAEA;;AAEA;AACA,8CAA8C;AAC9C,8CAA8C;AAC9C;;AAEA;AACA;AACA,oOAAoO,gCAAgC,uBAAuB,8CAA8C,8CAA8C,8CAA8C,8CAA8C,iEAAiE,SAAS;AAC7hB;;AAEA;;AAEA,wBAAwB;;AAExB;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,2CAA2C;;AAE3C;AACA;;AAEA,sGAAsG;AACtG;;AAEA;AACA,2EAA2E,uBAAuB,WAAW,iFAAiF,gDAAgD,2DAA2D,SAAS,uBAAuB,2CAA2C,gCAAgC,iCAAiC,iDAAiD,iCAAiC,4BAA4B,oBAAoB,SAAS,qCAAqC,4MAA4M,uCAAuC,kDAAkD,qCAAqC,sEAAsE,wCAAwC,gCAAgC,wHAAwH,wCAAwC,gCAAgC,4JAA4J,uCAAuC,8BAA8B,SAAS;AACv+C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mDAAmD,wCAAwC,UAAU,qCAAqC,6EAA6E,uCAAuC,UAAU,MAAM,4DAA4D,qEAAqE,8DAA8D,wCAAwC,2DAA2D,sCAAsC,aAAa,WAAW,SAAS;AACvnB;AACA,gFAAgF,gEAAgE,6EAA6E,uGAAuG,gEAAgE,6EAA6E;AACjd;AACA,2EAA2E,uCAAuC,WAAW,sFAAsF,mDAAmD,gDAAgD,4DAA4D,SAAS,uBAAuB,2CAA2C,gCAAgC,iCAAiC,iDAAiD,oDAAoD,gCAAgC,+BAA+B,+BAA+B,+BAA+B,4BAA4B,oBAAoB,SAAS,qCAAqC,gOAAgO,uCAAuC,kDAAkD,qCAAqC,2MAA2M,wCAAwC,gCAAgC,kNAAkN,wCAAwC,gCAAgC,yNAAyN,uCAAuC,oCAAoC,SAAS;AACt/D;;AAEA;;AAEA;AACA;;AAEA,SAAS,gDAAgD;AACzD;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;AACA;AACA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,gDAAgD,wCAAwC,OAAO;AACvI;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA;;AAEA,yBAAyB,mBAAmB;AAC5C;AACA;;AAEA;AACA;AACA;AACA,wCAAwC,6CAA6C,+BAA+B,mCAAmC,4BAA4B,qCAAqC,SAAS,wCAAwC,6EAA6E,qCAAqC,8BAA8B,uCAAuC,WAAW,SAAS,0BAA0B,OAAO;AACrf;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA,uBAAuB,mBAAmB;AAC1C;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI;AACJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,yBAAyB;;AAEzB;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,wDAAwD,iBAAiB,KAAK,mBAAmB;AACjG;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,+DAA+D,oCAAoC;AACnG;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,wBAAwB;AACxB;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,oEAAoE;AACpE,KAAK;AACL;AACA,0CAA0C,oFAAoF,4BAA4B,SAAS;AACnK;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,mEAAmE;AACnE,KAAK;AACL;AACA,0CAA0C,mFAAmF,4BAA4B,SAAS;AAClK;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN,mHAAmH,2CAA2C,gCAAgC,iCAAiC,gDAAgD,qCAAqC,mDAAmD,4BAA4B,oBAAoB,MAAM,oDAAoD,iDAAiD,KAAK,iDAAiD,2EAA2E,oCAAoC,gCAAgC,aAAa,WAAW,sCAAsC,SAAS;AAC9wB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA,mGAAmG,kCAAkC,yFAAyF,kCAAkC,yFAAyF,kCAAkC,yFAAyF,kCAAkC;AACtf,MAAM,6DAA6D,kCAAkC,6CAA6C,kCAAkC,6CAA6C,kCAAkC,6CAA6C,kCAAkC;;AAElV;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2UAA2U;AAC3U;AACA,kFAAkF,sJAAsJ,SAAS;AACjP,sEAAsE,gIAAgI,SAAS,2CAA2C,oDAAoD,8EAA8E,8EAA8E,2LAA2L,+BAA+B,uCAAuC,0CAA0C,4BAA4B,oBAAoB,MAAM,2BAA2B,sEAAsE,yCAAyC,sHAAsH,mRAAmR,mEAAmE,qBAAqB,WAAW,+BAA+B,SAAS;AACz8C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,wDAAwD,iBAAiB,KAAK,mBAAmB;AACjG;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qEAAqE;AACrE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qDAAqD;AACrD;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,8DAA8D;AAC9D,iDAAiD,iEAAiE,6DAA6D;AAC/K;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,+EAA+E,6CAA6C;AAC5H;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,yIAAyI,mEAAmE,yBAAyB,6CAA6C,kCAAkC,8BAA8B,2DAA2D,uCAAuC,uCAAuC,+HAA+H,yCAAyC,mCAAmC,iCAAiC,+BAA+B,oBAAoB,uCAAuC,qCAAqC,8DAA8D,yBAAyB,eAAe,iCAAiC,oBAAoB,yCAAyC,uCAAuC,+DAA+D,2BAA2B,iBAAiB,uDAAuD,gOAAgO,+CAA+C,sCAAsC,yCAAyC,sQAAsQ,iBAAiB,eAAe,aAAa,6CAA6C,WAAW;AAC55D;AACA;AACA;AACA;AACA,yCAAyC,wCAAwC,UAAU,MAAM,iDAAiD,SAAS;AAC3J,wFAAwF,iEAAiE,yDAAyD,mDAAmD,4BAA4B,4DAA4D,uDAAuD,uCAAuC,WAAW,uBAAuB,wCAAwC,SAAS,uBAAuB,2CAA2C,gCAAgC,4BAA4B,yDAAyD,qCAAqC,qCAAqC,yIAAyI,+BAA+B,sBAAsB,6BAA6B,oBAAoB,qCAAqC,mCAAmC,4DAA4D,uBAAuB,aAAa,+BAA+B,qBAAqB,UAAU,sDAAsD,yRAAyR,2CAA2C,iDAAiD,uCAAuC,uMAAuM,4CAA4C,gCAAgC,+NAA+N,4CAA4C,gCAAgC,2PAA2P,2CAA2C,WAAW,oCAAoC,SAAS;AACllF;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,sKAAsK,mFAAmF,yBAAyB,6CAA6C,iCAAiC,8BAA8B,mFAAmF,qCAAqC,qCAAqC,qCAAqC,wIAAwI,yCAAyC,mCAAmC,+BAA+B,oBAAoB,uCAAuC,qCAAqC,6DAA6D,yBAAyB,eAAe,iCAAiC,oBAAoB,yCAAyC,uCAAuC,gEAAgE,2BAA2B,iBAAiB,mCAAmC,oBAAoB,2CAA2C,yCAAyC,iEAAiE,6BAA6B,mBAAmB,8DAA8D,wOAAwO,iDAAiD,wCAAwC,2CAA2C,wYAAwY,mBAAmB,iBAAiB,eAAe,aAAa,6CAA6C,WAAW;AAC/4E;AACA;AACA;AACA;AACA,yCAAyC,wCAAwC,UAAU,MAAM,iDAAiD,SAAS;AAC3J,iHAAiH,iFAAiF,yDAAyD,mDAAmD,4BAA4B,qEAAqE,uDAAuD,uCAAuC,WAAW,uBAAuB,6CAA6C,SAAS,uBAAuB,2CAA2C,+BAA+B,4BAA4B,iFAAiF,mCAAmC,mCAAmC,mCAAmC,iJAAiJ,+BAA+B,sBAAsB,6BAA6B,oBAAoB,qCAAqC,mCAAmC,2DAA2D,uBAAuB,aAAa,+BAA+B,oBAAoB,qCAAqC,qCAAqC,8DAA8D,yBAAyB,eAAe,iCAAiC,qBAAqB,UAAU,wDAAwD,yTAAyT,+CAA+C,mDAAmD,yCAAyC,wNAAwN,gDAAgD,gCAAgC,qPAAqP,gDAAgD,gCAAgC,sRAAsR,+CAA+C,aAAa,sCAAsC,WAAW,SAAS;AACljG;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,8HAA8H,6FAA6F,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gDAAgD,uCAAuC,uCAAuC,oLAAoL,2BAA2B,oBAAoB,oDAAoD,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,+BAA+B,oBAAoB,kDAAkD,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,wDAAwD,mDAAmD,aAAa,WAAW,6BAA6B,SAAS;AACn+C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oKAAoK,6GAA6G,uBAAuB,2CAA2C,+BAA+B,4BAA4B,wEAAwE,qCAAqC,qCAAqC,qCAAqC,4MAA4M,6BAA6B,oBAAoB,mDAAmD,6EAA6E,sFAAsF,uBAAuB,aAAa,gCAAgC,+BAA+B,oBAAoB,sDAAsD,gFAAgF,0GAA0G,yBAAyB,eAAe,kCAAkC,iCAAiC,oBAAoB,uDAAuD,iFAAiF,6GAA6G,2BAA2B,iBAAiB,oCAAoC,qEAAqE,qDAAqD,eAAe,aAAa,WAAW,6BAA6B,SAAS;AAC1gE;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,gLAAgL,sCAAsC,4CAA4C,oDAAoD,uCAAuC,uCAAuC,4EAA4E,oEAAoE,SAAS;AAC7hB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,gLAAgL,sCAAsC,sCAAsC,uCAAuC,2CAA2C,mDAAmD,4EAA4E,iDAAiD,SAAS;AACvgB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,2CAA2C,oDAAoD,6HAA6H,mEAAmE,sEAAsE,SAAS;AAC9W;;AAEA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,8CAA8C,8EAA8E,0CAA0C,oCAAoC,0CAA0C,SAAS;AAC7P,2EAA2E,8EAA8E,0CAA0C,oCAAoC,gFAAgF,4CAA4C,sCAAsC,WAAW,SAAS;AAC7Z,4IAA4I,mFAAmF;AAC/N,0CAA0C,mDAAmD,mCAAmC,yDAAyD,4EAA4E,SAAS;AAC9Q;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,sCAAsC;AACtC;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC;;AAEnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,2BAA2B;AAC3B;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,iEAAiE,0CAA0C,6BAA6B,6BAA6B,mBAAmB,WAAW,oDAAoD,SAAS;AACrQ;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,+DAA+D,yCAAyC,oCAAoC,6BAA6B,mBAAmB,WAAW,gEAAgE,SAAS;AACrR;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,uGAAuG,+CAA+C,+CAA+C,iCAAiC,+RAA+R,SAAS;AAC9gB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA,kEAAkE;;AAElE,yBAAyB,mBAAmB;AAC5C,yHAAyH;AACzH;;AAEA,0FAA0F,2CAA2C,2CAA2C,4BAA4B,4BAA4B,qDAAqD;AAC7R;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA,qDAAqD,8FAA8F,WAAW;;AAE9J,yBAAyB,mBAAmB;AAC5C;AACA,oHAAoH,8KAA8K,WAAW;AAC7S;;AAEA;AACA,2KAA2K,iFAAiF,iCAAiC,uBAAuB,oDAAoD,mEAAmE,8DAA8D,8DAA8D,gDAAgD,WAAW,8DAA8D,8DAA8D,gDAAgD,WAAW,8DAA8D,4HAA4H,gDAAgD,WAAW,4BAA4B,SAAS;AACnjC;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8CAA8C,6DAA6D,oCAAoC,qCAAqC,qDAAqD,oCAAoC,6CAA6C,wCAAwC,+CAA+C;AACjZ,gDAAgD;AAChD,8PAA8P,iEAAiE,uBAAuB,2CAA2C,gCAAgC,0CAA0C,kHAAkH,qCAAqC,qCAAqC,kLAAkL,2BAA2B,qBAAqB,OAAO,oDAAoD,4DAA4D,uBAAuB,aAAa,+BAA+B,qBAAqB,OAAO,sDAAsD,6DAA6D,yBAAyB,eAAe,iCAAiC,qBAAqB,UAAU,6NAA6N,uCAAuC,qPAAqP,mDAAmD,kBAAkB,MAAM,qPAAqP,mDAAmD,iBAAiB,eAAe,2CAA2C,uCAAuC,8IAA8I,kBAAkB,MAAM,8IAA8I,iBAAiB,kBAAkB,gCAAgC,+JAA+J,uCAAuC,6KAA6K,mDAAmD,kBAAkB,MAAM,6KAA6K,mDAAmD,iBAAiB,kBAAkB,gCAAgC,sNAAsN,uCAAuC,yOAAyO,mDAAmD,kBAAkB,MAAM,yOAAyO,mDAAmD,iBAAiB,iBAAiB,aAAa,WAAW,mCAAmC,4EAA4E,SAAS;AAC/0I;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wGAAwG,iFAAiF,uBAAuB,2CAA2C,+BAA+B,4BAA4B,oFAAoF,sCAAsC,sCAAsC,sCAAsC,wMAAwM,2BAA2B,qBAAqB,OAAO,oDAAoD,2DAA2D,uBAAuB,aAAa,+BAA+B,qBAAqB,OAAO,sDAAsD,8DAA8D,yBAAyB,eAAe,iCAAiC,qBAAqB,OAAO,wDAAwD,+DAA+D,2BAA2B,iBAAiB,mCAAmC,qBAAqB,UAAU,qQAAqQ,yPAAyP,qDAAqD,iBAAiB,6CAA6C,kJAAkJ,kBAAkB,gCAAgC,qLAAqL,+KAA+K,mDAAmD,kBAAkB,gCAAgC,qPAAqP,4OAA4O,mDAAmD,iBAAiB,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACvvG;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA,wBAAwB,YAAY;AACpC,0BAA0B,YAAY;AACtC,gEAAgE,4CAA4C,6EAA6E,6FAA6F,qEAAqE,uDAAuD,6GAA6G,0GAA0G,yDAAyD,+DAA+D,yCAAyC,6CAA6C,4KAA4K,oBAAoB,MAAM,6CAA6C,4KAA4K,mBAAmB,iBAAiB,eAAe,aAAa;AACtxC;AACA;;AAEA,0CAA0C,uCAAuC,kCAAkC,8DAA8D,yBAAyB,oEAAoE,SAAS;AACvR;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL,IAAI;AACJ;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK,EAAE;AACP;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,8GAA8G,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,oLAAoL,4BAA4B,6BAA6B,MAAM,6BAA6B,+BAA+B,OAAO,0FAA0F,8DAA8D,yBAAyB,eAAe,iCAAiC,8BAA8B,OAAO,4FAA4F,+DAA+D,2BAA2B,iBAAiB,qEAAqE,uDAAuD,qDAAqD,gDAAgD,kBAAkB,MAAM,uDAAuD,qDAAqD,gDAAgD,iBAAiB,iBAAiB,aAAa,WAAW,6BAA6B,SAAS;AACjgD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,8HAA8H,uBAAuB,2CAA2C,gCAAgC,kDAAkD,0GAA0G,qCAAqC,qCAAqC,wLAAwL,2BAA2B,qBAAqB,OAAO,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,mDAAmD,+BAA+B,qBAAqB,OAAO,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,qDAAqD,iCAAiC,iCAAiC,OAAO,uCAAuC,8DAA8D,8DAA8D,6CAA6C,kBAAkB,MAAM,8DAA8D,8DAA8D,6CAA6C,iBAAiB,iBAAiB,aAAa,WAAW,6BAA6B,SAAS;AAC73D;;AAEA;;AAEA;AACA;AACA,8GAA8G,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,4BAA4B,gCAAgC,4BAA4B,6BAA6B,MAAM,6BAA6B,8BAA8B,OAAO,2FAA2F,6DAA6D,yBAAyB,eAAe,iCAAiC,+BAA+B,OAAO,4FAA4F,gEAAgE,2BAA2B,iBAAiB,mCAAmC,8BAA8B,OAAO,8FAA8F,iEAAiE,6BAA6B,mBAAmB,6DAA6D,yDAAyD,gDAAgD,iBAAiB,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACh7C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oKAAoK,uBAAuB,2CAA2C,+BAA+B,4BAA4B,0EAA0E,qCAAqC,qCAAqC,qCAAqC,gCAAgC,2BAA2B,qBAAqB,OAAO,6EAA6E,sFAAsF,uBAAuB,aAAa,gCAAgC,mDAAmD,+BAA+B,qBAAqB,OAAO,gFAAgF,wGAAwG,yBAAyB,eAAe,kCAAkC,qDAAqD,iCAAiC,qBAAqB,OAAO,iFAAiF,6GAA6G,2BAA2B,iBAAiB,oCAAoC,uDAAuD,mCAAmC,iCAAiC,OAAO,oEAAoE,sEAAsE,6CAA6C,iBAAiB,eAAe,aAAa,WAAW,6BAA6B,SAAS;AAC/5D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,uDAAuD;AACvD;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,oCAAoC,mCAAmC;AACvE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6EAA6E,wDAAwD,qBAAqB,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,8DAA8D,mCAAmC,mCAAmC,mCAAmC,gFAAgF,kDAAkD,mBAAmB,WAAW,gDAAgD,6CAA6C,wCAAwC,qDAAqD,6CAA6C,mBAAmB,WAAW,sCAAsC,qDAAqD,6CAA6C,mBAAmB,WAAW,qDAAqD,mCAAmC,2GAA2G,gEAAgE,+EAA+E,+EAA+E,6EAA6E,+EAA+E,oEAAoE,oEAAoE,8EAA8E,6DAA6D,gCAAgC,YAAY,MAAM,oKAAoK,kFAAkF,gCAAgC,WAAW,SAAS;AACntE;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,oNAAoN,sEAAsE,4DAA4D,qCAAqC,0CAA0C,+BAA+B,qCAAqC,0DAA0D,6DAA6D,WAAW,yBAAyB,SAAS;AAC7oB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;;AAEH,uBAAuB,uCAAuC;AAC9D;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,0JAA0J,yCAAyC,0BAA0B,wDAAwD,wDAAwD,wDAAwD,wCAAwC,+CAA+C,sCAAsC,+CAA+C,gHAAgH,gCAAgC,oEAAoE,0BAA0B,OAAO;AACtyB;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8CAA8C,6DAA6D,oCAAoC,qCAAqC,qDAAqD,oCAAoC,6CAA6C,wCAAwC,+CAA+C;AACjZ,gDAAgD;AAChD,8PAA8P,iEAAiE,uBAAuB,2CAA2C,+BAA+B,uDAAuD,4BAA4B,uCAAuC,2CAA2C,uCAAuC,qCAAqC,iLAAiL,2GAA2G,qBAAqB,OAAO,oDAAoD,mDAAmD,uBAAuB,aAAa,+BAA+B,qBAAqB,OAAO,sDAAsD,qDAAqD,yBAAyB,eAAe,qDAAqD,+CAA+C,qCAAqC,aAAa,WAAW,mCAAmC,4EAA4E,SAAS;AAC3jD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6B,QAAQ,aAAa,qBAAqB,eAAe,WAAW;;AAEjG,yBAAyB,YAAY;AACrC,2DAA2D,oDAAoD,oDAAoD,wDAAwD,uCAAuC;AAClQ;;AAEA,yBAAyB,YAAY;AACrC,2BAA2B,YAAY;AACvC,oEAAoE,oDAAoD,2DAA2D,wDAAwD,8CAA8C;AACzR;;AAEA,6DAA6D,8CAA8C;;AAE3G,0BAA0B,qBAAqB;AAC/C;AACA;;AAEA,gEAAgE;AAChE,oFAAoF,yGAAyG,8EAA8E,2LAA2L,+DAA+D,qBAAqB,uDAAuD,mBAAmB,8JAA8J,kEAAkE,sEAAsE,+DAA+D,iMAAiM,gDAAgD,uBAAuB,+FAA+F,sBAAsB,MAAM,0FAA0F,qBAAqB,6HAA6H,wEAAwE,mDAAmD,+DAA+D,qBAAqB,uDAAuD,mBAAmB,qEAAqE;AACt5D;;AAEA,sJAAsJ,kHAAkH,oFAAoF,iMAAiM,qEAAqE,uBAAuB,6DAA6D,qBAAqB,4EAA4E,6GAA6G,kFAAkF,2DAA2D,uBAAuB,oJAAoJ,yHAAyH,qFAAqF,oHAAoH,sFAAsF,6DAA6D,uEAAuE,yBAAyB,+DAA+D,uBAAuB,iFAAiF;AACx5D;AACA,UAAU,2FAA2F,yGAAyG,8EAA8E,yLAAyL,+DAA+D,qBAAqB,uDAAuD,mBAAmB,2GAA2G,gFAAgF,mLAAmL,mEAAmE,qBAAqB,2DAA2D,mBAAmB,gHAAgH,kFAAkF,sDAAsD,oEAAoE,4DAA4D,qBAAqB,gGAAgG,4HAA4H,wEAAwE,mDAAmD,+DAA+D,qBAAqB,uDAAuD,mBAAmB,mDAAmD,6GAA6G,kFAAkF,yDAAyD,kEAAkE,qBAAqB,2DAA2D,mBAAmB,oIAAoI,+JAA+J;;AAE5vF,uGAAuG,2EAA2E,yHAAyH,iFAAiF;AAC5X;;AAEA,uBAAuB;AACvB;;AAEA;AACA;AACA,4CAA4C,4DAA4D,oCAAoC,mCAAmC,oDAAoD,oCAAoC,+BAA+B,oCAAoC,qCAAqC;AAC/W,gDAAgD;AAChD,8PAA8P,iEAAiE,uBAAuB,6CAA6C,+BAA+B,uDAAuD,4BAA4B,uCAAuC,2CAA2C,qCAAqC,qCAAqC,wIAAwI,sFAAsF,4EAA4E,SAAS;AACp8B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,8GAA8G,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,sEAAsE,gCAAgC,wEAAwE,8BAA8B,MAAM,6BAA6B,+BAA+B,OAAO,0FAA0F,8DAA8D,yBAAyB,eAAe,iCAAiC,8BAA8B,OAAO,4FAA4F,+DAA+D,2BAA2B,iBAAiB,uDAAuD,mDAAmD,8CAA8C,eAAe,aAAa,WAAW,6BAA6B,SAAS;AAChtC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,8HAA8H,uBAAuB,2CAA2C,gCAAgC,6BAA6B,4CAA4C,qCAAqC,qCAAqC,gCAAgC,6BAA6B,qBAAqB,OAAO,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,mDAAmD,+BAA+B,qBAAqB,OAAO,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,qDAAqD,iFAAiF,qBAAqB,OAAO,kDAAkD,4DAA4D,4DAA4D,2CAA2C,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACz+C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,iGAAiG,6CAA6C,uEAAuE,2BAA2B,SAAS;AACzP;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN,wFAAwF,iEAAiE,2CAA2C,uBAAuB,2CAA2C,+BAA+B,4BAA4B,2EAA2E,wCAAwC,wCAAwC,wCAAwC,0BAA0B,oBAAoB,MAAM,gDAAgD,qDAAqD,8BAA8B,oBAAoB,MAAM,oDAAoD,yDAAyD,yDAAyD,8CAA8C,4CAA4C,qCAAqC,iCAAiC,mBAAmB,iBAAiB,eAAe,aAAa,WAAW,kCAAkC,4BAA4B,SAAS;AAC7qC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,uBAAuB,YAAY;AACnC;AACA;AACA;AACA;AACA,QAAQ;;AAER;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;;AAEP;;AAEA,2BAA2B,uBAAuB;AAClD;AACA;;AAEA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA,SAAS;AACT;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,kDAAkD;AAClD,kCAAkC,uDAAuD,qDAAqD,qDAAqD,qDAAqD,oBAAoB;AAC5Q;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qDAAqD;AACrD,8EAA8E,6EAA6E;AAC3J;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iDAAiD;AACjD,kCAAkC;AAClC;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,oPAAoP,mCAAmC,mCAAmC,mCAAmC,mCAAmC,mCAAmC,2BAA2B,eAAe,kCAAkC,+EAA+E;AAC9jB;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,yBAAyB;AACzB;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,iCAAiC;AACjC;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4DAA4D,EAAE;AAC9D;AACA,4CAA4C;AAC5C;AACA,4EAA4E,gFAAgF,iCAAiC,iDAAiD,kEAAkE,iGAAiG,+BAA+B,4BAA4B,oBAAoB,MAAM,+CAA+C,mEAAmE,gCAAgC,gCAAgC,2CAA2C,2CAA2C,+FAA+F,WAAW,0BAA0B,SAAS,uBAAuB,2CAA2C,qDAAqD,SAAS;AAC3/B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK,2FAA2F,iFAAiF,SAAS;AAC1L;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,kEAAkE,6CAA6C,8BAA8B,iDAAiD,8BAA8B,wDAAwD,8EAA8E,cAAc,MAAM,iFAAiF,aAAa,mCAAmC,WAAW;AAClgB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,CAAC;AACD,6BAA6B;AAC7B;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,+CAA+C,sBAAsB,sBAAsB,kBAAkB,kHAAkH,MAAM,MAAM,iBAAiB,KAAK;AACjQ,8CAA8C,wBAAwB,wCAAwC,4BAA4B,+BAA+B,gGAAgG,2CAA2C,KAAK,kBAAkB,2CAA2C,KAAK,kBAAkB,2CAA2C,KAAK,kBAAkB,2CAA2C,KAAK,wBAAwB;AACvhB;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,gEAAgE,2CAA2C,+BAA+B,+BAA+B,gCAAgC,0FAA0F,0DAA0D,sBAAsB,2BAA2B,6BAA6B,YAAY,sBAAsB,6BAA6B,YAAY,sBAAsB,6BAA6B,YAAY,sBAAsB,6BAA6B,WAAW,kDAAkD,SAAS;AAC9qB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,gEAAgE,2CAA2C,+BAA+B,+BAA+B,gCAAgC,mCAAmC,2BAA2B,QAAQ,QAAQ,2BAA2B,QAAQ,QAAQ,qCAAqC,sCAAsC,wHAAwH,4DAA4D,0BAA0B,+BAA+B,iCAAiC,gBAAgB,sBAAsB,iCAAiC,gBAAgB,sBAAsB,iCAAiC,gBAAgB,sBAAsB,iCAAiC,eAAe,mEAAmE,aAAa,WAAW,2CAA2C,SAAS;AAC9gC;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK,EAAE;AACP;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,iGAAiG,wBAAwB,sDAAsD,iCAAiC,4BAA4B,gCAAgC,MAAM,0DAA0D,gGAAgG,aAAa,qDAAqD,WAAW;AACzf;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;;AAEH;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,0CAA0C,kDAAkD,0CAA0C,SAAS;AAC/I;;AAEA;;AAEA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC;AACnC,yDAAyD;AACzD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,0CAA0C;AAC1C,oEAAoE;AACpE;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,sDAAsD;AACtD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qCAAqC;AACrC;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qCAAqC;AACrC;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,gCAAgC;AAChC,mDAAmD;AACnD;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uCAAuC;AACvC,8DAA8D;AAC9D;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC,kBAAkB;AACrD,2CAA2C,8CAA8C,+CAA+C,+CAA+C,+CAA+C,+CAA+C,oBAAoB;AACzS;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iCAAiC;AACjC;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,sDAAsD;AACtD,kIAAkI;AAClI;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,6CAA6C;AAC7C;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,qDAAqD;AACrD,gJAAgJ;AAChJ;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wIAAwI,0CAA0C,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,qCAAqC,0BAA0B,uCAAuC,qBAAqB,MAAM,4BAA4B,qDAAqD,2CAA2C,2BAA2B,aAAa,WAAW,yCAAyC,yBAAyB,SAAS;AACxsB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,wIAAwI,0CAA0C,2CAA2C,2BAA2B,2BAA2B,2BAA2B,2BAA2B,kEAAkE,iEAAiE,gCAAgC,sDAAsD,oZAAoZ,kDAAkD,gCAAgC,gCAAgC,gEAAgE,0EAA0E,6BAA6B,kFAAkF,eAAe,WAAW,0CAA0C,yCAAyC,qBAAqB,MAAM,kCAAkC,oEAAoE,8EAA8E,yEAAyE,8EAA8E,sDAAsD,gCAAgC,uCAAuC,8BAA8B,oDAAoD,+GAA+G,sEAAsE,+BAA+B,4EAA4E,iBAAiB,eAAe,8BAA8B,2BAA2B,aAAa,WAAW,yDAAyD,4BAA4B,SAAS;AACl7E;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,wOAAwO,2CAA2C,4BAA4B,4BAA4B,4BAA4B,+BAA+B,0BAA0B,4BAA4B,MAAM,sEAAsE,kHAAkH,4CAA4C,8DAA8D,+BAA+B,0CAA0C,mBAAmB,MAAM,kCAAkC,yBAAyB,eAAe,yDAAyD,8EAA8E,eAAe,oBAAoB,sBAAsB,eAAe,aAAa,0EAA0E,2CAA2C,mBAAmB,KAAK,kCAAkC,yBAAyB,eAAe,wDAAwD,oMAAoM,6BAA6B,0DAA0D,iBAAiB,qCAAqC,2CAA2C,gCAAgC,iBAAiB,eAAe,oBAAoB,sBAAsB,eAAe,aAAa,SAAS,0BAA0B,SAAS;AACz6D;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,2BAA2B,uBAAuB;AAClD;AACA;;AAEA;;AAEA;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,0DAA0D;AAC1D,wDAAwD,iEAAiE,+CAA+C;AACxK;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,8HAA8H,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gDAAgD,uCAAuC,uCAAuC,oLAAoL,2BAA2B,oBAAoB,kDAAkD,8EAA8E,uFAAuF,uBAAuB,aAAa,gCAAgC,+BAA+B,qBAAqB,OAAO,+EAA+E,yGAAyG,yBAAyB,eAAe,kCAAkC,wDAAwD,0FAA0F,uKAAuK,yEAAyE,0CAA0C,aAAa,WAAW,6BAA6B,SAAS;AAC3pD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,oKAAoK,uBAAuB,2CAA2C,+BAA+B,4BAA4B,wEAAwE,qCAAqC,qCAAqC,qCAAqC,4MAA4M,6BAA6B,oBAAoB,kDAAkD,6EAA6E,sFAAsF,uBAAuB,aAAa,gCAAgC,+BAA+B,oBAAoB,sDAAsD,gFAAgF,0GAA0G,yBAAyB,eAAe,kCAAkC,iCAAiC,oBAAoB,uDAAuD,iFAAiF,6GAA6G,2BAA2B,iBAAiB,oCAAoC,qEAAqE,8HAA8H,0PAA0P,2EAA2E,4CAA4C,eAAe,aAAa,WAAW,6BAA6B,SAAS;AACt1E;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,6BAA6B,uBAAuB;AACpD;AACA;;AAEA;;AAEA;AACA,QAAQ;;AAER;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,0DAA0D;AAC1D,wDAAwD,iEAAiE,+CAA+C;AACxK;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,4FAA4F,4DAA4D,uBAAuB,kDAAkD,0BAA0B,oBAAoB,MAAM,qCAAqC,gEAAgE,cAAc,4BAA4B,oEAAoE,aAAa,WAAW,+CAA+C,0CAA0C,SAAS,gDAAgD,mCAAmC,yBAAyB,yCAAyC,+BAA+B,uDAAuD,cAAc,sBAAsB,2DAA2D,aAAa,0CAA0C,WAAW;AAC9+B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,wDAAwD,+BAA+B,yDAAyD,YAAY,yBAAyB,6DAA6D,WAAW,0BAA0B;;AAEvR,kDAAkD,2GAA2G,qCAAqC,8BAA8B,+GAA+G,WAAW;AAC1V,MAAM;AACN,wDAAwD,sEAAsE,6EAA6E,+CAA+C,kKAAkK,0BAA0B;;AAEtb,kDAAkD,2GAA2G,qCAAqC,8BAA8B,+GAA+G,WAAW,yBAAyB,qCAAqC,4EAA4E,+GAA+G,uCAAuC,gCAAgC,mHAAmH,aAAa,WAAW;AACryB;;AAEA,wFAAwF,kEAAkE,uBAAuB,uDAAuD,iCAAiC,oDAAoD,SAAS;AACtU;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,oCAAoC,qBAAqB;AACzD,8CAA8C,2CAA2C,+CAA+C;AACxI;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK,oEAAoE,2CAA2C,gCAAgC,mCAAmC,0BAA0B,4BAA4B,uBAAuB,MAAM,sCAAsC,4BAA4B,kCAAkC,qBAAqB,aAAa,WAAW,8GAA8G,SAAS;AAClhB;;AAEA;;AAEA,0BAA0B,eAAe,IAAI,eAAe;AAC5D,sDAAsD,8CAA8C,wBAAwB,oBAAoB,oBAAoB,KAAK,oBAAoB,oBAAoB,KAAK,oBAAoB,oBAAoB,KAAK,oBAAoB,oBAAoB,KAAK,oBAAoB;AACpU;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,wBAAwB;AACxB;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,qBAAqB;;AAErB;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,uGAAuG,2CAA2C,kDAAkD,uHAAuH,SAAS;AACpU;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA;AACA;AACA;AACA,OAAO;AACP;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,4FAA4F,4DAA4D,uBAAuB,kDAAkD,+EAA+E,6BAA6B,YAAY,MAAM,iDAAiD,4CAA4C,WAAW,SAAS,gDAAgD,mCAAmC,yBAAyB,yCAAyC,8CAA8C,+BAA+B,cAAc,MAAM,4CAA4C,aAAa,WAAW;AAC1wB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2C,+BAA+B,6BAA6B,6BAA6B,wBAAwB,mCAAmC,2EAA2E,gDAAgD,+BAA+B;AACzV;AACA;;AAEA,kDAAkD,gBAAgB;AAClE,0EAA0E,uDAAuD,YAAY,MAAM,+CAA+C,kGAAkG,WAAW;AAC/S;;AAEA,sBAAsB,OAAO,uFAAuF,kEAAkE,uBAAuB,uDAAuD,iCAAiC,oDAAoD,SAAS;AAClW;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,2CAA2C,iBAAiB,KAAK,mBAAmB,iBAAiB,KAAK,wFAAwF;AAClM,qKAAqK,wEAAwE,8CAA8C,yHAAyH,4CAA4C,4CAA4C,4CAA4C,4CAA4C,8EAA8E,+CAA+C;AACjsB;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iCAAiC;AACjC;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,iEAAiE;AACjE,8EAA8E,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AAC/R;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,4EAA4E;AAC5E,8FAA8F,2BAA2B,0CAA0C,wCAAwC,wCAAwC,wCAAwC,oBAAoB;AAC/S;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0RAA0R,2EAA2E,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gCAAgC,yFAAyF,yHAAyH,mGAAmG,yEAAyE,yEAAyE,uEAAuE,yEAAyE,kEAAkE,kEAAkE,4EAA4E,2DAA2D,gCAAgC,SAAS;AAC14C;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+TAA+T,mIAAmI,uDAAuD,0DAA0D,SAAS,uBAAuB,2CAA2C,4BAA4B,4BAA4B,yGAAyG,yFAAyF,yHAAyH,mGAAmG,sIAAsI,0DAA0D,2gBAA2gB,kaAAka,gaAAga,+ZAA+Z,kEAAkE,2DAA2D,kEAAkE,qDAAqD,gCAAgC,SAAS;AAC7xG;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0CAA0C,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,oCAAoC,2DAA2D,yDAAyD,+DAA+D,4DAA4D,sDAAsD,mDAAmD,wHAAwH,gEAAgE,+DAA+D,+DAA+D,6DAA6D,uBAAuB,cAAc,2CAA2C,uHAAuH,uBAAuB,aAAa,sCAAsC,sBAAsB,cAAc,6CAA6C,2HAA2H,yBAAyB,eAAe,qDAAqD,gDAAgD,6EAA6E,uDAAuD,mDAAmD,oDAAoD,iDAAiD,4EAA4E,wDAAwD,mDAAmD,4DAA4D,kIAAkI,eAAe,6DAA6D,2GAA2G,eAAe,+DAA+D,6GAA6G,eAAe,gEAAgE,uGAAuG,eAAe,aAAa,WAAW,gEAAgE,SAAS;AACv0F;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8RAA8R,2EAA2E,uBAAuB,2CAA2C,4BAA4B,4BAA4B,gCAAgC,yFAAyF,0MAA0M,4EAA4E,gCAAgC,SAAS;AAC35B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mUAAmU,mIAAmI,uDAAuD,0DAA0D,SAAS,uBAAuB,2CAA2C,4BAA4B,4BAA4B,yGAAyG,yFAAyF,0MAA0M,sIAAsI,0DAA0D,obAAob,gCAAgC,SAAS;AACnuD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0CAA0C,2CAA2C,4BAA4B,4BAA4B,4BAA4B,4BAA4B,oCAAoC,2DAA2D,yDAAyD,+DAA+D,4DAA4D,sDAAsD,mDAAmD,wHAAwH,uEAAuE,+DAA+D,sEAAsE,6DAA6D,uBAAuB,cAAc,2CAA2C,uHAAuH,uBAAuB,aAAa,sCAAsC,sBAAsB,cAAc,6CAA6C,2HAA2H,yBAAyB,eAAe,yIAAyI,6IAA6I,kOAAkO,kOAAkO,qEAAqE,qDAAqD,eAAe,aAAa,WAAW,gEAAgE,SAAS;AAC34E;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,4FAA4F,0CAA0C,0DAA0D,WAAW;AAC3M;AACA;AACA,0CAA0C,mDAAmD,0CAA0C,SAAS;AAChJ;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;;AAEA,qDAAqD,uCAAuC,mCAAmC,4GAA4G,+BAA+B,mIAAmI,aAAa,8BAA8B,WAAW,qCAAqC,iDAAiD,mCAAmC;AAC5jB;AACA,KAAK,eAAe,+BAA+B;AACnD;AACA,KAAK,eAAe,aAAa,gCAAgC;AACjE;AACA,KAAK,eAAe,kCAAkC;AACtD;AACA,KAAK,eAAe,eAAe,aAAa,8BAA8B,WAAW;AACzF;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,6EAA6E,0DAA0D,8CAA8C,4CAA4C,6CAA6C,8BAA8B,8BAA8B,qHAAqH,qHAAqH,6DAA6D,6DAA6D,2HAA2H,2EAA2E,aAAa,mCAAmC,WAAW;AAC/6B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,2IAA2I,2BAA2B,sBAAsB,MAAM,4BAA4B,qBAAqB,MAAM,MAAM,kCAAkC,oBAAoB,QAAQ,MAAM,0BAA0B,OAAO,KAAK;AACzW;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC;AACnC;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2IAA2I,yBAAyB,sDAAsD,4BAA4B,+BAA+B,4BAA4B,oBAAoB,MAAM,qCAAqC,8BAA8B,oBAAoB,MAAM,8DAA8D,wFAAwF,eAAe,gDAAgD,kDAAkD,6BAA6B,eAAe,aAAa,iEAAiE,WAAW;AACpzB;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,0CAA0C;AAC1C;AACA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;;AAEA;AACA;AACA;AACA,0CAA0C,kDAAkD,4CAA4C,4BAA4B,4CAA4C,YAAY,MAAM,4CAA4C,WAAW,SAAS;AAClS;;AAEA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,kLAAkL,0CAA0C,gEAAgE;AAC5R;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,kDAAkD;AAClD;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,gCAAgC,aAAa,mBAAmB;AAChE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,uDAAuD;AACvD;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,mCAAmC,mCAAmC;AACtE;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,0DAA0D,yCAAyC,sCAAsC,mCAAmC,mBAAmB,yBAAyB,qBAAqB,iBAAiB,KAAK,wBAAwB,qBAAqB,KAAK,SAAS,gCAAgC,KAAK,kBAAkB;AACrX;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA,kCAAkC,yBAAyB;AAC3D;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,2BAA2B;AAC3B;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,2BAA2B;AAC3B;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,mDAAmD;AACnD;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ,qHAAqH;AACrH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,gDAAgD;AAChD;AACA;AACA;AACA,kFAAkF,gEAAgE,uBAAuB,oDAAoD,0CAA0C,SAAS;AAChR;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;AACL,IAAI,oEAAoE;AACxE;AACA;AACA;;AAEA;AACA,IAAI;AACJ;;AAEA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,CAAC;AACD,yBAAyB;AACzB;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA,CAAC;AACD,+CAA+C,+CAA+C;AAC9F;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;AACA;AACA;AACA,0CAA0C,kDAAkD,0CAA0C,SAAS;AAC/I;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,sBAAsB,kBAAkB;AACxC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,gEAAgE,4CAA4C,iCAAiC,mCAAmC,o0BAAo0B,2DAA2D,qEAAqE,+EAA+E,6DAA6D,6DAA6D,yIAAyI,6DAA6D,uCAAuC,sEAAsE,qBAAqB,wBAAwB,YAAY,+BAA+B,mCAAmC,aAAa,MAAM,mCAAmC,YAAY,UAAU;AAC5zD;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK,6DAA6D,iHAAiH,iCAAiC,mCAAmC,2wCAA2wC,mEAAmE,2EAA2E,wCAAwC,oDAAoD,yDAAyD,UAAU;AAC/yD;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,uBAAuB,YAAY;AACnC;;AAEA,6BAA6B,YAAY;AACzC;AACA;AACA;;AAEA,uBAAuB,YAAY;AACnC;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA,6BAA6B,aAAa;AAC1C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA,KAAK;AACL;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,8EAA8E,yCAAyC,wCAAwC,sCAAsC,qCAAqC,oCAAoC,sBAAsB,MAAM,4CAA4C,0CAA0C,2GAA2G,uBAAuB,gFAAgF,qBAAqB,oBAAoB,+BAA+B,qCAAqC,oCAAoC,sBAAsB,MAAM,4CAA4C,wEAAwE,2CAA2C,sDAAsD,uBAAuB,qBAAqB,mBAAmB,wDAAwD,kBAAkB,+BAA+B,sCAAsC,qCAAqC,oCAAoC,sBAAsB,MAAM,2CAA2C,gFAAgF,qBAAqB,oBAAoB,+BAA+B,qCAAqC,oCAAoC,sBAAsB,MAAM,2CAA2C,uEAAuE,qBAAqB,mBAAmB,wDAAwD,kBAAkB,+BAA+B,yDAAyD,kBAAkB,MAAM,kCAAkC,iBAAiB,eAAe,wGAAwG,kCAAkC,uGAAuG,2EAA2E,kBAAkB,MAAM,sDAAsD,iBAAiB,mCAAmC,eAAe,6BAA6B,iDAAiD,kCAAkC,sCAAsC,kCAAkC,kCAAkC,wCAAwC,oCAAoC,oCAAoC,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,mDAAmD,2DAA2D,wCAAwC,sDAAsD,kBAAkB,MAAM,oEAAoE,oEAAoE,oEAAoE,oEAAoE,8CAA8C,kDAAkD,kDAAkD,yGAAyG,oBAAoB,MAAM,+CAA+C,+CAA+C,+CAA+C,+CAA+C,6PAA6P,0PAA0P,iHAAiH,mBAAmB,iBAAiB,uCAAuC,eAAe;AACxsJ;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;;AAEA,uBAAuB,YAAY;AACnC;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,uBAAuB,mBAAmB;AAC1C;;AAEA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;AACA,KAAK;;AAEL;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0DAA0D;AAC1D;AACA,2EAA2E,uCAAuC,WAAW;AAC7H;AACA,2EAA2E,wBAAwB,WAAW,4EAA4E,gDAAgD,2DAA2D,SAAS,gDAAgD,8DAA8D,SAAS,uBAAuB,2CAA2C,gCAAgC,iCAAiC,+GAA+G,0EAA0E,iCAAiC,4BAA4B,oBAAoB,SAAS,qCAAqC,4MAA4M,6UAA6U,uCAAuC,kDAAkD,qCAAqC,uLAAuL,6DAA6D,qKAAqK,wCAAwC,gCAAgC,8LAA8L,iOAAiO,wCAAwC,gCAAgC,qMAAqM,qRAAqR,uCAAuC,8BAA8B,SAAS;AACv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uBAAuB,cAAc;AACrC;AACA;AACA;;AAEA,sBAAsB,cAAc;AACpC;AACA;;AAEA;;AAEA,uBAAuB,cAAc;AACrC;AACA;;AAEA,wBAAwB,cAAc;AACtC;AACA;;AAEA,uBAAuB,6BAA6B;AACpD;AACA;;AAEA;AACA;AACA;;AAEA,WAAW,eAAe;AAC1B;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA,kBAAkB,cAAc;AAChC;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,OAAO;AACzB;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,8FAA8F,iBAAiB;AAC/G;AACA;;AAEA,qBAAqB,MAAM;AAC3B;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,oBAAoB,sBAAsB;AAC1C;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,UAAU;AACV;AACA;AACA,UAAU;AACV,gBAAgB,mCAAmC;;AAEnD,sBAAsB,EAAE,UAAU;AAClC,2DAA2D;;AAE3D;;AAEA;AACA;AACA;AACA;;AAEA;AACA,UAAU;AACV,gGAAgG,wEAAwE;AACxK;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA,UAAU;;AAEV;AACA;AACA;AACA,QAAQ;AACR;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,mIAAmI;AACnI;;AAEA;;AAEA;AACA;AACA,qMAAqM,gBAAgB,yBAAyB,4FAA4F,4FAA4F;AACta;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA,aAAa,mCAAmC;AAChD;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,MAAM;AACN;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,oXAAoX,EAAE;AACtX;;AAEA;AACA,wDAAwD,yBAAyB;AACjF;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,8BAA8B,GAAG,GAAG,EAAE,YAAY,EAAE,GAAG,EAAE;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,iDAAiD,mBAAmB;AACpE;AACA,QAAQ,0BAA0B,uBAAuB;AACzD;AACA;AACA,MAAM;AACN;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,WAAW;AACX;;AAEA;AACA,yDAAyD,EAAE;AAC3D;AACA;;AAEA;AACA;AACA,wDAAwD,EAAE;AAC1D;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,2GAA2G,6EAA6E;AACxL;;AAEA;;AAEA,sBAAsB,UAAU;AAChC;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,yBAAyB,mBAAmB;AAC5C;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,qGAAqG,iDAAiD,cAAc;AACpK;AACA;AACA,QAAQ;AACR,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,UAAU;;AAEV;;AAEA;AACA;AACA;AACA;AACA,UAAU;;AAEV;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,2KAA2K,EAAE;AAC7K;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,+BAA+B;AAC/B;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,yKAAyK,yBAAyB;AAClM;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;;AAEA;;AAEA,wBAAwB,kBAAkB;AAC1C;AACA;;AAEA;;AAEA;AACA;;AAEA,0BAA0B,kBAAkB;AAC5C;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,0BAA0B,GAAG,EAAE,GAAG,KAAK,EAAE;AACzC,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,UAAU;;AAEV;AACA,QAAQ;AACR,KAAK;AACL;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,oDAAoD,GAAG;AACvD,kBAAkB,cAAc,GAAG,EAAE,EAAE,MAAM,EAAE,cAAc,GAAG;AAChE,kBAAkB,+BAA+B,GAAG,QAAQ,GAAG,OAAO,GAAG,UAAU,GAAG,UAAU,GAAG,OAAO,UAAU,EAAE,mCAAmC,WAAW,GAAG,OAAO,UAAU,GAAG,YAAY,EAAE,GAAG;AAC5M;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA,6BAA6B;AAC7B;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;;AAEA,oBAAoB,cAAc;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,oBAAoB;AACpB;;AAEA;AACA,4DAA4D,6PAA6P,iBAAiB,GAAG,0BAA0B,GAAG;AAC1W;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,sBAAsB,+GAA+G;AACrI;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA,wCAAwC;AACxC;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,qBAAqB,cAAc;AACnC;AACA;AACA;AACA;AACA,aAAa;AACb,aAAa;AACb;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,aAAa;AACb;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,mCAAmC,IAAI;AACvC;AACA;;AAEA,uBAAuB,mBAAmB;AAC1C;AACA;AACA;;AAEA;AACA;AACA,CAAC;AACD;AACA;AACA,gDAAgD,eAAe,6GAA6G,IAAI,kTAAkT,GAAG,wIAAwI,IAAI,uBAAuB,eAAe,6CAA6C,eAAe;AACntB,CAAC;AACD;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,SAAS,gBAAgB;AACzB;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,YAAY;AACZ;AACA;;AAEA;AACA,+DAA+D;;AAE/D,uBAAuB,YAAY;AACnC;AACA;;AAEA;AACA;;AAEA,SAAS,4EAA4E;AACrF;AACA;;AAEA,YAAY;AACZ;;AAEA;AACA;AACA;;AAEA;AACA,wHAAwH,iCAAiC,qQAAqQ,iCAAiC;;AAE/b;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,cAAc;AACd;;AAEA;AACA;AACA;;AAEA;;AAEA,yBAAyB,cAAc;AACvC;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA,aAAa,sDAAsD;AACnE;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,2FAA2F;AAC3F;AACA;AACA;;AAEA;AACA,cAAc;AACd,YAAY;AACZ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,oHAAoH,GAAG;AACvH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,qCAAqC,qCAAqC,mBAAmB,wCAAwC;AACrI,IAAI;;AAEJ;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA,0CAA0C,qCAAqC,mBAAmB,yCAAyC;;AAE3I;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,yCAAyC,WAAW;AACpD;AACA,UAAU,yBAAyB,eAAe;AAClD;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,6CAA6C,eAAe;AAC5D;AACA,UAAU,yBAAyB,eAAe;AAClD;AACA;AACA;AACA;;AAEA;AACA,mEAAmE;;AAEnE;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,CAAC,uBAAuB;AACxB;AACA,CAAC,iBAAiB;AAClB;AACA,CAAC,yBAAyB;AAC1B;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,yEAAyE,iBAAiB;AAC1F;AACA;;AAEA;AACA;AACA;AACA;AACA,QAAQ;AACR,MAAM;AACN;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA,gGAAgG;AAChG;AACA;;AAEA;AACA;AACA;AACA;;AAEA,qCAAqC,aAAa;AAClD;AACA;AACA,QAAQ;AACR;;AAEA;AACA,GAAG;AACH;AACA;;AAEA;AACA;;AAEA;AACA,8QAA8Q;AAC9Q;;AAEA;AACA;AACA;;AAEA;AACA,uHAAuH;AACvH;;AAEA;AACA;AACA;AACA;AACA,UAAU;AACV;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,yBAAyB,4EAA4E;AACrG;;AAEA;;AAEA,4EAA4E,gBAAgB;AAC5F;AACA,YAAY;AACZ;AACA;;AAEA,gCAAgC,gBAAgB;AAChD;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA,iCAAiC,YAAY;AAC7C;AACA;AACA,YAAY;AACZ;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH,aAAa;AACb,aAAa;AACb,gBAAgB;AAChB;AACA,CAAC;AACD;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,0BAA0B;AAC5C;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA,kBAAkB,wBAAwB;AAC1C;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,uBAAuB,oBAAoB;AAC3C;;AAEA,wBAAwB,WAAW;AACnC,0BAA0B,WAAW;AACrC,4BAA4B,WAAW;AACvC;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,yBAAyB,UAAU;AACnC;AACA;;AAEA;AACA,KAAK;AACL,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,yBAAyB,YAAY;AACrC;;AAEA;AACA;;AAEA;AACA;;AAEA,0BAA0B,WAAW;AACrC,4BAA4B,WAAW;AACvC;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA,KAAK;AACL,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,uBAAuB,kBAAkB;AACzC;;AAEA,wBAAwB,WAAW;AACnC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;AACA;;AAEA;;AAEA;;AAEA;;AAEA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA,uBAAuB,YAAY;AACnC;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,yDAAyD;AACzD;AACA,MAAM;;AAEN;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,uFAAuF,qBAAqB,sVAAsV,eAAe,6BAA6B,kiBAAkiB,IAAI;AACphC;AACA;;AAEA;AACA;AACA;AACA,IAAI;;AAEJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;;AAEA;;AAEA,sBAAsB,OAAO;AAC7B;AACA;;AAEA,0BAA0B,WAAW;AACrC;AACA;AACA,KAAK;AACL;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,2BAA2B,aAAa;AACxC;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,gjCAAgjC,SAAS,wDAAwD,YAAY,mHAAmH,YAAY,unJAAunJ,UAAU;AAC73L;AACA;AACA,KAAK,qRAAqR,OAAO;AACjS;AACA;;AAEA,aAAa,4BAA4B;AACzC;AACA;;AAEA;AACA;;AAEA,aAAa,MAAM;AACnB;AACA;;AAEA;AACA,KAAK;AACL;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA,cAAc;AACd;;AAEA,oBAAoB,oBAAoB;AACxC;;AAEA;AACA;AACA;AACA;;AAEA,iBAAiB,8DAA8D;AAC/E;;AAEA;AACA;;AAEA,wBAAwB,kBAAkB,YAAY;AACtD;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;;AAEA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA,qjBAAqjB,SAAS,8OAA8O,wvBAAwvB,KAAK;AACziD;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA,kEAAkE,IAAI,uGAAuG,oCAAoC,6MAA6M,GAAG,sCAAsC,yBAAyB,GAAG,qBAAqB,GAAG,qBAAqB,OAAO,iBAAiB,OAAO,iBAAiB,GAAG,6RAA6R,OAAO,wFAAwF,GAAG,2DAA2D,GAAG,2nBAA2nB,GAAG,kBAAkB,GAAG,kBAAkB,GAAG,kBAAkB,GAAG,kBAAkB,GAAG,kBAAkB,OAAO,iBAAiB,GAAG,iBAAiB,GAAG,iBAAiB,GAAG,iBAAiB,GAAG,iBAAiB,GAAG,iBAAiB,OAAO;AACv2D;;AAEA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA,SAAS,MAAM;AACf;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,uBAAuB,uBAAuB;AAC9C;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;;AAEA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA,uBAAuB,yOAAyO;AAChQ;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA,IAAI;AACJ;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA,kBAAkB,cAAc;AAChC;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA,oCAAoC,sDAAsD,0BAA0B;AACpH;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA,OAAO;AACP;;AAEA;AACA;;AAEA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;AACA;AACA,OAAO;AACP;;AAEA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;AACA;AACA;AACA;AACA,GAAG;AACH;AACA,GAAG;AACH;AACA;AACA,GAAG;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,IAAI;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,KAAK;AACL;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;;AAEP;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR,iGAAiG;AACjG;AACA,SAAS;AACT;;AAEA;AACA,wBAAwB;AACxB;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,QAAQ;AACR,8LAA8L;AAC9L;AACA,OAAO;AACP;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR,0FAA0F;AAC1F;AACA,SAAS;AACT;;AAEA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT;AACA,SAAS;AACT,QAAQ;AACR,sFAAsF;AACtF;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAAS;AACT,QAAQ;AACR;AACA;;AAEA,wBAAwB;AACxB;AACA;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;;AAEA;AACA,GAAG;AACH;AACA;AACA;AACA;AACA,QAAQ;AACR;AACA;AACA;AACA;AACA,OAAO;AACP;AACA;AACA;;AAEA;AACA;AACA,QAAQ;AACR,4FAA4F;AAC5F;AACA,SAAS;AACT;;AAEA;AACA;AACA;AACA;AACA;;AAEA,2BAA2B,YAAY;AACvC;;AAEA,4BAA4B,WAAW;AACvC;AACA;;AAEA;AACA;;AAEA,wBAAwB;AACxB;AACA;AACA;AACA;AACA,OAAO;AACP,KAAK;;AAEL;AACA;AACA;;AAEA;AACA,GAAG;AACH;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA,MAAM;AACN;AACA;AACA;AACA,GAAG;AACH;AACA;;AAEA;AACA;AACA;AACA;AACA,GAAG;AACH;;AAEA","sources":["webpack://jacdac-docs/./src/workers/tf/dist/node_modules/tf-worker.js"],"sourcesContent":["var _asyncToGenerator = require(\"/home/runner/work/jacdac-docs/jacdac-docs/node_modules/@babel/runtime/helpers/asyncToGenerator\");\n\nrequire(\"core-js/modules/es.math.hypot.js\");\n\nfunction _extends() {\n  return (_extends = Object.assign || function (e) {\n    for (var t = 1; t < arguments.length; t++) {\n      var n = arguments[t];\n\n      for (var r in n) {\n        Object.prototype.hasOwnProperty.call(n, r) && (e[r] = n[r]);\n      }\n    }\n\n    return e;\n  }).apply(this, arguments);\n}\n\nfunction _objectWithoutPropertiesLoose(e, t) {\n  if (null == e) return {};\n  var n,\n      r,\n      a = {},\n      s = Object.keys(e);\n\n  for (r = 0; r < s.length; r++) {\n    t.indexOf(n = s[r]) >= 0 || (a[n] = e[n]);\n  }\n\n  return a;\n}\n\nvar EPSILON_FLOAT32$3 = 1e-7,\n    EPSILON_FLOAT16$3 = 1e-4;\n\nclass DataStorage$1 {\n  constructor(e, t) {\n    this.backend = e, this.dataMover = t, this.data = new WeakMap(), this.dataIdsCount = 0;\n  }\n\n  get(e) {\n    return this.data.has(e) || this.dataMover.moveData(this.backend, e), this.data.get(e);\n  }\n\n  set(e, t) {\n    this.dataIdsCount++, this.data.set(e, t);\n  }\n\n  has(e) {\n    return this.data.has(e);\n  }\n\n  delete(e) {\n    return this.dataIdsCount--, this.data.delete(e);\n  }\n\n  numDataIds() {\n    return this.dataIdsCount;\n  }\n\n}\n\nclass KernelBackend$1 {\n  refCount(e) {\n    return notYetImplemented$1(\"refCount\");\n  }\n\n  incRef(e) {\n    return notYetImplemented$1(\"incRef\");\n  }\n\n  timerAvailable() {\n    return !0;\n  }\n\n  time(e) {\n    return notYetImplemented$1(\"time\");\n  }\n\n  read(e) {\n    return notYetImplemented$1(\"read\");\n  }\n\n  readSync(e) {\n    return notYetImplemented$1(\"readSync\");\n  }\n\n  numDataIds() {\n    return notYetImplemented$1(\"numDataIds\");\n  }\n\n  disposeData(e, t) {\n    return notYetImplemented$1(\"disposeData\");\n  }\n\n  write(e, t, n) {\n    return notYetImplemented$1(\"write\");\n  }\n\n  move(e, t, n, r, a) {\n    return notYetImplemented$1(\"move\");\n  }\n\n  memory() {\n    return notYetImplemented$1(\"memory\");\n  }\n\n  floatPrecision() {\n    return notYetImplemented$1(\"floatPrecision\");\n  }\n\n  epsilon() {\n    return 32 === this.floatPrecision() ? EPSILON_FLOAT32$3 : EPSILON_FLOAT16$3;\n  }\n\n  dispose() {\n    return notYetImplemented$1(\"dispose\");\n  }\n\n}\n\nfunction notYetImplemented$1(e) {\n  throw new Error(\"'\".concat(e, \"' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen\"));\n}\n\nfunction shuffle$1(e) {\n  var t = e.length,\n      n = 0;\n\n  for (; t > 0;) {\n    n = Math.random() * t | 0, t--, swap$1(e, t, n);\n  }\n}\n\nfunction clamp$1(e, t, n) {\n  return Math.max(e, Math.min(t, n));\n}\n\nfunction nearestLargerEven$1(e) {\n  return e % 2 == 0 ? e : e + 1;\n}\n\nfunction swap$1(e, t, n) {\n  var r = e[t];\n  e[t] = e[n], e[n] = r;\n}\n\nfunction sum$7(e) {\n  var t = 0;\n\n  for (var n = 0; n < e.length; n++) {\n    t += e[n];\n  }\n\n  return t;\n}\n\nfunction assert$6(e, t) {\n  if (!e) throw new Error(\"string\" == typeof t ? t : t());\n}\n\nfunction assertShapesMatch$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"\";\n  assert$6(arraysEqual$1(e, t), () => n + \" Shapes \".concat(e, \" and \").concat(t, \" must match\"));\n}\n\nfunction assertNonNull$1(e) {\n  assert$6(null != e, () => \"The input to the tensor constructor must be a non-null value.\");\n}\n\nfunction flatten$6(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : [];\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  if (null == t && (t = []), Array.isArray(e) || isTypedArray$1(e) && !n) for (var r = 0; r < e.length; ++r) {\n    flatten$6(e[r], t, n);\n  } else t.push(e);\n  return t;\n}\n\nfunction sizeFromShape$1(e) {\n  if (0 === e.length) return 1;\n  var t = e[0];\n\n  for (var n = 1; n < e.length; n++) {\n    t *= e[n];\n  }\n\n  return t;\n}\n\nfunction arraysEqual$1(e, t) {\n  if (e === t) return !0;\n  if (null == e || null == t) return !1;\n  if (e.length !== t.length) return !1;\n\n  for (var n = 0; n < e.length; n++) {\n    if (e[n] !== t[n]) return !1;\n  }\n\n  return !0;\n}\n\nfunction isInt$1(e) {\n  return e % 1 == 0;\n}\n\nfunction sizeToSquarishShape$1(e) {\n  var t = Math.ceil(Math.sqrt(e));\n  return [t, Math.ceil(e / t)];\n}\n\nfunction rightPad$1(e, t) {\n  return t <= e.length ? e : e + \" \".repeat(t - e.length);\n}\n\nfunction repeatedTry$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e => 0;\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  return new Promise((r, a) => {\n    var s = 0;\n\n    var o = () => {\n      if (e()) return void r();\n      s++;\n      var i = t(s);\n      null != n && s >= n ? a() : setTimeout(o, i);\n    };\n\n    o();\n  });\n}\n\nfunction inferFromImplicitShape$1(e, t) {\n  var n = 1,\n      r = -1;\n\n  for (var _t = 0; _t < e.length; ++_t) {\n    if (e[_t] >= 0) n *= e[_t];else if (-1 === e[_t]) {\n      if (-1 !== r) throw Error(\"Shapes can only have 1 implicit size. Found -1 at dim \".concat(r, \" and dim \").concat(_t));\n      r = _t;\n    } else if (e[_t] < 0) throw Error(\"Shapes can not be < 0. Found \".concat(e[_t], \" at dim \").concat(_t));\n  }\n\n  if (-1 === r) {\n    if (t > 0 && t !== n) throw Error(\"Size(\".concat(t, \") must match the product of shape \").concat(e));\n    return e;\n  }\n\n  if (0 === n) throw Error(\"Cannot infer the missing size in [\".concat(e, \"] when there are 0 elements\"));\n  if (t % n != 0) throw Error(\"The implicit shape can't be a fractional number. Got \".concat(t, \" / \").concat(n));\n  var a = e.slice();\n  return a[r] = t / n, a;\n}\n\nfunction parseAxisParam$1(e, t) {\n  var n = t.length;\n  return assert$6((e = null == e ? t.map((e, t) => t) : [].concat(e)).every(e => e >= -n && e < n), () => \"All values in axis param must be in range [-\".concat(n, \", \").concat(n, \") but got axis \").concat(e)), assert$6(e.every(e => isInt$1(e)), () => \"All values in axis param must be integers but got axis \".concat(e)), e.map(e => e < 0 ? n + e : e);\n}\n\nfunction squeezeShape$1(e, t) {\n  var n = [],\n      r = [],\n      a = null != t && Array.isArray(t) && 0 === t.length,\n      s = null == t || a ? null : parseAxisParam$1(t, e).sort();\n  var o = 0;\n\n  for (var _t2 = 0; _t2 < e.length; ++_t2) {\n    if (null != s) {\n      if (s[o] === _t2 && 1 !== e[_t2]) throw new Error(\"Can't squeeze axis \".concat(_t2, \" since its dim '\").concat(e[_t2], \"' is not 1\"));\n      (null == s[o] || s[o] > _t2) && 1 === e[_t2] && (n.push(e[_t2]), r.push(_t2)), s[o] <= _t2 && o++;\n    }\n\n    1 !== e[_t2] && (n.push(e[_t2]), r.push(_t2));\n  }\n\n  return {\n    newShape: n,\n    keptDims: r\n  };\n}\n\nfunction getTypedArrayFromDType$1(e, t) {\n  var n = null;\n  if (null == e || \"float32\" === e) n = new Float32Array(t);else if (\"int32\" === e) n = new Int32Array(t);else {\n    if (\"bool\" !== e) throw new Error(\"Unknown data type \".concat(e));\n    n = new Uint8Array(t);\n  }\n  return n;\n}\n\nfunction getArrayFromDType$1(e, t) {\n  var n = null;\n  if (null == e || \"float32\" === e) n = new Float32Array(t);else if (\"int32\" === e) n = new Int32Array(t);else if (\"bool\" === e) n = new Uint8Array(t);else {\n    if (\"string\" !== e) throw new Error(\"Unknown data type \".concat(e));\n    n = new Array(t);\n  }\n  return n;\n}\n\nfunction checkConversionForErrors$1(e, t) {\n  for (var n = 0; n < e.length; n++) {\n    var r = e[n];\n    if (isNaN(r) || !isFinite(r)) throw Error(\"A tensor of type \".concat(t, \" being uploaded contains \").concat(r, \".\"));\n  }\n}\n\nfunction isValidDtype$1(e) {\n  return \"bool\" === e || \"complex64\" === e || \"float32\" === e || \"int32\" === e || \"string\" === e;\n}\n\nfunction hasEncodingLoss$1(e, t) {\n  return !(\"complex64\" === t || \"float32\" === t && \"complex64\" !== e || \"int32\" === t && \"float32\" !== e && \"complex64\" !== e || \"bool\" === t && \"bool\" === e);\n}\n\nfunction isTypedArray$1(e) {\n  return e instanceof Float32Array || e instanceof Int32Array || e instanceof Uint8Array;\n}\n\nfunction bytesPerElement$1(e) {\n  if (\"float32\" === e || \"int32\" === e) return 4;\n  if (\"complex64\" === e) return 8;\n  if (\"bool\" === e) return 1;\n  throw new Error(\"Unknown dtype \".concat(e));\n}\n\nfunction bytesFromStringArray$1(e) {\n  if (null == e) return 0;\n  var t = 0;\n  return e.forEach(e => t += e.length), t;\n}\n\nfunction isString$1(e) {\n  return \"string\" == typeof e || e instanceof String;\n}\n\nfunction isBoolean$1(e) {\n  return \"boolean\" == typeof e;\n}\n\nfunction isNumber$1(e) {\n  return \"number\" == typeof e;\n}\n\nfunction inferDtype$1(e) {\n  return Array.isArray(e) ? inferDtype$1(e[0]) : e instanceof Float32Array ? \"float32\" : e instanceof Int32Array || e instanceof Uint8Array ? \"int32\" : isNumber$1(e) ? \"float32\" : isString$1(e) ? \"string\" : isBoolean$1(e) ? \"bool\" : \"float32\";\n}\n\nfunction isFunction$1(e) {\n  return !!(e && e.constructor && e.call && e.apply);\n}\n\nfunction nearestDivisor$1(e, t) {\n  for (var n = t; n < e; ++n) {\n    if (e % n == 0) return n;\n  }\n\n  return e;\n}\n\nfunction computeStrides$1(e) {\n  var t = e.length;\n  if (t < 2) return [];\n  var n = new Array(t - 1);\n  n[t - 2] = e[t - 1];\n\n  for (var r = t - 3; r >= 0; --r) {\n    n[r] = n[r + 1] * e[r + 1];\n  }\n\n  return n;\n}\n\nfunction createNestedArray$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = new Array();\n\n  if (1 === t.length) {\n    var s = t[0] * (r ? 2 : 1);\n\n    for (var _t3 = 0; _t3 < s; _t3++) {\n      a[_t3] = n[e + _t3];\n    }\n  } else {\n    var _s = t[0],\n        o = t.slice(1),\n        i = o.reduce((e, t) => e * t) * (r ? 2 : 1);\n\n    for (var _t4 = 0; _t4 < _s; _t4++) {\n      a[_t4] = createNestedArray$1(e + _t4 * i, o, n, r);\n    }\n  }\n\n  return a;\n}\n\nfunction toNestedArray$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  if (0 === e.length) return t[0];\n  var r = e.reduce((e, t) => e * t) * (n ? 2 : 1);\n  if (0 === r) return [];\n  if (r !== t.length) throw new Error(\"[\".concat(e, \"] does not match the input size \").concat(t.length).concat(n ? \" for a complex tensor\" : \"\", \".\"));\n  return createNestedArray$1(0, e, t, n);\n}\n\nfunction makeOnesTypedArray$1(e, t) {\n  var n = makeZerosTypedArray$1(e, t);\n\n  for (var _e = 0; _e < n.length; _e++) {\n    n[_e] = 1;\n  }\n\n  return n;\n}\n\nfunction makeZerosTypedArray$1(e, t) {\n  if (null == t || \"float32\" === t || \"complex64\" === t) return new Float32Array(e);\n  if (\"int32\" === t) return new Int32Array(e);\n  if (\"bool\" === t) return new Uint8Array(e);\n  throw new Error(\"Unknown data type \".concat(t));\n}\n\nfunction makeZerosNestedTypedArray$1(e, t) {\n  var n = e.reduce((e, t) => e * t, 1);\n  if (null == t || \"float32\" === t) return toNestedArray$1(e, new Float32Array(n));\n  if (\"int32\" === t) return toNestedArray$1(e, new Int32Array(n));\n  if (\"bool\" === t) return toNestedArray$1(e, new Uint8Array(n));\n  throw new Error(\"Unknown data type \".concat(t));\n}\n\nfunction assertNonNegativeIntegerDimensions$1(e) {\n  e.forEach(t => {\n    assert$6(Number.isInteger(t) && t >= 0, () => \"Tensor must have a shape comprised of positive integers but got shape [\".concat(e, \"].\"));\n  });\n}\n\nfunction locToIndex$1(e, t, n) {\n  if (0 === t) return 0;\n  if (1 === t) return e[0];\n  var r = e[e.length - 1];\n\n  for (var _t5 = 0; _t5 < e.length - 1; ++_t5) {\n    r += n[_t5] * e[_t5];\n  }\n\n  return r;\n}\n\nfunction indexToLoc$1(e, t, n) {\n  if (0 === t) return [];\n  if (1 === t) return [e];\n  var r = new Array(t);\n\n  for (var _t6 = 0; _t6 < r.length - 1; ++_t6) {\n    r[_t6] = Math.floor(e / n[_t6]), e -= r[_t6] * n[_t6];\n  }\n\n  return r[r.length - 1] = e, r;\n}\n\nfunction isPromise$1(e) {\n  return e && e.then && \"function\" == typeof e.then;\n}\n\nvar TENSORFLOWJS_FLAGS_PREFIX$1 = \"tfjsflags\";\n\nclass Environment$1 {\n  constructor(e) {\n    this.global = e, this.flags = {}, this.flagRegistry = {}, this.urlFlags = {}, this.getQueryParams = getQueryParams$1, this.populateURLFlags();\n  }\n\n  setPlatform(e, t) {\n    null != this.platform && console.warn(\"Platform \".concat(this.platformName, \" has already been set. Overwriting the platform with \").concat(t, \".\")), this.platformName = e, this.platform = t;\n  }\n\n  registerFlag(e, t, n) {\n    if (this.flagRegistry[e] = {\n      evaluationFn: t,\n      setHook: n\n    }, null != this.urlFlags[e]) {\n      var _t7 = this.urlFlags[e];\n      console.warn(\"Setting feature override from URL \".concat(e, \": \").concat(_t7, \".\")), this.set(e, _t7);\n    }\n  }\n\n  getAsync(e) {\n    var _this = this;\n\n    return _asyncToGenerator(function* () {\n      return e in _this.flags || (_this.flags[e] = yield _this.evaluateFlag(e)), _this.flags[e];\n    })();\n  }\n\n  get(e) {\n    if (e in this.flags) return this.flags[e];\n    var t = this.evaluateFlag(e);\n    if (isPromise$1(t)) throw new Error(\"Flag \".concat(e, \" cannot be synchronously evaluated. Please use getAsync() instead.\"));\n    return this.flags[e] = t, this.flags[e];\n  }\n\n  getNumber(e) {\n    return this.get(e);\n  }\n\n  getBool(e) {\n    return this.get(e);\n  }\n\n  getFlags() {\n    return this.flags;\n  }\n\n  get features() {\n    return this.flags;\n  }\n\n  set(e, t) {\n    if (null == this.flagRegistry[e]) throw new Error(\"Cannot set flag \".concat(e, \" as it has not been registered.\"));\n    this.flags[e] = t, null != this.flagRegistry[e].setHook && this.flagRegistry[e].setHook(t);\n  }\n\n  evaluateFlag(e) {\n    if (null == this.flagRegistry[e]) throw new Error(\"Cannot evaluate flag '\".concat(e, \"': no evaluation function found.\"));\n    return this.flagRegistry[e].evaluationFn();\n  }\n\n  setFlags(e) {\n    this.flags = Object.assign({}, e);\n  }\n\n  reset() {\n    this.flags = {}, this.urlFlags = {}, this.populateURLFlags();\n  }\n\n  populateURLFlags() {\n    if (void 0 === this.global || void 0 === this.global.location || void 0 === this.global.location.search) return;\n    var e = this.getQueryParams(this.global.location.search);\n    TENSORFLOWJS_FLAGS_PREFIX$1 in e && e[TENSORFLOWJS_FLAGS_PREFIX$1].split(\",\").forEach(e => {\n      var [t, n] = e.split(\":\");\n      this.urlFlags[t] = parseValue$1(t, n);\n    });\n  }\n\n}\n\nfunction getQueryParams$1(e) {\n  var t = {};\n  return e.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g, function (e) {\n    for (var _len = arguments.length, n = new Array(_len > 1 ? _len - 1 : 0), _key = 1; _key < _len; _key++) {\n      n[_key - 1] = arguments[_key];\n    }\n\n    return decodeParam$1(t, n[0], n[1]), n.join(\"=\");\n  }), t;\n}\n\nfunction decodeParam$1(e, t, n) {\n  e[decodeURIComponent(t)] = decodeURIComponent(n || \"\");\n}\n\nfunction parseValue$1(e, t) {\n  if (\"true\" === (t = t.toLowerCase()) || \"false\" === t) return \"true\" === t;\n  if (\"\" + +t === t) return +t;\n  throw new Error(\"Could not parse value flag value \".concat(t, \" for flag \").concat(e, \".\"));\n}\n\nfunction env$1() {\n  return ENV$5;\n}\n\nvar ENV$5 = null,\n    globalNameSpace$1;\n\nfunction setEnvironmentGlobal$1(e) {\n  ENV$5 = e;\n}\n\nfunction getGlobalNamespace$1() {\n  if (null == globalNameSpace$1) {\n    var e;\n    if (\"undefined\" != typeof window) e = window;else if (\"undefined\" != typeof global) e = global;else if (\"undefined\" != typeof process) e = process;else {\n      if (\"undefined\" == typeof self) throw new Error(\"Could not find a global object\");\n      e = self;\n    }\n    globalNameSpace$1 = e;\n  }\n\n  return globalNameSpace$1;\n}\n\nfunction getGlobalMap$1() {\n  var e = getGlobalNamespace$1();\n  return null == e._tfGlobals && (e._tfGlobals = new Map()), e._tfGlobals;\n}\n\nfunction getGlobal$1(e, t) {\n  var n = getGlobalMap$1();\n  if (n.has(e)) return n.get(e);\n  {\n    var r = t();\n    return n.set(e, r), n.get(e);\n  }\n}\n\nvar Abs$1 = \"Abs\",\n    Acos$1 = \"Acos\",\n    Acosh$1 = \"Acosh\",\n    Add$3 = \"Add\",\n    AddN$1 = \"AddN\",\n    All$1 = \"All\",\n    Any$1 = \"Any\",\n    ArgMax$1 = \"ArgMax\",\n    ArgMin$1 = \"ArgMin\",\n    Asin$1 = \"Asin\",\n    Asinh$1 = \"Asinh\",\n    Atan$1 = \"Atan\",\n    Atanh$1 = \"Atanh\",\n    Atan2$1 = \"Atan2\",\n    AvgPool$1 = \"AvgPool\",\n    AvgPoolGrad$1 = \"AvgPoolGrad\",\n    AvgPool3D$1 = \"AvgPool3D\",\n    AvgPool3DGrad$1 = \"AvgPool3DGrad\",\n    BatchMatMul$1 = \"BatchMatMul\",\n    BatchToSpaceND$1 = \"BatchToSpaceND\",\n    Bincount$1 = \"Bincount\",\n    BroadcastTo$1 = \"BroadcastTo\",\n    Cast$1 = \"Cast\",\n    Ceil$1 = \"Ceil\",\n    ClipByValue$1 = \"ClipByValue\",\n    Complex$1 = \"Complex\",\n    ComplexAbs$1 = \"ComplexAbs\",\n    Concat$1 = \"Concat\",\n    Conv2D$3 = \"Conv2D\",\n    Conv2DBackpropFilter$1 = \"Conv2DBackpropFilter\",\n    Conv2DBackpropInput$1 = \"Conv2DBackpropInput\",\n    Conv3D$3 = \"Conv3D\",\n    Conv3DBackpropFilterV2$1 = \"Conv3DBackpropFilterV2\",\n    Conv3DBackpropInputV2$1 = \"Conv3DBackpropInputV2\",\n    Cos$1 = \"Cos\",\n    Cosh$1 = \"Cosh\",\n    Cumsum$1 = \"Cumsum\",\n    CropAndResize$1 = \"CropAndResize\",\n    DenseBincount$1 = \"DenseBincount\",\n    DepthToSpace$1 = \"DepthToSpace\",\n    DepthwiseConv2dNative$1 = \"DepthwiseConv2dNative\",\n    DepthwiseConv2dNativeBackpropFilter$1 = \"DepthwiseConv2dNativeBackpropFilter\",\n    DepthwiseConv2dNativeBackpropInput$1 = \"DepthwiseConv2dNativeBackpropInput\",\n    Diag$1 = \"Diag\",\n    Dilation2D$1 = \"Dilation2D\",\n    Dilation2DBackpropInput$1 = \"Dilation2DBackpropInput\",\n    Dilation2DBackpropFilter$1 = \"Dilation2DBackpropFilter\",\n    RealDiv$1 = \"RealDiv\",\n    Einsum$1 = \"Einsum\",\n    Elu$3 = \"Elu\",\n    EluGrad$1 = \"EluGrad\",\n    Erf$1 = \"Erf\",\n    Equal$1 = \"Equal\",\n    Exp$1 = \"Exp\",\n    ExpandDims$1 = \"ExpandDims\",\n    Expm1$1 = \"Expm1\",\n    FFT$1 = \"FFT\",\n    Fill$1 = \"Fill\",\n    FlipLeftRight$1 = \"FlipLeftRight\",\n    Floor$1 = \"Floor\",\n    FloorDiv$1 = \"FloorDiv\",\n    FusedBatchNorm$1 = \"FusedBatchNorm\",\n    GatherV2$1 = \"GatherV2\",\n    GatherNd$1 = \"GatherNd\",\n    Greater$1 = \"Greater\",\n    GreaterEqual$1 = \"GreaterEqual\",\n    Identity$3 = \"Identity\",\n    IFFT$1 = \"IFFT\",\n    Imag$1 = \"Imag\",\n    IsFinite$1 = \"IsFinite\",\n    IsInf$1 = \"IsInf\",\n    IsNan$1 = \"IsNan\",\n    LeakyRelu$1 = \"LeakyRelu\",\n    Less$1 = \"Less\",\n    LessEqual$1 = \"LessEqual\",\n    LinSpace$1 = \"LinSpace\",\n    Log$1 = \"Log\",\n    Log1p$1 = \"Log1p\",\n    LogicalAnd$1 = \"LogicalAnd\",\n    LogicalNot$1 = \"LogicalNot\",\n    LogicalOr$1 = \"LogicalOr\",\n    LogSoftmax$3 = \"LogSoftmax\",\n    LRN$1 = \"LRN\",\n    LRNGrad$1 = \"LRNGrad\",\n    Max$1 = \"Max\",\n    Maximum$3 = \"Maximum\",\n    MaxPool$1 = \"MaxPool\",\n    MaxPoolGrad$1 = \"MaxPoolGrad\",\n    MaxPool3D$1 = \"MaxPool3D\",\n    MaxPool3DGrad$1 = \"MaxPool3DGrad\",\n    MaxPoolWithArgmax$1 = \"MaxPoolWithArgmax\",\n    Mean$1 = \"Mean\",\n    Min$1 = \"Min\",\n    Minimum$3 = \"Minimum\",\n    MirrorPad$1 = \"MirrorPad\",\n    Mod$1 = \"Mod\",\n    Multinomial$1 = \"Multinomial\",\n    Multiply$3 = \"Multiply\",\n    Neg$1 = \"Neg\",\n    NotEqual$1 = \"NotEqual\",\n    NonMaxSuppressionV3$1 = \"NonMaxSuppressionV3\",\n    NonMaxSuppressionV4$1 = \"NonMaxSuppressionV4\",\n    NonMaxSuppressionV5$1 = \"NonMaxSuppressionV5\",\n    OnesLike$1 = \"OnesLike\",\n    OneHot$1 = \"OneHot\",\n    Pack$1 = \"Pack\",\n    PadV2$1 = \"PadV2\",\n    Pow$1 = \"Pow\",\n    Prelu$1 = \"Prelu\",\n    Prod$1 = \"Prod\",\n    Range$1 = \"Range\",\n    Real$1 = \"Real\",\n    Reciprocal$1 = \"Reciprocal\",\n    Relu$3 = \"Relu\",\n    Reshape$3 = \"Reshape\",\n    ResizeNearestNeighbor$1 = \"ResizeNearestNeighbor\",\n    ResizeNearestNeighborGrad$1 = \"ResizeNearestNeighborGrad\",\n    ResizeBilinear$1 = \"ResizeBilinear\",\n    ResizeBilinearGrad$1 = \"ResizeBilinearGrad\",\n    Relu6$3 = \"Relu6\",\n    Reverse$1 = \"Reverse\",\n    Round$1 = \"Round\",\n    Rsqrt$1 = \"Rsqrt\",\n    ScatterNd$1 = \"ScatterNd\",\n    Select$1 = \"Select\",\n    Selu$3 = \"Selu\",\n    Slice$1 = \"Slice\",\n    Sin$1 = \"Sin\",\n    Sinh$1 = \"Sinh\",\n    Sign$1 = \"Sign\",\n    Sigmoid$3 = \"Sigmoid\",\n    Softplus$3 = \"Softplus\",\n    Sqrt$1 = \"Sqrt\",\n    Sum$1 = \"Sum\",\n    SpaceToBatchND$1 = \"SpaceToBatchND\",\n    SplitV$1 = \"SplitV\",\n    Softmax$5 = \"Softmax\",\n    SparseFillEmptyRows$1 = \"SparseFillEmptyRows\",\n    SparseReshape$1 = \"SparseReshape\",\n    SparseSegmentMean$1 = \"SparseSegmentMean\",\n    SparseSegmentSum$1 = \"SparseSegmentSum\",\n    SparseToDense$1 = \"SparseToDense\",\n    SquaredDifference$1 = \"SquaredDifference\",\n    Square$1 = \"Square\",\n    StridedSlice$1 = \"StridedSlice\",\n    StringNGrams$1 = \"StringNGrams\",\n    StringSplit$1 = \"StringSplit\",\n    StringToHashBucketFast$1 = \"StringToHashBucketFast\",\n    Sub$1 = \"Sub\",\n    Tan$1 = \"Tan\",\n    Tanh$3 = \"Tanh\",\n    Tile$1 = \"Tile\",\n    TopK$1 = \"TopK\",\n    Transform$1 = \"Transform\",\n    Transpose$1 = \"Transpose\",\n    Unique$1 = \"Unique\",\n    Unpack$1 = \"Unpack\",\n    UnsortedSegmentSum$1 = \"UnsortedSegmentSum\",\n    ZerosLike$1 = \"ZerosLike\",\n    Step$1 = \"Step\",\n    FromPixels$1 = \"FromPixels\",\n    RotateWithOffset$1 = \"RotateWithOffset\",\n    _FusedMatMul$1 = \"_FusedMatMul\",\n    FusedConv2D$1 = \"FusedConv2D\",\n    FusedDepthwiseConv2D$1 = \"FusedDepthwiseConv2D\",\n    kernelRegistry$1 = getGlobal$1(\"kernelRegistry\", () => new Map()),\n    gradRegistry$1 = getGlobal$1(\"gradRegistry\", () => new Map());\n\nfunction getKernel$1(e, t) {\n  var n = makeKey$1(e, t);\n  return kernelRegistry$1.get(n);\n}\n\nfunction getGradient$1(e) {\n  return gradRegistry$1.get(e);\n}\n\nfunction getKernelsForBackend$1(e) {\n  var t = kernelRegistry$1.entries(),\n      n = [];\n\n  for (;;) {\n    var {\n      done: r,\n      value: a\n    } = t.next();\n    if (r) break;\n    var [s, o] = a,\n        [i] = s.split(\"_\");\n    i === e && n.push(o);\n  }\n\n  return n;\n}\n\nfunction registerKernel$1(e) {\n  var {\n    kernelName: t,\n    backendName: n\n  } = e,\n      r = makeKey$1(t, n);\n  kernelRegistry$1.has(r) && console.warn(\"The kernel '\".concat(t, \"' for backend '\").concat(n, \"' is already registered\")), kernelRegistry$1.set(r, e);\n}\n\nfunction registerGradient$1(e) {\n  var {\n    kernelName: t\n  } = e;\n  gradRegistry$1.has(t) && env$1().getBool(\"DEBUG\") && console.warn(\"Overriding the gradient for '\".concat(t, \"'\")), gradRegistry$1.set(t, e);\n}\n\nfunction makeKey$1(e, t) {\n  return \"\".concat(t, \"_\").concat(e);\n}\n\nvar long$2 = Long$3,\n    wasm$1 = null;\n\ntry {\n  wasm$1 = new WebAssembly.Instance(new WebAssembly.Module(new Uint8Array([0, 97, 115, 109, 1, 0, 0, 0, 1, 13, 2, 96, 0, 1, 127, 96, 4, 127, 127, 127, 127, 1, 127, 3, 7, 6, 0, 1, 1, 1, 1, 1, 6, 6, 1, 127, 1, 65, 0, 11, 7, 50, 6, 3, 109, 117, 108, 0, 1, 5, 100, 105, 118, 95, 115, 0, 2, 5, 100, 105, 118, 95, 117, 0, 3, 5, 114, 101, 109, 95, 115, 0, 4, 5, 114, 101, 109, 95, 117, 0, 5, 8, 103, 101, 116, 95, 104, 105, 103, 104, 0, 0, 10, 191, 1, 6, 4, 0, 35, 0, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 126, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 127, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 128, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 129, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 130, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11])), {}).exports;\n} catch (e) {}\n\nfunction Long$3(e, t, n) {\n  this.low = 0 | e, this.high = 0 | t, this.unsigned = !!n;\n}\n\nfunction isLong$1(e) {\n  return !0 === (e && e.__isLong__);\n}\n\nObject.defineProperty(Long$3.prototype, \"__isLong__\", {\n  value: !0\n}), Long$3.isLong = isLong$1;\nvar INT_CACHE$1 = {},\n    UINT_CACHE$1 = {};\n\nfunction fromInt$1(e, t) {\n  var n, r, a;\n  return t ? (a = 0 <= (e >>>= 0) && e < 256) && (r = UINT_CACHE$1[e]) ? r : (n = fromBits$1(e, (0 | e) < 0 ? -1 : 0, !0), a && (UINT_CACHE$1[e] = n), n) : (a = -128 <= (e |= 0) && e < 128) && (r = INT_CACHE$1[e]) ? r : (n = fromBits$1(e, e < 0 ? -1 : 0, !1), a && (INT_CACHE$1[e] = n), n);\n}\n\nfunction fromNumber$1(e, t) {\n  if (isNaN(e)) return t ? UZERO$1 : ZERO$1;\n\n  if (t) {\n    if (e < 0) return UZERO$1;\n    if (e >= TWO_PWR_64_DBL$1) return MAX_UNSIGNED_VALUE$1;\n  } else {\n    if (e <= -TWO_PWR_63_DBL$1) return MIN_VALUE$1;\n    if (e + 1 >= TWO_PWR_63_DBL$1) return MAX_VALUE$1;\n  }\n\n  return e < 0 ? fromNumber$1(-e, t).neg() : fromBits$1(e % TWO_PWR_32_DBL$1 | 0, e / TWO_PWR_32_DBL$1 | 0, t);\n}\n\nfunction fromBits$1(e, t, n) {\n  return new Long$3(e, t, n);\n}\n\nLong$3.fromInt = fromInt$1, Long$3.fromNumber = fromNumber$1, Long$3.fromBits = fromBits$1;\nvar pow_dbl$1 = Math.pow;\n\nfunction fromString$1(e, t, n) {\n  if (0 === e.length) throw Error(\"empty string\");\n  if (\"NaN\" === e || \"Infinity\" === e || \"+Infinity\" === e || \"-Infinity\" === e) return ZERO$1;\n  if (\"number\" == typeof t ? (n = t, t = !1) : t = !!t, (n = n || 10) < 2 || 36 < n) throw RangeError(\"radix\");\n  var r;\n  if ((r = e.indexOf(\"-\")) > 0) throw Error(\"interior hyphen\");\n  if (0 === r) return fromString$1(e.substring(1), t, n).neg();\n\n  for (var a = fromNumber$1(pow_dbl$1(n, 8)), s = ZERO$1, o = 0; o < e.length; o += 8) {\n    var i = Math.min(8, e.length - o),\n        l = parseInt(e.substring(o, o + i), n);\n\n    if (i < 8) {\n      var u = fromNumber$1(pow_dbl$1(n, i));\n      s = s.mul(u).add(fromNumber$1(l));\n    } else s = (s = s.mul(a)).add(fromNumber$1(l));\n  }\n\n  return s.unsigned = t, s;\n}\n\nfunction fromValue$1(e, t) {\n  return \"number\" == typeof e ? fromNumber$1(e, t) : \"string\" == typeof e ? fromString$1(e, t) : fromBits$1(e.low, e.high, \"boolean\" == typeof t ? t : e.unsigned);\n}\n\nLong$3.fromString = fromString$1, Long$3.fromValue = fromValue$1;\nvar TWO_PWR_16_DBL$1 = 65536,\n    TWO_PWR_24_DBL$1 = 1 << 24,\n    TWO_PWR_32_DBL$1 = TWO_PWR_16_DBL$1 * TWO_PWR_16_DBL$1,\n    TWO_PWR_64_DBL$1 = TWO_PWR_32_DBL$1 * TWO_PWR_32_DBL$1,\n    TWO_PWR_63_DBL$1 = TWO_PWR_64_DBL$1 / 2,\n    TWO_PWR_24$1 = fromInt$1(TWO_PWR_24_DBL$1),\n    ZERO$1 = fromInt$1(0);\nLong$3.ZERO = ZERO$1;\nvar UZERO$1 = fromInt$1(0, !0);\nLong$3.UZERO = UZERO$1;\nvar ONE$1 = fromInt$1(1);\nLong$3.ONE = ONE$1;\nvar UONE$1 = fromInt$1(1, !0);\nLong$3.UONE = UONE$1;\nvar NEG_ONE$1 = fromInt$1(-1);\nLong$3.NEG_ONE = NEG_ONE$1;\nvar MAX_VALUE$1 = fromBits$1(-1, 2147483647, !1);\nLong$3.MAX_VALUE = MAX_VALUE$1;\nvar MAX_UNSIGNED_VALUE$1 = fromBits$1(-1, -1, !0);\nLong$3.MAX_UNSIGNED_VALUE = MAX_UNSIGNED_VALUE$1;\nvar MIN_VALUE$1 = fromBits$1(0, -2147483648, !1);\nLong$3.MIN_VALUE = MIN_VALUE$1;\nvar LongPrototype$1 = Long$3.prototype;\nLongPrototype$1.toInt = function () {\n  return this.unsigned ? this.low >>> 0 : this.low;\n}, LongPrototype$1.toNumber = function () {\n  return this.unsigned ? (this.high >>> 0) * TWO_PWR_32_DBL$1 + (this.low >>> 0) : this.high * TWO_PWR_32_DBL$1 + (this.low >>> 0);\n}, LongPrototype$1.toString = function (e) {\n  if ((e = e || 10) < 2 || 36 < e) throw RangeError(\"radix\");\n  if (this.isZero()) return \"0\";\n\n  if (this.isNegative()) {\n    if (this.eq(MIN_VALUE$1)) {\n      var t = fromNumber$1(e),\n          n = this.div(t),\n          r = n.mul(t).sub(this);\n      return n.toString(e) + r.toInt().toString(e);\n    }\n\n    return \"-\" + this.neg().toString(e);\n  }\n\n  for (var a = fromNumber$1(pow_dbl$1(e, 6), this.unsigned), s = this, o = \"\";;) {\n    var i = s.div(a),\n        l = (s.sub(i.mul(a)).toInt() >>> 0).toString(e);\n    if ((s = i).isZero()) return l + o;\n\n    for (; l.length < 6;) {\n      l = \"0\" + l;\n    }\n\n    o = \"\" + l + o;\n  }\n}, LongPrototype$1.getHighBits = function () {\n  return this.high;\n}, LongPrototype$1.getHighBitsUnsigned = function () {\n  return this.high >>> 0;\n}, LongPrototype$1.getLowBits = function () {\n  return this.low;\n}, LongPrototype$1.getLowBitsUnsigned = function () {\n  return this.low >>> 0;\n}, LongPrototype$1.getNumBitsAbs = function () {\n  if (this.isNegative()) return this.eq(MIN_VALUE$1) ? 64 : this.neg().getNumBitsAbs();\n\n  for (var e = 0 != this.high ? this.high : this.low, t = 31; t > 0 && 0 == (e & 1 << t); t--) {\n    ;\n  }\n\n  return 0 != this.high ? t + 33 : t + 1;\n}, LongPrototype$1.isZero = function () {\n  return 0 === this.high && 0 === this.low;\n}, LongPrototype$1.eqz = LongPrototype$1.isZero, LongPrototype$1.isNegative = function () {\n  return !this.unsigned && this.high < 0;\n}, LongPrototype$1.isPositive = function () {\n  return this.unsigned || this.high >= 0;\n}, LongPrototype$1.isOdd = function () {\n  return 1 == (1 & this.low);\n}, LongPrototype$1.isEven = function () {\n  return 0 == (1 & this.low);\n}, LongPrototype$1.equals = function (e) {\n  return isLong$1(e) || (e = fromValue$1(e)), (this.unsigned === e.unsigned || this.high >>> 31 != 1 || e.high >>> 31 != 1) && this.high === e.high && this.low === e.low;\n}, LongPrototype$1.eq = LongPrototype$1.equals, LongPrototype$1.notEquals = function (e) {\n  return !this.eq(e);\n}, LongPrototype$1.neq = LongPrototype$1.notEquals, LongPrototype$1.ne = LongPrototype$1.notEquals, LongPrototype$1.lessThan = function (e) {\n  return this.comp(e) < 0;\n}, LongPrototype$1.lt = LongPrototype$1.lessThan, LongPrototype$1.lessThanOrEqual = function (e) {\n  return this.comp(e) <= 0;\n}, LongPrototype$1.lte = LongPrototype$1.lessThanOrEqual, LongPrototype$1.le = LongPrototype$1.lessThanOrEqual, LongPrototype$1.greaterThan = function (e) {\n  return this.comp(e) > 0;\n}, LongPrototype$1.gt = LongPrototype$1.greaterThan, LongPrototype$1.greaterThanOrEqual = function (e) {\n  return this.comp(e) >= 0;\n}, LongPrototype$1.gte = LongPrototype$1.greaterThanOrEqual, LongPrototype$1.ge = LongPrototype$1.greaterThanOrEqual, LongPrototype$1.compare = function (e) {\n  if (isLong$1(e) || (e = fromValue$1(e)), this.eq(e)) return 0;\n  var t = this.isNegative(),\n      n = e.isNegative();\n  return t && !n ? -1 : !t && n ? 1 : this.unsigned ? e.high >>> 0 > this.high >>> 0 || e.high === this.high && e.low >>> 0 > this.low >>> 0 ? -1 : 1 : this.sub(e).isNegative() ? -1 : 1;\n}, LongPrototype$1.comp = LongPrototype$1.compare, LongPrototype$1.negate = function () {\n  return !this.unsigned && this.eq(MIN_VALUE$1) ? MIN_VALUE$1 : this.not().add(ONE$1);\n}, LongPrototype$1.neg = LongPrototype$1.negate, LongPrototype$1.add = function (e) {\n  isLong$1(e) || (e = fromValue$1(e));\n  var t = 0,\n      n = 0,\n      r = 0,\n      a = 0;\n  return r += (a += (65535 & this.low) + (65535 & e.low)) >>> 16, n += (r += (this.low >>> 16) + (e.low >>> 16)) >>> 16, t += (n += (65535 & this.high) + (65535 & e.high)) >>> 16, t += (this.high >>> 16) + (e.high >>> 16), fromBits$1((r &= 65535) << 16 | (a &= 65535), (t &= 65535) << 16 | (n &= 65535), this.unsigned);\n}, LongPrototype$1.subtract = function (e) {\n  return isLong$1(e) || (e = fromValue$1(e)), this.add(e.neg());\n}, LongPrototype$1.sub = LongPrototype$1.subtract, LongPrototype$1.multiply = function (e) {\n  if (this.isZero()) return ZERO$1;\n  if (isLong$1(e) || (e = fromValue$1(e)), wasm$1) return fromBits$1(wasm$1.mul(this.low, this.high, e.low, e.high), wasm$1.get_high(), this.unsigned);\n  if (e.isZero()) return ZERO$1;\n  if (this.eq(MIN_VALUE$1)) return e.isOdd() ? MIN_VALUE$1 : ZERO$1;\n  if (e.eq(MIN_VALUE$1)) return this.isOdd() ? MIN_VALUE$1 : ZERO$1;\n  if (this.isNegative()) return e.isNegative() ? this.neg().mul(e.neg()) : this.neg().mul(e).neg();\n  if (e.isNegative()) return this.mul(e.neg()).neg();\n  if (this.lt(TWO_PWR_24$1) && e.lt(TWO_PWR_24$1)) return fromNumber$1(this.toNumber() * e.toNumber(), this.unsigned);\n  var t = 65535 & this.high,\n      n = this.low >>> 16,\n      r = 65535 & this.low,\n      a = 65535 & e.high,\n      s = e.low >>> 16,\n      o = 65535 & e.low,\n      i = 0,\n      l = 0,\n      u = 0,\n      c = 0;\n  return u += (c += r * o) >>> 16, l += (u += n * o) >>> 16, u &= 65535, l += (u += r * s) >>> 16, i += (l += t * o) >>> 16, l &= 65535, i += (l += n * s) >>> 16, l &= 65535, i += (l += r * a) >>> 16, i += (this.high >>> 16) * o + t * s + n * a + r * (e.high >>> 16), fromBits$1((u &= 65535) << 16 | (c &= 65535), (i &= 65535) << 16 | (l &= 65535), this.unsigned);\n}, LongPrototype$1.mul = LongPrototype$1.multiply, LongPrototype$1.divide = function (e) {\n  if (isLong$1(e) || (e = fromValue$1(e)), e.isZero()) throw Error(\"division by zero\");\n  var t, n, r;\n  if (wasm$1) return this.unsigned || -2147483648 !== this.high || -1 !== e.low || -1 !== e.high ? fromBits$1((this.unsigned ? wasm$1.div_u : wasm$1.div_s)(this.low, this.high, e.low, e.high), wasm$1.get_high(), this.unsigned) : this;\n  if (this.isZero()) return this.unsigned ? UZERO$1 : ZERO$1;\n\n  if (this.unsigned) {\n    if (e.unsigned || (e = e.toUnsigned()), e.gt(this)) return UZERO$1;\n    if (e.gt(this.shru(1))) return UONE$1;\n    r = UZERO$1;\n  } else {\n    if (this.eq(MIN_VALUE$1)) return e.eq(ONE$1) || e.eq(NEG_ONE$1) ? MIN_VALUE$1 : e.eq(MIN_VALUE$1) ? ONE$1 : (t = this.shr(1).div(e).shl(1)).eq(ZERO$1) ? e.isNegative() ? ONE$1 : NEG_ONE$1 : (n = this.sub(e.mul(t)), r = t.add(n.div(e)));\n    if (e.eq(MIN_VALUE$1)) return this.unsigned ? UZERO$1 : ZERO$1;\n    if (this.isNegative()) return e.isNegative() ? this.neg().div(e.neg()) : this.neg().div(e).neg();\n    if (e.isNegative()) return this.div(e.neg()).neg();\n    r = ZERO$1;\n  }\n\n  for (n = this; n.gte(e);) {\n    t = Math.max(1, Math.floor(n.toNumber() / e.toNumber()));\n\n    for (var a = Math.ceil(Math.log(t) / Math.LN2), s = a <= 48 ? 1 : pow_dbl$1(2, a - 48), o = fromNumber$1(t), i = o.mul(e); i.isNegative() || i.gt(n);) {\n      i = (o = fromNumber$1(t -= s, this.unsigned)).mul(e);\n    }\n\n    o.isZero() && (o = ONE$1), r = r.add(o), n = n.sub(i);\n  }\n\n  return r;\n}, LongPrototype$1.div = LongPrototype$1.divide, LongPrototype$1.modulo = function (e) {\n  return isLong$1(e) || (e = fromValue$1(e)), wasm$1 ? fromBits$1((this.unsigned ? wasm$1.rem_u : wasm$1.rem_s)(this.low, this.high, e.low, e.high), wasm$1.get_high(), this.unsigned) : this.sub(this.div(e).mul(e));\n}, LongPrototype$1.mod = LongPrototype$1.modulo, LongPrototype$1.rem = LongPrototype$1.modulo, LongPrototype$1.not = function () {\n  return fromBits$1(~this.low, ~this.high, this.unsigned);\n}, LongPrototype$1.and = function (e) {\n  return isLong$1(e) || (e = fromValue$1(e)), fromBits$1(this.low & e.low, this.high & e.high, this.unsigned);\n}, LongPrototype$1.or = function (e) {\n  return isLong$1(e) || (e = fromValue$1(e)), fromBits$1(this.low | e.low, this.high | e.high, this.unsigned);\n}, LongPrototype$1.xor = function (e) {\n  return isLong$1(e) || (e = fromValue$1(e)), fromBits$1(this.low ^ e.low, this.high ^ e.high, this.unsigned);\n}, LongPrototype$1.shiftLeft = function (e) {\n  return isLong$1(e) && (e = e.toInt()), 0 == (e &= 63) ? this : e < 32 ? fromBits$1(this.low << e, this.high << e | this.low >>> 32 - e, this.unsigned) : fromBits$1(0, this.low << e - 32, this.unsigned);\n}, LongPrototype$1.shl = LongPrototype$1.shiftLeft, LongPrototype$1.shiftRight = function (e) {\n  return isLong$1(e) && (e = e.toInt()), 0 == (e &= 63) ? this : e < 32 ? fromBits$1(this.low >>> e | this.high << 32 - e, this.high >> e, this.unsigned) : fromBits$1(this.high >> e - 32, this.high >= 0 ? 0 : -1, this.unsigned);\n}, LongPrototype$1.shr = LongPrototype$1.shiftRight, LongPrototype$1.shiftRightUnsigned = function (e) {\n  if (isLong$1(e) && (e = e.toInt()), 0 == (e &= 63)) return this;\n  var t = this.high;\n  return e < 32 ? fromBits$1(this.low >>> e | t << 32 - e, t >>> e, this.unsigned) : fromBits$1(32 === e ? t : t >>> e - 32, 0, this.unsigned);\n}, LongPrototype$1.shru = LongPrototype$1.shiftRightUnsigned, LongPrototype$1.shr_u = LongPrototype$1.shiftRightUnsigned, LongPrototype$1.toSigned = function () {\n  return this.unsigned ? fromBits$1(this.low, this.high, !1) : this;\n}, LongPrototype$1.toUnsigned = function () {\n  return this.unsigned ? this : fromBits$1(this.low, this.high, !0);\n}, LongPrototype$1.toBytes = function (e) {\n  return e ? this.toBytesLE() : this.toBytesBE();\n}, LongPrototype$1.toBytesLE = function () {\n  var e = this.high,\n      t = this.low;\n  return [255 & t, t >>> 8 & 255, t >>> 16 & 255, t >>> 24, 255 & e, e >>> 8 & 255, e >>> 16 & 255, e >>> 24];\n}, LongPrototype$1.toBytesBE = function () {\n  var e = this.high,\n      t = this.low;\n  return [e >>> 24, e >>> 16 & 255, e >>> 8 & 255, 255 & e, t >>> 24, t >>> 16 & 255, t >>> 8 & 255, 255 & t];\n}, Long$3.fromBytes = function (e, t, n) {\n  return n ? Long$3.fromBytesLE(e, t) : Long$3.fromBytesBE(e, t);\n}, Long$3.fromBytesLE = function (e, t) {\n  return new Long$3(e[0] | e[1] << 8 | e[2] << 16 | e[3] << 24, e[4] | e[5] << 8 | e[6] << 16 | e[7] << 24, t);\n}, Long$3.fromBytesBE = function (e, t) {\n  return new Long$3(e[4] << 24 | e[5] << 16 | e[6] << 8 | e[7], e[0] << 24 | e[1] << 16 | e[2] << 8 | e[3], t);\n};\nvar long$3 = long$2,\n    LongExports$1 = /*#__PURE__*/Object.assign( /*#__PURE__*/Object.create(null), long$2, {\n  default: long$3\n});\nvar Long$2 = long$3 || LongExports$1;\n\nfunction hexToLong$1(e) {\n  return Long$2.fromString(e, !0, 16);\n}\n\nvar k0$1 = hexToLong$1(\"c3a5c85c97cb3127\"),\n    k1$1 = hexToLong$1(\"b492b66fbe98f273\"),\n    k2$1 = hexToLong$1(\"9ae16a3b2f90404f\");\n\nfunction shiftMix$1(e) {\n  return e.xor(e.shru(47));\n}\n\nfunction fetch$3(e, t, n) {\n  var r = e.slice(t, t + n);\n  return Long$2.fromBytes(Array.from(r), !0, !0);\n}\n\nfunction fetch64$1(e, t) {\n  return fetch$3(e, t, 8);\n}\n\nfunction fetch32$1(e, t) {\n  return fetch$3(e, t, 4);\n}\n\nfunction rotate64$1(e, t) {\n  return 0 === t ? e : e.shru(t).or(e.shl(64 - t));\n}\n\nfunction hashLen16$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : hexToLong$1(\"9ddfea08eb382d69\");\n  var r = e.xor(t).mul(n);\n  r = r.xor(r.shru(47));\n  var a = t.xor(r).mul(n);\n  return a = a.xor(a.shru(47)), a = a.mul(n), a;\n}\n\nfunction weakHashLen32WithSeeds$1(e, t, n, r, a, s) {\n  a = a.add(e), s = rotate64$1(s.add(a).add(r), 21);\n  var o = a;\n  return a = (a = a.add(t)).add(n), s = s.add(rotate64$1(a, 44)), [a.add(r), s.add(o)];\n}\n\nfunction weakHashLen32WithSeedsStr$1(e, t, n, r) {\n  return weakHashLen32WithSeeds$1(fetch64$1(e, t), fetch64$1(e, t + 8), fetch64$1(e, t + 16), fetch64$1(e, t + 24), n, r);\n}\n\nfunction hashLen0to16$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n\n  if (t >= 8) {\n    var n = k2$1.add(2 * t),\n        r = fetch64$1(e, 0).add(k2$1),\n        a = fetch64$1(e, t - 8);\n    return hashLen16$1(rotate64$1(a, 37).mul(n).add(r), rotate64$1(r, 25).add(a).mul(n), n);\n  }\n\n  if (t >= 4) {\n    var _n = k2$1.add(2 * t);\n\n    return hashLen16$1(fetch32$1(e, 0).shl(3).add(t), fetch32$1(e, t - 4), _n);\n  }\n\n  if (t > 0) {\n    var _n2 = t + (e[t - 1] << 2);\n\n    return shiftMix$1(k2$1.mul(e[0] + (e[t >> 1] << 8)).xor(k0$1.mul(_n2))).mul(k2$1);\n  }\n\n  return k2$1;\n}\n\nfunction hashLen17to32$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n  var n = k2$1.add(2 * t),\n      r = fetch64$1(e, 0).mul(k1$1),\n      a = fetch64$1(e, 8),\n      s = fetch64$1(e, t - 8).mul(n),\n      o = fetch64$1(e, t - 16).mul(k2$1);\n  return hashLen16$1(rotate64$1(r.add(a), 43).add(rotate64$1(s, 30)).add(o), r.add(rotate64$1(a.add(k2$1), 18)).add(s), n);\n}\n\nfunction hashLen33to64$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n  var n = k2$1.add(2 * t),\n      r = fetch64$1(e, 0).mul(k2$1),\n      a = fetch64$1(e, 8),\n      s = fetch64$1(e, t - 8).mul(n),\n      o = fetch64$1(e, t - 16).mul(k2$1),\n      i = rotate64$1(r.add(a), 43).add(rotate64$1(s, 30)).add(o),\n      l = hashLen16$1(i, r.add(rotate64$1(a.add(k2$1), 18)).add(s), n),\n      u = fetch64$1(e, 16).mul(n),\n      c = fetch64$1(e, 24),\n      p = i.add(fetch64$1(e, t - 32)).mul(n),\n      d = l.add(fetch64$1(e, t - 24)).mul(n);\n  return hashLen16$1(rotate64$1(u.add(c), 43).add(rotate64$1(p, 30)).add(d), u.add(rotate64$1(c.add(r), 18)).add(p), n);\n}\n\nfunction fingerPrint64$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n  var n = Long$2.fromNumber(81, !0);\n  if (t <= 32) return t <= 16 ? hashLen0to16$1(e, t) : hashLen17to32$1(e, t);\n  if (t <= 64) return hashLen33to64$1(e, t);\n  var r = n,\n      a = n.mul(k1$1).add(113),\n      s = shiftMix$1(a.mul(k2$1).add(113)).mul(k2$1),\n      o = [Long$2.UZERO, Long$2.UZERO],\n      i = [Long$2.UZERO, Long$2.UZERO];\n  r = r.mul(k2$1).add(fetch64$1(e, 0));\n  var l = 0;\n  var u = 64 * (t - 1 >> 6),\n      c = u + (t - 1 & 63) - 63;\n\n  do {\n    r = rotate64$1(r.add(a).add(o[0]).add(fetch64$1(e, l + 8)), 37).mul(k1$1), a = rotate64$1(a.add(o[1]).add(fetch64$1(e, l + 48)), 42).mul(k1$1), r = r.xor(i[1]), a = a.add(o[0]).add(fetch64$1(e, l + 40)), s = rotate64$1(s.add(i[0]), 33).mul(k1$1), o = weakHashLen32WithSeedsStr$1(e, l, o[1].mul(k1$1), r.add(i[0])), i = weakHashLen32WithSeedsStr$1(e, l + 32, s.add(i[1]), a.add(fetch64$1(e, l + 16))), [s, r] = [r, s], l += 64;\n  } while (l !== u);\n\n  var p = k1$1.add(s.and(255).shl(1));\n  return l = c, i[0] = i[0].add(t - 1 & 63), o[0] = o[0].add(i[0]), i[0] = i[0].add(o[0]), r = rotate64$1(r.add(a).add(o[0]).add(fetch64$1(e, l + 8)), 37).mul(p), a = rotate64$1(a.add(o[1]).add(fetch64$1(e, l + 48)), 42).mul(p), r = r.xor(i[1].mul(9)), a = a.add(o[0].mul(9).add(fetch64$1(e, l + 40))), s = rotate64$1(s.add(i[0]), 33).mul(p), o = weakHashLen32WithSeedsStr$1(e, l, o[1].mul(p), r.add(i[0])), i = weakHashLen32WithSeedsStr$1(e, l + 32, s.add(i[1]), a.add(fetch64$1(e, l + 16))), [s, r] = [r, s], hashLen16$1(hashLen16$1(o[0], i[0], p).add(shiftMix$1(a).mul(k0$1)).add(s), hashLen16$1(o[1], i[1], p).add(r), p);\n}\n\nfunction createScalarValue$1(e, t) {\n  return \"string\" === t ? encodeString$1(e) : toTypedArray$1([e], t);\n}\n\nfunction noConversionNeeded$1(e, t) {\n  return e instanceof Float32Array && \"float32\" === t || e instanceof Int32Array && \"int32\" === t || e instanceof Uint8Array && \"bool\" === t;\n}\n\nfunction toTypedArray$1(e, t) {\n  if (\"string\" === t) throw new Error(\"Cannot convert a string[] to a TypedArray\");\n  if (Array.isArray(e) && (e = flatten$6(e)), env$1().getBool(\"DEBUG\") && checkConversionForErrors$1(e, t), noConversionNeeded$1(e, t)) return e;\n  if (null == t || \"float32\" === t || \"complex64\" === t) return new Float32Array(e);\n  if (\"int32\" === t) return new Int32Array(e);\n\n  if (\"bool\" === t) {\n    var _t8 = new Uint8Array(e.length);\n\n    for (var n = 0; n < _t8.length; ++n) {\n      0 !== Math.round(e[n]) && (_t8[n] = 1);\n    }\n\n    return _t8;\n  }\n\n  throw new Error(\"Unknown data type \".concat(t));\n}\n\nfunction now$1() {\n  return env$1().platform.now();\n}\n\nfunction encodeString$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"utf-8\";\n  return t = t || \"utf-8\", env$1().platform.encode(e, t);\n}\n\nfunction decodeString$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"utf-8\";\n  return t = t || \"utf-8\", env$1().platform.decode(e, t);\n}\n\nclass Profiler$1 {\n  constructor(e, t) {\n    this.backendTimer = e, this.logger = t, null == t && (this.logger = new Logger$1());\n  }\n\n  profileKernel(e, t, n) {\n    var r;\n\n    var a = () => {\n      r = n();\n    };\n\n    var s;\n    var o = now$1();\n    if (this.backendTimer.timerAvailable()) s = this.backendTimer.time(a);else {\n      a();\n\n      for (var _e2 of r) {\n        _e2.dataSync();\n      }\n\n      s = Promise.resolve({\n        kernelMs: now$1() - o\n      });\n    }\n\n    if (env$1().getBool(\"CHECK_COMPUTATION_FOR_ERRORS\")) {\n      var _loop = function _loop(_t9) {\n        var n = r[_t9];\n        n.data().then(t => {\n          checkComputationForErrors$1(t, n.dtype, e);\n        });\n      };\n\n      for (var _t9 = 0; _t9 < r.length; _t9++) {\n        _loop(_t9);\n      }\n    }\n\n    return {\n      kernelName: e,\n      outputs: r,\n      inputs: t,\n      timeMs: s.then(e => e.kernelMs),\n      extraInfo: s.then(e => null != e.getExtraProfileInfo ? e.getExtraProfileInfo() : \"\")\n    };\n  }\n\n  logKernelProfile(e) {\n    var {\n      kernelName: t,\n      outputs: n,\n      timeMs: r,\n      inputs: a,\n      extraInfo: s\n    } = e;\n    n.forEach(e => {\n      Promise.all([e.data(), r, s]).then(n => {\n        this.logger.logKernelProfile(t, e, n[0], n[1], a, n[2]);\n      });\n    });\n  }\n\n}\n\nfunction checkComputationForErrors$1(e, t, n) {\n  if (\"float32\" !== t) return !1;\n\n  for (var _t10 = 0; _t10 < e.length; _t10++) {\n    var r = e[_t10];\n    if (isNaN(r) || !isFinite(r)) return console.warn(\"Found \".concat(r, \" in the result of '\").concat(n, \"'\")), !0;\n  }\n\n  return !1;\n}\n\nclass Logger$1 {\n  logKernelProfile(e, t, n, r, a, s) {\n    var o = \"number\" == typeof r ? rightPad$1(\"\".concat(r, \"ms\"), 9) : r.error,\n        i = rightPad$1(e, 25),\n        l = t.rank,\n        u = t.size,\n        c = rightPad$1(t.shape.toString(), 14);\n    var p = \"\";\n\n    for (var _e3 in a) {\n      var _n3 = a[_e3];\n\n      if (null != _n3) {\n        var _r = _n3.shape || t.shape,\n            _a = _r.length;\n\n        p += \"\".concat(_e3, \": \").concat(_a, \"D \").concat(_a > 0 ? _r : \"\", \" \");\n      }\n    }\n\n    console.log(\"%c\".concat(i, \"\\t%c\").concat(o, \"\\t%c\").concat(l, \"D \").concat(c, \"\\t%c\").concat(u, \"\\t%c\").concat(p, \"\\t%c\").concat(s), \"font-weight:bold\", \"color:red\", \"color:blue\", \"color: orange\", \"color: green\", \"color: steelblue\");\n  }\n\n}\n\nfunction getFilteredNodesXToY$1(e, t, n) {\n  var r = {},\n      a = {};\n\n  for (var _e4 = 0; _e4 < t.length; _e4++) {\n    r[t[_e4].id] = !0;\n  }\n\n  for (var _n4 = 0; _n4 < e.length; _n4++) {\n    var _s2 = e[_n4],\n        _o = _s2.inputs;\n\n    for (var _e5 in _o) {\n      var _n5 = _o[_e5];\n\n      var _i = !1;\n\n      for (var _e6 = 0; _e6 < t.length; _e6++) {\n        if (r[_n5.id]) {\n          _s2.outputs.forEach(e => r[e.id] = !0), _i = !0, a[_s2.id] = !0;\n          break;\n        }\n      }\n\n      if (_i) break;\n    }\n  }\n\n  var s = {};\n  s[n.id] = !0;\n  var o = {};\n\n  for (var _t11 = e.length - 1; _t11 >= 0; _t11--) {\n    var _n6 = e[_t11],\n        _r2 = _n6.inputs;\n\n    for (var _e7 = 0; _e7 < _n6.outputs.length; _e7++) {\n      if (s[_n6.outputs[_e7].id]) {\n        for (var _e8 in _r2) {\n          s[_r2[_e8].id] = !0, o[_n6.id] = !0;\n        }\n\n        break;\n      }\n    }\n  }\n\n  var i = [];\n\n  for (var _t12 = 0; _t12 < e.length; _t12++) {\n    var _n7 = e[_t12];\n\n    if (a[_n7.id] && o[_n7.id]) {\n      var _e9 = {};\n\n      for (var _t14 in _n7.inputs) {\n        var _a2 = _n7.inputs[_t14];\n        r[_a2.id] && (_e9[_t14] = _a2);\n      }\n\n      var _t13 = Object.assign({}, _n7);\n\n      _t13.inputs = _e9, _t13.outputs = _n7.outputs, i.push(_t13);\n    }\n  }\n\n  return i;\n}\n\nfunction backpropagateGradients$1(e, t, n, r) {\n  var _loop2 = function _loop2(a) {\n    var s = t[a],\n        o = [];\n    if (s.outputs.forEach(t => {\n      var n = e[t.id];\n      o.push(null != n ? n : null);\n    }), null == s.gradient) throw new Error(\"Cannot compute gradient: gradient function not found for \".concat(s.kernelName, \".\"));\n    var i = s.gradient(o);\n\n    var _loop3 = function _loop3(_t15) {\n      if (!(_t15 in i)) throw new Error(\"Cannot backprop through input \".concat(_t15, \". Available gradients found: \").concat(Object.keys(i), \".\"));\n      var a = n(() => i[_t15]());\n      if (\"float32\" !== a.dtype) throw new Error(\"Error in gradient for op \".concat(s.kernelName, \". The gradient of input \").concat(_t15, \" must have 'float32' dtype, but has '\").concat(a.dtype, \"'\"));\n      var o = s.inputs[_t15];\n      if (!arraysEqual$1(a.shape, o.shape)) throw new Error(\"Error in gradient for op \".concat(s.kernelName, \". The gradient of input '\").concat(_t15, \"' has shape '\").concat(a.shape, \"', which does not match the shape of the input '\").concat(o.shape, \"'\"));\n      if (null == e[o.id]) e[o.id] = a;else {\n        var _t16 = e[o.id];\n        e[o.id] = r(_t16, a), _t16.dispose();\n      }\n    };\n\n    for (var _t15 in s.inputs) {\n      _loop3(_t15);\n    }\n  };\n\n  for (var a = t.length - 1; a >= 0; a--) {\n    _loop2(a);\n  }\n}\n\nvar FORMAT_LIMIT_NUM_VALS$1 = 20,\n    FORMAT_NUM_FIRST_LAST_VALS$1 = 3,\n    FORMAT_NUM_SIG_DIGITS$1 = 7;\n\nfunction tensorToString$1(e, t, n, r) {\n  var a = computeStrides$1(t),\n      s = computeMaxSizePerColumn$1(e, t, n, a),\n      o = t.length,\n      i = subTensorToString$1(e, t, n, a, s),\n      l = [\"Tensor\"];\n  return r && (l.push(\"  dtype: \".concat(n)), l.push(\"  rank: \".concat(o)), l.push(\"  shape: [\".concat(t, \"]\")), l.push(\"  values:\")), l.push(i.map(e => \"    \" + e).join(\"\\n\")), l.join(\"\\n\");\n}\n\nfunction computeMaxSizePerColumn$1(e, t, n, r) {\n  var a = sizeFromShape$1(t),\n      s = r[r.length - 1],\n      o = new Array(s).fill(0),\n      i = t.length,\n      l = \"complex64\" === n ? createComplexTuples$1(e) : e;\n  if (i > 1) for (var _e10 = 0; _e10 < a / s; _e10++) {\n    var _t17 = _e10 * s;\n\n    for (var _e11 = 0; _e11 < s; _e11++) {\n      o[_e11] = Math.max(o[_e11], valToString$1(l[_t17 + _e11], 0, n).length);\n    }\n  }\n  return o;\n}\n\nfunction valToString$1(e, t, n) {\n  var r;\n  return r = Array.isArray(e) ? \"\".concat(parseFloat(e[0].toFixed(FORMAT_NUM_SIG_DIGITS$1)), \" + \").concat(parseFloat(e[1].toFixed(FORMAT_NUM_SIG_DIGITS$1)), \"j\") : isString$1(e) ? \"'\".concat(e, \"'\") : \"bool\" === n ? boolNumToString$1(e) : parseFloat(e.toFixed(FORMAT_NUM_SIG_DIGITS$1)).toString(), rightPad$1(r, t);\n}\n\nfunction boolNumToString$1(e) {\n  return 0 === e ? \"false\" : \"true\";\n}\n\nfunction subTensorToString$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !0;\n  var o = \"complex64\" === n ? 2 : 1,\n      i = t[0],\n      l = t.length;\n  if (0 === l) return \"complex64\" === n ? [valToString$1(createComplexTuples$1(e)[0], 0, n)] : \"bool\" === n ? [boolNumToString$1(e[0])] : [e[0].toString()];\n\n  if (1 === l) {\n    if (i > FORMAT_LIMIT_NUM_VALS$1) {\n      var _t18 = Array.from(e.slice(0, FORMAT_NUM_FIRST_LAST_VALS$1 * o)),\n          _r3 = Array.from(e.slice((i - FORMAT_NUM_FIRST_LAST_VALS$1) * o, i * o));\n\n      return \"complex64\" === n && (_t18 = createComplexTuples$1(_t18), _r3 = createComplexTuples$1(_r3)), [\"[\" + _t18.map((e, t) => valToString$1(e, a[t], n)).join(\", \") + \", ..., \" + _r3.map((e, t) => valToString$1(e, a[i - FORMAT_NUM_FIRST_LAST_VALS$1 + t], n)).join(\", \") + \"]\"];\n    }\n\n    return [\"[\" + (\"complex64\" === n ? createComplexTuples$1(e) : Array.from(e)).map((e, t) => valToString$1(e, a[t], n)).join(\", \") + \"]\"];\n  }\n\n  var u = t.slice(1),\n      c = r.slice(1),\n      p = r[0] * o,\n      d = [];\n\n  if (i > FORMAT_LIMIT_NUM_VALS$1) {\n    for (var _t19 = 0; _t19 < FORMAT_NUM_FIRST_LAST_VALS$1; _t19++) {\n      var _r4 = _t19 * p;\n\n      d.push(...subTensorToString$1(e.slice(_r4, _r4 + p), u, n, c, a, !1));\n    }\n\n    d.push(\"...\");\n\n    for (var _t20 = i - FORMAT_NUM_FIRST_LAST_VALS$1; _t20 < i; _t20++) {\n      var _r5 = _t20 * p;\n\n      d.push(...subTensorToString$1(e.slice(_r5, _r5 + p), u, n, c, a, _t20 === i - 1));\n    }\n  } else for (var _t21 = 0; _t21 < i; _t21++) {\n    var _r6 = _t21 * p;\n\n    d.push(...subTensorToString$1(e.slice(_r6, _r6 + p), u, n, c, a, _t21 === i - 1));\n  }\n\n  var h = 2 === l ? \",\" : \"\";\n  d[0] = \"[\" + d[0] + h;\n\n  for (var _e12 = 1; _e12 < d.length - 1; _e12++) {\n    d[_e12] = \" \" + d[_e12] + h;\n  }\n\n  var m = \",\\n\";\n\n  for (var _e13 = 2; _e13 < l; _e13++) {\n    m += \"\\n\";\n  }\n\n  return d[d.length - 1] = \" \" + d[d.length - 1] + \"]\" + (s ? \"\" : m), d;\n}\n\nfunction createComplexTuples$1(e) {\n  var t = [];\n\n  for (var n = 0; n < e.length; n += 2) {\n    t.push([e[n], e[n + 1]]);\n  }\n\n  return t;\n}\n\nclass TensorBuffer$1 {\n  constructor(e, t, n) {\n    if (this.dtype = t, this.shape = e.slice(), this.size = sizeFromShape$1(e), null != n) {\n      var _e14 = n.length;\n      assert$6(_e14 === this.size, () => \"Length of values '\".concat(_e14, \"' does not match the size inferred by the shape '\").concat(this.size, \"'.\"));\n    }\n\n    if (\"complex64\" === t) throw new Error(\"complex64 dtype TensorBuffers are not supported. Please create a TensorBuffer for the real and imaginary parts separately and call tf.complex(real, imag).\");\n    this.values = n || getArrayFromDType$1(t, this.size), this.strides = computeStrides$1(e);\n  }\n\n  set(e) {\n    for (var _len2 = arguments.length, t = new Array(_len2 > 1 ? _len2 - 1 : 0), _key2 = 1; _key2 < _len2; _key2++) {\n      t[_key2 - 1] = arguments[_key2];\n    }\n\n    0 === t.length && (t = [0]), assert$6(t.length === this.rank, () => \"The number of provided coordinates (\".concat(t.length, \") must match the rank (\").concat(this.rank, \")\"));\n    var n = this.locToIndex(t);\n    this.values[n] = e;\n  }\n\n  get() {\n    for (var _len3 = arguments.length, e = new Array(_len3), _key3 = 0; _key3 < _len3; _key3++) {\n      e[_key3] = arguments[_key3];\n    }\n\n    0 === e.length && (e = [0]);\n    var t = 0;\n\n    for (var _n8 of e) {\n      if (_n8 < 0 || _n8 >= this.shape[t]) throw new Error(\"Requested out of range element at \".concat(e, \".   Buffer shape=\").concat(this.shape));\n      t++;\n    }\n\n    var n = e[e.length - 1];\n\n    for (var _t22 = 0; _t22 < e.length - 1; ++_t22) {\n      n += this.strides[_t22] * e[_t22];\n    }\n\n    return this.values[n];\n  }\n\n  locToIndex(e) {\n    if (0 === this.rank) return 0;\n    if (1 === this.rank) return e[0];\n    var t = e[e.length - 1];\n\n    for (var n = 0; n < e.length - 1; ++n) {\n      t += this.strides[n] * e[n];\n    }\n\n    return t;\n  }\n\n  indexToLoc(e) {\n    if (0 === this.rank) return [];\n    if (1 === this.rank) return [e];\n    var t = new Array(this.shape.length);\n\n    for (var n = 0; n < t.length - 1; ++n) {\n      t[n] = Math.floor(e / this.strides[n]), e -= t[n] * this.strides[n];\n    }\n\n    return t[t.length - 1] = e, t;\n  }\n\n  get rank() {\n    return this.shape.length;\n  }\n\n  toTensor() {\n    return trackerFn$1().makeTensor(this.values, this.shape, this.dtype);\n  }\n\n}\n\nvar trackerFn$1 = null,\n    opHandler$3 = null;\n\nfunction setTensorTracker$1(e) {\n  trackerFn$1 = e;\n}\n\nfunction setOpHandler$1(e) {\n  opHandler$3 = e;\n}\n\nclass Tensor$1 {\n  constructor(e, t, n, r) {\n    this.kept = !1, this.isDisposedInternal = !1, this.shape = e.slice(), this.dtype = t || \"float32\", this.size = sizeFromShape$1(e), this.strides = computeStrides$1(e), this.dataId = n, this.id = r, this.rankType = this.rank < 5 ? this.rank.toString() : \"higher\";\n  }\n\n  get rank() {\n    return this.shape.length;\n  }\n\n  buffer() {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this2.data();\n      return opHandler$3.buffer(_this2.shape, _this2.dtype, e);\n    })();\n  }\n\n  bufferSync() {\n    return opHandler$3.buffer(this.shape, this.dtype, this.dataSync());\n  }\n\n  array() {\n    var _this3 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this3.data();\n      return toNestedArray$1(_this3.shape, e, \"complex64\" === _this3.dtype);\n    })();\n  }\n\n  arraySync() {\n    return toNestedArray$1(this.shape, this.dataSync(), \"complex64\" === this.dtype);\n  }\n\n  data() {\n    var _this4 = this;\n\n    return _asyncToGenerator(function* () {\n      _this4.throwIfDisposed();\n\n      var e = trackerFn$1().read(_this4.dataId);\n\n      if (\"string\" === _this4.dtype) {\n        var t = yield e;\n\n        try {\n          return t.map(e => decodeString$1(e));\n        } catch (e) {\n          throw new Error(\"Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().\");\n        }\n      }\n\n      return e;\n    })();\n  }\n\n  dataSync() {\n    this.throwIfDisposed();\n    var e = trackerFn$1().readSync(this.dataId);\n    if (\"string\" === this.dtype) try {\n      return e.map(e => decodeString$1(e));\n    } catch (e) {\n      throw new Error(\"Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().\");\n    }\n    return e;\n  }\n\n  bytes() {\n    var _this5 = this;\n\n    return _asyncToGenerator(function* () {\n      _this5.throwIfDisposed();\n\n      var e = yield trackerFn$1().read(_this5.dataId);\n      return \"string\" === _this5.dtype ? e : new Uint8Array(e.buffer);\n    })();\n  }\n\n  dispose() {\n    this.isDisposed || (trackerFn$1().disposeTensor(this), this.isDisposedInternal = !0);\n  }\n\n  get isDisposed() {\n    return this.isDisposedInternal;\n  }\n\n  throwIfDisposed() {\n    if (this.isDisposed) throw new Error(\"Tensor is disposed.\");\n  }\n\n  print() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !1;\n    return opHandler$3.print(this, e);\n  }\n\n  clone() {\n    return this.throwIfDisposed(), opHandler$3.clone(this);\n  }\n\n  toString() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !1;\n    return tensorToString$1(this.dataSync(), this.shape, this.dtype, e);\n  }\n\n  cast(e) {\n    return this.throwIfDisposed(), opHandler$3.cast(this, e);\n  }\n\n  variable() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !0;\n    var t = arguments.length > 1 ? arguments[1] : undefined;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    return this.throwIfDisposed(), trackerFn$1().makeVariable(this, e, t, n);\n  }\n\n}\n\nfunction getGlobalTensorClass$1() {\n  return getGlobal$1(\"Tensor\", () => Tensor$1);\n}\n\nObject.defineProperty(Tensor$1, Symbol.hasInstance, {\n  value: e => !!e && null != e.data && null != e.dataSync && null != e.throwIfDisposed\n}), getGlobalTensorClass$1();\n\nclass Variable$1 extends Tensor$1 {\n  constructor(e, t, n, r) {\n    super(e.shape, e.dtype, e.dataId, r), this.trainable = t, this.name = n;\n  }\n\n  assign(e) {\n    if (e.dtype !== this.dtype) throw new Error(\"dtype of the new value (\".concat(e.dtype, \") and previous value (\").concat(this.dtype, \") must match\"));\n    if (!arraysEqual$1(e.shape, this.shape)) throw new Error(\"shape of the new value (\".concat(e.shape, \") and previous value (\").concat(this.shape, \") must match\"));\n    trackerFn$1().disposeTensor(this), this.dataId = e.dataId, trackerFn$1().incRef(this, null);\n  }\n\n  dispose() {\n    trackerFn$1().disposeVariable(this), this.isDisposedInternal = !0;\n  }\n\n}\n\nvar Rank$1, UpcastInt32AndMap$1, UpcastBoolAndMap$1, UpcastFloat32AndMap$1, UpcastComplex64AndMap$1;\nObject.defineProperty(Variable$1, Symbol.hasInstance, {\n  value: e => e instanceof Tensor$1 && null != e.assign && e.assign instanceof Function\n}), function (e) {\n  e.R0 = \"R0\", e.R1 = \"R1\", e.R2 = \"R2\", e.R3 = \"R3\", e.R4 = \"R4\", e.R5 = \"R5\", e.R6 = \"R6\";\n}(Rank$1 || (Rank$1 = {})), function (e) {\n  e.float32 = \"float32\", e.int32 = \"int32\", e.bool = \"int32\", e.complex64 = \"complex64\";\n}(UpcastInt32AndMap$1 || (UpcastInt32AndMap$1 = {})), function (e) {\n  e.float32 = \"float32\", e.int32 = \"int32\", e.bool = \"bool\", e.complex64 = \"complex64\";\n}(UpcastBoolAndMap$1 || (UpcastBoolAndMap$1 = {})), function (e) {\n  e.float32 = \"float32\", e.int32 = \"float32\", e.bool = \"float32\", e.complex64 = \"complex64\";\n}(UpcastFloat32AndMap$1 || (UpcastFloat32AndMap$1 = {})), function (e) {\n  e.float32 = \"complex64\", e.int32 = \"complex64\", e.bool = \"complex64\", e.complex64 = \"complex64\";\n}(UpcastComplex64AndMap$1 || (UpcastComplex64AndMap$1 = {}));\nvar upcastTypeMap$1 = {\n  float32: UpcastFloat32AndMap$1,\n  int32: UpcastInt32AndMap$1,\n  bool: UpcastBoolAndMap$1,\n  complex64: UpcastComplex64AndMap$1\n};\n\nfunction upcastType$1(e, t) {\n  if (\"string\" === e || \"string\" === t) {\n    if (\"string\" === e && \"string\" === t) return \"string\";\n    throw new Error(\"Can not upcast \".concat(e, \" with \").concat(t));\n  }\n\n  return upcastTypeMap$1[e][t];\n}\n\nfunction sumOutType$1(e) {\n  return upcastType$1(e, \"int32\");\n}\n\nfunction makeTypesMatch$1(e, t) {\n  if (e.dtype === t.dtype) return [e, t];\n  var n = upcastType$1(e.dtype, t.dtype);\n  return [e.cast(n), t.cast(n)];\n}\n\nfunction getTensorsInContainer$1(e) {\n  var t = [];\n  return walkTensorContainer$1(e, t, new Set()), t;\n}\n\nfunction walkTensorContainer$1(e, t, n) {\n  if (null == e) return;\n  if (e instanceof Tensor$1) return void t.push(e);\n  if (!isIterable$2(e)) return;\n  var r = e;\n\n  for (var _e15 in r) {\n    var a = r[_e15];\n    n.has(a) || (n.add(a), walkTensorContainer$1(a, t, n));\n  }\n}\n\nfunction isIterable$2(e) {\n  return Array.isArray(e) || \"object\" == typeof e;\n}\n\nfunction isRegisteredKernelInvocation$1(e) {\n  return null != e.kernelName;\n}\n\nclass EngineState$1 {\n  constructor() {\n    this.registeredVariables = {}, this.nextTapeNodeId = 0, this.numBytes = 0, this.numTensors = 0, this.numStringTensors = 0, this.numDataBuffers = 0, this.gradientDepth = 0, this.kernelDepth = 0, this.scopeStack = [], this.numDataMovesStack = [], this.nextScopeId = 0, this.tensorInfo = new WeakMap(), this.profiling = !1, this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null,\n\n      get kernelNames() {\n        return Array.from(new Set(this.kernels.map(e => e.name)));\n      }\n\n    };\n  }\n\n  dispose() {\n    for (var e in this.registeredVariables) {\n      this.registeredVariables[e].dispose();\n    }\n  }\n\n}\n\nclass Engine$1 {\n  constructor(e) {\n    this.ENV = e, this.registry = {}, this.registryFactory = {}, this.pendingBackendInitId = 0, this.state = new EngineState$1();\n  }\n\n  ready() {\n    var _this6 = this;\n\n    return _asyncToGenerator(function* () {\n      if (null != _this6.pendingBackendInit) return _this6.pendingBackendInit.then(() => {});\n      if (null != _this6.backendInstance) return;\n\n      var e = _this6.getSortedBackends();\n\n      for (var t = 0; t < e.length; t++) {\n        var n = e[t];\n        if (yield _this6.initializeBackend(n).success) return void (yield _this6.setBackend(n));\n      }\n\n      throw new Error(\"Could not initialize any backends, all backend initializations failed.\");\n    })();\n  }\n\n  get backend() {\n    if (null != this.pendingBackendInit) throw new Error(\"Backend '\".concat(this.backendName, \"' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods\"));\n\n    if (null == this.backendInstance) {\n      var {\n        name: e,\n        asyncInit: t\n      } = this.initializeBackendsAndReturnBest();\n      if (t) throw new Error(\"The highest priority backend '\".concat(e, \"' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods\"));\n      this.setBackend(e);\n    }\n\n    return this.backendInstance;\n  }\n\n  backendNames() {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(e) {\n    if (!(e in this.registry)) {\n      if (!(e in this.registryFactory)) return null;\n      {\n        var {\n          asyncInit: t\n        } = this.initializeBackend(e);\n        if (t) return null;\n      }\n    }\n\n    return this.registry[e];\n  }\n\n  findBackendFactory(e) {\n    return e in this.registryFactory ? this.registryFactory[e].factory : null;\n  }\n\n  registerBackend(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n    return e in this.registryFactory ? (console.warn(\"\".concat(e, \" backend was already registered. Reusing existing backend factory.\")), !1) : (this.registryFactory[e] = {\n      factory: t,\n      priority: n\n    }, !0);\n  }\n\n  setBackend(e) {\n    var _this7 = this;\n\n    return _asyncToGenerator(function* () {\n      if (null == _this7.registryFactory[e]) throw new Error(\"Backend name '\".concat(e, \"' not found in registry\"));\n\n      if (_this7.backendName = e, null == _this7.registry[e]) {\n        _this7.backendInstance = null;\n\n        var {\n          success: t,\n          asyncInit: n\n        } = _this7.initializeBackend(e);\n\n        if (!(n ? yield t : t)) return !1;\n      }\n\n      return _this7.backendInstance = _this7.registry[e], _this7.setupRegisteredKernels(), _this7.profiler = new Profiler$1(_this7.backendInstance), !0;\n    })();\n  }\n\n  setupRegisteredKernels() {\n    getKernelsForBackend$1(this.backendName).forEach(e => {\n      null != e.setupFunc && e.setupFunc(this.backendInstance);\n    });\n  }\n\n  disposeRegisteredKernels(e) {\n    getKernelsForBackend$1(e).forEach(t => {\n      null != t.disposeFunc && t.disposeFunc(this.registry[e]);\n    });\n  }\n\n  initializeBackend(e) {\n    var t = this.registryFactory[e];\n    if (null == t) throw new Error(\"Cannot initialize backend \".concat(e, \", no registration found.\"));\n\n    try {\n      var n = t.factory();\n      if (!n || n instanceof KernelBackend$1 || \"function\" != typeof n.then) return this.registry[e] = n, {\n        success: !0,\n        asyncInit: !1\n      };\n      {\n        var _t23 = ++this.pendingBackendInitId,\n            r = n.then(n => !(_t23 < this.pendingBackendInitId || (this.registry[e] = n, this.pendingBackendInit = null, 0))).catch(n => (_t23 < this.pendingBackendInitId || (this.pendingBackendInit = null, console.warn(\"Initialization of backend \".concat(e, \" failed\")), console.warn(n.stack || n.message)), !1));\n\n        return this.pendingBackendInit = r, {\n          success: r,\n          asyncInit: !0\n        };\n      }\n    } catch (t) {\n      return console.warn(\"Initialization of backend \".concat(e, \" failed\")), console.warn(t.stack || t.message), {\n        success: !1,\n        asyncInit: !1\n      };\n    }\n  }\n\n  removeBackend(e) {\n    if (!(e in this.registryFactory)) throw new Error(\"\".concat(e, \" backend not found in registry\"));\n    this.backendName === e && null != this.pendingBackendInit && this.pendingBackendInitId++, e in this.registry && (this.disposeRegisteredKernels(e), this.registry[e].dispose(), delete this.registry[e]), delete this.registryFactory[e], this.backendName === e && (this.pendingBackendInit = null, this.backendName = null, this.backendInstance = null);\n  }\n\n  getSortedBackends() {\n    if (0 === Object.keys(this.registryFactory).length) throw new Error(\"No backend found in registry.\");\n    return Object.keys(this.registryFactory).sort((e, t) => this.registryFactory[t].priority - this.registryFactory[e].priority);\n  }\n\n  initializeBackendsAndReturnBest() {\n    var e = this.getSortedBackends();\n\n    for (var t = 0; t < e.length; t++) {\n      var n = e[t],\n          {\n        success: r,\n        asyncInit: a\n      } = this.initializeBackend(n);\n      if (a || r) return {\n        name: n,\n        asyncInit: a\n      };\n    }\n\n    throw new Error(\"Could not initialize any backends, all backend initializations failed.\");\n  }\n\n  moveData(e, t) {\n    var n = this.state.tensorInfo.get(t),\n        r = n.backend,\n        a = this.readSync(t),\n        s = r.refCount(t);\n    r.disposeData(t, !0), n.backend = e, e.move(t, a, n.shape, n.dtype, s), this.shouldCheckForMemLeaks() && this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n  }\n\n  tidy(e, t) {\n    var n,\n        r = null;\n\n    if (null == t) {\n      if (\"function\" != typeof e) throw new Error(\"Please provide a function to tidy()\");\n      t = e;\n    } else {\n      if (\"string\" != typeof e && !(e instanceof String)) throw new Error(\"When calling with two arguments, the first argument to tidy() must be a string\");\n      if (\"function\" != typeof t) throw new Error(\"When calling with two arguments, the 2nd argument to tidy() must be a function\");\n      r = e;\n    }\n\n    return this.scopedRun(() => this.startScope(r), () => this.endScope(n), () => (n = t(), n instanceof Promise && console.error(\"Cannot return a Promise inside of tidy.\"), n));\n  }\n\n  scopedRun(e, t, n) {\n    e();\n\n    try {\n      var _e16 = n();\n\n      return t(), _e16;\n    } catch (e) {\n      throw t(), e;\n    }\n  }\n\n  nextTensorId() {\n    return Engine$1.nextTensorId++;\n  }\n\n  nextVariableId() {\n    return Engine$1.nextVariableId++;\n  }\n\n  clone(e) {\n    var t = ENGINE$1.runKernel(Identity$3, {\n      x: e\n    });\n    return this.addTapeNode(this.state.activeScope.name, {\n      x: e\n    }, [t], e => ({\n      x: () => ENGINE$1.runKernel(Cast$1, {\n        x: e\n      }, {\n        dtype: \"float32\"\n      })\n    }), [], {}), t;\n  }\n\n  runKernel(e, t, n) {\n    if (null == getKernel$1(e, this.backendName)) throw new Error(\"Kernel '\".concat(e, \"' not registered for backend '\").concat(this.backendName, \"'\"));\n    return this.runKernelFunc({\n      kernelName: e,\n      inputs: t,\n      attrs: n\n    });\n  }\n\n  shouldCheckForMemLeaks() {\n    return this.ENV.getBool(\"IS_TEST\");\n  }\n\n  checkKernelForMemLeak(e, t, n) {\n    var r = this.backend.numDataIds();\n    var a = 0;\n    n.forEach(e => {\n      a += \"complex64\" === e.dtype ? 3 : 1;\n    });\n    var s = r - t - a - this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    if (s > 0) throw new Error(\"Backend '\".concat(this.backendName, \"' has an internal memory leak (\").concat(s, \" data ids) after running '\").concat(e, \"'\"));\n  }\n\n  runKernelFunc(e) {\n    var t,\n        n = [];\n    var r = this.isTapeOn(),\n        a = this.state.numBytes,\n        s = this.state.numTensors;\n    var o, i;\n    this.shouldCheckForMemLeaks() && this.state.numDataMovesStack.push(0);\n    var l = isRegisteredKernelInvocation$1(e) ? e.kernelName : null != this.state.activeScope ? this.state.activeScope.name : \"\";\n\n    if (isRegisteredKernelInvocation$1(e)) {\n      var {\n        kernelName: _t24,\n        inputs: _a3,\n        attrs: _s3\n      } = e,\n          _l = getKernel$1(_t24, this.backendName);\n\n      assert$6(null != _l, () => \"Cannot find registered kernel '\".concat(_t24, \"' for backend '\").concat(this.backendName, \"'\")), o = () => {\n        var e = this.backend.numDataIds();\n        i = _l.kernelFunc({\n          inputs: _a3,\n          attrs: _s3,\n          backend: this.backend\n        });\n        var o = Array.isArray(i) ? i : [i];\n        this.shouldCheckForMemLeaks() && this.checkKernelForMemLeak(_t24, e, o);\n        var u = o.map(e => {\n          if (null != e.rank) return e;\n          var {\n            dataId: t,\n            shape: n,\n            dtype: r\n          } = e;\n          return this.makeTensorFromDataId(t, n, r);\n        });\n\n        if (r) {\n          var _e17 = this.getTensorsForGradient(_t24, _a3, u);\n\n          n = this.saveTensorsForBackwardMode(_e17);\n        }\n\n        return u;\n      };\n    } else {\n      var {\n        forwardFunc: _t25\n      } = e,\n          _a4 = e => {\n        r && (n = e.map(e => this.keep(this.clone(e))));\n      };\n\n      o = () => {\n        var e = this.backend.numDataIds();\n        i = this.tidy(() => _t25(this.backend, _a4));\n        var n = Array.isArray(i) ? i : [i];\n        return this.shouldCheckForMemLeaks() && this.checkKernelForMemLeak(l, e, n), n;\n      };\n    }\n\n    var {\n      inputs: u,\n      attrs: c\n    } = e,\n        p = isRegisteredKernelInvocation$1(e) ? null : e.backwardsFunc;\n    var d;\n    return this.scopedRun(() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n      this.ENV.getBool(\"DEBUG\") || this.state.profiling ? (d = this.profiler.profileKernel(l, u, () => o()), this.ENV.getBool(\"DEBUG\") && this.profiler.logKernelProfile(d), t = d.outputs) : t = o();\n    }), r && this.addTapeNode(l, u, t, p, n, c), this.state.profiling && this.state.activeProfile.kernels.push({\n      name: l,\n      bytesAdded: this.state.numBytes - a,\n      totalBytesSnapshot: this.state.numBytes,\n      tensorsAdded: this.state.numTensors - s,\n      totalTensorsSnapshot: this.state.numTensors,\n      inputShapes: Object.keys(u).map(e => null != u[e] ? u[e].shape : null),\n      outputShapes: t.map(e => e.shape),\n      kernelTimeMs: d.timeMs,\n      extraInfo: d.extraInfo\n    }), Array.isArray(i) ? t : t[0];\n  }\n\n  saveTensorsForBackwardMode(e) {\n    return e.map(e => this.keep(this.clone(e)));\n  }\n\n  getTensorsForGradient(e, t, n) {\n    var r = getGradient$1(e);\n\n    if (null != r) {\n      var _e18 = r.inputsToSave || [],\n          a = r.outputsToSave || [];\n\n      var s;\n      r.saveAllInputs ? (assert$6(Array.isArray(t), () => \"saveAllInputs is true, expected inputs to be an array.\"), s = Object.keys(t).map(e => t[e])) : s = _e18.map(e => t[e]);\n      var o = n.filter((e, t) => a[t]);\n      return s.concat(o);\n    }\n\n    return [];\n  }\n\n  makeTensor(e, t, n, r) {\n    if (null == e) throw new Error(\"Values passed to engine.makeTensor() are null\");\n    r = r || this.backend;\n    var a = e;\n    \"string\" === (n = n || \"float32\") && isString$1(e[0]) && (a = e.map(e => encodeString$1(e)));\n    var s = r.write(a, t, n),\n        o = new Tensor$1(t, n, s, this.nextTensorId());\n\n    if (this.trackTensor(o, r), \"string\" === n) {\n      var _e19 = this.state.tensorInfo.get(s),\n          _t26 = bytesFromStringArray$1(a);\n\n      this.state.numBytes += _t26 - _e19.bytes, _e19.bytes = _t26;\n    }\n\n    return o;\n  }\n\n  makeTensorFromDataId(e, t, n, r) {\n    var a = new Tensor$1(t, n = n || \"float32\", e, this.nextTensorId());\n    return this.trackTensor(a, r), a;\n  }\n\n  makeVariable(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    var r = arguments.length > 3 ? arguments[3] : undefined;\n    n = n || this.nextVariableId().toString(), null != r && r !== e.dtype && (e = e.cast(r));\n    var a = new Variable$1(e, t, n, this.nextTensorId());\n    if (null != this.state.registeredVariables[a.name]) throw new Error(\"Variable with name \".concat(a.name, \" was already registered\"));\n    return this.state.registeredVariables[a.name] = a, this.incRef(a, this.backend), a;\n  }\n\n  trackTensor(e, t) {\n    this.state.numTensors++, \"string\" === e.dtype && this.state.numStringTensors++;\n    var n = 0;\n    \"complex64\" !== e.dtype && \"string\" !== e.dtype && (n = e.size * bytesPerElement$1(e.dtype)), this.state.numBytes += n, this.state.tensorInfo.has(e.dataId) || (this.state.numDataBuffers++, this.state.tensorInfo.set(e.dataId, {\n      backend: t || this.backend,\n      dtype: e.dtype,\n      shape: e.shape,\n      bytes: n\n    })), e instanceof Variable$1 || this.track(e);\n  }\n\n  incRef(e, t) {\n    this.trackTensor(e, t), this.backend.incRef(e.dataId);\n  }\n\n  removeDataId(e, t) {\n    this.state.tensorInfo.has(e) && this.state.tensorInfo.get(e).backend === t && (this.state.tensorInfo.delete(e), this.state.numDataBuffers--);\n  }\n\n  disposeTensor(e) {\n    if (!this.state.tensorInfo.has(e.dataId)) return;\n    var t = this.state.tensorInfo.get(e.dataId);\n\n    if (this.state.numTensors--, \"string\" === e.dtype && (this.state.numStringTensors--, this.state.numBytes -= t.bytes), \"complex64\" !== e.dtype && \"string\" !== e.dtype) {\n      var _t27 = e.size * bytesPerElement$1(e.dtype);\n\n      this.state.numBytes -= _t27;\n    }\n\n    t.backend.disposeData(e.dataId) && this.removeDataId(e.dataId, t.backend);\n  }\n\n  disposeVariables() {\n    for (var e in this.state.registeredVariables) {\n      this.disposeVariable(this.state.registeredVariables[e]);\n    }\n  }\n\n  disposeVariable(e) {\n    this.disposeTensor(e), null != this.state.registeredVariables[e.name] && delete this.state.registeredVariables[e.name];\n  }\n\n  memory() {\n    var e = this.backend.memory();\n    return e.numTensors = this.state.numTensors, e.numDataBuffers = this.state.numDataBuffers, e.numBytes = this.state.numBytes, this.state.numStringTensors > 0 && (e.unreliable = !0, null == e.reasons && (e.reasons = []), e.reasons.push(\"Memory usage by string tensors is approximate (2 bytes per character)\")), e;\n  }\n\n  profile(e) {\n    var _this8 = this;\n\n    return _asyncToGenerator(function* () {\n      _this8.state.profiling = !0;\n      var t = _this8.state.numBytes,\n          n = _this8.state.numTensors;\n      _this8.state.activeProfile.kernels = [], _this8.state.activeProfile.result = yield e(), _this8.state.profiling = !1, _this8.state.activeProfile.peakBytes = Math.max(..._this8.state.activeProfile.kernels.map(e => e.totalBytesSnapshot)), _this8.state.activeProfile.newBytes = _this8.state.numBytes - t, _this8.state.activeProfile.newTensors = _this8.state.numTensors - n;\n\n      for (var _e20 of _this8.state.activeProfile.kernels) {\n        _e20.kernelTimeMs = yield _e20.kernelTimeMs, _e20.extraInfo = yield _e20.extraInfo;\n      }\n\n      return _this8.state.activeProfile;\n    })();\n  }\n\n  isTapeOn() {\n    return this.state.gradientDepth > 0 && 0 === this.state.kernelDepth;\n  }\n\n  addTapeNode(e, t, n, r, a, s) {\n    var o = {\n      id: this.state.nextTapeNodeId++,\n      kernelName: e,\n      inputs: t,\n      outputs: n,\n      saved: a\n    },\n        i = getGradient$1(e);\n    null != i && (r = i.gradFunc), null != r && (o.gradient = e => (e = e.map((e, t) => {\n      if (null == e) {\n        var _e21 = n[t],\n            _r7 = makeZerosTypedArray$1(_e21.size, _e21.dtype);\n\n        return this.makeTensor(_r7, _e21.shape, _e21.dtype);\n      }\n\n      return e;\n    }), r(e.length > 1 ? e : e[0], a, s))), this.state.activeTape.push(o);\n  }\n\n  keep(e) {\n    return e.kept = !0, e;\n  }\n\n  startTape() {\n    0 === this.state.gradientDepth && (this.state.activeTape = []), this.state.gradientDepth++;\n  }\n\n  endTape() {\n    this.state.gradientDepth--;\n  }\n\n  startScope(e) {\n    var t = {\n      track: [],\n      name: \"unnamed scope\",\n      id: this.state.nextScopeId++\n    };\n    e && (t.name = e), this.state.scopeStack.push(t), this.state.activeScope = t;\n  }\n\n  endScope(e) {\n    var t = getTensorsInContainer$1(e),\n        n = new Set(t.map(e => e.id));\n\n    for (var _e22 = 0; _e22 < this.state.activeScope.track.length; _e22++) {\n      var _t28 = this.state.activeScope.track[_e22];\n      _t28.kept || n.has(_t28.id) || _t28.dispose();\n    }\n\n    var r = this.state.scopeStack.pop();\n    this.state.activeScope = 0 === this.state.scopeStack.length ? null : this.state.scopeStack[this.state.scopeStack.length - 1], t.forEach(e => {\n      e.kept || e.scopeId !== r.id || this.track(e);\n    });\n  }\n\n  gradients(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    if (assert$6(t.length > 0, () => \"gradients() received an empty list of xs.\"), null != n && \"float32\" !== n.dtype) throw new Error(\"dy must have 'float32' dtype, but has '\".concat(n.dtype, \"'\"));\n    var a = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy(\"forward\", e));\n    assert$6(a instanceof Tensor$1, () => \"The result y returned by f() must be a tensor.\");\n    var s = getFilteredNodesXToY$1(this.state.activeTape, t, a);\n    if (!r && 0 === s.length && t.length > 0) throw new Error(\"Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.\");\n    return this.tidy(\"backward\", () => {\n      var e = {};\n      e[a.id] = null == n ? ones$4(a.shape) : n, backpropagateGradients$1(e, s, e => this.tidy(e), add$6);\n      var r = t.map(t => e[t.id]);\n      return 0 === this.state.gradientDepth && (this.state.activeTape.forEach(e => {\n        for (var _t29 of e.saved) {\n          _t29.dispose();\n        }\n      }), this.state.activeTape = null), {\n        value: a,\n        grads: r\n      };\n    });\n  }\n\n  customGrad(e) {\n    var _this9 = this;\n\n    return assert$6(isFunction$1(e), () => \"The f passed in customGrad(f) must be a function.\"), function () {\n      for (var _len4 = arguments.length, t = new Array(_len4), _key4 = 0; _key4 < _len4; _key4++) {\n        t[_key4] = arguments[_key4];\n      }\n\n      var n;\n      assert$6(t.every(e => e instanceof Tensor$1), () => \"The args passed in customGrad(f)(x1, x2,...) must all be tensors\");\n      var r = {};\n      return t.forEach((e, t) => {\n        r[t] = e;\n      }), _this9.runKernelFunc({\n        forwardFunc: (r, a) => (n = e(...t, a), assert$6(n.value instanceof Tensor$1, () => \"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor\"), assert$6(isFunction$1(n.gradFunc), () => \"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.\"), n.value),\n        backwardsFunc: (e, r) => {\n          var a = n.gradFunc(e, r),\n              s = Array.isArray(a) ? a : [a];\n          assert$6(s.length === t.length, () => \"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).\"), assert$6(s.every(e => e instanceof Tensor$1), () => \"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors.\");\n          var o = {};\n          return s.forEach((e, t) => {\n            o[t] = () => e;\n          }), o;\n        },\n        inputs: r\n      });\n    };\n  }\n\n  readSync(e) {\n    return this.state.tensorInfo.get(e).backend.readSync(e);\n  }\n\n  read(e) {\n    return this.state.tensorInfo.get(e).backend.read(e);\n  }\n\n  time(e) {\n    var _this10 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = now$1(),\n          n = yield _this10.backend.time(e);\n      return n.wallMs = now$1() - t, n;\n    })();\n  }\n\n  track(e) {\n    return null != this.state.activeScope && (e.scopeId = this.state.activeScope.id, this.state.activeScope.track.push(e)), e;\n  }\n\n  get registeredVariables() {\n    return this.state.registeredVariables;\n  }\n\n  reset() {\n    this.pendingBackendInitId++, this.state.dispose(), this.ENV.reset(), this.state = new EngineState$1();\n\n    for (var e in this.registry) {\n      this.disposeRegisteredKernels(e), this.registry[e].dispose(), delete this.registry[e];\n    }\n\n    this.backendName = null, this.backendInstance = null, this.pendingBackendInit = null;\n  }\n\n}\n\nfunction ones$4(e) {\n  var t = makeOnesTypedArray$1(sizeFromShape$1(e), \"float32\");\n  return ENGINE$1.makeTensor(t, e, \"float32\");\n}\n\nfunction getOrMakeEngine$1() {\n  var e = getGlobalNamespace$1();\n\n  if (null == e._tfengine) {\n    var t = new Environment$1(e);\n    e._tfengine = new Engine$1(t);\n  }\n\n  return setEnvironmentGlobal$1(e._tfengine.ENV), setTensorTracker$1(() => e._tfengine), e._tfengine;\n}\n\nEngine$1.nextTensorId = 0, Engine$1.nextVariableId = 0;\nvar ENGINE$1 = getOrMakeEngine$1();\n\nfunction add$6(e, t) {\n  return ENGINE$1.runKernel(Add$3, {\n    a: e,\n    b: t\n  });\n}\n\nfunction _isNavigatorDefined$1() {\n  return \"undefined\" != typeof navigator && null != navigator;\n}\n\nfunction isMobile$1(e) {\n  if (e || _isNavigatorDefined$1()) {\n    if (e || (e = navigator), \"ReactNative\" === e.product) return !0;\n    var t = e.userAgent || e.vendor || window.opera;\n    return /(android|bb\\d+|meego).+mobile|avantgo|bada\\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i.test(t) || /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\/|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\/)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\/(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\/|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\/|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-/i.test(t.substr(0, 4));\n  }\n\n  return !1;\n}\n\nfunction isBrowser$1() {\n  return \"undefined\" != typeof window && null != window.document || \"undefined\" != typeof WorkerGlobalScope;\n}\n\nvar ENV$4 = env$1();\n\nfunction inferShape$1(e, t) {\n  var n = e;\n  if (isTypedArray$1(e)) return \"string\" === t ? [] : [e.length];\n  if (!Array.isArray(e)) return [];\n  var r = [];\n\n  for (; Array.isArray(n) || isTypedArray$1(n) && \"string\" !== t;) {\n    r.push(n.length), n = n[0];\n  }\n\n  return Array.isArray(e) && env$1().getBool(\"TENSORLIKE_CHECK_SHAPE_CONSISTENCY\") && deepAssertShapeConsistency$1(e, r, []), r;\n}\n\nfunction deepAssertShapeConsistency$1(e, t, n) {\n  if (n = n || [], !Array.isArray(e) && !isTypedArray$1(e)) return void assert$6(0 === t.length, () => \"Element arr[\".concat(n.join(\"][\"), \"] is a primitive, but should be an array/TypedArray of \").concat(t[0], \" elements\"));\n  assert$6(t.length > 0, () => \"Element arr[\".concat(n.join(\"][\"), \"] should be a primitive, but is an array of \").concat(e.length, \" elements\")), assert$6(e.length === t[0], () => \"Element arr[\".concat(n.join(\"][\"), \"] should have \").concat(t[0], \" elements, but has \").concat(e.length, \" elements\"));\n  var r = t.slice(1);\n\n  for (var _t30 = 0; _t30 < e.length; ++_t30) {\n    deepAssertShapeConsistency$1(e[_t30], r, n.concat(_t30));\n  }\n}\n\nfunction assertDtype$1(e, t, n, r) {\n  if (\"string_or_numeric\" !== e) {\n    if (null == e) throw new Error(\"Expected dtype cannot be null.\");\n    if (\"numeric\" !== e && e !== t || \"numeric\" === e && \"string\" === t) throw new Error(\"Argument '\".concat(n, \"' passed to '\").concat(r, \"' must be \").concat(e, \" tensor, but got \").concat(t, \" tensor\"));\n  }\n}\n\nfunction convertToTensor$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"numeric\";\n  if (e instanceof Tensor$1) return assertDtype$1(r, e.dtype, t, n), e;\n  var a = inferDtype$1(e);\n  if (\"string\" !== a && [\"bool\", \"int32\", \"float32\"].indexOf(r) >= 0 && (a = r), assertDtype$1(r, a, t, n), null == e || !isTypedArray$1(e) && !Array.isArray(e) && \"number\" != typeof e && \"boolean\" != typeof e && \"string\" != typeof e) throw new Error(\"Argument '\".concat(t, \"' passed to '\").concat(n, \"' must be a Tensor or TensorLike, but got '\").concat(null == e ? \"null\" : e.constructor.name, \"'\"));\n  var s = inferShape$1(e, a);\n  isTypedArray$1(e) || Array.isArray(e) || (e = [e]);\n  var o = \"string\" !== a ? toTypedArray$1(e, a) : flatten$6(e, [], !0);\n  return ENGINE$1.makeTensor(o, s, a);\n}\n\nfunction convertToTensorArray$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"numeric\";\n  if (!Array.isArray(e)) throw new Error(\"Argument \".concat(t, \" passed to \").concat(n, \" must be a `Tensor[]` or `TensorLike[]`\"));\n  return e.map((e, a) => convertToTensor$1(e, \"\".concat(t, \"[\").concat(a, \"]\"), n, r));\n}\n\nENV$4.registerFlag(\"DEBUG\", () => !1, e => {\n  e && console.warn(\"Debugging mode is ON. The output of every math call will be downloaded to CPU and checked for NaNs. This significantly impacts performance.\");\n}), ENV$4.registerFlag(\"IS_BROWSER\", () => isBrowser$1()), ENV$4.registerFlag(\"IS_NODE\", () => \"undefined\" != typeof process && void 0 !== process.versions && void 0 !== process.versions.node), ENV$4.registerFlag(\"IS_CHROME\", () => \"undefined\" != typeof navigator && null != navigator && null != navigator.userAgent && /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor)), ENV$4.registerFlag(\"PROD\", () => !1), ENV$4.registerFlag(\"TENSORLIKE_CHECK_SHAPE_CONSISTENCY\", () => ENV$4.getBool(\"DEBUG\")), ENV$4.registerFlag(\"DEPRECATION_WARNINGS_ENABLED\", () => !0), ENV$4.registerFlag(\"IS_TEST\", () => !1), ENV$4.registerFlag(\"CHECK_COMPUTATION_FOR_ERRORS\", () => !0), ENV$4.registerFlag(\"WRAP_TO_IMAGEBITMAP\", () => !1);\nvar OP_SCOPE_SUFFIX$1 = \"__op\";\n\nfunction op$1(e) {\n  var t = Object.keys(e);\n  if (1 !== t.length) throw new Error(\"Please provide an object with a single key (operation name) mapping to a function. Got an object with \".concat(t.length, \" keys.\"));\n  var n = t[0];\n  var r = e[n];\n  n.endsWith(\"_\") && (n = n.substring(0, n.length - 1)), n += OP_SCOPE_SUFFIX$1;\n\n  var a = function a() {\n    ENGINE$1.startScope(n);\n\n    try {\n      var _t31 = r(...arguments);\n\n      return isPromise$1(_t31) && console.error(\"Cannot return a Promise inside of tidy.\"), ENGINE$1.endScope(_t31), _t31;\n    } catch (e) {\n      throw ENGINE$1.endScope(null), e;\n    }\n  };\n\n  return Object.defineProperty(a, \"name\", {\n    value: n,\n    configurable: !0\n  }), a;\n}\n\nfunction complex_$1(e, t) {\n  var n = convertToTensor$1(e, \"real\", \"complex\"),\n      r = convertToTensor$1(t, \"imag\", \"complex\");\n  return assertShapesMatch$1(n.shape, r.shape, \"real and imag shapes, \".concat(n.shape, \" and \").concat(r.shape, \", must match in call to tf.complex().\")), ENGINE$1.runKernel(Complex$1, {\n    real: n,\n    imag: r\n  });\n}\n\nvar complex$5 = op$1({\n  complex_: complex_$1\n});\n\nfunction makeTensor$1(e, t, n, r) {\n  if (null == r && (r = inferDtype$1(e)), \"complex64\" === r) throw new Error(\"Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).\");\n  if (!isTypedArray$1(e) && !Array.isArray(e) && \"number\" != typeof e && \"boolean\" != typeof e && \"string\" != typeof e) throw new Error(\"values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray\");\n\n  if (null != t) {\n    assertNonNegativeIntegerDimensions$1(t);\n\n    var _e23 = sizeFromShape$1(t),\n        _r8 = sizeFromShape$1(n);\n\n    assert$6(_e23 === _r8, () => \"Based on the provided shape, [\".concat(t, \"], the tensor should have \").concat(_e23, \" values but has \").concat(_r8));\n\n    for (var _e24 = 0; _e24 < n.length; ++_e24) {\n      var _r9 = n[_e24],\n          a = _e24 !== n.length - 1 || _r9 !== sizeFromShape$1(t.slice(_e24));\n\n      assert$6(n[_e24] === t[_e24] || !a, () => \"Error creating a new Tensor. Inferred shape (\".concat(n, \") does not match the provided shape (\").concat(t, \"). \"));\n    }\n  }\n\n  return isTypedArray$1(e) || Array.isArray(e) || (e = [e]), t = t || n, e = \"string\" !== r ? toTypedArray$1(e, r) : flatten$6(e, [], !0), ENGINE$1.makeTensor(e, t, r);\n}\n\nfunction tensor$1(e, t, n) {\n  return makeTensor$1(e, t, inferShape$1(e, n), n);\n}\n\nvar DTYPE_VALUE_SIZE_MAP$1 = {\n  float32: 4,\n  float16: 2,\n  int32: 4,\n  uint16: 2,\n  uint8: 1,\n  bool: 1,\n  complex64: 8\n},\n    NUM_BYTES_STRING_LENGTH$1 = 4;\n\nfunction encodeWeights$1(_x, _x2) {\n  return _encodeWeights$.apply(this, arguments);\n}\n\nfunction _encodeWeights$() {\n  _encodeWeights$ = _asyncToGenerator(function* (e, t) {\n    var n = [],\n        r = [],\n        a = Array.isArray(e) ? e.map(e => e.name) : Object.keys(e);\n\n    var _loop60 = function _loop60(s) {\n      var o = a[s],\n          i = Array.isArray(e) ? e[s].tensor : e[o];\n      if (\"float32\" !== i.dtype && \"int32\" !== i.dtype && \"bool\" !== i.dtype && \"string\" !== i.dtype && \"complex64\" !== i.dtype) throw new Error(\"Unsupported dtype in weight '\".concat(o, \"': \").concat(i.dtype));\n      var l = {\n        name: o,\n        shape: i.shape,\n        dtype: i.dtype\n      };\n\n      if (\"string\" === i.dtype) {\n        var _e1127 = new Promise( /*#__PURE__*/function () {\n          var _ref75 = _asyncToGenerator(function* (e) {\n            var t = yield i.bytes(),\n                n = t.reduce((e, t) => e + t.length, 0) + NUM_BYTES_STRING_LENGTH$1 * t.length,\n                r = new Uint8Array(n);\n            var a = 0;\n\n            for (var _e1128 = 0; _e1128 < t.length; _e1128++) {\n              var _n450 = t[_e1128],\n                  _s223 = new Uint8Array(new Uint32Array([_n450.length]).buffer);\n\n              r.set(_s223, a), a += NUM_BYTES_STRING_LENGTH$1, r.set(_n450, a), a += _n450.length;\n            }\n\n            e(r);\n          });\n\n          return function (_x145) {\n            return _ref75.apply(this, arguments);\n          };\n        }());\n\n        r.push(_e1127);\n      } else r.push(i.data());\n\n      null != t && (l.group = t), n.push(l);\n    };\n\n    for (var s = 0; s < a.length; ++s) {\n      _loop60(s);\n    }\n\n    return {\n      data: concatenateTypedArrays$1(yield Promise.all(r)),\n      specs: n\n    };\n  });\n  return _encodeWeights$.apply(this, arguments);\n}\n\nfunction decodeWeights$1(e, t) {\n  var n = {};\n  var r,\n      a = 0;\n\n  for (var s of t) {\n    var _t32 = s.name,\n        o = s.dtype,\n        i = s.shape,\n        l = sizeFromShape$1(i);\n    var u = void 0;\n\n    if (\"quantization\" in s) {\n      var _n9 = s.quantization;\n\n      if (\"uint8\" === _n9.dtype || \"uint16\" === _n9.dtype) {\n        if (!(\"min\" in _n9) || !(\"scale\" in _n9)) throw new Error(\"Weight \".concat(s.name, \" with quantization \").concat(_n9.dtype, \" doesn't have corresponding metadata min and scale.\"));\n      } else {\n        if (\"float16\" !== _n9.dtype) throw new Error(\"Weight \".concat(s.name, \" has unknown quantization dtype \").concat(_n9.dtype, \". Supported quantization dtypes are: 'uint8', 'uint16', and 'float16'.\"));\n        if (\"float32\" !== o) throw new Error(\"Weight \".concat(s.name, \" is quantized with \").concat(_n9.dtype, \" which only supports weights of type float32 not \").concat(o, \".\"));\n      }\n\n      var _i2 = DTYPE_VALUE_SIZE_MAP$1[_n9.dtype],\n          c = e.slice(a, a + l * _i2),\n          _p = \"uint8\" === _n9.dtype ? new Uint8Array(c) : new Uint16Array(c);\n\n      if (\"float32\" === o) {\n        if (\"uint8\" === _n9.dtype || \"uint16\" === _n9.dtype) {\n          u = new Float32Array(_p.length);\n\n          for (var _e25 = 0; _e25 < _p.length; _e25++) {\n            u[_e25] = _p[_e25] * _n9.scale + _n9.min;\n          }\n        } else {\n          if (\"float16\" !== _n9.dtype) throw new Error(\"Unsupported quantization type \".concat(_n9.dtype, \" for weight type float32.\"));\n          void 0 === r && (r = getFloat16Decoder$1()), u = r(_p);\n        }\n      } else {\n        if (\"int32\" !== o) throw new Error(\"Unsupported dtype in weight '\".concat(_t32, \"': \").concat(o));\n        if (\"uint8\" !== _n9.dtype && \"uint16\" !== _n9.dtype) throw new Error(\"Unsupported quantization type \".concat(_n9.dtype, \" for weight type int32.\"));\n        u = new Int32Array(_p.length);\n\n        for (var _e26 = 0; _e26 < _p.length; _e26++) {\n          u[_e26] = Math.round(_p[_e26] * _n9.scale + _n9.min);\n        }\n      }\n      a += l * _i2;\n    } else if (\"string\" === o) {\n      var _t33 = sizeFromShape$1(s.shape);\n\n      u = [];\n\n      for (var _n10 = 0; _n10 < _t33; _n10++) {\n        var _t34 = new Uint32Array(e.slice(a, a + NUM_BYTES_STRING_LENGTH$1))[0];\n        a += NUM_BYTES_STRING_LENGTH$1;\n\n        var _n11 = new Uint8Array(e.slice(a, a + _t34));\n\n        u.push(_n11), a += _t34;\n      }\n    } else {\n      var _r10 = DTYPE_VALUE_SIZE_MAP$1[o],\n          _s4 = e.slice(a, a + l * _r10);\n\n      if (\"float32\" === o) u = new Float32Array(_s4);else if (\"int32\" === o) u = new Int32Array(_s4);else if (\"bool\" === o) u = new Uint8Array(_s4);else {\n        if (\"complex64\" !== o) throw new Error(\"Unsupported dtype in weight '\".concat(_t32, \"': \").concat(o));\n        {\n          u = new Float32Array(_s4);\n\n          var _e27 = new Float32Array(u.length / 2),\n              _r11 = new Float32Array(u.length / 2);\n\n          for (var _t35 = 0; _t35 < _e27.length; _t35++) {\n            _e27[_t35] = u[2 * _t35], _r11[_t35] = u[2 * _t35 + 1];\n          }\n\n          var _a5 = tensor$1(_e27, i, \"float32\"),\n              _o2 = tensor$1(_r11, i, \"float32\");\n\n          n[_t32] = complex$5(_a5, _o2), _a5.dispose(), _o2.dispose();\n        }\n      }\n      a += l * _r10;\n    }\n\n    \"complex64\" !== o && (n[_t32] = tensor$1(u, i, o));\n  }\n\n  return n;\n}\n\nfunction concatenateTypedArrays$1(e) {\n  if (null === e) throw new Error(\"Invalid input value: \".concat(JSON.stringify(e)));\n  var t = 0;\n  var n = [];\n  e.forEach(e => {\n    if (t += e.byteLength, n.push(e.byteLength === e.buffer.byteLength ? e : new e.constructor(e)), !(e instanceof Float32Array || e instanceof Int32Array || e instanceof Uint8Array)) throw new Error(\"Unsupported TypedArray subtype: \".concat(e.constructor.name));\n  });\n  var r = new Uint8Array(t);\n  var a = 0;\n  return n.forEach(e => {\n    r.set(new Uint8Array(e.buffer), a), a += e.byteLength;\n  }), r.buffer;\n}\n\nvar useNodeBuffer$1 = \"undefined\" != typeof Buffer && (\"undefined\" == typeof Blob || \"undefined\" == typeof atob || \"undefined\" == typeof btoa);\n\nfunction stringByteLength$1(e) {\n  return useNodeBuffer$1 ? Buffer.byteLength(e) : new Blob([e]).size;\n}\n\nfunction arrayBufferToBase64String$1(e) {\n  if (useNodeBuffer$1) return Buffer.from(e).toString(\"base64\");\n  var t = new Uint8Array(e);\n  var n = \"\";\n\n  for (var _e28 = 0, r = t.length; _e28 < r; _e28++) {\n    n += String.fromCharCode(t[_e28]);\n  }\n\n  return btoa(n);\n}\n\nfunction base64StringToArrayBuffer$1(e) {\n  if (useNodeBuffer$1) {\n    var _t36 = Buffer.from(e, \"base64\");\n\n    return _t36.buffer.slice(_t36.byteOffset, _t36.byteOffset + _t36.byteLength);\n  }\n\n  var t = atob(e),\n      n = new Uint8Array(t.length);\n\n  for (var _e29 = 0; _e29 < t.length; ++_e29) {\n    n.set([t.charCodeAt(_e29)], _e29);\n  }\n\n  return n.buffer;\n}\n\nfunction concatenateArrayBuffers$1(e) {\n  if (1 === e.length) return e[0];\n  var t = 0;\n  e.forEach(e => {\n    t += e.byteLength;\n  });\n  var n = new Uint8Array(t);\n  var r = 0;\n  return e.forEach(e => {\n    n.set(new Uint8Array(e), r), r += e.byteLength;\n  }), n.buffer;\n}\n\nfunction getModelJSONForModelArtifacts$1(e, t) {\n  var n = {\n    modelTopology: e.modelTopology,\n    format: e.format,\n    generatedBy: e.generatedBy,\n    convertedBy: e.convertedBy,\n    weightsManifest: t\n  };\n  return null != e.signature && (n.signature = e.signature), null != e.userDefinedMetadata && (n.userDefinedMetadata = e.userDefinedMetadata), null != e.modelInitializer && (n.modelInitializer = e.modelInitializer), null != e.trainingConfig && (n.trainingConfig = e.trainingConfig), n;\n}\n\nfunction getModelArtifactsForJSON$1(_x3, _x4) {\n  return _getModelArtifactsForJSON$.apply(this, arguments);\n}\n\nfunction _getModelArtifactsForJSON$() {\n  _getModelArtifactsForJSON$ = _asyncToGenerator(function* (e, t) {\n    var n = {\n      modelTopology: e.modelTopology,\n      format: e.format,\n      generatedBy: e.generatedBy,\n      convertedBy: e.convertedBy\n    };\n\n    if (null != e.trainingConfig && (n.trainingConfig = e.trainingConfig), null != e.weightsManifest) {\n      var [r, a] = yield t(e.weightsManifest);\n      n.weightSpecs = r, n.weightData = a;\n    }\n\n    return null != e.signature && (n.signature = e.signature), null != e.userDefinedMetadata && (n.userDefinedMetadata = e.userDefinedMetadata), null != e.modelInitializer && (n.modelInitializer = e.modelInitializer), n;\n  });\n  return _getModelArtifactsForJSON$.apply(this, arguments);\n}\n\nfunction getModelArtifactsInfoForJSON$1(e) {\n  if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"Expected JSON model topology, received ArrayBuffer.\");\n  return {\n    dateSaved: new Date(),\n    modelTopologyType: \"JSON\",\n    modelTopologyBytes: null == e.modelTopology ? 0 : stringByteLength$1(JSON.stringify(e.modelTopology)),\n    weightSpecsBytes: null == e.weightSpecs ? 0 : stringByteLength$1(JSON.stringify(e.weightSpecs)),\n    weightDataBytes: null == e.weightData ? 0 : e.weightData.byteLength\n  };\n}\n\nfunction computeFloat16MantisaTable$1() {\n  var e = e => {\n    var t = e << 13,\n        n = 0;\n\n    for (; 0 == (8388608 & t);) {\n      n -= 8388608, t <<= 1;\n    }\n\n    return t &= -8388609, n += 947912704, t | n;\n  },\n      t = new Uint32Array(2048);\n\n  t[0] = 0;\n\n  for (var n = 1; n < 1024; n++) {\n    t[n] = e(n);\n  }\n\n  for (var _e30 = 1024; _e30 < 2048; _e30++) {\n    t[_e30] = 939524096 + (_e30 - 1024 << 13);\n  }\n\n  return t;\n}\n\nfunction computeFloat16ExponentTable$1() {\n  var e = new Uint32Array(64);\n  e[0] = 0, e[31] = 1199570944, e[32] = 2147483648, e[63] = 3347054592;\n\n  for (var t = 1; t < 31; t++) {\n    e[t] = t << 23;\n  }\n\n  for (var _t37 = 33; _t37 < 63; _t37++) {\n    e[_t37] = 2147483648 + (_t37 - 32 << 23);\n  }\n\n  return e;\n}\n\nfunction computeFloat16OffsetTable$1() {\n  var e = new Uint32Array(64);\n\n  for (var t = 0; t < 64; t++) {\n    e[t] = 1024;\n  }\n\n  return e[0] = e[32] = 0, e;\n}\n\nfunction getFloat16Decoder$1() {\n  var e = computeFloat16MantisaTable$1(),\n      t = computeFloat16ExponentTable$1(),\n      n = computeFloat16OffsetTable$1();\n  return r => {\n    var a = new ArrayBuffer(4 * r.length),\n        s = new Uint32Array(a);\n\n    for (var _a6 = 0; _a6 < r.length; _a6++) {\n      var o = r[_a6];\n      s[_a6] = e[n[o >> 10] + (1023 & o)] + t[o >> 10];\n    }\n\n    return new Float32Array(a);\n  };\n}\n\nclass IORouterRegistry$1 {\n  constructor() {\n    this.saveRouters = [], this.loadRouters = [];\n  }\n\n  static getInstance() {\n    return null == IORouterRegistry$1.instance && (IORouterRegistry$1.instance = new IORouterRegistry$1()), IORouterRegistry$1.instance;\n  }\n\n  static registerSaveRouter(e) {\n    IORouterRegistry$1.getInstance().saveRouters.push(e);\n  }\n\n  static registerLoadRouter(e) {\n    IORouterRegistry$1.getInstance().loadRouters.push(e);\n  }\n\n  static getSaveHandlers(e) {\n    return IORouterRegistry$1.getHandlers(e, \"save\");\n  }\n\n  static getLoadHandlers(e, t) {\n    return IORouterRegistry$1.getHandlers(e, \"load\", t);\n  }\n\n  static getHandlers(e, t, n) {\n    var r = [];\n    return (\"load\" === t ? IORouterRegistry$1.getInstance().loadRouters : IORouterRegistry$1.getInstance().saveRouters).forEach(t => {\n      var a = t(e, n);\n      null !== a && r.push(a);\n    }), r;\n  }\n\n}\n\nvar getSaveHandlers$1 = e => IORouterRegistry$1.getSaveHandlers(e),\n    getLoadHandlers$1 = (e, t) => IORouterRegistry$1.getLoadHandlers(e, t),\n    DATABASE_NAME$1 = \"tensorflowjs\",\n    DATABASE_VERSION$1 = 1,\n    MODEL_STORE_NAME$1 = \"models_store\",\n    INFO_STORE_NAME$1 = \"model_info_store\";\n\nfunction getIndexedDBFactory$1() {\n  if (!env$1().getBool(\"IS_BROWSER\")) throw new Error(\"Failed to obtain IndexedDB factory because the current environmentis not a web browser.\");\n  var e = \"undefined\" == typeof window ? self : window,\n      t = e.indexedDB || e.mozIndexedDB || e.webkitIndexedDB || e.msIndexedDB || e.shimIndexedDB;\n  if (null == t) throw new Error(\"The current browser does not appear to support IndexedDB.\");\n  return t;\n}\n\nfunction setUpDatabase$1(e) {\n  var t = e.result;\n  t.createObjectStore(MODEL_STORE_NAME$1, {\n    keyPath: \"modelPath\"\n  }), t.createObjectStore(INFO_STORE_NAME$1, {\n    keyPath: \"modelPath\"\n  });\n}\n\nclass BrowserIndexedDB$1 {\n  constructor(e) {\n    if (this.indexedDB = getIndexedDBFactory$1(), null == e || !e) throw new Error(\"For IndexedDB, modelPath must not be null, undefined or empty.\");\n    this.modelPath = e;\n  }\n\n  save(e) {\n    var _this11 = this;\n\n    return _asyncToGenerator(function* () {\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserLocalStorage.save() does not support saving model topology in binary formats yet.\");\n      return _this11.databaseAction(_this11.modelPath, e);\n    })();\n  }\n\n  load() {\n    var _this12 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this12.databaseAction(_this12.modelPath);\n    })();\n  }\n\n  databaseAction(e, t) {\n    return new Promise((e, n) => {\n      var r = this.indexedDB.open(DATABASE_NAME$1, DATABASE_VERSION$1);\n      r.onupgradeneeded = () => setUpDatabase$1(r), r.onsuccess = () => {\n        var a = r.result;\n\n        if (null == t) {\n          var _t38 = a.transaction(MODEL_STORE_NAME$1, \"readonly\"),\n              _r12 = _t38.objectStore(MODEL_STORE_NAME$1).get(this.modelPath);\n\n          _r12.onsuccess = () => {\n            if (null == _r12.result) return a.close(), n(new Error(\"Cannot find model with path '\".concat(this.modelPath, \"' in IndexedDB.\")));\n            e(_r12.result.modelArtifacts);\n          }, _r12.onerror = e => (a.close(), n(_r12.error)), _t38.oncomplete = () => a.close();\n        } else {\n          var _r13 = getModelArtifactsInfoForJSON$1(t),\n              s = a.transaction(INFO_STORE_NAME$1, \"readwrite\");\n\n          var o = s.objectStore(INFO_STORE_NAME$1);\n          var i = o.put({\n            modelPath: this.modelPath,\n            modelArtifactsInfo: _r13\n          });\n          var l;\n          i.onsuccess = () => {\n            l = a.transaction(MODEL_STORE_NAME$1, \"readwrite\");\n            var i = l.objectStore(MODEL_STORE_NAME$1).put({\n              modelPath: this.modelPath,\n              modelArtifacts: t,\n              modelArtifactsInfo: _r13\n            });\n            i.onsuccess = () => e({\n              modelArtifactsInfo: _r13\n            }), i.onerror = e => {\n              o = s.objectStore(INFO_STORE_NAME$1);\n              var t = o.delete(this.modelPath);\n              t.onsuccess = () => (a.close(), n(i.error)), t.onerror = e => (a.close(), n(i.error));\n            };\n          }, i.onerror = e => (a.close(), n(i.error)), s.oncomplete = () => {\n            null == l ? a.close() : l.oncomplete = () => a.close();\n          };\n        }\n      }, r.onerror = e => n(r.error);\n    });\n  }\n\n}\n\nBrowserIndexedDB$1.URL_SCHEME = \"indexeddb://\";\n\nvar indexedDBRouter$1 = e => env$1().getBool(\"IS_BROWSER\") && !Array.isArray(e) && e.startsWith(BrowserIndexedDB$1.URL_SCHEME) ? browserIndexedDB$1(e.slice(BrowserIndexedDB$1.URL_SCHEME.length)) : null;\n\nfunction browserIndexedDB$1(e) {\n  return new BrowserIndexedDB$1(e);\n}\n\nfunction maybeStripScheme$3(e) {\n  return e.startsWith(BrowserIndexedDB$1.URL_SCHEME) ? e.slice(BrowserIndexedDB$1.URL_SCHEME.length) : e;\n}\n\nIORouterRegistry$1.registerSaveRouter(indexedDBRouter$1), IORouterRegistry$1.registerLoadRouter(indexedDBRouter$1);\n\nclass BrowserIndexedDBManager$1 {\n  constructor() {\n    this.indexedDB = getIndexedDBFactory$1();\n  }\n\n  listModels() {\n    var _this13 = this;\n\n    return _asyncToGenerator(function* () {\n      return new Promise((e, t) => {\n        var n = _this13.indexedDB.open(DATABASE_NAME$1, DATABASE_VERSION$1);\n\n        n.onupgradeneeded = () => setUpDatabase$1(n), n.onsuccess = () => {\n          var r = n.result,\n              a = r.transaction(INFO_STORE_NAME$1, \"readonly\"),\n              s = a.objectStore(INFO_STORE_NAME$1).getAll();\n          s.onsuccess = () => {\n            var t = {};\n\n            for (var _e31 of s.result) {\n              t[_e31.modelPath] = _e31.modelArtifactsInfo;\n            }\n\n            e(t);\n          }, s.onerror = e => (r.close(), t(s.error)), a.oncomplete = () => r.close();\n        }, n.onerror = e => t(n.error);\n      });\n    })();\n  }\n\n  removeModel(e) {\n    var _this14 = this;\n\n    return _asyncToGenerator(function* () {\n      return e = maybeStripScheme$3(e), new Promise((t, n) => {\n        var r = _this14.indexedDB.open(DATABASE_NAME$1, DATABASE_VERSION$1);\n\n        r.onupgradeneeded = () => setUpDatabase$1(r), r.onsuccess = () => {\n          var a = r.result,\n              s = a.transaction(INFO_STORE_NAME$1, \"readwrite\"),\n              o = s.objectStore(INFO_STORE_NAME$1),\n              i = o.get(e);\n          var l;\n          i.onsuccess = () => {\n            if (null == i.result) return a.close(), n(new Error(\"Cannot find model with path '\".concat(e, \"' in IndexedDB.\")));\n            {\n              var _r14 = o.delete(e),\n                  _s5 = () => {\n                l = a.transaction(MODEL_STORE_NAME$1, \"readwrite\");\n                var r = l.objectStore(MODEL_STORE_NAME$1).delete(e);\n                r.onsuccess = () => t(i.result.modelArtifactsInfo), r.onerror = e => n(i.error);\n              };\n\n              _r14.onsuccess = _s5, _r14.onerror = e => (_s5(), a.close(), n(i.error));\n            }\n          }, i.onerror = e => (a.close(), n(i.error)), s.oncomplete = () => {\n            null == l ? a.close() : l.oncomplete = () => a.close();\n          };\n        }, r.onerror = e => n(r.error);\n      });\n    })();\n  }\n\n}\n\nvar PATH_SEPARATOR$1 = \"/\",\n    PATH_PREFIX$1 = \"tensorflowjs_models\",\n    INFO_SUFFIX$1 = \"info\",\n    MODEL_TOPOLOGY_SUFFIX$1 = \"model_topology\",\n    WEIGHT_SPECS_SUFFIX$1 = \"weight_specs\",\n    WEIGHT_DATA_SUFFIX$1 = \"weight_data\",\n    MODEL_METADATA_SUFFIX$1 = \"model_metadata\";\n\nfunction getModelKeys$1(e) {\n  return {\n    info: [PATH_PREFIX$1, e, INFO_SUFFIX$1].join(PATH_SEPARATOR$1),\n    topology: [PATH_PREFIX$1, e, MODEL_TOPOLOGY_SUFFIX$1].join(PATH_SEPARATOR$1),\n    weightSpecs: [PATH_PREFIX$1, e, WEIGHT_SPECS_SUFFIX$1].join(PATH_SEPARATOR$1),\n    weightData: [PATH_PREFIX$1, e, WEIGHT_DATA_SUFFIX$1].join(PATH_SEPARATOR$1),\n    modelMetadata: [PATH_PREFIX$1, e, MODEL_METADATA_SUFFIX$1].join(PATH_SEPARATOR$1)\n  };\n}\n\nfunction removeItems$1(e) {\n  for (var t of Object.values(e)) {\n    window.localStorage.removeItem(t);\n  }\n}\n\nfunction getModelPathFromKey$1(e) {\n  var t = e.split(PATH_SEPARATOR$1);\n  if (t.length < 3) throw new Error(\"Invalid key format: \".concat(e));\n  return t.slice(1, t.length - 1).join(PATH_SEPARATOR$1);\n}\n\nfunction maybeStripScheme$2(e) {\n  return e.startsWith(BrowserLocalStorage$1.URL_SCHEME) ? e.slice(BrowserLocalStorage$1.URL_SCHEME.length) : e;\n}\n\nclass BrowserLocalStorage$1 {\n  constructor(e) {\n    if (!env$1().getBool(\"IS_BROWSER\") || \"undefined\" == typeof window || void 0 === window.localStorage) throw new Error(\"The current environment does not support local storage.\");\n    if (this.LS = window.localStorage, null == e || !e) throw new Error(\"For local storage, modelPath must not be null, undefined or empty.\");\n    this.modelPath = e, this.keys = getModelKeys$1(this.modelPath);\n  }\n\n  save(e) {\n    var _this15 = this;\n\n    return _asyncToGenerator(function* () {\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserLocalStorage.save() does not support saving model topology in binary formats yet.\");\n      {\n        var t = JSON.stringify(e.modelTopology),\n            n = JSON.stringify(e.weightSpecs),\n            r = getModelArtifactsInfoForJSON$1(e);\n\n        try {\n          return _this15.LS.setItem(_this15.keys.info, JSON.stringify(r)), _this15.LS.setItem(_this15.keys.topology, t), _this15.LS.setItem(_this15.keys.weightSpecs, n), _this15.LS.setItem(_this15.keys.weightData, arrayBufferToBase64String$1(e.weightData)), _this15.LS.setItem(_this15.keys.modelMetadata, JSON.stringify({\n            format: e.format,\n            generatedBy: e.generatedBy,\n            convertedBy: e.convertedBy,\n            signature: null != e.signature ? e.signature : void 0,\n            userDefinedMetadata: null != e.userDefinedMetadata ? e.userDefinedMetadata : void 0,\n            modelInitializer: null != e.modelInitializer ? e.modelInitializer : void 0,\n            trainingConfig: null != e.trainingConfig ? e.trainingConfig : void 0\n          })), {\n            modelArtifactsInfo: r\n          };\n        } catch (e) {\n          throw removeItems$1(_this15.keys), new Error(\"Failed to save model '\".concat(_this15.modelPath, \"' to local storage: size quota being exceeded is a possible cause of this failure: modelTopologyBytes=\").concat(r.modelTopologyBytes, \", weightSpecsBytes=\").concat(r.weightSpecsBytes, \", weightDataBytes=\").concat(r.weightDataBytes, \".\"));\n        }\n      }\n    })();\n  }\n\n  load() {\n    var _this16 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = JSON.parse(_this16.LS.getItem(_this16.keys.info));\n      if (null == e) throw new Error(\"In local storage, there is no model with name '\".concat(_this16.modelPath, \"'\"));\n      if (\"JSON\" !== e.modelTopologyType) throw new Error(\"BrowserLocalStorage does not support loading non-JSON model topology yet.\");\n      var t = {},\n          n = JSON.parse(_this16.LS.getItem(_this16.keys.topology));\n      if (null == n) throw new Error(\"In local storage, the topology of model '\".concat(_this16.modelPath, \"' is missing.\"));\n      t.modelTopology = n;\n      var r = JSON.parse(_this16.LS.getItem(_this16.keys.weightSpecs));\n      if (null == r) throw new Error(\"In local storage, the weight specs of model '\".concat(_this16.modelPath, \"' are missing.\"));\n      t.weightSpecs = r;\n\n      var a = _this16.LS.getItem(_this16.keys.modelMetadata);\n\n      if (null != a) {\n        var _e32 = JSON.parse(a);\n\n        t.format = _e32.format, t.generatedBy = _e32.generatedBy, t.convertedBy = _e32.convertedBy, null != _e32.signature && (t.signature = _e32.signature), null != _e32.userDefinedMetadata && (t.userDefinedMetadata = _e32.userDefinedMetadata), null != _e32.modelInitializer && (t.modelInitializer = _e32.modelInitializer), null != _e32.trainingConfig && (t.trainingConfig = _e32.trainingConfig);\n      }\n\n      var s = _this16.LS.getItem(_this16.keys.weightData);\n\n      if (null == s) throw new Error(\"In local storage, the binary weight values of model '\".concat(_this16.modelPath, \"' are missing.\"));\n      return t.weightData = base64StringToArrayBuffer$1(s), t;\n    })();\n  }\n\n}\n\nBrowserLocalStorage$1.URL_SCHEME = \"localstorage://\";\n\nvar localStorageRouter$1 = e => env$1().getBool(\"IS_BROWSER\") && !Array.isArray(e) && e.startsWith(BrowserLocalStorage$1.URL_SCHEME) ? browserLocalStorage$1(e.slice(BrowserLocalStorage$1.URL_SCHEME.length)) : null;\n\nfunction browserLocalStorage$1(e) {\n  return new BrowserLocalStorage$1(e);\n}\n\nIORouterRegistry$1.registerSaveRouter(localStorageRouter$1), IORouterRegistry$1.registerLoadRouter(localStorageRouter$1);\n\nclass BrowserLocalStorageManager$1 {\n  constructor() {\n    assert$6(env$1().getBool(\"IS_BROWSER\"), () => \"Current environment is not a web browser\"), assert$6(\"undefined\" == typeof window || void 0 !== window.localStorage, () => \"Current browser does not appear to support localStorage\"), this.LS = window.localStorage;\n  }\n\n  listModels() {\n    var _this17 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = {},\n          t = PATH_PREFIX$1 + PATH_SEPARATOR$1,\n          n = PATH_SEPARATOR$1 + INFO_SUFFIX$1;\n\n      for (var r = 0; r < _this17.LS.length; ++r) {\n        var a = _this17.LS.key(r);\n\n        a.startsWith(t) && a.endsWith(n) && (e[getModelPathFromKey$1(a)] = JSON.parse(_this17.LS.getItem(a)));\n      }\n\n      return e;\n    })();\n  }\n\n  removeModel(e) {\n    var _this18 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = getModelKeys$1(e = maybeStripScheme$2(e));\n      if (null == _this18.LS.getItem(t.info)) throw new Error(\"Cannot find model at path '\".concat(e, \"'\"));\n      var n = JSON.parse(_this18.LS.getItem(t.info));\n      return removeItems$1(t), n;\n    })();\n  }\n\n}\n\nvar URL_SCHEME_SUFFIX$1 = \"://\";\n\nclass ModelStoreManagerRegistry$1 {\n  constructor() {\n    this.managers = {};\n  }\n\n  static getInstance() {\n    return null == ModelStoreManagerRegistry$1.instance && (ModelStoreManagerRegistry$1.instance = new ModelStoreManagerRegistry$1()), ModelStoreManagerRegistry$1.instance;\n  }\n\n  static registerManager(e, t) {\n    assert$6(null != e, () => \"scheme must not be undefined or null.\"), e.endsWith(URL_SCHEME_SUFFIX$1) && (e = e.slice(0, e.indexOf(URL_SCHEME_SUFFIX$1))), assert$6(e.length > 0, () => \"scheme must not be an empty string.\");\n    var n = ModelStoreManagerRegistry$1.getInstance();\n    assert$6(null == n.managers[e], () => \"A model store manager is already registered for scheme '\".concat(e, \"'.\")), n.managers[e] = t;\n  }\n\n  static getManager(e) {\n    var t = this.getInstance().managers[e];\n    if (null == t) throw new Error(\"Cannot find model manager for scheme '\".concat(e, \"'\"));\n    return t;\n  }\n\n  static getSchemes() {\n    return Object.keys(this.getInstance().managers);\n  }\n\n}\n\nclass PlatformBrowser$1 {\n  fetch(e, t) {\n    return fetch(e, t);\n  }\n\n  now() {\n    return performance.now();\n  }\n\n  encode(e, t) {\n    if (\"utf-8\" !== t && \"utf8\" !== t) throw new Error(\"Browser's encoder only supports utf-8, but got \".concat(t));\n    return null == this.textEncoder && (this.textEncoder = new TextEncoder()), this.textEncoder.encode(e);\n  }\n\n  decode(e, t) {\n    return new TextDecoder(t).decode(e);\n  }\n\n}\n\nif (env$1().get(\"IS_BROWSER\")) {\n  env$1().setPlatform(\"browser\", new PlatformBrowser$1());\n\n  try {\n    ModelStoreManagerRegistry$1.registerManager(BrowserLocalStorage$1.URL_SCHEME, new BrowserLocalStorageManager$1());\n  } catch (e) {}\n\n  try {\n    ModelStoreManagerRegistry$1.registerManager(BrowserIndexedDB$1.URL_SCHEME, new BrowserIndexedDBManager$1());\n  } catch (e) {}\n}\n\nvar getNodeFetch$1 = {\n  importFetch: () => require(\"node-fetch\")\n};\nvar systemFetch$1;\n\nclass PlatformNode$1 {\n  constructor() {\n    this.util = require(\"util\"), this.textEncoder = new this.util.TextEncoder();\n  }\n\n  fetch(e, t) {\n    return null != env$1().global.fetch ? env$1().global.fetch(e, t) : (null == systemFetch$1 && (systemFetch$1 = getNodeFetch$1.importFetch()), systemFetch$1(e, t));\n  }\n\n  now() {\n    var e = process.hrtime();\n    return 1e3 * e[0] + e[1] / 1e6;\n  }\n\n  encode(e, t) {\n    if (\"utf-8\" !== t && \"utf8\" !== t) throw new Error(\"Node built-in encoder only supports utf-8, but got \".concat(t));\n    return this.textEncoder.encode(e);\n  }\n\n  decode(e, t) {\n    return 0 === e.length ? \"\" : new this.util.TextDecoder(t).decode(e);\n  }\n\n}\n\nfunction buffer$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  return t = t || \"float32\", assertNonNegativeIntegerDimensions$1(e), new TensorBuffer$1(e, t, n);\n}\n\nfunction cast_$1(e, t) {\n  var n = convertToTensor$1(e, \"x\", \"cast\");\n  if (!isValidDtype$1(t)) throw new Error(\"Failed to cast to unknown dtype \".concat(t));\n  if (\"string\" === t && \"string\" !== n.dtype || \"string\" !== t && \"string\" === n.dtype) throw new Error(\"Only strings can be casted to strings\");\n  return ENGINE$1.runKernel(Cast$1, {\n    x: n\n  }, {\n    dtype: t\n  });\n}\n\nenv$1().get(\"IS_NODE\") && env$1().setPlatform(\"node\", new PlatformNode$1());\nvar cast$7 = op$1({\n  cast_: cast_$1\n});\n\nfunction clone_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"clone\", \"string_or_numeric\");\n  return ENGINE$1.runKernel(Identity$3, {\n    x: t\n  });\n}\n\nvar clone$1 = op$1({\n  clone_: clone_$1\n});\n\nfunction print$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  console.log(e.toString(t));\n}\n\ngetOrMakeEngine$1();\nvar opHandler$2 = {\n  buffer: buffer$1,\n  cast: cast$7,\n  clone: clone$1,\n  print: print$1\n};\nsetOpHandler$1(opHandler$2);\nvar DEFAULT_FILE_NAME_PREFIX$1 = \"model\",\n    DEFAULT_JSON_EXTENSION_NAME$1 = \".json\",\n    DEFAULT_WEIGHT_DATA_EXTENSION_NAME$1 = \".weights.bin\";\n\nfunction defer$1(e) {\n  return new Promise(e => setTimeout(e)).then(e);\n}\n\nclass BrowserDownloads$1 {\n  constructor(e) {\n    if (!env$1().getBool(\"IS_BROWSER\")) throw new Error(\"browserDownloads() cannot proceed because the current environment is not a browser.\");\n    e.startsWith(BrowserDownloads$1.URL_SCHEME) && (e = e.slice(BrowserDownloads$1.URL_SCHEME.length)), null != e && 0 !== e.length || (e = DEFAULT_FILE_NAME_PREFIX$1), this.modelJsonFileName = e + DEFAULT_JSON_EXTENSION_NAME$1, this.weightDataFileName = e + DEFAULT_WEIGHT_DATA_EXTENSION_NAME$1;\n  }\n\n  save(e) {\n    var _this19 = this;\n\n    return _asyncToGenerator(function* () {\n      if (\"undefined\" == typeof document) throw new Error(\"Browser downloads are not supported in this environment since `document` is not present\");\n      var t = window.URL.createObjectURL(new Blob([e.weightData], {\n        type: \"application/octet-stream\"\n      }));\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserDownloads.save() does not support saving model topology in binary formats yet.\");\n      {\n        var n = getModelJSONForModelArtifacts$1(e, [{\n          paths: [\"./\" + _this19.weightDataFileName],\n          weights: e.weightSpecs\n        }]),\n            r = window.URL.createObjectURL(new Blob([JSON.stringify(n)], {\n          type: \"application/json\"\n        })),\n            a = null == _this19.modelJsonAnchor ? document.createElement(\"a\") : _this19.modelJsonAnchor;\n\n        if (a.download = _this19.modelJsonFileName, a.href = r, yield defer$1(() => a.dispatchEvent(new MouseEvent(\"click\"))), null != e.weightData) {\n          var _e33 = null == _this19.weightDataAnchor ? document.createElement(\"a\") : _this19.weightDataAnchor;\n\n          _e33.download = _this19.weightDataFileName, _e33.href = t, yield defer$1(() => _e33.dispatchEvent(new MouseEvent(\"click\")));\n        }\n\n        return {\n          modelArtifactsInfo: getModelArtifactsInfoForJSON$1(e)\n        };\n      }\n    })();\n  }\n\n}\n\nBrowserDownloads$1.URL_SCHEME = \"downloads://\";\n\nvar browserDownloadsRouter$1 = e => env$1().getBool(\"IS_BROWSER\") && !Array.isArray(e) && e.startsWith(BrowserDownloads$1.URL_SCHEME) ? browserDownloads$1(e.slice(BrowserDownloads$1.URL_SCHEME.length)) : null;\n\nfunction browserDownloads$1() {\n  var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : \"model\";\n  return new BrowserDownloads$1(e);\n}\n\nfunction monitorPromisesProgress$1(e, t, n, r) {\n  !function (e) {\n    assert$6(null != e && Array.isArray(e) && e.length > 0, () => \"promises must be a none empty array\");\n  }(e), function (e, t) {\n    assert$6(e >= 0 && e <= 1, () => \"Progress fraction must be in range [0, 1], but got startFraction \".concat(e)), assert$6(t >= 0 && t <= 1, () => \"Progress fraction must be in range [0, 1], but got endFraction \".concat(t)), assert$6(t >= e, () => \"startFraction must be no more than endFraction, but got startFraction \".concat(e, \" and endFraction \").concat(t));\n  }(n = null == n ? 0 : n, r = null == r ? 1 : r);\n  var a = 0;\n  return Promise.all(e.map(s => (s.then(s => {\n    var o = n + ++a / e.length * (r - n);\n    return t(o), s;\n  }), s)));\n}\n\nfunction loadWeightsAsArrayBuffer$1(_x5, _x6) {\n  return _loadWeightsAsArrayBuffer$.apply(this, arguments);\n}\n\nfunction _loadWeightsAsArrayBuffer$() {\n  _loadWeightsAsArrayBuffer$ = _asyncToGenerator(function* (e, t) {\n    null == t && (t = {});\n    var n = null == t.fetchFunc ? env$1().platform.fetch : t.fetchFunc,\n        r = e.map(e => n(e, t.requestInit, {\n      isBinary: !0\n    })),\n        a = (null == t.onProgress ? yield Promise.all(r) : yield monitorPromisesProgress$1(r, t.onProgress, 0, .5)).map(e => e.arrayBuffer());\n    return null == t.onProgress ? yield Promise.all(a) : yield monitorPromisesProgress$1(a, t.onProgress, .5, 1);\n  });\n  return _loadWeightsAsArrayBuffer$.apply(this, arguments);\n}\n\nIORouterRegistry$1.registerSaveRouter(browserDownloadsRouter$1);\nvar OCTET_STREAM_MIME_TYPE$1 = \"application/octet-stream\",\n    JSON_TYPE$1 = \"application/json\";\n\nclass HTTPRequest$1 {\n  constructor(e, t) {\n    if (this.DEFAULT_METHOD = \"POST\", null == t && (t = {}), this.weightPathPrefix = t.weightPathPrefix, this.onProgress = t.onProgress, this.weightUrlConverter = t.weightUrlConverter, null != t.fetchFunc ? (assert$6(\"function\" == typeof t.fetchFunc, () => \"Must pass a function that matches the signature of `fetch` (see https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API)\"), this.fetch = t.fetchFunc) : this.fetch = env$1().platform.fetch, assert$6(null != e && e.length > 0, () => \"URL path for http must not be null, undefined or empty.\"), Array.isArray(e) && assert$6(2 === e.length, () => \"URL paths for http must have a length of 2, (actual length is \".concat(e.length, \").\")), this.path = e, null != t.requestInit && null != t.requestInit.body) throw new Error(\"requestInit is expected to have no pre-existing body, but has one.\");\n    this.requestInit = t.requestInit || {};\n  }\n\n  save(e) {\n    var _this20 = this;\n\n    return _asyncToGenerator(function* () {\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserHTTPRequest.save() does not support saving model topology in binary formats yet.\");\n      var t = Object.assign({\n        method: _this20.DEFAULT_METHOD\n      }, _this20.requestInit);\n      t.body = new FormData();\n      var n = getModelJSONForModelArtifacts$1(e, [{\n        paths: [\"./model.weights.bin\"],\n        weights: e.weightSpecs\n      }]);\n      t.body.append(\"model.json\", new Blob([JSON.stringify(n)], {\n        type: JSON_TYPE$1\n      }), \"model.json\"), null != e.weightData && t.body.append(\"model.weights.bin\", new Blob([e.weightData], {\n        type: OCTET_STREAM_MIME_TYPE$1\n      }), \"model.weights.bin\");\n      var r = yield _this20.fetch(_this20.path, t);\n      if (r.ok) return {\n        modelArtifactsInfo: getModelArtifactsInfoForJSON$1(e),\n        responses: [r]\n      };\n      throw new Error(\"BrowserHTTPRequest.save() failed due to HTTP response status \".concat(r.status, \".\"));\n    })();\n  }\n\n  load() {\n    var _this21 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this21.fetch(_this21.path, _this21.requestInit);\n      if (!e.ok) throw new Error(\"Request to \".concat(_this21.path, \" failed with status code \").concat(e.status, \". Please verify this URL points to the model JSON of the model to load.\"));\n      var t;\n\n      try {\n        t = yield e.json();\n      } catch (e) {\n        var _t39 = \"Failed to parse model JSON of response from \".concat(_this21.path, \".\");\n\n        throw _this21.path.endsWith(\".pb\") ? _t39 += \" Your path contains a .pb file extension. Support for .pb models have been removed in TensorFlow.js 1.0 in favor of .json models. You can re-convert your Python TensorFlow model using the TensorFlow.js 1.0 conversion scripts or you can convert your.pb models with the 'pb2json'NPM script in the tensorflow/tfjs-converter repository.\" : _t39 += \" Please make sure the server is serving valid JSON for this request.\", new Error(_t39);\n      }\n\n      if (null == t.modelTopology && null == t.weightsManifest) throw new Error(\"The JSON from HTTP path \".concat(_this21.path, \" contains neither model topology or manifest for weights.\"));\n      return getModelArtifactsForJSON$1(t, e => _this21.loadWeights(e));\n    })();\n  }\n\n  loadWeights(e) {\n    var _this22 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = Array.isArray(_this22.path) ? _this22.path[1] : _this22.path,\n          [n, r] = parseUrl$1(t),\n          a = _this22.weightPathPrefix || n,\n          s = [];\n\n      for (var _t40 of e) {\n        s.push(..._t40.weights);\n      }\n\n      var o = [],\n          i = [];\n\n      for (var _t41 of e) {\n        for (var _e34 of _t41.paths) {\n          null != _this22.weightUrlConverter ? i.push(_this22.weightUrlConverter(_e34)) : o.push(a + _e34 + r);\n        }\n      }\n\n      return _this22.weightUrlConverter && o.push(...(yield Promise.all(i))), [s, concatenateArrayBuffers$1(yield loadWeightsAsArrayBuffer$1(o, {\n        requestInit: _this22.requestInit,\n        fetchFunc: _this22.fetch,\n        onProgress: _this22.onProgress\n      }))];\n    })();\n  }\n\n}\n\nfunction parseUrl$1(e) {\n  var t = e.lastIndexOf(\"/\"),\n      n = e.lastIndexOf(\"?\");\n  return [e.substring(0, t) + \"/\", n > t ? e.substring(n) : \"\"];\n}\n\nfunction isHTTPScheme$1(e) {\n  return null != e.match(HTTPRequest$1.URL_SCHEME_REGEX);\n}\n\nHTTPRequest$1.URL_SCHEME_REGEX = /^https?:\\/\\//;\n\nvar httpRouter$1 = (e, t) => {\n  if (\"undefined\" == typeof fetch && (null == t || null == t.fetchFunc)) return null;\n  {\n    var n = !0;\n    if (n = Array.isArray(e) ? e.every(e => isHTTPScheme$1(e)) : isHTTPScheme$1(e), n) return http$1(e, t);\n  }\n  return null;\n};\n\nfunction http$1(e, t) {\n  return new HTTPRequest$1(e, t);\n}\n\nfunction browserHTTPRequest$1(e, t) {\n  return http$1(e, t);\n}\n\nfunction matMul_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor$1(e, \"a\", \"matMul\"),\n      s = convertToTensor$1(t, \"b\", \"matMul\");\n  return [a, s] = makeTypesMatch$1(a, s), ENGINE$1.runKernel(BatchMatMul$1, {\n    a,\n    b: s\n  }, {\n    transposeA: n,\n    transposeB: r\n  });\n}\n\nIORouterRegistry$1.registerSaveRouter(httpRouter$1), IORouterRegistry$1.registerLoadRouter(httpRouter$1);\nvar matMul$3 = op$1({\n  matMul_: matMul_$1\n});\n\nfunction oneHot_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  if (t < 2) throw new Error(\"Error in oneHot: depth must be >=2, but it is \".concat(t));\n  var a = convertToTensor$1(e, \"indices\", \"oneHot\", \"int32\");\n  return ENGINE$1.runKernel(OneHot$1, {\n    indices: a\n  }, {\n    depth: t,\n    onValue: n,\n    offValue: r\n  });\n}\n\nvar oneHot$5 = op$1({\n  oneHot_: oneHot_$1\n});\n\nfunction transpose_$1(e, t) {\n  var n = convertToTensor$1(e, \"x\", \"transpose\");\n  return null == t && (t = n.shape.map((e, t) => t).reverse()), assert$6(n.rank === t.length, () => \"Error in transpose: rank of input \".concat(n.rank, \" must match length of perm \").concat(t, \".\")), t.forEach(e => {\n    assert$6(e >= 0 && e < n.rank, () => \"All entries in 'perm' must be between 0 and \" + (n.rank - 1) + \" but got \".concat(t));\n  }), n.rank <= 1 ? n.clone() : ENGINE$1.runKernel(Transpose$1, {\n    x: n\n  }, {\n    perm: t\n  });\n}\n\nvar transpose$5 = op$1({\n  transpose_: transpose_$1\n});\n\nfunction tensor3d$1(e, t, n) {\n  if (assertNonNull$1(e), null != t && 3 !== t.length) throw new Error(\"tensor3d() requires shape to have three numbers\");\n  var r = inferShape$1(e, n);\n  if (3 !== r.length && 1 !== r.length) throw new Error(\"tensor3d() requires values to be number[][][] or flat/TypedArray\");\n  if (1 === r.length && null == t) throw new Error(\"tensor3d() requires shape to be provided when `values` are a flat array\");\n  return makeTensor$1(e, t, r, n);\n}\n\nfunction prepareAndValidate$1(e, t) {\n  var n = e.shape.length,\n      r = t.shape.length;\n  if (n < 1) throw new Error(\"tf.gatherND() expects the input to be rank 1 or higher, but the rank was \".concat(n, \".\"));\n  if (r < 1) throw new Error(\"tf.gatherND() expects the indices to be rank 1 or higher, but the rank was \".concat(r, \".\"));\n  if (\"int32\" !== t.dtype) throw new Error(\"tf.gatherND() expects the indices to be int32 type, but the dtype was \".concat(t.dtype, \".\"));\n  if (t.shape[r - 1] > n) throw new Error(\"index innermost dimension length must be <= tensor rank; saw: \".concat(t.shape[r - 1], \" vs. \").concat(n));\n  if (0 === sizeFromShape$1(e.shape)) throw new Error(\"Requested more than 0 entries, but input is empty. Input shape: \".concat(e.shape, \".\"));\n  var a = t.shape,\n      s = a[a.length - 1];\n  var o = 1;\n\n  for (var _e35 = 0; _e35 < a.length - 1; ++_e35) {\n    o *= a[_e35];\n  }\n\n  var i = e.shape,\n      l = a.slice();\n  l.pop();\n  var u = 1;\n\n  for (var _e36 = s; _e36 < n; ++_e36) {\n    u *= i[_e36], l.push(i[_e36]);\n  }\n\n  var c = [...computeStrides$1(e.shape).map(e => e / u), 1].slice(0, s);\n  return [l, o, u, c];\n}\n\nfunction validateUpdateShape$1(e, t, n) {\n  var r = t.rank > 1 ? t.shape[t.rank - 1] : 1,\n      a = t.rank > 1 ? t.rank - 1 : 1,\n      s = \"Must have updates.shape = indices.shape[:batchDim] + shape[sliceDim:], got updates.shape: \".concat(n.shape, \", indices.shape: \").concat(t.shape, \", shape: \").concat(e, \", sliceDim: \").concat(r, \", and batchDim: \").concat(a, \".\");\n  if (n.rank < a) throw new Error(s + \" update.rank < \".concat(a, \". \"));\n  if (e.length < r + (n.rank - a)) throw new Error(s + \" Output shape length < \".concat(r + (n.rank - a)));\n  if (n.rank !== a + e.length - r) throw new Error(s + \" update.rank != \" + (a + e.length - r));\n\n  for (var _e37 = 0; _e37 < a; ++_e37) {\n    if (n.shape[_e37] !== t.shape[_e37]) throw new Error(s + \" updates.shape[\".concat(_e37, \"] (\").concat(n.shape[_e37], \") != indices.shape[\").concat(_e37, \"] (\").concat(t.shape[_e37], \").\"));\n  }\n\n  for (var _t42 = 0; _t42 < n.rank - a; ++_t42) {\n    if (n.shape[_t42 + a] !== e[_t42 + r]) throw new Error(s + \" updates.shape[\".concat(_t42 + a, \"] (\").concat(n.shape[_t42 + a], \") != shape[\").concat(_t42 + a, \"] (\").concat(e[_t42 + a], \")\"));\n  }\n}\n\nfunction validateInput$2(e, t, n) {\n  if (t.rank < 1) throw new Error(\"tf.scatterND() expects the indices to be rank 1 or higher, but the rank was \".concat(t.rank, \".\"));\n  if (e.rank < 1) throw new Error(\"tf.scatterND() expects the updates to be rank 1 or higher, but the rank was \".concat(e.rank, \".\"));\n  if (\"int32\" !== t.dtype) throw new Error(\"The dtype of 'indices' should be int32, but got dtype: \".concat(t.dtype));\n  if (n.length < 1) throw new Error(\"Output rank must be greater or equal to 1, but got shape: \".concat(n));\n\n  if (0 === n.length) {\n    if (0 === t.size) throw new Error(\"Indices specified for empty output. indices shape: \".concat(t.shape));\n    if (0 === e.size) throw new Error(\"Updates specified for empty output. updates shape: \".concat(e.shape));\n  }\n\n  validateUpdateShape$1(n, t, e);\n}\n\nfunction calculateShapes$1(e, t, n) {\n  var r = t.shape.length,\n      a = r > 1 ? t.shape[r - 1] : 1,\n      s = n.length;\n  var o = 1;\n\n  for (var _e38 = a; _e38 < s; ++_e38) {\n    o *= n[_e38];\n  }\n\n  var i = a < 1 ? 1 : a;\n  return {\n    sliceRank: a,\n    numUpdates: sizeFromShape$1(t.shape) / i,\n    sliceSize: o,\n    strides: [...computeStrides$1(n.slice(0, a)), 1],\n    outputSize: sizeFromShape$1(n)\n  };\n}\n\nfunction assertParamsValid$1(e, t, n) {\n  var r = e.shape.length;\n  assert$6(r === t.length, () => \"Error in slice\".concat(r, \"D: Length of begin \").concat(t, \" must match the rank of the array (\").concat(r, \").\")), assert$6(r === n.length, () => \"Error in slice\".concat(r, \"D: Length of size \").concat(n, \" must match the rank of the array (\").concat(r, \").\"));\n\n  var _loop4 = function _loop4(a) {\n    assert$6(t[a] + n[a] <= e.shape[a], () => \"Error in slice\".concat(r, \"D: begin[\").concat(a, \"] + size[\").concat(a, \"] (\").concat(t[a] + n[a], \") would overflow input.shape[\").concat(a, \"] (\").concat(e.shape[a], \")\"));\n  };\n\n  for (var a = 0; a < r; ++a) {\n    _loop4(a);\n  }\n}\n\nfunction maskToAxes$1(e) {\n  var t = [];\n  var n = 0;\n\n  for (; e > 0;) {\n    1 & e && t.push(n), e /= 2, n++;\n  }\n\n  return t;\n}\n\nfunction computeOutShape$5(e, t, n) {\n  var r = [];\n\n  for (var a = 0; a < e.length; a++) {\n    r[a] = Math.ceil((t[a] - e[a]) / n[a]);\n  }\n\n  return r;\n}\n\nfunction stridesWithElidedDims$1(e, t, n, r) {\n  var a = [...e];\n\n  for (var _e39 = a.length; _e39 < r.length; _e39++) {\n    a.push(1);\n  }\n\n  for (var _e40 = 0; _e40 < n; _e40++) {\n    0 === _e40 ? a[t] = 1 : (a.splice(t, 0, 1), a.pop());\n  }\n\n  return a;\n}\n\nfunction unnormalizeAxis$1(e, t, n) {\n  return n <= e ? n : n - (t - 1);\n}\n\nfunction getElidedAxes$1(e, t) {\n  var n = [];\n\n  for (var r = 0; r < e; r++) {\n    n.push(t + r);\n  }\n\n  return n;\n}\n\nfunction getNormalizedAxes$1(e, t, n, r, a, s, o, i, l) {\n  var u = e.length;\n  var c = new Array(u),\n      p = new Array(u),\n      d = new Array(u);\n\n  if (t.length && n > 0) {\n    var _l2 = t[0],\n        _u = n + 1;\n\n    c = startIndicesWithElidedDims$1(o, _l2, _u, r, e), p = stopIndicesWithElidedDims$1(i, _l2, _u, a, e), d = stridesWithElidedDims$1(s, _l2, _u, e);\n  } else for (var _t43 = 0; _t43 < u; _t43++) {\n    c[_t43] = startForAxis$1(o, r, s, e, _t43, l), p[_t43] = stopForAxis$1(i, a, s, e, _t43, l), d[_t43] = stridesForAxis$1(s, _t43, l);\n  }\n\n  return {\n    begin: c,\n    end: p,\n    strides: d\n  };\n}\n\nfunction startIndicesWithElidedDims$1(e, t, n, r, a) {\n  var s = [...a],\n      o = getElidedAxes$1(n, t);\n\n  for (var _a7 = 0; _a7 < s.length; _a7++) {\n    if (o.indexOf(_a7) > -1) s[_a7] = 0;else {\n      var _o3 = unnormalizeAxis$1(t, n, _a7);\n\n      var i = r[_o3];\n      e & 1 << _o3 && (i = 0), s[_a7] = i;\n    }\n  }\n\n  return s;\n}\n\nfunction stopIndicesWithElidedDims$1(e, t, n, r, a) {\n  var s = [...a],\n      o = getElidedAxes$1(n, t);\n\n  for (var _a8 = 0; _a8 < s.length; _a8++) {\n    if (o.indexOf(_a8) > -1) s[_a8] = Number.MAX_SAFE_INTEGER;else {\n      var _o4 = unnormalizeAxis$1(t, n, _a8);\n\n      var i = r[_o4];\n      e & 1 << _o4 && (i = Number.MAX_SAFE_INTEGER), s[_a8] = i;\n    }\n  }\n\n  for (var _e41 = 0; _e41 < s.length; _e41++) {\n    var _t44 = a[_e41];\n    s[_e41] < 0 && (s[_e41] += _t44), s[_e41] = clamp$1(0, s[_e41], a[_e41]);\n  }\n\n  return s;\n}\n\nfunction stridesForAxis$1(e, t, n) {\n  var r = e[t];\n  return (n & 1 << t || null == r) && (r = 1), r;\n}\n\nfunction startForAxis$1(e, t, n, r, a, s) {\n  var o = t[a];\n  (e & 1 << a || s & 1 << a || null == o) && (o = (n[a] || 1) > 0 ? Number.MIN_SAFE_INTEGER : Number.MAX_SAFE_INTEGER);\n  var i = r[a];\n  return o < 0 && (o += i), o = clamp$1(0, o, i - 1), o;\n}\n\nfunction stopForAxis$1(e, t, n, r, a, s) {\n  var o = t[a];\n  var i = n[a] || 1;\n  (e & 1 << a || s & 1 << a || null == o) && (o = i > 0 ? Number.MAX_SAFE_INTEGER : Number.MIN_SAFE_INTEGER);\n  var l = r[a];\n  return o < 0 && (o += l), o = i > 0 ? clamp$1(0, o, l) : clamp$1(-1, o, l - 1), o;\n}\n\nfunction isSliceContinous$1(e, t, n) {\n  var r = n.length;\n\n  for (var _e42 = 0; _e42 < n.length; _e42++) {\n    if (n[_e42] > 1) {\n      r = _e42;\n      break;\n    }\n  }\n\n  for (var a = r + 1; a < n.length; a++) {\n    if (t[a] > 0 || n[a] !== e[a]) return !1;\n  }\n\n  return !0;\n}\n\nfunction computeFlatOffset$1(e, t) {\n  var n = e.length > 0 ? e[e.length - 1] : 1;\n\n  for (var r = 0; r < e.length - 1; r++) {\n    n += e[r] * t[r];\n  }\n\n  return n;\n}\n\nfunction parseSliceParams$1(e, t, n) {\n  var r;\n  var a = e.shape.length;\n  var s;\n  return r = \"number\" == typeof t ? [t, ...new Array(a - 1).fill(0)] : t.length < a ? t.concat(new Array(a - t.length).fill(0)) : t.slice(), r.forEach(e => {\n    assert$6(-1 !== e, () => \"slice() does not support negative begin indexing.\");\n  }), s = null == n ? new Array(a).fill(-1) : \"number\" == typeof n ? [n, ...new Array(a - 1).fill(-1)] : n.length < a ? n.concat(new Array(a - n.length).fill(-1)) : n, s = s.map((t, n) => t >= 0 ? t : (assert$6(-1 === t, () => \"Negative size values should be exactly -1 but got \".concat(t, \" for the slice() size at index \").concat(n, \".\")), e.shape[n] - r[n])), [r, s];\n}\n\nfunction sliceInfo$1(e, t, n, r, a, s, o, i, l) {\n  var u = t.slice(),\n      c = n.slice(),\n      p = r;\n  null == r && (p = new Array(u.length));\n  var d = maskToAxes$1(o);\n  if (d.length > 1) throw new Error(\"Multiple ellipses in slice is not allowed.\");\n  if (0 !== o && 0 !== i) throw new Error(\"Using both ellipsisMask and newAxisMask is not yet supported.\");\n  if (0 !== o && 0 !== l) throw new Error(\"Using both ellipsisMask and shrinkAxisMask is not yet supported.\");\n  var h = e.length - u.length,\n      m = maskToAxes$1(i),\n      f = e.slice();\n  m.forEach(e => {\n    u[e] = 0, c[e] = 1, f.splice(e, 0, 1);\n  });\n  var {\n    begin: g,\n    end: $,\n    strides: y\n  } = getNormalizedAxes$1(f, d, h, u, c, p, a, s, o);\n  u = g, c = $, p = y;\n  var b = maskToAxes$1(l);\n  b.forEach(e => {\n    c[e] = u[e] + 1, p[e] = 1;\n  });\n  var x = computeOutShape$5(u, c, p),\n      v = x.filter((e, t) => -1 === b.indexOf(t));\n  return {\n    nonStrided: p.every(e => 1 === e),\n    $begin: u,\n    $end: c,\n    $strides: p,\n    size: x,\n    newShape: f,\n    outShape: v\n  };\n}\n\nvar slice_util$1 = {\n  __proto__: null,\n  assertParamsValid: assertParamsValid$1,\n  maskToAxes: maskToAxes$1,\n  computeOutShape: computeOutShape$5,\n  stridesWithElidedDims: stridesWithElidedDims$1,\n  getNormalizedAxes: getNormalizedAxes$1,\n  startIndicesWithElidedDims: startIndicesWithElidedDims$1,\n  stopIndicesWithElidedDims: stopIndicesWithElidedDims$1,\n  stridesForAxis: stridesForAxis$1,\n  startForAxis: startForAxis$1,\n  stopForAxis: stopForAxis$1,\n  isSliceContinous: isSliceContinous$1,\n  computeFlatOffset: computeFlatOffset$1,\n  parseSliceParams: parseSliceParams$1,\n  sliceInfo: sliceInfo$1\n};\n\nclass Serializable$1 {\n  getClassName() {\n    return this.constructor.className;\n  }\n\n  static fromConfig(e, t) {\n    return new e(t);\n  }\n\n}\n\nclass SerializationMap$1 {\n  constructor() {\n    this.classNameMap = {};\n  }\n\n  static getMap() {\n    return null == SerializationMap$1.instance && (SerializationMap$1.instance = new SerializationMap$1()), SerializationMap$1.instance;\n  }\n\n  static register(e) {\n    SerializationMap$1.getMap().classNameMap[e.className] = [e, e.fromConfig];\n  }\n\n}\n\nfunction registerClass$1(e) {\n  assert$6(null != e.className, () => \"Class being registered does not have the static className property defined.\"), assert$6(\"string\" == typeof e.className, () => \"className is required to be a string, but got type \" + typeof e.className), assert$6(e.className.length > 0, () => \"Class being registered has an empty-string as its className, which is disallowed.\"), SerializationMap$1.register(e);\n}\n\nvar version$e = \"3.8.0\";\n\nfunction engine$1() {\n  return ENGINE$1;\n}\n\nfunction memory$1() {\n  return ENGINE$1.memory();\n}\n\nfunction tidy$1(e, t) {\n  return ENGINE$1.tidy(e, t);\n}\n\nfunction dispose$1(e) {\n  getTensorsInContainer$1(e).forEach(e => e.dispose());\n}\n\nfunction keep$1(e) {\n  return ENGINE$1.keep(e);\n}\n\nfunction registerBackend$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  return ENGINE$1.registerBackend(e, t, n);\n}\n\nfunction backend$1() {\n  return ENGINE$1.backend;\n}\n\nfunction add_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"add\"),\n      r = convertToTensor$1(t, \"b\", \"add\");\n  return [n, r] = makeTypesMatch$1(n, r), ENGINE$1.runKernel(Add$3, {\n    a: n,\n    b: r\n  });\n}\n\nvar add$5 = op$1({\n  add_: add_$1\n});\n\nfunction floorDiv_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"floorDiv\"),\n      r = convertToTensor$1(t, \"b\", \"floorDiv\");\n  return [n, r] = makeTypesMatch$1(n, r), ENGINE$1.runKernel(FloorDiv$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar floorDiv$5 = op$1({\n  floorDiv_: floorDiv_$1\n});\n\nfunction div_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"div\"),\n      r = convertToTensor$1(t, \"b\", \"div\");\n  return [n, r] = makeTypesMatch$1(n, r), \"int32\" === n.dtype && \"int32\" === r.dtype ? floorDiv$5(n, r) : ENGINE$1.runKernel(RealDiv$1, {\n    a: n,\n    b: r\n  }, {});\n}\n\nvar div$3 = op$1({\n  div_: div_$1\n});\n\nfunction mul_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"mul\"),\n      r = convertToTensor$1(t, \"b\", \"mul\");\n  return [n, r] = makeTypesMatch$1(n, r), ENGINE$1.runKernel(Multiply$3, {\n    a: n,\n    b: r\n  });\n}\n\nvar mul$1 = op$1({\n  mul_: mul_$1\n});\n\nfunction abs_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"abs\");\n  return ENGINE$1.runKernel(\"complex64\" === t.dtype ? ComplexAbs$1 : Abs$1, {\n    x: t\n  });\n}\n\nvar abs$5 = op$1({\n  abs_: abs_$1\n});\n\nfunction acos_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"acos\");\n  return ENGINE$1.runKernel(Acos$1, {\n    x: t\n  });\n}\n\nvar acos$5 = op$1({\n  acos_: acos_$1\n});\n\nfunction acosh_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"acosh\");\n  return ENGINE$1.runKernel(Acosh$1, {\n    x: t\n  });\n}\n\nvar acosh$5 = op$1({\n  acosh_: acosh_$1\n});\n\nfunction all_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"all\", \"bool\");\n  return ENGINE$1.runKernel(All$1, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar all$5 = op$1({\n  all_: all_$1\n});\n\nfunction any_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"any\", \"bool\");\n  return ENGINE$1.runKernel(Any$1, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar any$5 = op$1({\n  any_: any_$1\n});\n\nfunction argMax_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor$1(e, \"x\", \"argMax\");\n  return ENGINE$1.runKernel(ArgMax$1, {\n    x: n\n  }, {\n    axis: t\n  });\n}\n\nvar argMax$5 = op$1({\n  argMax_: argMax_$1\n});\n\nfunction argMin_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor$1(e, \"x\", \"argMin\");\n  return ENGINE$1.runKernel(ArgMin$1, {\n    x: n\n  }, {\n    axis: t\n  });\n}\n\nvar argMin$5 = op$1({\n  argMin_: argMin_$1\n});\n\nfunction asin_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"asin\");\n  return ENGINE$1.runKernel(Asin$1, {\n    x: t\n  });\n}\n\nvar asin$5 = op$1({\n  asin_: asin_$1\n});\n\nfunction asinh_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"asinh\");\n  return ENGINE$1.runKernel(Asinh$1, {\n    x: t\n  });\n}\n\nvar asinh$5 = op$1({\n  asinh_: asinh_$1\n});\n\nfunction atan_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"atan\");\n  return ENGINE$1.runKernel(Atan$1, {\n    x: t\n  });\n}\n\nvar atan$5 = op$1({\n  atan_: atan_$1\n});\n\nfunction atan2_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"atan2\"),\n      r = convertToTensor$1(t, \"b\", \"atan2\");\n  return [n, r] = makeTypesMatch$1(n, r), ENGINE$1.runKernel(Atan2$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar atan2$5 = op$1({\n  atan2_: atan2_$1\n});\n\nfunction atanh_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"atanh\");\n  return ENGINE$1.runKernel(Atanh$1, {\n    x: t\n  });\n}\n\nvar atanh$5 = op$1({\n  atanh_: atanh_$1\n});\n\nfunction computeDilation2DInfo$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NHWC\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  return computeConv2DInfo$1(e, [...t, e[3]], n, s, r, null, null, convertConv2DDataFormat$1(a));\n}\n\nfunction computePool2DInfo$1(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"channelsLast\";\n  var [i, l] = parseTupleParam$1(t);\n  var u;\n  if (\"channelsLast\" === o) u = [i, l, e[3], e[3]];else {\n    if (\"channelsFirst\" !== o) throw new Error(\"Unknown dataFormat \".concat(o));\n    u = [i, l, e[1], e[1]];\n  }\n  return computeConv2DInfo$1(e, u, n, r, a, s, !1, o);\n}\n\nfunction computePool3DInfo$1(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"NDHWC\";\n  var [i, l, u] = parse3TupleParam$1(t);\n  var c, p;\n  if (\"NDHWC\" === o) p = \"channelsLast\", c = [i, l, u, e[4], e[4]];else {\n    if (\"NCDHW\" !== o) throw new Error(\"Unknown dataFormat \".concat(o));\n    p = \"channelsFirst\", c = [i, l, u, e[1], e[1]];\n  }\n  return computeConv3DInfo$1(e, c, n, r, a, !1, p, s);\n}\n\nfunction computeConv2DInfo$1(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !1;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : \"channelsLast\";\n  var [l, u, c, p] = [-1, -1, -1, -1];\n  if (\"channelsLast\" === i) [l, u, c, p] = e;else {\n    if (\"channelsFirst\" !== i) throw new Error(\"Unknown dataFormat \".concat(i));\n    [l, p, u, c] = e;\n  }\n  var [d, h,, m] = t,\n      [f, g] = parseTupleParam$1(n),\n      [$, y] = parseTupleParam$1(r),\n      b = getEffectiveFilterSize$1(d, $),\n      x = getEffectiveFilterSize$1(h, y),\n      {\n    padInfo: v,\n    outHeight: I,\n    outWidth: C\n  } = getPadAndOutInfo$1(a, u, c, f, g, b, x, s, i),\n      S = o ? m * p : m;\n  var k;\n  return \"channelsFirst\" === i ? k = [l, S, I, C] : \"channelsLast\" === i && (k = [l, I, C, S]), {\n    batchSize: l,\n    dataFormat: i,\n    inHeight: u,\n    inWidth: c,\n    inChannels: p,\n    outHeight: I,\n    outWidth: C,\n    outChannels: S,\n    padInfo: v,\n    strideHeight: f,\n    strideWidth: g,\n    filterHeight: d,\n    filterWidth: h,\n    effectiveFilterHeight: b,\n    effectiveFilterWidth: x,\n    dilationHeight: $,\n    dilationWidth: y,\n    inShape: e,\n    outShape: k,\n    filterShape: t\n  };\n}\n\nfunction computeConv3DInfo$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"channelsLast\";\n  var i = arguments.length > 7 ? arguments[7] : undefined;\n  var [l, u, c, p, d] = [-1, -1, -1, -1, -1];\n  if (\"channelsLast\" === o) [l, u, c, p, d] = e;else {\n    if (\"channelsFirst\" !== o) throw new Error(\"Unknown dataFormat \".concat(o));\n    [l, d, u, c, p] = e;\n  }\n  var [h, m, f,, g] = t,\n      [$, y, b] = parse3TupleParam$1(n),\n      [x, v, I] = parse3TupleParam$1(r),\n      C = getEffectiveFilterSize$1(h, x),\n      S = getEffectiveFilterSize$1(m, v),\n      k = getEffectiveFilterSize$1(f, I),\n      {\n    padInfo: T,\n    outDepth: N,\n    outHeight: w,\n    outWidth: E\n  } = get3DPadAndOutInfo$1(a, u, c, p, $, y, b, C, S, k, i),\n      A = s ? g * d : g;\n  var D;\n  return \"channelsFirst\" === o ? D = [l, A, N, w, E] : \"channelsLast\" === o && (D = [l, N, w, E, A]), {\n    batchSize: l,\n    dataFormat: o,\n    inDepth: u,\n    inHeight: c,\n    inWidth: p,\n    inChannels: d,\n    outDepth: N,\n    outHeight: w,\n    outWidth: E,\n    outChannels: A,\n    padInfo: T,\n    strideDepth: $,\n    strideHeight: y,\n    strideWidth: b,\n    filterDepth: h,\n    filterHeight: m,\n    filterWidth: f,\n    effectiveFilterDepth: C,\n    effectiveFilterHeight: S,\n    effectiveFilterWidth: k,\n    dilationDepth: x,\n    dilationHeight: v,\n    dilationWidth: I,\n    inShape: e,\n    outShape: D,\n    filterShape: t\n  };\n}\n\nfunction computeOutputShape2D$1(e, t, n, r, a) {\n  null == r && (r = computeDefaultPad$1(e, t, n));\n  var s = e[1];\n  return [round$7((e[0] - t + 2 * r) / n + 1, a), round$7((s - t + 2 * r) / n + 1, a)];\n}\n\nfunction computeOutputShape4D$1(e, t, n, r, a, s) {\n  null == a && (a = computeDefaultPad$1(e, t, r));\n  var o = e[1],\n      i = e[2];\n  return [round$7((e[0] - t + 2 * a) / r + 1, s), round$7((o - t + 2 * a) / r + 1, s), round$7((i - t + 2 * a) / r + 1, s), n];\n}\n\nfunction computeDefaultPad$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = getEffectiveFilterSize$1(t, r);\n  return Math.floor((e[0] * (n - 1) - n + a) / 2);\n}\n\nfunction parseTupleParam$1(e) {\n  return \"number\" == typeof e ? [e, e, e] : 2 === e.length ? [e[0], e[1], 1] : e;\n}\n\nfunction parse3TupleParam$1(e) {\n  return \"number\" == typeof e ? [e, e, e] : e;\n}\n\nfunction getEffectiveFilterSize$1(e, t) {\n  return t <= 1 ? e : e + (e - 1) * (t - 1);\n}\n\nfunction getPadAndOutInfo$1(e, t, n, r, a, s, o, i, l) {\n  var u, c, p;\n\n  if (\"number\" == typeof e) {\n    u = {\n      top: e,\n      bottom: e,\n      left: e,\n      right: e,\n      type: 0 === e ? \"VALID\" : \"NUMBER\"\n    };\n\n    var _a9 = computeOutputShape2D$1([t, n], s, r, e, i);\n\n    c = _a9[0], p = _a9[1];\n  } else if (\"same\" === e) {\n    c = Math.ceil(t / r), p = Math.ceil(n / a);\n\n    var _e43 = Math.max(0, (c - 1) * r + s - t),\n        _i3 = Math.max(0, (p - 1) * a + o - n),\n        _l3 = Math.floor(_e43 / 2),\n        d = _e43 - _l3,\n        h = Math.floor(_i3 / 2);\n\n    u = {\n      top: _l3,\n      bottom: d,\n      left: h,\n      right: _i3 - h,\n      type: \"SAME\"\n    };\n  } else if (\"valid\" === e) u = {\n    top: 0,\n    bottom: 0,\n    left: 0,\n    right: 0,\n    type: \"VALID\"\n  }, c = Math.ceil((t - s + 1) / r), p = Math.ceil((n - o + 1) / a);else {\n    if (\"object\" != typeof e) throw Error(\"Unknown padding parameter: \".concat(e));\n    {\n      var _d = \"channelsLast\" === l ? e[1][0] : e[2][0],\n          _h = \"channelsLast\" === l ? e[1][1] : e[2][1],\n          m = \"channelsLast\" === l ? e[2][0] : e[3][0],\n          f = \"channelsLast\" === l ? e[2][1] : e[3][1];\n\n      u = {\n        top: _d,\n        bottom: _h,\n        left: m,\n        right: f,\n        type: 0 === _d && 0 === _h && 0 === m && 0 === f ? \"VALID\" : \"EXPLICIT\"\n      }, c = round$7((t - s + _d + _h) / r + 1, i), p = round$7((n - o + m + f) / a + 1, i);\n    }\n  }\n\n  return {\n    padInfo: u,\n    outHeight: c,\n    outWidth: p\n  };\n}\n\nfunction get3DPadAndOutInfo$1(e, t, n, r, a, s, o, i, l, u, c) {\n  var p, d, h, m;\n\n  if (\"number\" == typeof e) {\n    p = {\n      top: e,\n      bottom: e,\n      left: e,\n      right: e,\n      front: e,\n      back: e,\n      type: 0 === e ? \"VALID\" : \"NUMBER\"\n    };\n\n    var _s6 = computeOutputShape4D$1([t, n, r, 1], i, 1, a, e, c);\n\n    d = _s6[0], h = _s6[1], m = _s6[2];\n  } else if (\"same\" === e) {\n    d = Math.ceil(t / a), h = Math.ceil(n / s), m = Math.ceil(r / o);\n\n    var _e44 = (d - 1) * a + i - t,\n        _c = (h - 1) * s + l - n,\n        f = (m - 1) * o + u - r,\n        g = Math.floor(_e44 / 2),\n        $ = _e44 - g,\n        y = Math.floor(_c / 2),\n        b = _c - y,\n        x = Math.floor(f / 2);\n\n    p = {\n      top: y,\n      bottom: b,\n      left: x,\n      right: f - x,\n      front: g,\n      back: $,\n      type: \"SAME\"\n    };\n  } else {\n    if (\"valid\" !== e) throw Error(\"Unknown padding parameter: \".concat(e));\n    p = {\n      top: 0,\n      bottom: 0,\n      left: 0,\n      right: 0,\n      front: 0,\n      back: 0,\n      type: \"VALID\"\n    }, d = Math.ceil((t - i + 1) / a), h = Math.ceil((n - l + 1) / s), m = Math.ceil((r - u + 1) / o);\n  }\n\n  return {\n    padInfo: p,\n    outDepth: d,\n    outHeight: h,\n    outWidth: m\n  };\n}\n\nfunction round$7(e, t) {\n  if (!t) return Math.trunc(e);\n\n  switch (t) {\n    case \"round\":\n      return Math.round(e);\n\n    case \"ceil\":\n      return Math.ceil(e);\n\n    case \"floor\":\n      return Math.floor(e);\n\n    default:\n      throw new Error(\"Unknown roundingMode \".concat(t));\n  }\n}\n\nfunction tupleValuesAreOne$1(e) {\n  var [t, n, r] = parseTupleParam$1(e);\n  return 1 === t && 1 === n && 1 === r;\n}\n\nfunction eitherStridesOrDilationsAreOne$1(e, t) {\n  return tupleValuesAreOne$1(e) || tupleValuesAreOne$1(t);\n}\n\nfunction convertConv2DDataFormat$1(e) {\n  if (\"NHWC\" === e) return \"channelsLast\";\n  if (\"NCHW\" === e) return \"channelsFirst\";\n  throw new Error(\"Unknown dataFormat \".concat(e));\n}\n\nfunction reshape_$1(e, t) {\n  var n = convertToTensor$1(e, \"x\", \"reshape\", \"string_or_numeric\");\n  return ENGINE$1.runKernel(Reshape$3, {\n    x: n\n  }, {\n    shape: t\n  });\n}\n\nvar reshape$6 = op$1({\n  reshape_: reshape_$1\n});\n\nfunction avgPool_$1(e, t, n, r, a) {\n  var s = convertToTensor$1(e, \"x\", \"avgPool\", \"float32\");\n  assert$6(eitherStridesOrDilationsAreOne$1(n, 1), () => \"Error in avgPool: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '1'\"));\n  var o = s,\n      i = !1;\n  3 === s.rank && (i = !0, o = reshape$6(s, [1, s.shape[0], s.shape[1], s.shape[2]])), assert$6(4 === o.rank, () => \"Error in avgPool: x must be rank 4 but got rank \".concat(o.rank, \".\")), null != a && assert$6(isInt$1(r), () => \"Error in avgPool: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var l = ENGINE$1.runKernel(AvgPool$1, {\n    x: o\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a\n  });\n  return l = cast$7(l, s.dtype), i ? reshape$6(l, [l.shape[1], l.shape[2], l.shape[3]]) : l;\n}\n\nvar avgPool$5 = op$1({\n  avgPool_: avgPool_$1\n});\n\nfunction avgPool3d_$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NDHWC\";\n  var o = convertToTensor$1(e, \"x\", \"avgPool3d\", \"float32\");\n  var i = o,\n      l = !1;\n  4 === o.rank && (l = !0, i = reshape$6(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]])), assert$6(5 === i.rank, () => \"Error in avgPool3d: x must be rank 5 but got rank \".concat(i.rank, \".\")), assert$6(\"NDHWC\" === s, () => \"Error in avgPool3d: Only NDHWC is currently supported, but got dataFormat of \".concat(s)), null != a && assert$6(isInt$1(r), () => \"Error in avgPool3d: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var u = ENGINE$1.runKernel(AvgPool3D$1, {\n    x: i\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a,\n    dataFormat: s\n  });\n  return u = cast$7(u, i.dtype), l ? reshape$6(u, [u.shape[1], u.shape[2], u.shape[3], u.shape[4]]) : u;\n}\n\nvar avgPool3d$2 = op$1({\n  avgPool3d_: avgPool3d_$1\n});\n\nfunction concat_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  assert$6(e.length >= 1, () => \"Pass at least one tensor to concat\");\n  var n = convertToTensorArray$1(e, \"tensors\", \"concat\", \"string_or_numeric\");\n  return \"complex64\" === n[0].dtype && n.forEach(e => {\n    if (\"complex64\" !== e.dtype) throw new Error(\"Cannot concatenate complex64 tensors with a tensor\\n          with dtype \".concat(e.dtype, \". \"));\n  }), 1 === n.length ? clone$1(n[0]) : ENGINE$1.runKernel(Concat$1, n, {\n    axis: t\n  });\n}\n\nvar concat$5 = op$1({\n  concat_: concat_$1\n});\n\nfunction sigmoid_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"sigmoid\");\n  return ENGINE$1.runKernel(Sigmoid$3, {\n    x: t\n  });\n}\n\nvar sigmoid$5 = op$1({\n  sigmoid_: sigmoid_$1\n});\n\nfunction slice_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"slice\", \"string_or_numeric\");\n  if (0 === r.rank) throw new Error(\"Slicing scalar is not possible\");\n  return ENGINE$1.runKernel(Slice$1, {\n    x: r\n  }, {\n    begin: t,\n    size: n\n  });\n}\n\nvar slice$5 = op$1({\n  slice_: slice_$1\n});\n\nfunction tanh_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"tanh\");\n  return ENGINE$1.runKernel(Tanh$3, {\n    x: t\n  });\n}\n\nvar tanh$6 = op$1({\n  tanh_: tanh_$1\n});\n\nfunction batchToSpaceND_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"batchToSpaceND\"),\n      a = t.reduce((e, t) => e * t);\n  return assert$6(r.rank >= 1 + t.length, () => \"input rank is \".concat(r.rank, \" but should be > than blockShape.length \").concat(t.length)), assert$6(n.length === t.length, () => \"crops.length is \".concat(n.length, \" but should be equal to blockShape.length  \").concat(t.length)), assert$6(r.shape[0] % a == 0, () => \"input tensor batch is \".concat(r.shape[0], \" but is not divisible by the product of the elements of blockShape \").concat(t.join(\" * \"), \" === \").concat(a)), ENGINE$1.runKernel(BatchToSpaceND$1, {\n    x: r\n  }, {\n    blockShape: t,\n    crops: n\n  });\n}\n\nvar batchToSpaceND$5 = op$1({\n  batchToSpaceND_: batchToSpaceND_$1\n});\n\nfunction xAs4D$1(e) {\n  var t;\n  return t = 0 === e.rank || 1 === e.rank ? reshape$6(e, [1, 1, 1, e.size]) : 2 === e.rank ? reshape$6(e, [1, 1, e.shape[0], e.shape[1]]) : 3 === e.rank ? reshape$6(e, [1, e.shape[0], e.shape[1], e.shape[2]]) : e, t;\n}\n\nfunction batchNorm_$1(e, t, n, r, a, s) {\n  null == s && (s = .001);\n  var o = convertToTensor$1(e, \"x\", \"batchNorm\"),\n      i = convertToTensor$1(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor$1(n, \"variance\", \"batchNorm\");\n  var u, c;\n  null != a && (u = convertToTensor$1(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor$1(r, \"offset\", \"batchNorm\")), assert$6(i.rank === l.rank, () => \"Batch normalization gradient requires mean and variance to have equal ranks.\"), assert$6(null == c || i.rank === c.rank, () => \"Batch normalization gradient requires mean and offset to have equal ranks.\"), assert$6(null == u || i.rank === u.rank, () => \"Batch normalization gradient requires mean and scale to have equal ranks.\");\n  var p = xAs4D$1(o),\n      d = ENGINE$1.runKernel(FusedBatchNorm$1, {\n    x: p,\n    scale: u,\n    offset: c,\n    mean: i,\n    variance: l\n  }, {\n    varianceEpsilon: s\n  });\n  return reshape$6(d, o.shape);\n}\n\nvar batchNorm$5 = op$1({\n  batchNorm_: batchNorm_$1\n});\n\nfunction batchNorm2d_$1(e, t, n, r, a, s) {\n  var o = convertToTensor$1(e, \"x\", \"batchNorm\"),\n      i = convertToTensor$1(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor$1(n, \"variance\", \"batchNorm\");\n  var u, c;\n  return null != a && (u = convertToTensor$1(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor$1(r, \"offset\", \"batchNorm\")), assert$6(2 === o.rank, () => \"Error in batchNorm2D: x must be rank 2 but got rank \".concat(o.rank, \".\")), assert$6(2 === i.rank || 1 === i.rank, () => \"Error in batchNorm2D: mean must be rank 2 or rank 1 but got rank \".concat(i.rank, \".\")), assert$6(2 === l.rank || 1 === l.rank, () => \"Error in batchNorm2D: variance must be rank 2 or rank 1 but got rank \".concat(l.rank, \".\")), null != u && assert$6(2 === u.rank || 1 === u.rank, () => \"Error in batchNorm2D: scale must be rank 2 or rank 1 but got rank \".concat(u.rank, \".\")), null != c && assert$6(2 === c.rank || 1 === c.rank, () => \"Error in batchNorm2D: offset must be rank 2 or rank 1 but got rank \".concat(c.rank, \".\")), batchNorm$5(o, i, l, c, u, s);\n}\n\nvar batchNorm2d$1 = op$1({\n  batchNorm2d_: batchNorm2d_$1\n});\n\nfunction batchNorm3d_$1(e, t, n, r, a, s) {\n  var o = convertToTensor$1(e, \"x\", \"batchNorm\"),\n      i = convertToTensor$1(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor$1(n, \"variance\", \"batchNorm\");\n  var u, c;\n  return null != a && (u = convertToTensor$1(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor$1(r, \"offset\", \"batchNorm\")), assert$6(3 === o.rank, () => \"Error in batchNorm3D: x must be rank 3 but got rank \".concat(o.rank, \".\")), assert$6(3 === i.rank || 1 === i.rank, () => \"Error in batchNorm3D: mean must be rank 3 or rank 1 but got rank \".concat(i.rank, \".\")), assert$6(3 === l.rank || 1 === l.rank, () => \"Error in batchNorm3D: variance must be rank 3 or rank 1 but got rank \".concat(l.rank, \".\")), null != u && assert$6(3 === u.rank || 1 === u.rank, () => \"Error in batchNorm3D: scale must be rank 3 or rank 1 but got rank \".concat(u.rank, \".\")), null != c && assert$6(3 === c.rank || 1 === c.rank, () => \"Error in batchNorm3D: offset must be rank 3 or rank 1 but got rank \".concat(c.rank, \".\")), batchNorm$5(o, i, l, c, u, s);\n}\n\nvar batchNorm3d$1 = op$1({\n  batchNorm3d_: batchNorm3d_$1\n});\n\nfunction batchNorm4d_$1(e, t, n, r, a, s) {\n  var o = convertToTensor$1(e, \"x\", \"batchNorm\"),\n      i = convertToTensor$1(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor$1(n, \"variance\", \"batchNorm\");\n  var u, c;\n  return null != a && (u = convertToTensor$1(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor$1(r, \"offset\", \"batchNorm\")), assert$6(4 === o.rank, () => \"Error in batchNorm4D: x must be rank 4 but got rank \".concat(o.rank, \".\")), assert$6(4 === i.rank || 1 === i.rank, () => \"Error in batchNorm4D: mean must be rank 4 or rank 1 but got rank \".concat(i.rank, \".\")), assert$6(4 === l.rank || 1 === l.rank, () => \"Error in batchNorm4D: variance must be rank 4 or rank 1 but got rank \".concat(l.rank, \".\")), null != u && assert$6(4 === u.rank || 1 === u.rank, () => \"Error in batchNorm4D: scale must be rank 4 or rank 1 but got rank \".concat(u.rank, \".\")), null != c && assert$6(4 === c.rank || 1 === c.rank, () => \"Error in batchNorm4D: offset must be rank 4 or rank 1 but got rank \".concat(c.rank, \".\")), batchNorm$5(o, i, l, c, u, s);\n}\n\nvar batchNorm4d$1 = op$1({\n  batchNorm4d_: batchNorm4d_$1\n});\n\nfunction bincount_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"bincount\"),\n      a = convertToTensor$1(t, \"weights\", \"bincount\");\n  return assert$6(\"int32\" === r.dtype, () => \"Error in bincount: input dtype must be int32, but got \".concat(r.dtype)), assert$6(n >= 0, () => \"size must be non-negative, but got \".concat(n, \".\")), assert$6(a.size === r.size || 0 === a.size, () => \"Error in bincount: weights must have the same size as input or0-length, but got input shape: \".concat(r.shape, \", weights shape: \").concat(a.shape, \".\")), ENGINE$1.runKernel(Bincount$1, {\n    x: r,\n    weights: a\n  }, {\n    size: n\n  });\n}\n\nvar bincount$5 = op$1({\n  bincount_: bincount_$1\n});\n\nfunction broadcastTo_$1(e, t) {\n  var n = convertToTensor$1(e, \"broadcastTo\", \"x\");\n  var r = n.shape;\n  if (t.some(e => !(e > 0) || e % 1 != 0)) throw new Error(\"broadcastTo(): Invalid broadcast shape [\".concat(t, \"].\"));\n  if (t.length < n.rank) throw new Error(\"broadcastTo(): shape.length=\".concat(t.length, \" < input.rank=\").concat(n.rank, \".\"));\n\n  if (t.length > n.rank) {\n    var _e45 = n.shape.slice();\n\n    for (; _e45.length < t.length;) {\n      _e45.unshift(1);\n    }\n\n    n = reshape$6(n, _e45);\n  }\n\n  var a = n.shape,\n      s = Array.from(t);\n\n  for (var _e46 = t.length - 1; _e46 >= 0; _e46--) {\n    if (a[_e46] === t[_e46]) s[_e46] = 1;else if (1 !== n.shape[_e46]) throw new Error(\"broadcastTo(): [\".concat(r, \"] cannot be broadcast to [\").concat(t, \"].\"));\n  }\n\n  return 0 === s.map((e, t) => e > 1 ? t : -1).filter(e => e >= 0).length ? clone$1(n) : ENGINE$1.runKernel(Tile$1, {\n    x: n\n  }, {\n    reps: s\n  });\n}\n\nvar broadcastTo$1 = op$1({\n  broadcastTo_: broadcastTo_$1\n});\n\nfunction ceil_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"ceil\");\n  return ENGINE$1.runKernel(Ceil$1, {\n    x: t\n  });\n}\n\nvar ceil$5 = op$1({\n  ceil_: ceil_$1\n});\n\nfunction clipByValue_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"clipByValue\");\n  return assert$6(t <= n, () => \"Error in clip: min (\".concat(t, \") must be less than or equal to max (\").concat(n, \").\")), ENGINE$1.runKernel(ClipByValue$1, {\n    x: r\n  }, {\n    clipValueMin: t,\n    clipValueMax: n\n  });\n}\n\nvar clipByValue$3 = op$1({\n  clipByValue_: clipByValue_$1\n});\n\nfunction concat1d_$1(e) {\n  return concat$5(e, 0);\n}\n\nvar concat1d$1 = op$1({\n  concat1d_: concat1d_$1\n});\n\nfunction concat2d_$1(e, t) {\n  return concat$5(e, t);\n}\n\nvar concat2d$1 = op$1({\n  concat2d_: concat2d_$1\n});\n\nfunction concat3d_$1(e, t) {\n  return concat$5(e, t);\n}\n\nvar concat3d$1 = op$1({\n  concat3d_: concat3d_$1\n});\n\nfunction concat4d_$1(e, t) {\n  return concat$5(e, t);\n}\n\nvar concat4d$1 = op$1({\n  concat4d_: concat4d_$1\n});\n\nfunction conv2d_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NHWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = convertToTensor$1(e, \"x\", \"conv2d\"),\n      l = convertToTensor$1(t, \"filter\", \"conv2d\");\n  var u = i,\n      c = !1;\n  3 === i.rank && (c = !0, u = reshape$6(i, [1, i.shape[0], i.shape[1], i.shape[2]])), assert$6(4 === u.rank, () => \"Error in conv2d: input must be rank 4, but got rank \".concat(u.rank, \".\")), assert$6(4 === l.rank, () => \"Error in conv2d: filter must be rank 4, but got rank \".concat(l.rank, \".\")), null != o && assert$6(isInt$1(r), () => \"Error in conv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\"));\n  var p = \"NHWC\" === a ? u.shape[3] : u.shape[1];\n  assert$6(p === l.shape[2], () => \"Error in conv2d: depth of input (\".concat(p, \") must match input depth for filter \").concat(l.shape[2], \".\")), assert$6(eitherStridesOrDilationsAreOne$1(n, s), () => \"Error in conv2D: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '\").concat(s, \"'\"));\n  var d = ENGINE$1.runKernel(Conv2D$3, {\n    x: u,\n    filter: l\n  }, {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s,\n    dimRoundingMode: o\n  });\n  return c ? reshape$6(d, [d.shape[1], d.shape[2], d.shape[3]]) : d;\n}\n\nvar conv2d$7 = op$1({\n  conv2d_: conv2d_$1\n});\n\nfunction conv1d_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1;\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = convertToTensor$1(e, \"x\", \"conv1d\"),\n      l = convertToTensor$1(t, \"filter\", \"conv1d\");\n  var u = i,\n      c = !1;\n  2 === i.rank && (c = !0, u = reshape$6(i, [1, i.shape[0], i.shape[1]])), assert$6(3 === u.rank, () => \"Error in conv1d: input must be rank 3, but got rank \".concat(u.rank, \".\")), assert$6(3 === l.rank, () => \"Error in conv1d: filter must be rank 3, but got rank \".concat(l.rank, \".\")), null != o && assert$6(isInt$1(r), () => \"Error in conv1d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\")), assert$6(u.shape[2] === l.shape[1], () => \"Error in conv1d: depth of input (\".concat(u.shape[2], \") must match input depth for filter \").concat(l.shape[1], \".\")), assert$6(eitherStridesOrDilationsAreOne$1(n, s), () => \"Error in conv1D: Either stride or dilation must be 1. Got stride \".concat(n, \" and dilation '\").concat(s, \"'\")), assert$6(\"NWC\" === a, () => \"Error in conv1d: got dataFormat of \".concat(a, \" but only NWC is currently supported.\"));\n  var p = reshape$6(l, [1, l.shape[0], l.shape[1], l.shape[2]]),\n      d = reshape$6(u, [u.shape[0], 1, u.shape[1], u.shape[2]]),\n      h = conv2d$7(d, p, [1, n], r, \"NHWC\", [1, s], o);\n  return reshape$6(h, c ? [h.shape[2], h.shape[3]] : [h.shape[0], h.shape[2], h.shape[3]]);\n}\n\nvar conv1d$3 = op$1({\n  conv1d_: conv1d_$1\n});\n\nfunction conv2DBackpropInput_$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NHWC\";\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  assert$6(e.length === t.rank, () => \"Length of inShape (\".concat(e.length, \") and rank of dy (\").concat(t.rank, \") must match\"));\n  var i = e,\n      l = t,\n      u = !1;\n  3 === t.rank && (u = !0, l = reshape$6(t, [1, t.shape[0], t.shape[1], t.shape[2]]), i = [1, e[0], e[1], e[2]]), assert$6(4 === i.length, () => \"Error in conv2dDerInput: inShape must be length 4, but got length \".concat(i.length, \".\")), assert$6(4 === l.rank, () => \"Error in conv2dDerInput: dy must be rank 4, but got rank \".concat(l.rank)), assert$6(4 === n.rank, () => \"Error in conv2dDerInput: filter must be rank 4, but got rank \".concat(n.rank));\n  var c = \"NHWC\" === s ? i[3] : i[1],\n      p = \"NHWC\" === s ? l.shape[3] : l.shape[1];\n  assert$6(c === n.shape[2], () => \"Error in conv2dDerInput: depth of input (\".concat(c, \") must match input depth for filter \").concat(n.shape[2], \".\")), assert$6(p === n.shape[3], () => \"Error in conv2dDerInput: depth of output (\".concat(p, \") must match output depth for filter \").concat(n.shape[3], \".\")), null != o && assert$6(isInt$1(a), () => \"Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(a, \".\"));\n  var d = ENGINE$1.runKernel(Conv2DBackpropInput$1, {\n    dy: l,\n    filter: n\n  }, {\n    strides: r,\n    pad: a,\n    dataFormat: s,\n    dimRoundingMode: o,\n    inputShape: i\n  });\n  return u ? reshape$6(d, [d.shape[1], d.shape[2], d.shape[3]]) : d;\n}\n\nvar conv2DBackpropInput$5 = op$1({\n  conv2DBackpropInput_: conv2DBackpropInput_$1\n});\n\nfunction conv2dTranspose_$1(e, t, n, r, a, s) {\n  var o = convertToTensor$1(e, \"x\", \"conv2dTranspose\"),\n      i = convertToTensor$1(t, \"filter\", \"conv2dTranspose\");\n  return conv2DBackpropInput$5(n, o, i, r, a, \"NHWC\", s);\n}\n\nvar conv2dTranspose$2 = op$1({\n  conv2dTranspose_: conv2dTranspose_$1\n});\n\nfunction conv3d_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NDHWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1, 1];\n  var o = convertToTensor$1(e, \"x\", \"conv3d\"),\n      i = convertToTensor$1(t, \"filter\", \"conv3d\");\n  var l = o,\n      u = !1;\n  4 === o.rank && (u = !0, l = reshape$6(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]])), assert$6(5 === l.rank, () => \"Error in conv3d: input must be rank 5, but got rank \".concat(l.rank, \".\")), assert$6(5 === i.rank, () => \"Error in conv3d: filter must be rank 5, but got rank \".concat(i.rank, \".\")), assert$6(l.shape[4] === i.shape[3], () => \"Error in conv3d: depth of input (\".concat(l.shape[4], \") must match input depth for filter \").concat(i.shape[3], \".\")), assert$6(eitherStridesOrDilationsAreOne$1(n, s), () => \"Error in conv3D: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '\").concat(s, \"'\")), assert$6(\"NDHWC\" === a, () => \"Error in conv3d: got dataFormat of \".concat(a, \" but only NDHWC is currently supported.\"));\n  var c = ENGINE$1.runKernel(Conv3D$3, {\n    x: l,\n    filter: i\n  }, {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s\n  });\n  return u ? reshape$6(c, [c.shape[1], c.shape[2], c.shape[3], c.shape[4]]) : c;\n}\n\nvar conv3d$2 = op$1({\n  conv3d_: conv3d_$1\n});\n\nfunction conv3DBackpropInput_$1(e, t, n, r, a) {\n  assert$6(e.length === t.rank, () => \"Length of inShape (\".concat(e.length, \") and rank of dy (\").concat(t.rank, \") must match\"));\n  var s = e,\n      o = t,\n      i = !1;\n  4 === t.rank && (i = !0, o = reshape$6(t, [1, t.shape[0], t.shape[1], t.shape[2], t.shape[3]]), s = [1, e[0], e[1], e[2], e[3]]);\n  var l = s[4],\n      u = o.shape[4];\n  assert$6(5 === s.length, () => \"Error in conv3dDerInput: inShape must be length 5, but got length \".concat(s.length, \".\")), assert$6(5 === o.rank, () => \"Error in conv3dDerInput: dy must be rank 5, but got rank \".concat(o.rank)), assert$6(5 === n.rank, () => \"Error in conv3dDerInput: filter must be rank 5, but got rank \".concat(n.rank)), assert$6(l === n.shape[3], () => \"Error in conv3dDerInput: depth of input (\".concat(l, \") must match input depth for filter \").concat(n.shape[3], \".\")), assert$6(u === n.shape[4], () => \"Error in conv3dDerInput: depth of output (\".concat(u, \") must match output depth for filter \").concat(n.shape[4], \".\"));\n  var c = ENGINE$1.runKernel(Conv3DBackpropInputV2$1, {\n    dy: o,\n    filter: n\n  }, {\n    pad: a,\n    strides: r,\n    inputShape: s\n  });\n  return i ? reshape$6(c, [c.shape[1], c.shape[2], c.shape[3], c.shape[4]]) : c;\n}\n\nvar conv3DBackpropInput$3 = op$1({\n  conv3DBackpropInput_: conv3DBackpropInput_$1\n});\n\nfunction conv3dTranspose_$1(e, t, n, r, a) {\n  var s = convertToTensor$1(e, \"x\", \"conv3dTranspose\"),\n      o = convertToTensor$1(t, \"filter\", \"conv3dTranspose\");\n  return conv3DBackpropInput$3(n, s, o, r, a);\n}\n\nvar conv3dTranspose$2 = op$1({\n  conv3dTranspose_: conv3dTranspose_$1\n});\n\nfunction cos_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"cos\");\n  return ENGINE$1.runKernel(Cos$1, {\n    x: t\n  });\n}\n\nvar cos$5 = op$1({\n  cos_: cos_$1\n});\n\nfunction cosh_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"cosh\");\n  return ENGINE$1.runKernel(Cosh$1, {\n    x: t\n  });\n}\n\nvar cosh$5 = op$1({\n  cosh_: cosh_$1\n});\n\nfunction cumsum_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor$1(e, \"x\", \"cumsum\");\n  return ENGINE$1.runKernel(Cumsum$1, {\n    x: a\n  }, {\n    axis: t,\n    exclusive: n,\n    reverse: r\n  });\n}\n\nvar cumsum$5 = op$1({\n  cumsum_: cumsum_$1\n});\n\nfunction depthToSpace_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"NHWC\";\n  var r = convertToTensor$1(e, \"x\", \"depthToSpace\"),\n      a = \"NHWC\" === n ? r.shape[1] : r.shape[2],\n      s = \"NHWC\" === n ? r.shape[2] : r.shape[3],\n      o = \"NHWC\" === n ? r.shape[3] : r.shape[1];\n  return assert$6(a * t >= 0, () => \"Negative dimension size caused by overflow when multiplying\\n    \".concat(a, \" and \").concat(t, \"  for depthToSpace with input shape\\n    \").concat(r.shape)), assert$6(s * t >= 0, () => \"Negative dimension size caused by overflow when multiplying\\n    \".concat(s, \" and \").concat(t, \" for depthToSpace with input shape\\n        \").concat(r.shape)), assert$6(o % (t * t) == 0, () => \"Dimension size must be evenly divisible by \".concat(t * t, \" but is \").concat(o, \" for depthToSpace with input shape \").concat(r.shape)), ENGINE$1.runKernel(DepthToSpace$1, {\n    x: r\n  }, {\n    blockSize: t,\n    dataFormat: n\n  });\n}\n\nvar depthToSpace$5 = op$1({\n  depthToSpace_: depthToSpace_$1\n});\n\nfunction depthwiseConv2d_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NHWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = convertToTensor$1(e, \"x\", \"depthwiseConv2d\"),\n      l = convertToTensor$1(t, \"filter\", \"depthwiseConv2d\");\n  var u = i,\n      c = !1;\n  3 === i.rank && (c = !0, u = reshape$6(i, [1, i.shape[0], i.shape[1], i.shape[2]])), assert$6(4 === u.rank, () => \"Error in depthwiseConv2d: input must be rank 4, but got rank \".concat(u.rank, \".\")), assert$6(4 === l.rank, () => \"Error in depthwiseConv2d: filter must be rank 4, but got rank \".concat(l.rank, \".\")), assert$6(u.shape[3] === l.shape[2], () => \"Error in depthwiseConv2d: number of input channels (\".concat(u.shape[3], \") must match the inChannels dimension in filter \").concat(l.shape[2], \".\")), null != o && assert$6(isInt$1(r), () => \"Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\"));\n  var p = ENGINE$1.runKernel(DepthwiseConv2dNative$1, {\n    x: u,\n    filter: l\n  }, {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s,\n    dimRoundingMode: o\n  });\n  return c ? reshape$6(p, [p.shape[1], p.shape[2], p.shape[3]]) : p;\n}\n\nvar depthwiseConv2d$5 = op$1({\n  depthwiseConv2d_: depthwiseConv2d_$1\n});\n\nfunction dilation2d_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : [1, 1];\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NHWC\";\n  var o = convertToTensor$1(e, \"x\", \"dilation2d\"),\n      i = convertToTensor$1(t, \"filter\", \"dilation2d\");\n  assert$6(3 === o.rank || 4 === o.rank, () => \"Error in dilation2d: input must be rank 3 or 4, but got rank \".concat(o.rank, \".\")), assert$6(3 === i.rank, () => \"Error in dilation2d: filter must be rank 3, but got rank \".concat(i.rank, \".\")), assert$6(\"NHWC\" === s, () => \"Error in dilation2d: Only NHWC is currently supported, but got dataFormat of \".concat(s));\n  var l = o,\n      u = !1;\n  3 === o.rank && (l = reshape$6(o, [1, o.shape[0], o.shape[1], o.shape[2]]), u = !0);\n  var c = ENGINE$1.runKernel(Dilation2D$1, {\n    x: l,\n    filter: i\n  }, {\n    strides: n,\n    pad: r,\n    dilations: a\n  });\n  return u ? reshape$6(c, [c.shape[1], c.shape[2], c.shape[3]]) : c;\n}\n\nvar dilation2d$1 = op$1({\n  dilation2d_: dilation2d_$1\n});\n\nfunction getBroadcastDims$3(e, t) {\n  var n = e.length,\n      r = [];\n\n  for (var a = 0; a < n; a++) {\n    var s = n - 1 - a,\n        o = e[s] || 1;\n    (t[t.length - 1 - a] || 1) > 1 && 1 === o && r.unshift(s);\n  }\n\n  return r;\n}\n\nfunction getReductionAxes$1(e, t) {\n  var n = [];\n\n  for (var r = 0; r < t.length; r++) {\n    var a = e[e.length - r - 1],\n        s = t.length - r - 1,\n        o = t[s];\n    (null == a || 1 === a && o > 1) && n.unshift(s);\n  }\n\n  return n;\n}\n\nfunction assertAndGetBroadcastShape$1(e, t) {\n  var n = [],\n      r = Math.max(e.length, t.length);\n\n  for (var a = 0; a < r; a++) {\n    var _r15 = e[e.length - a - 1];\n    null == _r15 && (_r15 = 1);\n    var s = t[t.length - a - 1];\n    if (null == s && (s = 1), 1 === _r15) n.unshift(s);else if (1 === s) n.unshift(_r15);else {\n      if (_r15 !== s) throw Error(\"Operands could not be broadcast together with shapes \".concat(e, \" and \").concat(t, \".\"));\n      n.unshift(_r15);\n    }\n  }\n\n  return n;\n}\n\nfunction equal_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"equal\", \"string_or_numeric\"),\n      r = convertToTensor$1(t, \"b\", \"equal\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch$1(n, r), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(Equal$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar equal$5 = op$1({\n  equal_: equal_$1\n});\n\nfunction where_$1(e, t, n) {\n  var r = convertToTensor$1(t, \"a\", \"where\"),\n      a = convertToTensor$1(n, \"b\", \"where\"),\n      s = convertToTensor$1(e, \"condition\", \"where\", \"bool\"),\n      o = assertAndGetBroadcastShape$1(assertAndGetBroadcastShape$1(s.shape, r.shape), a.shape),\n      i = broadcastTo$1(s, o),\n      l = broadcastTo$1(r, o),\n      u = broadcastTo$1(a, o);\n  return ENGINE$1.runKernel(Select$1, {\n    condition: i,\n    t: l,\n    e: u\n  });\n}\n\nvar where$1 = op$1({\n  where_: where_$1\n});\n\nfunction zerosLike_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"zerosLike\");\n  return ENGINE$1.runKernel(ZerosLike$1, {\n    x: t\n  });\n}\n\nvar zerosLike$5 = op$1({\n  zerosLike_: zerosLike_$1\n});\n\nfunction divNoNan_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"div\"),\n      r = convertToTensor$1(t, \"b\", \"div\");\n  [n, r] = makeTypesMatch$1(n, r);\n  var a = div$3(n, r),\n      s = zerosLike$5(a),\n      o = equal$5(r, s);\n  return where$1(o, s, a);\n}\n\nvar divNoNan$1 = op$1({\n  divNoNan_: divNoNan_$1\n});\n\nfunction dot_$1(e, t) {\n  var n = convertToTensor$1(e, \"t1\", \"dot\"),\n      r = convertToTensor$1(t, \"t2\", \"dot\");\n  assert$6(!(1 !== n.rank && 2 !== n.rank || 1 !== r.rank && 2 !== r.rank), () => \"Error in dot: inputs must all be rank 1 or 2, but got ranks \".concat(n.rank, \" and \").concat(r.rank, \".\"));\n  var a = 1 === n.rank ? n.size : n.shape[1],\n      s = 1 === r.rank ? r.size : r.shape[0];\n\n  if (assert$6(a === s, () => \"Error in dot: inner dimensions of inputs must match, but got \".concat(a, \" and \").concat(s, \".\")), 1 === n.rank && 1 === r.rank) {\n    var _e47 = reshape$6(n, [1, -1]),\n        _t45 = reshape$6(r, [-1, 1]),\n        _a10 = matMul$3(_e47, _t45);\n\n    return reshape$6(_a10, []);\n  }\n\n  if (1 === n.rank && 2 === r.rank) {\n    var _e48 = reshape$6(n, [1, -1]),\n        _t46 = reshape$6(r, [r.shape[0], r.shape[1]]),\n        _a11 = matMul$3(_e48, _t46);\n\n    return reshape$6(_a11, [_a11.size]);\n  }\n\n  if (2 === n.rank && 1 === r.rank) {\n    var _e49 = reshape$6(r, [-1, 1]),\n        _t47 = matMul$3(n, _e49);\n\n    return reshape$6(_t47, [_t47.size]);\n  }\n\n  {\n    var _e50 = reshape$6(r, [r.shape[0], r.shape[1]]);\n\n    return matMul$3(n, _e50);\n  }\n}\n\nvar dot$4 = op$1({\n  dot_: dot_$1\n});\n\nfunction elu_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"elu\");\n  return ENGINE$1.runKernel(Elu$3, {\n    x: t\n  });\n}\n\nvar elu$8 = op$1({\n  elu_: elu_$1\n});\n\nfunction erf_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"erf\");\n  return assert$6(\"int32\" === t.dtype || \"float32\" === t.dtype, () => \"Input dtype must be `int32` or `float32`.\"), \"int32\" === t.dtype && (t = cast$7(t, \"float32\")), ENGINE$1.runKernel(Erf$1, {\n    x: t\n  });\n}\n\nvar erf$5 = op$1({\n  erf_: erf_$1\n});\n\nfunction exp_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"exp\");\n  return ENGINE$1.runKernel(Exp$1, {\n    x: t\n  });\n}\n\nvar exp$5 = op$1({\n  exp_: exp_$1\n});\n\nfunction expandDims_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor$1(e, \"x\", \"expandDims\", \"string_or_numeric\");\n  return assert$6(t <= n.rank, () => \"Axis must be <= rank of the tensor\"), ENGINE$1.runKernel(ExpandDims$1, {\n    input: n\n  }, {\n    dim: t\n  });\n}\n\nvar expandDims$7 = op$1({\n  expandDims_: expandDims_$1\n});\n\nfunction expm1_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"expm1\");\n  return ENGINE$1.runKernel(Expm1$1, {\n    x: t\n  });\n}\n\nvar expm1$5 = op$1({\n  expm1_: expm1_$1\n});\n\nfunction tile_$1(e, t) {\n  var n = convertToTensor$1(e, \"x\", \"tile\", \"string_or_numeric\");\n  return assert$6(n.rank === t.length, () => \"Error in transpose: rank of input \".concat(n.rank, \" must match length of reps \").concat(t, \".\")), ENGINE$1.runKernel(Tile$1, {\n    x: n\n  }, {\n    reps: t\n  });\n}\n\nvar tile$7 = op$1({\n  tile_: tile_$1\n});\n\nfunction eye_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"float32\";\n  null == t && (t = e);\n  var a = buffer$1([e, t], r),\n      s = e <= t ? e : t;\n\n  for (var _e51 = 0; _e51 < s; ++_e51) {\n    a.set(1, _e51, _e51);\n  }\n\n  var o = reshape$6(a.toTensor(), [e, t]);\n  if (null == n) return o;\n  if (1 === n.length) return tile$7(expandDims$7(o, 0), [n[0], 1, 1]);\n  if (2 === n.length) return tile$7(expandDims$7(expandDims$7(o, 0), 0), [n[0], n[1], 1, 1]);\n  if (3 === n.length) return tile$7(expandDims$7(expandDims$7(expandDims$7(o, 0), 0), 0), [n[0], n[1], n[2], 1, 1]);\n  throw new Error(\"eye() currently supports only 1D and 2D batchShapes, but received \".concat(n.length, \"D.\"));\n}\n\nvar eye$1 = op$1({\n  eye_: eye_$1\n});\n\nfunction fill$5(e, t, n) {\n  return ENGINE$1.runKernel(Fill$1, {}, {\n    shape: e,\n    value: t,\n    dtype: n\n  });\n}\n\nfunction floor_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"floor\");\n  return ENGINE$1.runKernel(Floor$1, {\n    x: t\n  });\n}\n\nvar floor$5 = op$1({\n  floor_: floor_$1\n});\n\nfunction gather_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var a = convertToTensor$1(e, \"x\", \"gather\"),\n      s = convertToTensor$1(t, \"indices\", \"gather\", \"int32\");\n  return ENGINE$1.runKernel(GatherV2$1, {\n    x: a,\n    indices: s\n  }, {\n    axis: n,\n    batchDims: r\n  });\n}\n\nvar gather$3 = op$1({\n  gather_: gather_$1\n});\n\nfunction greater_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"greater\", \"string_or_numeric\"),\n      r = convertToTensor$1(t, \"b\", \"greater\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch$1(n, r), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(Greater$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar greater$6 = op$1({\n  greater_: greater_$1\n});\n\nfunction greaterEqual_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"greaterEqual\", \"string_or_numeric\"),\n      r = convertToTensor$1(t, \"b\", \"greaterEqual\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch$1(n, r), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(GreaterEqual$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar greaterEqual$5 = op$1({\n  greaterEqual_: greaterEqual_$1\n});\n\nfunction imag_$1(e) {\n  var t = convertToTensor$1(e, \"input\", \"imag\");\n  return ENGINE$1.runKernel(Imag$1, {\n    input: t\n  });\n}\n\nvar imag$5 = op$1({\n  imag_: imag_$1\n});\n\nfunction isFinite_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"isFinite\");\n  return ENGINE$1.runKernel(IsFinite$1, {\n    x: t\n  });\n}\n\nvar isFinite$6 = op$1({\n  isFinite_: isFinite_$1\n});\n\nfunction isInf_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"isInf\");\n  return ENGINE$1.runKernel(IsInf$1, {\n    x: t\n  });\n}\n\nvar isInf$5 = op$1({\n  isInf_: isInf_$1\n});\n\nfunction isNaN_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"isNaN\");\n  return ENGINE$1.runKernel(IsNan$1, {\n    x: t\n  });\n}\n\nvar isNaN$6 = op$1({\n  isNaN_: isNaN_$1\n});\n\nfunction leakyRelu_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .2;\n  var n = convertToTensor$1(e, \"x\", \"leakyRelu\");\n  return ENGINE$1.runKernel(LeakyRelu$1, {\n    x: n\n  }, {\n    alpha: t\n  });\n}\n\nvar leakyRelu$5 = op$1({\n  leakyRelu_: leakyRelu_$1\n});\n\nfunction less_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"less\", \"string_or_numeric\"),\n      r = convertToTensor$1(t, \"b\", \"less\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch$1(n, r), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(Less$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar less$6 = op$1({\n  less_: less_$1\n});\n\nfunction lessEqual_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"lessEqual\", \"string_or_numeric\"),\n      r = convertToTensor$1(t, \"b\", \"lessEqual\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch$1(n, r), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(LessEqual$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar lessEqual$5 = op$1({\n  lessEqual_: lessEqual_$1\n});\n\nfunction localResponseNormalization_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 5;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .5;\n  var s = convertToTensor$1(e, \"x\", \"localResponseNormalization\");\n  assert$6(4 === s.rank || 3 === s.rank, () => \"Error in localResponseNormalization: x must be rank 3 or 4 but got\\n               rank \".concat(s.rank, \".\")), assert$6(isInt$1(t), () => \"Error in localResponseNormalization: depthRadius must be an integer but got depthRadius \".concat(t, \".\"));\n  var o = s,\n      i = !1;\n  3 === s.rank && (i = !0, o = reshape$6(s, [1, s.shape[0], s.shape[1], s.shape[2]]));\n  var l = ENGINE$1.runKernel(LRN$1, {\n    x: o\n  }, {\n    depthRadius: t,\n    bias: n,\n    alpha: r,\n    beta: a\n  });\n  return i ? reshape$6(l, [l.shape[1], l.shape[2], l.shape[3]]) : l;\n}\n\nvar localResponseNormalization$1 = op$1({\n  localResponseNormalization_: localResponseNormalization_$1\n});\n\nfunction log_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"log\");\n  return ENGINE$1.runKernel(Log$1, {\n    x: t\n  });\n}\n\nvar log$7 = op$1({\n  log_: log_$1\n});\n\nfunction log1p_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"log1p\");\n  return ENGINE$1.runKernel(Log1p$1, {\n    x: t\n  });\n}\n\nvar log1p$5 = op$1({\n  log1p_: log1p_$1\n});\n\nfunction variableGrads$1(e, t) {\n  assert$6(isFunction$1(e), () => \"The f passed in variableGrads(f) must be a function\"), assert$6(null == t || Array.isArray(t) && t.every(e => e instanceof Variable$1), () => \"The varList passed in variableGrads(f, varList) must be an array of variables\");\n  var n = null != t;\n\n  if (!n) {\n    t = [];\n\n    for (var _e52 in ENGINE$1.registeredVariables) {\n      t.push(ENGINE$1.registeredVariables[_e52]);\n    }\n  }\n\n  var r = n ? t.filter(e => !e.trainable) : null,\n      a = t.length;\n  assert$6((t = t.filter(e => e.trainable)).length > 0, () => \"variableGrads() expects at least one of the input variables to be trainable, but none of the \".concat(a, \" variables is trainable.\"));\n  var {\n    value: s,\n    grads: o\n  } = ENGINE$1.gradients(e, t, null, !0);\n  assert$6(o.some(e => null != e), () => \"Cannot find a connection between any variable and the result of the loss function y=f(x). Please make sure the operations that use variables are inside the function f passed to minimize().\"), assert$6(0 === s.rank, () => \"The f passed in variableGrads(f) must return a scalar, but it returned a rank-\".concat(s.rank, \" tensor\"));\n  var i = {};\n  return t.forEach((e, t) => {\n    null != o[t] && (i[e.name] = o[t]);\n  }), null != r && r.forEach(e => i[e.name] = null), {\n    value: s,\n    grads: i\n  };\n}\n\nfunction customGrad$1(e) {\n  return ENGINE$1.customGrad(e);\n}\n\nfunction neg_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"neg\");\n  return ENGINE$1.runKernel(Neg$1, {\n    x: t\n  });\n}\n\nvar neg$5 = op$1({\n  neg_: neg_$1\n});\n\nfunction softplus_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"softplus\");\n  return ENGINE$1.runKernel(Softplus$3, {\n    x: t\n  });\n}\n\nvar softplus$5 = op$1({\n  softplus_: softplus_$1\n});\n\nfunction logSigmoid_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"logSigmoid\");\n  return customGrad$1(e => ({\n    value: neg$5(softplus$5(neg$5(e))),\n    gradFunc: t => mul$1(t, sigmoid$5(neg$5(e)))\n  }))(t);\n}\n\nvar logSigmoid$1 = op$1({\n  logSigmoid_: logSigmoid_$1\n});\n\nfunction max_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"max\");\n  return ENGINE$1.runKernel(Max$1, {\n    x: r\n  }, {\n    reductionIndices: t,\n    keepDims: n\n  });\n}\n\nvar max$7 = op$1({\n  max_: max_$1\n});\n\nfunction sub_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"sub\"),\n      r = convertToTensor$1(t, \"b\", \"sub\");\n  return [n, r] = makeTypesMatch$1(n, r), ENGINE$1.runKernel(Sub$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar sub$5 = op$1({\n  sub_: sub_$1\n});\n\nfunction sum_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"sum\");\n  return \"bool\" === r.dtype && (r = cast$7(r, \"int32\")), ENGINE$1.runKernel(Sum$1, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar sum$6 = op$1({\n  sum_: sum_$1\n});\n\nfunction logSoftmax_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n = convertToTensor$1(e, \"logits\", \"logSoftmax\");\n  if (-1 === t && (t = n.rank - 1), t !== n.rank - 1) throw Error(\"Log Softmax along a non-last dimension is not yet supported. Logits was rank \".concat(n.rank, \" and axis was \").concat(t));\n  return customGrad$1((e, n) => {\n    var r = max$7(e, t, !0),\n        a = sub$5(e, r),\n        s = sub$5(cast$7(a, \"float32\"), log$7(sum$6(exp$5(a), t, !0)));\n    return n([s]), {\n      value: s,\n      gradFunc: (e, n) => {\n        var [r] = n,\n            a = exp$5(r);\n        return sub$5(e, mul$1(sum$6(e, t, !0), a));\n      }\n    };\n  })(n);\n}\n\nvar logSoftmax$1 = op$1({\n  logSoftmax_: logSoftmax_$1\n});\n\nfunction axesAreInnerMostDims$1(e, t) {\n  for (var n = 0; n < e.length; ++n) {\n    if (e[e.length - n - 1] !== t - 1 - n) return !1;\n  }\n\n  return !0;\n}\n\nfunction combineLocations$1(e, t, n) {\n  var r = e.length + t.length,\n      a = [];\n  var s = 0,\n      o = 0;\n\n  for (var i = 0; i < r; i++) {\n    -1 === n.indexOf(i) ? a.push(e[s++]) : a.push(t[o++]);\n  }\n\n  return a;\n}\n\nfunction computeOutAndReduceShapes$1(e, t) {\n  var n = [],\n      r = e.length;\n\n  for (var a = 0; a < r; a++) {\n    -1 === t.indexOf(a) && n.push(e[a]);\n  }\n\n  return [n, t.map(t => e[t])];\n}\n\nfunction expandShapeToKeepDim$1(e, t) {\n  return combineLocations$1(e, t.map(e => 1), t);\n}\n\nfunction assertAxesAreInnerMostDims$1(e, t, n) {\n  assert$6(axesAreInnerMostDims$1(t, n), () => \"\".concat(e, \" supports only inner-most axes for now. Got axes \").concat(t, \" and rank-\").concat(n, \" input.\"));\n}\n\nfunction getAxesPermutation$1(e, t) {\n  if (axesAreInnerMostDims$1(e, t)) return null;\n  var n = [];\n\n  for (var r = 0; r < t; ++r) {\n    -1 === e.indexOf(r) && n.push(r);\n  }\n\n  return e.forEach(e => n.push(e)), n;\n}\n\nfunction getUndoAxesPermutation$1(e) {\n  return e.map((e, t) => [t, e]).sort((e, t) => e[1] - t[1]).map(e => e[0]);\n}\n\nfunction getInnerMostAxes$1(e, t) {\n  var n = [];\n\n  for (var r = t - e; r < t; ++r) {\n    n.push(r);\n  }\n\n  return n;\n}\n\nfunction logSumExp_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"logSumExp\"),\n      a = parseAxisParam$1(t, r.shape),\n      s = max$7(r, a, !0),\n      o = sub$5(r, s),\n      i = exp$5(o),\n      l = sum$6(i, a),\n      u = log$7(l),\n      c = add$5(reshape$6(s, u.shape), u);\n\n  if (n) {\n    var _e53 = expandShapeToKeepDim$1(c.shape, a);\n\n    return reshape$6(c, _e53);\n  }\n\n  return c;\n}\n\nvar logSumExp$1 = op$1({\n  logSumExp_: logSumExp_$1\n});\n\nfunction logicalAnd_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"logicalAnd\", \"bool\"),\n      r = convertToTensor$1(t, \"b\", \"logicalAnd\", \"bool\");\n  return assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(LogicalAnd$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar logicalAnd$5 = op$1({\n  logicalAnd_: logicalAnd_$1\n});\n\nfunction logicalNot_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"logicalNot\", \"bool\");\n  return ENGINE$1.runKernel(LogicalNot$1, {\n    x: t\n  });\n}\n\nvar logicalNot$5 = op$1({\n  logicalNot_: logicalNot_$1\n});\n\nfunction logicalOr_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"logicalOr\", \"bool\"),\n      r = convertToTensor$1(t, \"b\", \"logicalOr\", \"bool\");\n  return assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(LogicalOr$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar logicalOr$5 = op$1({\n  logicalOr_: logicalOr_$1\n});\n\nfunction logicalXor_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"logicalXor\", \"bool\"),\n      r = convertToTensor$1(t, \"b\", \"logicalXor\", \"bool\");\n  return assertAndGetBroadcastShape$1(n.shape, r.shape), logicalAnd$5(logicalOr$5(e, t), logicalNot$5(logicalAnd$5(e, t)));\n}\n\nvar logicalXor$1 = op$1({\n  logicalXor_: logicalXor_$1\n});\n\nfunction maxPool_$1(e, t, n, r, a) {\n  var s = convertToTensor$1(e, \"x\", \"maxPool\");\n  var o = s,\n      i = !1;\n  3 === s.rank && (i = !0, o = reshape$6(s, [1, s.shape[0], s.shape[1], s.shape[2]])), assert$6(4 === o.rank, () => \"Error in maxPool: input must be rank 4 but got rank \".concat(o.rank, \".\")), assert$6(eitherStridesOrDilationsAreOne$1(n, 1), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '1'\")), null != a && assert$6(isInt$1(r), () => \"Error in maxPool: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var l = ENGINE$1.runKernel(MaxPool$1, {\n    x: o\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a\n  });\n  return i ? reshape$6(l, [l.shape[1], l.shape[2], l.shape[3]]) : l;\n}\n\nvar maxPool$5 = op$1({\n  maxPool_: maxPool_$1\n});\n\nfunction maxPool3d_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : [1, 1, 1];\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NDHWC\";\n  var o = convertToTensor$1(e, \"x\", \"maxPool3d\");\n  var i = o,\n      l = !1;\n  4 === o.rank && (l = !0, i = reshape$6(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]])), assert$6(5 === i.rank, () => \"Error in maxPool3d: x must be rank 5 but got rank \".concat(i.rank, \".\")), assert$6(\"NDHWC\" === s, () => \"Error in maxPool3d: Only NDHWC is currently supported, but got dataFormat of \".concat(s)), null != a && assert$6(isInt$1(r), () => \"Error in maxPool3d: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var u = ENGINE$1.runKernel(MaxPool3D$1, {\n    x: i\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a,\n    dataFormat: s\n  });\n  return l ? reshape$6(u, [u.shape[1], u.shape[2], u.shape[3], u.shape[4]]) : u;\n}\n\nvar maxPool3d$3 = op$1({\n  maxPool3d_: maxPool3d_$1\n});\n\nfunction maximum_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"maximum\"),\n      r = convertToTensor$1(t, \"b\", \"maximum\");\n  return [n, r] = makeTypesMatch$1(n, r), \"bool\" === n.dtype && (n = cast$7(n, \"int32\"), r = cast$7(r, \"int32\")), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(Maximum$3, {\n    a: n,\n    b: r\n  });\n}\n\nvar maximum$6 = op$1({\n  maximum_: maximum_$1\n});\n\nfunction mean_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"mean\");\n  return ENGINE$1.runKernel(Mean$1, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar mean$3 = op$1({\n  mean_: mean_$1\n});\n\nfunction zeros$4(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n\n  if (\"complex64\" === t) {\n    var _t48 = zeros$4(e, \"float32\"),\n        _n12 = zeros$4(e, \"float32\");\n\n    return complex$5(_t48, _n12);\n  }\n\n  var n = makeZerosTypedArray$1(sizeFromShape$1(e), t);\n  return ENGINE$1.makeTensor(n, e, t);\n}\n\nfunction ones$3(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n\n  if (\"complex64\" === t) {\n    var _t49 = ones$3(e, \"float32\"),\n        _n13 = zeros$4(e, \"float32\");\n\n    return complex$5(_t49, _n13);\n  }\n\n  var n = makeOnesTypedArray$1(sizeFromShape$1(e), t);\n  return ENGINE$1.makeTensor(n, e, t);\n}\n\nfunction min_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"min\");\n  return ENGINE$1.runKernel(Min$1, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar min$7 = op$1({\n  min_: min_$1\n});\n\nfunction minimum_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"minimum\"),\n      r = convertToTensor$1(t, \"b\", \"minimum\");\n  return [n, r] = makeTypesMatch$1(n, r), \"bool\" === n.dtype && (n = cast$7(n, \"int32\"), r = cast$7(r, \"int32\")), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(Minimum$3, {\n    a: n,\n    b: r\n  });\n}\n\nvar minimum$6 = op$1({\n  minimum_: minimum_$1\n});\n\nfunction mirrorPad_$1(e, t, n) {\n  assert$6(\"reflect\" === n || \"symmetric\" === n, () => \"Invalid mode. Mode must be either reflect or symmetric. Got \".concat(n, \".\"));\n  var r = convertToTensor$1(e, \"x\", \"mirrorPad\");\n  if (0 === r.rank) throw new Error(\"mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad\");\n  assert$6(t.length === r.rank, () => \"Padding doesn't match input. Must be \".concat(r.rank, \". Got \").concat(t.length, \".\"));\n  var a = \"reflect\" === n ? 1 : 0;\n\n  var _loop5 = function _loop5(_e54) {\n    assert$6(2 === t[_e54].length, () => \"Invalid number of paddings. Must be length of 2 each.\"), assert$6(t[_e54][0] >= 0 && t[_e54][0] <= r.shape[_e54] - a && t[_e54][1] >= 0 && t[_e54][1] <= r.shape[_e54] - a, () => \"Padding in dimension \".concat(_e54, \" cannot be greater than or equal to \").concat(r.shape[_e54] - a, \" or less than 0 for input of shape \").concat(r.shape));\n  };\n\n  for (var _e54 = 0; _e54 < r.rank; _e54++) {\n    _loop5(_e54);\n  }\n\n  return ENGINE$1.runKernel(MirrorPad$1, {\n    x: r\n  }, {\n    paddings: t,\n    mode: n\n  });\n}\n\nvar mirrorPad$3 = op$1({\n  mirrorPad_: mirrorPad_$1\n});\n\nfunction mod_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"mod\"),\n      r = convertToTensor$1(t, \"b\", \"mod\");\n  return [n, r] = makeTypesMatch$1(n, r), ENGINE$1.runKernel(Mod$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar mod$5 = op$1({\n  mod_: mod_$1\n});\n\nfunction square_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"square\");\n  return ENGINE$1.runKernel(\"Square\", {\n    x: t\n  }, {});\n}\n\nvar square$5 = op$1({\n  square_: square_$1\n});\n\nfunction moments_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = parseAxisParam$1(t, (e = convertToTensor$1(e, \"x\", \"moments\")).shape),\n      a = mean$3(e, r, n);\n  var s = a.shape;\n  n || (s = expandShapeToKeepDim$1(a.shape, r));\n  var o = square$5(sub$5(cast$7(e, \"float32\"), reshape$6(a, s)));\n  return {\n    mean: a,\n    variance: mean$3(o, r, n)\n  };\n}\n\nvar moments$1 = op$1({\n  moments_: moments_$1\n});\n\nfunction notEqual_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"notEqual\", \"string_or_numeric\"),\n      r = convertToTensor$1(t, \"b\", \"notEqual\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch$1(n, r), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(NotEqual$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar notEqual$5 = op$1({\n  notEqual_: notEqual_$1\n});\n\nfunction onesLike_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"onesLike\");\n  return ENGINE$1.runKernel(OnesLike$1, {\n    x: t\n  });\n}\n\nvar onesLike$5 = op$1({\n  onesLike_: onesLike_$1\n});\n\nfunction pad_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = convertToTensor$1(e, \"x\", \"pad\");\n  if (0 === r.rank) throw new Error(\"pad(scalar) is not defined. Pass non-scalar to pad\");\n  return ENGINE$1.runKernel(PadV2$1, {\n    x: r\n  }, {\n    paddings: t,\n    constantValue: n\n  });\n}\n\nvar pad$1 = op$1({\n  pad_: pad_$1\n});\n\nfunction spaceToBatchND_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"spaceToBatchND\");\n  return assert$6(r.rank >= 1 + t.length, () => \"input rank \".concat(r.rank, \" should be > than [blockShape] \").concat(t.length)), assert$6(n.length === t.length, () => \"paddings.shape[0] \".concat(n.length, \" must be equal to [blockShape] \").concat(t.length)), assert$6(r.shape.reduce((e, r, a) => a > 0 && a <= t.length ? e && (r + n[a - 1][0] + n[a - 1][1]) % t[a - 1] == 0 : e, !0), () => \"input spatial dimensions \".concat(r.shape.slice(1), \" with paddings \").concat(n.toString(), \" must be divisible by blockShapes \").concat(t.toString())), ENGINE$1.runKernel(SpaceToBatchND$1, {\n    x: r\n  }, {\n    blockShape: t,\n    paddings: n\n  });\n}\n\nvar spaceToBatchND$5 = op$1({\n  spaceToBatchND_: spaceToBatchND_$1\n});\n\nfunction pool_$1(e, t, n, r, a, s) {\n  null == a && (a = [1, 1]), null == s && (s = 1), 0 === r && (r = \"valid\");\n  var o = convertToTensor$1(e, \"x\", \"maxPool\");\n  var i = o,\n      l = !1;\n  3 === o.rank && (l = !0, i = reshape$6(o, [1, o.shape[0], o.shape[1], o.shape[2]])), assert$6(eitherStridesOrDilationsAreOne$1(s, a), () => \"Error in pool: Either strides or dilations must be 1. Got strides \".concat(s, \" and dilations '\").concat(a, \"'\"));\n  var u = computePool2DInfo$1(i.shape, t, s, a, r),\n      c = [u.dilationHeight, u.dilationWidth];\n  var p;\n  p = \"same\" === r ? withSpaceToBatchBasePaddings$1([u.filterHeight, u.filterWidth], c) : [[0, 0], [0, 0]];\n  var d = 1 === c[0] && 1 === c[1],\n      [h, m] = requiredSpaceToBatchPaddings$1([u.inHeight, u.inWidth], c, p),\n      f = d ? r : \"valid\",\n      g = d ? i : spaceToBatchND$5(i, c, h),\n      $ = (\"avg\" === n ? () => avgPool$5(g, t, s, f) : () => maxPool$5(g, t, s, f))(),\n      y = d ? $ : batchToSpaceND$5($, c, m);\n  return l ? reshape$6(y, [y.shape[1], y.shape[2], y.shape[3]]) : y;\n}\n\nfunction requiredSpaceToBatchPaddings$1(e, t, n) {\n  var r = n.map(e => e[0]),\n      a = n.map(e => e[1]),\n      s = e.concat(r, a),\n      o = t.map((e, t) => (e - s[t] % e) % e),\n      i = a.map((e, t) => e + o[t]);\n  return [t.map((e, t) => [r[t], i[t]]), t.map((e, t) => [0, o[t]])];\n}\n\nfunction withSpaceToBatchBasePaddings$1(e, t) {\n  var n = e.map((e, n) => e + (e - 1) * (t[n] - 1)).map(e => e - 1),\n      r = n.map(e => Math.floor(e / 2)),\n      a = n.map((e, t) => e - r[t]);\n  return n.map((e, t) => [r[t], a[t]]);\n}\n\nvar pool$3 = op$1({\n  pool_: pool_$1\n});\n\nfunction pow_$1(e, t) {\n  var n = convertToTensor$1(e, \"base\", \"pow\"),\n      r = convertToTensor$1(t, \"exp\", \"pow\");\n  return [n, r] = makeTypesMatch$1(n, r), ENGINE$1.runKernel(Pow$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar pow$5 = op$1({\n  pow_: pow_$1\n});\n\nfunction prelu_$1(e, t) {\n  var n = convertToTensor$1(e, \"x\", \"prelu\"),\n      r = convertToTensor$1(t, \"alpha\", \"prelu\");\n  return ENGINE$1.runKernel(Prelu$1, {\n    x: n,\n    alpha: r\n  });\n}\n\nvar prelu$6 = op$1({\n  prelu_: prelu_$1\n});\n\nfunction prod_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor$1(e, \"x\", \"prod\");\n  return \"bool\" === r.dtype && (r = cast$7(r, \"int32\")), ENGINE$1.runKernel(Prod$1, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar prod$5 = op$1({\n  prod_: prod_$1\n});\nvar commonjsGlobal = \"undefined\" != typeof globalThis ? globalThis : \"undefined\" != typeof window ? window : \"undefined\" != typeof global ? global : \"undefined\" != typeof self ? self : {};\n\nfunction createCommonjsModule(e) {\n  var t = {\n    exports: {}\n  };\n  return e(t, t.exports), t.exports;\n}\n\nvar alea$1 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t,\n          n = this,\n          r = (t = 4022871197, function (e) {\n        e = e.toString();\n\n        for (var n = 0; n < e.length; n++) {\n          var r = .02519603282416938 * (t += e.charCodeAt(n));\n          r -= t = r >>> 0, t = (r *= t) >>> 0, t += 4294967296 * (r -= t);\n        }\n\n        return 2.3283064365386963e-10 * (t >>> 0);\n      });\n      n.next = function () {\n        var e = 2091639 * n.s0 + 2.3283064365386963e-10 * n.c;\n        return n.s0 = n.s1, n.s1 = n.s2, n.s2 = e - (n.c = 0 | e);\n      }, n.c = 1, n.s0 = r(\" \"), n.s1 = r(\" \"), n.s2 = r(\" \"), n.s0 -= r(e), n.s0 < 0 && (n.s0 += 1), n.s1 -= r(e), n.s1 < 0 && (n.s1 += 1), n.s2 -= r(e), n.s2 < 0 && (n.s2 += 1), r = null;\n    }\n\n    function a(e, t) {\n      return t.c = e.c, t.s0 = e.s0, t.s1 = e.s1, t.s2 = e.s2, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = n.next;\n      return o.int32 = function () {\n        return 4294967296 * n.next() | 0;\n      }, o.double = function () {\n        return o() + 11102230246251565e-32 * (2097152 * o() | 0);\n      }, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.alea = s;\n  }(0, e);\n}),\n    xor128$1 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this,\n          n = \"\";\n      t.x = 0, t.y = 0, t.z = 0, t.w = 0, t.next = function () {\n        var e = t.x ^ t.x << 11;\n        return t.x = t.y, t.y = t.z, t.z = t.w, t.w ^= t.w >>> 19 ^ e ^ e >>> 8;\n      }, e === (0 | e) ? t.x = e : n += e;\n\n      for (var r = 0; r < n.length + 64; r++) {\n        t.x ^= 0 | n.charCodeAt(r), t.next();\n      }\n    }\n\n    function a(e, t) {\n      return t.x = e.x, t.y = e.y, t.z = e.z, t.w = e.w, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xor128 = s;\n  }(0, e);\n}),\n    xorwow$1 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this,\n          n = \"\";\n      t.next = function () {\n        var e = t.x ^ t.x >>> 2;\n        return t.x = t.y, t.y = t.z, t.z = t.w, t.w = t.v, (t.d = t.d + 362437 | 0) + (t.v = t.v ^ t.v << 4 ^ e ^ e << 1) | 0;\n      }, t.x = 0, t.y = 0, t.z = 0, t.w = 0, t.v = 0, e === (0 | e) ? t.x = e : n += e;\n\n      for (var r = 0; r < n.length + 64; r++) {\n        t.x ^= 0 | n.charCodeAt(r), r == n.length && (t.d = t.x << 10 ^ t.x >>> 4), t.next();\n      }\n    }\n\n    function a(e, t) {\n      return t.x = e.x, t.y = e.y, t.z = e.z, t.w = e.w, t.v = e.v, t.d = e.d, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xorwow = s;\n  }(0, e);\n}),\n    xorshift7$1 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this;\n      t.next = function () {\n        var e,\n            n,\n            r = t.x,\n            a = t.i;\n        return e = r[a], n = (e ^= e >>> 7) ^ e << 24, n ^= (e = r[a + 1 & 7]) ^ e >>> 10, n ^= (e = r[a + 3 & 7]) ^ e >>> 3, n ^= (e = r[a + 4 & 7]) ^ e << 7, e = r[a + 7 & 7], r[a] = n ^= (e ^= e << 13) ^ e << 9, t.i = a + 1 & 7, n;\n      }, function (e, t) {\n        var n,\n            r = [];\n        if (t === (0 | t)) r[0] = t;else for (t = \"\" + t, n = 0; n < t.length; ++n) {\n          r[7 & n] = r[7 & n] << 15 ^ t.charCodeAt(n) + r[n + 1 & 7] << 13;\n        }\n\n        for (; r.length < 8;) {\n          r.push(0);\n        }\n\n        for (n = 0; n < 8 && 0 === r[n]; ++n) {\n          ;\n        }\n\n        for (8 == n && (r[7] = -1), e.x = r, e.i = 0, n = 256; n > 0; --n) {\n          e.next();\n        }\n      }(t, e);\n    }\n\n    function a(e, t) {\n      return t.x = e.x.slice(), t.i = e.i, t;\n    }\n\n    function s(e, t) {\n      null == e && (e = +new Date());\n\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (s.x && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xorshift7 = s;\n  }(0, e);\n}),\n    xor4096$1 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this;\n      t.next = function () {\n        var e,\n            n,\n            r = t.w,\n            a = t.X,\n            s = t.i;\n        return t.w = r = r + 1640531527 | 0, n = a[s + 34 & 127], e = a[s = s + 1 & 127], n ^= n << 13, e ^= e << 17, n = a[s] = (n ^= n >>> 15) ^ (e ^= e >>> 12), t.i = s, n + (r ^ r >>> 16) | 0;\n      }, function (e, t) {\n        var n,\n            r,\n            a,\n            s,\n            o,\n            i = [],\n            l = 128;\n\n        for (t === (0 | t) ? (r = t, t = null) : (t += \"\\0\", r = 0, l = Math.max(l, t.length)), a = 0, s = -32; s < l; ++s) {\n          t && (r ^= t.charCodeAt((s + 32) % t.length)), 0 === s && (o = r), r ^= r << 10, r ^= r >>> 15, r ^= r << 4, r ^= r >>> 13, s >= 0 && (a = 0 == (n = i[127 & s] ^= r + (o = o + 1640531527 | 0)) ? a + 1 : 0);\n        }\n\n        for (a >= 128 && (i[127 & (t && t.length || 0)] = -1), a = 127, s = 512; s > 0; --s) {\n          r = i[a + 34 & 127], n = i[a = a + 1 & 127], r ^= r << 13, n ^= n << 17, i[a] = (r ^= r >>> 15) ^ (n ^= n >>> 12);\n        }\n\n        e.w = o, e.X = i, e.i = a;\n      }(t, e);\n    }\n\n    function a(e, t) {\n      return t.i = e.i, t.w = e.w, t.X = e.X.slice(), t;\n    }\n\n    function s(e, t) {\n      null == e && (e = +new Date());\n\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (s.X && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xor4096 = s;\n  }(0, e);\n}),\n    tychei$1 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this,\n          n = \"\";\n      t.next = function () {\n        var e = t.b,\n            n = t.c,\n            r = t.d,\n            a = t.a;\n        return e = e << 25 ^ e >>> 7 ^ n, n = n - r | 0, r = r << 24 ^ r >>> 8 ^ a, a = a - e | 0, t.b = e = e << 20 ^ e >>> 12 ^ n, t.c = n = n - r | 0, t.d = r << 16 ^ n >>> 16 ^ a, t.a = a - e | 0;\n      }, t.a = 0, t.b = 0, t.c = -1640531527, t.d = 1367130551, e === Math.floor(e) ? (t.a = e / 4294967296 | 0, t.b = 0 | e) : n += e;\n\n      for (var r = 0; r < n.length + 20; r++) {\n        t.b ^= 0 | n.charCodeAt(r), t.next();\n      }\n    }\n\n    function a(e, t) {\n      return t.a = e.a, t.b = e.b, t.c = e.c, t.d = e.d, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.tychei = s;\n  }(0, e);\n}),\n    _nodeResolve_empty = {},\n    _nodeResolve_empty$1 = {\n  __proto__: null,\n  default: _nodeResolve_empty\n},\n    seedrandom$3 = createCommonjsModule(function (e) {\n  !function (t, n) {\n    var r,\n        a = this,\n        s = 256,\n        o = n.pow(s, 6),\n        i = n.pow(2, 52),\n        l = 2 * i,\n        u = 255;\n\n    function c(e, u, c) {\n      var g = [],\n          $ = m(h((u = 1 == u ? {\n        entropy: !0\n      } : u || {}).entropy ? [e, f(t)] : null == e ? function () {\n        try {\n          var e;\n          return r && (e = r.randomBytes) ? e = e(s) : (e = new Uint8Array(s), (a.crypto || a.msCrypto).getRandomValues(e)), f(e);\n        } catch (e) {\n          var n = a.navigator,\n              o = n && n.plugins;\n          return [+new Date(), a, o, a.screen, f(t)];\n        }\n      }() : e, 3), g),\n          y = new p(g),\n          b = function b() {\n        for (var e = y.g(6), t = o, n = 0; e < i;) {\n          e = (e + n) * s, t *= s, n = y.g(1);\n        }\n\n        for (; e >= l;) {\n          e /= 2, t /= 2, n >>>= 1;\n        }\n\n        return (e + n) / t;\n      };\n\n      return b.int32 = function () {\n        return 0 | y.g(4);\n      }, b.quick = function () {\n        return y.g(4) / 4294967296;\n      }, b.double = b, m(f(y.S), t), (u.pass || c || function (e, t, r, a) {\n        return a && (a.S && d(a, y), e.state = function () {\n          return d(y, {});\n        }), r ? (n.random = e, t) : e;\n      })(b, $, \"global\" in u ? u.global : this == n, u.state);\n    }\n\n    function p(e) {\n      var t,\n          n = e.length,\n          r = this,\n          a = 0,\n          o = r.i = r.j = 0,\n          i = r.S = [];\n\n      for (n || (e = [n++]); a < s;) {\n        i[a] = a++;\n      }\n\n      for (a = 0; a < s; a++) {\n        i[a] = i[o = u & o + e[a % n] + (t = i[a])], i[o] = t;\n      }\n\n      (r.g = function (e) {\n        for (var t, n = 0, a = r.i, o = r.j, i = r.S; e--;) {\n          t = i[a = u & a + 1], n = n * s + i[u & (i[a] = i[o = u & o + t]) + (i[o] = t)];\n        }\n\n        return r.i = a, r.j = o, n;\n      })(s);\n    }\n\n    function d(e, t) {\n      return t.i = e.i, t.j = e.j, t.S = e.S.slice(), t;\n    }\n\n    function h(e, t) {\n      var n,\n          r = [],\n          a = typeof e;\n      if (t && \"object\" == a) for (n in e) {\n        try {\n          r.push(h(e[n], t - 1));\n        } catch (e) {}\n      }\n      return r.length ? r : \"string\" == a ? e : e + \"\\0\";\n    }\n\n    function m(e, t) {\n      for (var n, r = e + \"\", a = 0; a < r.length;) {\n        t[u & a] = u & (n ^= 19 * t[u & a]) + r.charCodeAt(a++);\n      }\n\n      return f(t);\n    }\n\n    function f(e) {\n      return String.fromCharCode.apply(0, e);\n    }\n\n    if (n.seedrandom = c, m(n.random(), t), e.exports) {\n      e.exports = c;\n\n      try {\n        r = _nodeResolve_empty$1;\n      } catch (e) {}\n    }\n  }([], Math);\n});\nseedrandom$3.alea = alea$1, seedrandom$3.xor128 = xor128$1, seedrandom$3.xorwow = xorwow$1, seedrandom$3.xorshift7 = xorshift7$1, seedrandom$3.xor4096 = xor4096$1, seedrandom$3.tychei = tychei$1;\nvar seedrandom$2 = seedrandom$3;\n\nclass MPRandGauss$1 {\n  constructor(e, t, n, r, a) {\n    this.mean = e, this.stdDev = t, this.dtype = n, this.nextVal = NaN, this.truncated = r, this.truncated && (this.upper = this.mean + 2 * this.stdDev, this.lower = this.mean - 2 * this.stdDev);\n    var s = a || Math.random();\n    this.random = seedrandom$2.alea(s.toString());\n  }\n\n  nextValue() {\n    if (!isNaN(this.nextVal)) {\n      var _e55 = this.nextVal;\n      return this.nextVal = NaN, _e55;\n    }\n\n    var e,\n        t,\n        n = !1;\n\n    for (; !n;) {\n      var r = void 0,\n          a = void 0,\n          s = void 0;\n\n      do {\n        r = 2 * this.random() - 1, a = 2 * this.random() - 1, s = r * r + a * a;\n      } while (s >= 1 || 0 === s);\n\n      var o = Math.sqrt(-2 * Math.log(s) / s);\n      e = this.mean + this.stdDev * r * o, t = this.mean + this.stdDev * a * o, this.truncated && !this.isValidTruncated(e) || (n = !0);\n    }\n\n    return this.truncated && !this.isValidTruncated(t) || (this.nextVal = this.convertValue(t)), this.convertValue(e);\n  }\n\n  convertValue(e) {\n    return null == this.dtype || \"float32\" === this.dtype ? e : Math.round(e);\n  }\n\n  isValidTruncated(e) {\n    return e <= this.upper && e >= this.lower;\n  }\n\n}\n\nclass UniformRandom$1 {\n  constructor() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    var r = arguments.length > 3 ? arguments[3] : undefined;\n    if (this.canReturnFloat = () => null == this.dtype || \"float32\" === this.dtype, this.min = e, this.range = t - e, this.dtype = n, null == r && (r = Math.random()), \"number\" == typeof r && (r = r.toString()), !this.canReturnFloat() && this.range <= 1) throw new Error(\"The difference between \".concat(e, \" - \").concat(t, \" <= 1 and dtype is not float\"));\n    this.random = seedrandom$2.alea(r);\n  }\n\n  convertValue(e) {\n    return this.canReturnFloat() ? e : Math.round(e);\n  }\n\n  nextValue() {\n    return this.convertValue(this.min + this.range * this.random());\n  }\n\n}\n\nfunction randomNormal_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  if (null != r && \"bool\" === r) throw new Error(\"Unsupported data type \".concat(r));\n  var s = new MPRandGauss$1(t, n, r, !1, a),\n      o = buffer$1(e, r);\n\n  for (var _e56 = 0; _e56 < o.values.length; _e56++) {\n    o.values[_e56] = s.nextValue();\n  }\n\n  return o.toTensor();\n}\n\nvar randomNormal$4 = op$1({\n  randomNormal_: randomNormal_$1\n});\n\nfunction randomUniform_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"float32\";\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = buffer$1(e, r),\n      o = new UniformRandom$1(t, n, null, a);\n\n  for (var _e57 = 0; _e57 < s.values.length; _e57++) {\n    s.values[_e57] = o.nextValue();\n  }\n\n  return s.toTensor();\n}\n\nvar randomUniform$2 = op$1({\n  randomUniform_: randomUniform_$1\n});\n\nfunction range$8(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"float32\";\n  if (0 === n) throw new Error(\"Cannot have a step of zero\");\n  return ENGINE$1.runKernel(Range$1, {}, {\n    start: e,\n    stop: t,\n    step: n,\n    dtype: r\n  });\n}\n\nfunction real_$1(e) {\n  var t = convertToTensor$1(e, \"input\", \"real\");\n  return ENGINE$1.runKernel(Real$1, {\n    input: t\n  });\n}\n\nvar real$5 = op$1({\n  real_: real_$1\n});\n\nfunction reciprocal_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"reciprocal\");\n  return ENGINE$1.runKernel(Reciprocal$1, {\n    x: t\n  });\n}\n\nvar reciprocal$5 = op$1({\n  reciprocal_: reciprocal_$1\n});\n\nfunction relu_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"relu\");\n  return ENGINE$1.runKernel(Relu$3, {\n    x: t\n  });\n}\n\nvar relu$6 = op$1({\n  relu_: relu_$1\n});\n\nfunction relu6_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"relu6\");\n  return ENGINE$1.runKernel(Relu6$3, {\n    x: t\n  });\n}\n\nvar relu6$5 = op$1({\n  relu6_: relu6_$1\n});\n\nfunction reverse_$1(e, t) {\n  var n = convertToTensor$1(e, \"x\", \"reverse\");\n  return ENGINE$1.runKernel(Reverse$1, {\n    x: n\n  }, {\n    dims: t\n  });\n}\n\nvar reverse$5 = op$1({\n  reverse_: reverse_$1\n});\n\nfunction round_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"round\");\n  return ENGINE$1.runKernel(Round$1, {\n    x: t\n  });\n}\n\nvar round$6 = op$1({\n  round_: round_$1\n});\n\nfunction rsqrt_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"rsqrt\");\n  return ENGINE$1.runKernel(Rsqrt$1, {\n    x: t\n  });\n}\n\nvar rsqrt$5 = op$1({\n  rsqrt_: rsqrt_$1\n});\n\nfunction scalar$1(e, t) {\n  if ((isTypedArray$1(e) && \"string\" !== t || Array.isArray(e)) && \"complex64\" !== t) throw new Error(\"Error creating a new Scalar: value must be a primitive (number|boolean|string)\");\n  if (\"string\" === t && isTypedArray$1(e) && !(e instanceof Uint8Array)) throw new Error(\"When making a scalar from encoded string, the value must be `Uint8Array`.\");\n  return makeTensor$1(e, [], [], t);\n}\n\nfunction selu_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"selu\");\n  return ENGINE$1.runKernel(Selu$3, {\n    x: t\n  });\n}\n\nvar selu$5 = op$1({\n  selu_: selu_$1\n});\n\nfunction separableConv2d_$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"NHWC\";\n  var i = convertToTensor$1(e, \"x\", \"separableConv2d\"),\n      l = convertToTensor$1(t, \"depthwiseFilter\", \"separableConv2d\"),\n      u = convertToTensor$1(n, \"pointwiseFilter\", \"separableConv2d\");\n  var c = i,\n      p = !1;\n  if (3 === i.rank && (p = !0, c = reshape$6(i, [1, i.shape[0], i.shape[1], i.shape[2]])), \"NCHW\" === o) throw new Error(\"separableConv2d currently does not support dataFormat NCHW; only NHWC is supported\");\n  assert$6(4 === c.rank, () => \"Error in separableConv2d: input must be rank 4, but got rank \".concat(c.rank, \".\")), assert$6(4 === l.rank, () => \"Error in separableConv2d: depthwise filter must be rank 4, but got rank \".concat(l.rank, \".\")), assert$6(4 === u.rank, () => \"Error in separableConv2d: pointwise filter must be rank 4, but got rank \".concat(l.rank, \".\")), assert$6(1 === u.shape[0], () => \"Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got \".concat(u.shape[0], \".\")), assert$6(1 === u.shape[1], () => \"Error in separableConv2d: the second dimension of pointwise filter must be 1, but got \".concat(u.shape[1], \".\"));\n  var d = l.shape[2],\n      h = l.shape[3];\n  assert$6(u.shape[2] === d * h, () => \"Error in separableConv2d: the third dimension of pointwise filter must be \".concat(d * h, \", but got \").concat(u.shape[2], \".\"));\n  var m = depthwiseConv2d$5(c, l, r, a, o, s),\n      f = conv2d$7(m, u, 1, \"valid\", o);\n  return p ? reshape$6(f, [f.shape[1], f.shape[2], f.shape[3]]) : f;\n}\n\nvar separableConv2d$2 = op$1({\n  separableConv2d_: separableConv2d_$1\n});\n\nfunction sign_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"sign\");\n  return ENGINE$1.runKernel(Sign$1, {\n    x: t\n  });\n}\n\nvar sign$5 = op$1({\n  sign_: sign_$1\n});\n\nfunction sin_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"sin\");\n  return ENGINE$1.runKernel(Sin$1, {\n    x: t\n  });\n}\n\nvar sin$5 = op$1({\n  sin_: sin_$1\n});\n\nfunction sinh_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"sinh\");\n  return ENGINE$1.runKernel(Sinh$1, {\n    x: t\n  });\n}\n\nvar sinh$5 = op$1({\n  sinh_: sinh_$1\n});\n\nfunction slice1d_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"slice1d\");\n  return assert$6(1 === r.rank, () => \"slice1d expects a rank-1 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$5(r, [t], [n]);\n}\n\nvar slice1d$1 = op$1({\n  slice1d_: slice1d_$1\n});\n\nfunction slice2d_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"slice2d\");\n  return assert$6(2 === r.rank, () => \"slice2d expects a rank-2 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$5(r, t, n);\n}\n\nvar slice2d$1 = op$1({\n  slice2d_: slice2d_$1\n});\n\nfunction slice3d_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"slice3d\");\n  return assert$6(3 === r.rank, () => \"slice3d expects a rank-3 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$5(r, t, n);\n}\n\nvar slice3d$1 = op$1({\n  slice3d_: slice3d_$1\n});\n\nfunction slice4d_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"slice4d\");\n  return assert$6(4 === r.rank, () => \"slice4d expects a rank-4 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$5(r, t, n);\n}\n\nvar slice4d$1 = op$1({\n  slice4d_: slice4d_$1\n});\n\nfunction softmax_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n = convertToTensor$1(e, \"logits\", \"softmax\", \"float32\");\n  if (-1 === t && (t = n.rank - 1), t !== n.rank - 1) throw Error(\"Softmax along a non-last dimension is not yet supported. Logits was rank \".concat(n.rank, \" and dim was \").concat(t));\n  return ENGINE$1.runKernel(Softmax$5, {\n    logits: n\n  }, {\n    dim: t\n  });\n}\n\nvar softmax$6 = op$1({\n  softmax_: softmax_$1\n});\n\nfunction fft_$1(e) {\n  return assert$6(\"complex64\" === e.dtype, () => \"The dtype for tf.spectral.fft() must be complex64 but got \".concat(e.dtype, \".\")), ENGINE$1.runKernel(FFT$1, {\n    input: e\n  });\n}\n\nvar fft$5 = op$1({\n  fft_: fft_$1\n});\n\nfunction ifft_$1(e) {\n  return assert$6(\"complex64\" === e.dtype, () => \"The dtype for tf.spectral.ifft() must be complex64 but got \".concat(e.dtype, \".\")), ENGINE$1.runKernel(IFFT$1, {\n    input: e\n  });\n}\n\nvar ifft$5 = op$1({\n  ifft_: ifft_$1\n});\n\nfunction irfft_$1(e) {\n  var t = e.shape[e.shape.length - 1],\n      n = e.size / t;\n  var r;\n\n  if (t <= 2) {\n    var a = reshape$6(e, [n, t]);\n    r = ifft$5(a);\n  } else {\n    var _a12 = [n, 2 * (t - 1)],\n        s = reshape$6(real$5(e), [n, t]),\n        o = reshape$6(imag$5(e), [n, t]),\n        i = reverse$5(slice$5(s, [0, 1], [n, t - 2]), 1),\n        l = mul$1(reverse$5(slice$5(o, [0, 1], [n, t - 2]), 1), scalar$1(-1)),\n        u = concat$5([s, i], 1),\n        c = concat$5([o, l], 1),\n        _p2 = reshape$6(complex$5(u, c), [_a12[0], _a12[1]]);\n\n    r = ifft$5(_p2);\n  }\n\n  if (r = real$5(r), 3 === e.rank && 0 !== e.shape[0]) {\n    var _t50 = r,\n        _n14 = e.shape[0];\n    r = reshape$6(r, [_n14, r.shape[0] / _n14, r.shape[1]]), _t50.dispose();\n  }\n\n  return r;\n}\n\nvar irfft$1 = op$1({\n  irfft_: irfft_$1\n});\n\nfunction split_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = convertToTensor$1(e, \"x\", \"split\");\n  return ENGINE$1.runKernel(SplitV$1, {\n    x: r\n  }, {\n    numOrSizeSplits: t,\n    axis: n\n  });\n}\n\nvar split$4 = op$1({\n  split_: split_$1\n});\n\nfunction rfft_$1(e, t) {\n  assert$6(\"float32\" === e.dtype, () => \"The dtype for rfft() must be real value but got \".concat(e.dtype));\n  var n = e.shape[e.shape.length - 1];\n  var r = e.size / n;\n  var a;\n\n  if (null != t && t < n) {\n    var _r16 = e.shape.map(e => 0),\n        _s7 = e.shape.map(e => e);\n\n    _s7[e.shape.length - 1] = t, a = slice$5(e, _r16, _s7), n = t;\n  } else if (null != t && t > n) {\n    var _r17 = e.shape.map(e => e);\n\n    _r17[e.shape.length - 1] = t - n, a = concat$5([e, zeros$4(_r17)], e.shape.length - 1), n = t;\n  } else a = e;\n\n  var s = zerosLike$5(a),\n      o = reshape$6(complex$5(a, s), [r, n]),\n      i = fft$5(o),\n      l = Math.floor(n / 2) + 1,\n      u = real$5(i),\n      c = imag$5(i),\n      p = split$4(u, [l, n - l], u.shape.length - 1),\n      d = split$4(c, [l, n - l], c.shape.length - 1),\n      h = a.shape.slice();\n  return h[a.shape.length - 1] = l, reshape$6(complex$5(p[0], d[0]), h);\n}\n\nvar rfft$1 = op$1({\n  rfft_: rfft_$1\n});\n\nfunction sqrt_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"sqrt\");\n  return ENGINE$1.runKernel(Sqrt$1, {\n    x: t\n  });\n}\n\nvar sqrt$5 = op$1({\n  sqrt_: sqrt_$1\n});\n\nfunction squaredDifference_$1(e, t) {\n  var n = convertToTensor$1(e, \"a\", \"squaredDifference\"),\n      r = convertToTensor$1(t, \"b\", \"squaredDifference\");\n  return [n, r] = makeTypesMatch$1(n, r), assertAndGetBroadcastShape$1(n.shape, r.shape), ENGINE$1.runKernel(SquaredDifference$1, {\n    a: n,\n    b: r\n  }, {});\n}\n\nvar squaredDifference$5 = op$1({\n  squaredDifference_: squaredDifference_$1\n});\n\nfunction squeeze_$1(e, t) {\n  var n = convertToTensor$1(e, \"x\", \"squeeze\");\n  return reshape$6(n, squeezeShape$1(n.shape, t).newShape);\n}\n\nvar squeeze$1 = op$1({\n  squeeze_: squeeze_$1\n});\n\nfunction stack_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensorArray$1(e, \"tensors\", \"stack\", \"string_or_numeric\");\n  return assert$6(n.length >= 1, () => \"Pass at least one tensor to tf.stack\"), n.length > 0 && assert$6(t <= n[0].rank, () => \"Axis must be <= rank of the tensor\"), ENGINE$1.runKernel(Pack$1, n, {\n    axis: t\n  });\n}\n\nvar stack$1 = op$1({\n  stack_: stack_$1\n});\n\nfunction step_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor$1(e, \"x\", \"step\");\n  return ENGINE$1.runKernel(Step$1, {\n    x: n\n  }, {\n    alpha: t\n  });\n}\n\nvar step$5 = op$1({\n  step_: step_$1\n});\n\nfunction stridedSlice_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : 0;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : 0;\n  var l = arguments.length > 8 && arguments[8] !== undefined ? arguments[8] : 0;\n  var u = convertToTensor$1(e, \"x\", \"stridedSlice\", \"string_or_numeric\");\n  return ENGINE$1.runKernel(StridedSlice$1, {\n    x: u\n  }, {\n    begin: t,\n    end: n,\n    strides: r,\n    beginMask: a,\n    endMask: s,\n    ellipsisMask: o,\n    newAxisMask: i,\n    shrinkAxisMask: l\n  });\n}\n\nvar stridedSlice$5 = op$1({\n  stridedSlice_: stridedSlice_$1\n});\n\nfunction tan_$1(e) {\n  var t = convertToTensor$1(e, \"x\", \"tan\");\n  return ENGINE$1.runKernel(Tan$1, {\n    x: t\n  });\n}\n\nvar tan$5 = op$1({\n  tan_: tan_$1\n});\n\nfunction tensor1d$1(e, t) {\n  assertNonNull$1(e);\n  var n = inferShape$1(e, t);\n  if (1 !== n.length) throw new Error(\"tensor1d() requires values to be a flat/TypedArray\");\n  return makeTensor$1(e, null, n, t);\n}\n\nfunction tensor2d$1(e, t, n) {\n  if (assertNonNull$1(e), null != t && 2 !== t.length) throw new Error(\"tensor2d() requires shape to have two numbers\");\n  var r = inferShape$1(e, n);\n  if (2 !== r.length && 1 !== r.length) throw new Error(\"tensor2d() requires values to be number[][] or flat/TypedArray\");\n  if (1 === r.length && null == t) throw new Error(\"tensor2d() requires shape to be provided when `values` are a flat/TypedArray\");\n  return makeTensor$1(e, t, r, n);\n}\n\nfunction topk_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n  var r = convertToTensor$1(e, \"x\", \"topk\");\n  if (0 === r.rank) throw new Error(\"topk() expects the input to be of rank 1 or higher\");\n  var a = r.shape[r.shape.length - 1];\n  if (t < 0) throw new Error(\"'k' passed to topk() must be >= 0 but got \".concat(t));\n  if (t > a) throw new Error(\"'k' passed to topk() must be <= the last dimension (\".concat(a, \") but got \").concat(t));\n  var s = {\n    x: r\n  },\n      o = {\n    k: t,\n    sorted: n\n  },\n      [i, l] = ENGINE$1.runKernel(TopK$1, s, o);\n  return {\n    values: i,\n    indices: l\n  };\n}\n\nvar topk$1 = op$1({\n  topk_: topk_$1\n});\n\nfunction truncatedNormal_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  if (null != r && \"bool\" === r) throw new Error(\"Unsupported data type $ { dtype }\");\n  var s = new MPRandGauss$1(t, n, r, !0, a),\n      o = buffer$1(e, r);\n\n  for (var _e58 = 0; _e58 < o.values.length; _e58++) {\n    o.values[_e58] = s.nextValue();\n  }\n\n  return o.toTensor();\n}\n\nvar truncatedNormal$2 = op$1({\n  truncatedNormal_: truncatedNormal_$1\n});\n\nfunction unique_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor$1(e, \"x\", \"unique\", \"string_or_numeric\");\n  assert$6(n.rank > 0, () => \"The input tensor must be at least 1D\");\n  var r = {\n    x: n\n  },\n      a = {\n    axis: t\n  },\n      [s, o] = ENGINE$1.runKernel(Unique$1, r, a);\n  return {\n    values: s,\n    indices: o\n  };\n}\n\nvar unique$7 = op$1({\n  unique_: unique_$1\n});\n\nfunction unsortedSegmentSum_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"x\", \"unsortedSegmentSum\"),\n      a = convertToTensor$1(t, \"segmentIds\", \"unsortedSegmentSum\", \"int32\");\n  return assert$6(isInt$1(n), () => \"numSegments must be of dtype int\"), ENGINE$1.runKernel(UnsortedSegmentSum$1, {\n    x: r,\n    segmentIds: a\n  }, {\n    numSegments: n\n  });\n}\n\nvar unsortedSegmentSum$5 = op$1({\n  unsortedSegmentSum_: unsortedSegmentSum_$1\n});\n\nfunction unstack_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor$1(e, \"x\", \"unstack\", \"string_or_numeric\");\n  return assert$6(t >= -n.shape.length && t < n.shape.length, () => \"Axis = \".concat(t, \" is not in [-\").concat(n.shape.length, \", \").concat(n.shape.length, \")\")), ENGINE$1.runKernel(Unpack$1, {\n    value: n\n  }, {\n    axis: t\n  });\n}\n\nvar unstack$1 = op$1({\n  unstack_: unstack_$1\n});\n\nfunction variable$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  return ENGINE$1.makeVariable(e, t, n, r);\n}\n\nfunction whereImpl$5(e, t) {\n  var n = [];\n\n  for (var _e59 = 0; _e59 < t.length; _e59++) {\n    t[_e59] && n.push(_e59);\n  }\n\n  var r = buffer$1(e, \"int32\"),\n      a = buffer$1([n.length, e.length], \"int32\");\n\n  for (var _t51 = 0; _t51 < n.length; _t51++) {\n    var s = r.indexToLoc(n[_t51]);\n    a.values.set(s, _t51 * e.length);\n  }\n\n  return a.toTensor();\n}\n\nfunction norm_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"euclidean\";\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = normImpl$1(e = convertToTensor$1(e, \"x\", \"norm\"), t, n);\n  var s = a.shape;\n\n  if (r) {\n    var _t52 = parseAxisParam$1(n, e.shape);\n\n    s = expandShapeToKeepDim$1(a.shape, _t52);\n  }\n\n  return reshape$6(a, s);\n}\n\nfunction normImpl$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n  if (0 === e.rank) return abs$5(e);\n  if (1 !== e.rank && null === n) return normImpl$1(reshape$6(e, [-1]), t, n);\n\n  if (1 === e.rank || \"number\" == typeof n || Array.isArray(n) && 1 === n.length) {\n    if (1 === t) return sum$6(abs$5(e), n);\n    if (Infinity === t) return max$7(abs$5(e), n);\n    if (-Infinity === t) return min$7(abs$5(e), n);\n    if (\"euclidean\" === t || 2 === t) return sqrt$5(sum$6(pow$5(abs$5(e), scalar$1(2, \"int32\")), n));\n    throw new Error(\"Error in norm: invalid ord value: \".concat(t));\n  }\n\n  if (Array.isArray(n) && 2 === n.length) {\n    if (1 === t) return max$7(sum$6(abs$5(e), n[0]), n[1] - 1);\n    if (Infinity === t) return max$7(sum$6(abs$5(e), n[1]), n[0]);\n    if (-Infinity === t) return min$7(sum$6(abs$5(e), n[1]), n[0]);\n    if (\"fro\" === t || \"euclidean\" === t) return sqrt$5(sum$6(square$5(e), n));\n    throw new Error(\"Error in norm: invalid ord value: \".concat(t));\n  }\n\n  throw new Error(\"Error in norm: invalid axis: \".concat(n));\n}\n\nvar norm$1 = op$1({\n  norm_: norm_$1\n});\n\nfunction getNoiseShape$1(e, t) {\n  if (null == t) return e.shape.slice();\n  if (arraysEqual$1(e.shape, t)) return t;\n\n  if (e.shape.length === t.length) {\n    var n = [];\n\n    for (var r = 0; r < e.shape.length; r++) {\n      n.push(null == t[r] && null != e.shape[r] ? e.shape[r] : t[r]);\n    }\n\n    return n;\n  }\n\n  return t;\n}\n\nfunction dropout_$1(e, t, n, r) {\n  var a = convertToTensor$1(e, \"x\", \"dropout\");\n  if (assert$6(\"float32\" === a.dtype, () => \"x has to be a floating point tensor since it's going to be scaled, but got a \".concat(a.dtype, \" tensor instead.\")), assert$6(t >= 0 && t < 1, () => \"rate must be a float in the range [0, 1), but got \".concat(t, \".\")), 0 === t) return e instanceof Tensor$1 ? a.clone() : a;\n  var s = getNoiseShape$1(a, n),\n      o = 1 - t,\n      i = div$3(floor$5(add$5(randomUniform$2(s, 0, 1, \"float32\", r), o)), o);\n  return mul$1(a, i);\n}\n\nvar dropout$5 = op$1({\n  dropout_: dropout_$1\n});\n\nfunction enclosingPowerOfTwo$1(e) {\n  return Math.floor(Math.pow(2, Math.ceil(Math.log(e) / Math.log(2))));\n}\n\nfunction cosineWindow$1(e, t, n) {\n  var r = 1 - e % 2,\n      a = new Float32Array(e);\n\n  for (var s = 0; s < e; ++s) {\n    var o = 2 * Math.PI * s / (e + r - 1);\n    a[s] = t - n * Math.cos(o);\n  }\n\n  return tensor1d$1(a, \"float32\");\n}\n\nfunction conv2DBackpropFilter_$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NHWC\";\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = e;\n  3 === e.rank && (i = reshape$6(e, [1, e.shape[0], e.shape[1], e.shape[2]]));\n  var l = t;\n  3 === l.rank && (l = reshape$6(t, [1, t.shape[0], t.shape[1], t.shape[2]])), assert$6(4 === i.rank, () => \"Error in conv2dDerFilter: input must be rank 4, but got shape \".concat(i.shape, \".\")), assert$6(4 === l.rank, () => \"Error in conv2dDerFilter: dy must be rank 4, but got shape \".concat(l.shape, \".\")), assert$6(4 === n.length, () => \"Error in conv2dDerFilter: filterShape must be length 4, but got \".concat(n, \".\"));\n  var u = \"NHWC\" === s ? i.shape[3] : i.shape[1],\n      c = \"NHWC\" === s ? l.shape[3] : l.shape[1];\n  return assert$6(u === n[2], () => \"Error in conv2dDerFilter: depth of input \".concat(u, \") must match input depth in filter (\").concat(n[2], \".\")), assert$6(c === n[3], () => \"Error in conv2dDerFilter: depth of dy (\".concat(c, \") must match output depth for filter (\").concat(n[3], \").\")), null != o && assert$6(isInt$1(a), () => \"Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(a, \".\")), ENGINE$1.runKernel(Conv2DBackpropFilter$1, {\n    x: i,\n    dy: l\n  }, {\n    strides: r,\n    pad: a,\n    dataFormat: s,\n    dimRoundingMode: o,\n    filterShape: n\n  });\n}\n\nvar conv2DBackpropFilter$5 = op$1({\n  conv2DBackpropFilter_: conv2DBackpropFilter_$1\n});\n\nfunction getFusedDyActivation$1(e, t, n) {\n  if (null == n || \"linear\" === n) return e;\n  if (\"relu\" === n) return mul$1(e, step$5(t));\n  throw new Error(\"Cannot compute gradient for fused activation \".concat(n, \".\"));\n}\n\nfunction getFusedBiasGradient$1(e, t) {\n  var n = t;\n  var r = getReductionAxes$1(e.shape, t.shape);\n  return r.length > 0 && (n = sum$6(n, r)), reshape$6(n, e.shape);\n}\n\nfunction applyActivation$3(e, t, n, r) {\n  if (\"linear\" === t) return e;\n  if (\"relu\" === t) return relu$6(e);\n  if (\"elu\" === t) return elu$8(e);\n  if (\"relu6\" === t) return relu6$5(e);\n  if (\"prelu\" === t) return prelu$6(e, n);\n  if (\"leakyrelu\" === t) return leakyRelu$5(e, r);\n  if (\"sigmoid\" === t) return sigmoid$5(e);\n  throw new Error(\"Unknown fused activation \".concat(t, \".\"));\n}\n\nvar shouldFuse$1 = (e, t) => !(e > 0) || \"linear\" === t;\n\nfunction fusedConv2d_$1(_ref) {\n  var {\n    x: e,\n    filter: t,\n    strides: n,\n    pad: r,\n    dataFormat: a = \"NHWC\",\n    dilations: s = [1, 1],\n    dimRoundingMode: o,\n    bias: i,\n    activation: l = \"linear\",\n    preluActivationWeights: u,\n    leakyreluAlpha: c\n  } = _ref;\n\n  if (!1 === shouldFuse$1(ENGINE$1.state.gradientDepth, l = l || \"linear\")) {\n    var _p3 = conv2d$7(e, t, n, r, a, s, o);\n\n    return null != i && (_p3 = add$5(_p3, i)), applyActivation$3(_p3, l, u, c);\n  }\n\n  var p = convertToTensor$1(e, \"x\", \"conv2d\"),\n      d = convertToTensor$1(t, \"filter\", \"conv2d\");\n  var h = p,\n      m = !1;\n  3 === p.rank && (m = !0, h = reshape$6(p, [1, p.shape[0], p.shape[1], p.shape[2]])), assert$6(4 === h.rank, () => \"Error in fused conv2d: input must be rank 4, but got rank \".concat(h.rank, \".\")), assert$6(4 === d.rank, () => \"Error in fused conv2d: filter must be rank 4, but got rank \".concat(d.rank, \".\")), null != o && assert$6(isInt$1(r), () => \"Error in fused conv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\")), assert$6(h.shape[3] === d.shape[2], () => \"Error in conv2d: depth of input (\".concat(h.shape[3], \") must match input depth for filter \").concat(d.shape[2], \".\")), assert$6(eitherStridesOrDilationsAreOne$1(n, s), () => \"Error in conv2D: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '\").concat(s, \"'\")), assert$6(\"NHWC\" === a, () => \"Error in conv2d: got dataFormat of \".concat(a, \" but only NHWC is currently supported.\"));\n  var f = computeConv2DInfo$1(h.shape, d.shape, n, s, r, o);\n  var g, $;\n  null != i && (g = convertToTensor$1(i, \"bias\", \"fused conv2d\"), [g] = makeTypesMatch$1(g, p), assertAndGetBroadcastShape$1(f.outShape, g.shape)), null != u && ($ = convertToTensor$1(u, \"prelu weights\", \"fused conv2d\"));\n\n  var y = (e, t) => {\n    var [a, o, i, u] = t,\n        c = getFusedDyActivation$1(e, i, l);\n    assert$6(tupleValuesAreOne$1(s), () => \"Error in gradient of fused conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '\".concat(s, \"'\"));\n    var p = [conv2DBackpropInput$5(o.shape, c, a, n, r), conv2DBackpropFilter$5(o, c, a.shape, n, r)];\n\n    if (null != u) {\n      var _e60 = getFusedBiasGradient$1(u, c);\n\n      p.push(_e60);\n    }\n\n    return p;\n  },\n      b = {\n    x: h,\n    filter: d,\n    bias: g,\n    preluActivationWeights: $\n  },\n      x = {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s,\n    dimRoundingMode: o,\n    activation: l,\n    leakyreluAlpha: c\n  };\n\n  return null == i ? customGrad$1((e, t, n) => {\n    var r = ENGINE$1.runKernel(FusedConv2D$1, b, x);\n    return n([t, e, r]), m && (r = reshape$6(r, [r.shape[1], r.shape[2], r.shape[3]])), {\n      value: r,\n      gradFunc: y\n    };\n  })(h, d) : customGrad$1((e, t, n, r) => {\n    var a = ENGINE$1.runKernel(FusedConv2D$1, b, x);\n    return r([t, e, a, n]), m && (a = reshape$6(a, [a.shape[1], a.shape[2], a.shape[3]])), {\n      value: a,\n      gradFunc: y\n    };\n  })(h, d, g);\n}\n\nvar conv2d$6 = op$1({\n  fusedConv2d_: fusedConv2d_$1\n});\n\nfunction depthwiseConv2dNativeBackpropFilter_$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = e;\n  3 === e.rank && (i = reshape$6(e, [1, e.shape[0], e.shape[1], e.shape[2]]));\n  var l = t;\n  return 3 === l.rank && (l = reshape$6(t, [1, t.shape[0], t.shape[1], t.shape[2]])), ENGINE$1.runKernel(DepthwiseConv2dNativeBackpropFilter$1, {\n    x: i,\n    dy: l\n  }, {\n    strides: r,\n    pad: a,\n    dimRoundingMode: o,\n    dilations: s,\n    filterShape: n\n  });\n}\n\nvar depthwiseConv2dNativeBackpropFilter$5 = op$1({\n  depthwiseConv2dNativeBackpropFilter_: depthwiseConv2dNativeBackpropFilter_$1\n});\n\nfunction depthwiseConv2dNativeBackpropInput_$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = t,\n      l = !1;\n  3 === t.rank && (l = !0, i = reshape$6(t, [1, t.shape[0], t.shape[1], t.shape[2]]));\n  var u = ENGINE$1.runKernel(DepthwiseConv2dNativeBackpropInput$1, {\n    dy: i,\n    filter: n\n  }, {\n    strides: r,\n    pad: a,\n    dimRoundingMode: o,\n    dilations: s,\n    inputShape: e\n  });\n  return l ? reshape$6(u, [u.shape[1], u.shape[2], u.shape[3]]) : u;\n}\n\nvar depthwiseConv2dNativeBackpropInput$5 = op$1({\n  depthwiseConv2dNativeBackpropInput_: depthwiseConv2dNativeBackpropInput_$1\n});\n\nfunction fusedMatMul_$1(_ref2) {\n  var {\n    a: e,\n    b: t,\n    transposeA: n = !1,\n    transposeB: r = !1,\n    bias: a,\n    activation: s = \"linear\",\n    preluActivationWeights: o,\n    leakyreluAlpha: i\n  } = _ref2;\n\n  if (!1 === shouldFuse$1(ENGINE$1.state.gradientDepth, s)) {\n    var _l4 = matMul$3(e, t, n, r);\n\n    return null != a && (_l4 = add$5(_l4, a)), applyActivation$3(_l4, s, o, i);\n  }\n\n  var l = convertToTensor$1(e, \"a\", \"fused matMul\"),\n      u = convertToTensor$1(t, \"b\", \"fused matMul\");\n  [l, u] = makeTypesMatch$1(l, u);\n  var c = n ? l.shape[l.rank - 2] : l.shape[l.rank - 1],\n      p = r ? u.shape[u.rank - 1] : u.shape[u.rank - 2],\n      d = n ? l.shape[l.rank - 1] : l.shape[l.rank - 2],\n      h = r ? u.shape[u.rank - 2] : u.shape[u.rank - 1],\n      m = l.shape.slice(0, -2),\n      f = u.shape.slice(0, -2),\n      g = sizeFromShape$1(m),\n      $ = sizeFromShape$1(f);\n  assert$6(l.rank >= 2 && u.rank >= 2 && l.rank === u.rank, () => \"Error in fused matMul: inputs must have the same rank of at least 2, got ranks \".concat(l.rank, \" and \").concat(u.rank, \".\")), assert$6(arraysEqual$1(m, f), () => \"Error in fused matMul: outer dimensions (\".concat(m, \") and (\").concat(f, \") of Tensors with shapes \").concat(l.shape, \" and \").concat(u.shape, \" must match.\")), assert$6(c === p, () => \"Error in fused matMul: inner shapes (\".concat(c, \") and (\").concat(p, \") of Tensors with shapes \").concat(l.shape, \" and \").concat(u.shape, \" and transposeA=\").concat(n, \" and transposeB=\").concat(r, \" must match.\"));\n  var y = l.shape.slice(0, -2).concat([d, h]),\n      b = reshape$6(l, n ? [g, c, d] : [g, d, c]),\n      x = reshape$6(u, r ? [$, h, p] : [$, p, h]);\n  var v, I;\n  null != a && (v = convertToTensor$1(a, \"bias\", \"fused matMul\"), [v] = makeTypesMatch$1(v, l), assertAndGetBroadcastShape$1(y, v.shape)), null != o && (I = convertToTensor$1(o, \"prelu weights\", \"fused matMul\"));\n\n  var C = (e, t) => {\n    var [o, i, l, u] = t,\n        c = getFusedDyActivation$1(reshape$6(e, l.shape), l, s);\n    var p, d;\n    return n || r ? !n && r ? (p = matMul$3(c, i, !1, !1), d = matMul$3(c, o, !0, !1)) : n && !r ? (p = matMul$3(i, c, !1, !0), d = matMul$3(o, c, !1, !1)) : (p = matMul$3(i, c, !0, !0), d = matMul$3(c, o, !0, !0)) : (p = matMul$3(c, i, !1, !0), d = matMul$3(o, c, !0, !1)), null != a ? [p, d, getFusedBiasGradient$1(u, c)] : [p, d];\n  },\n      S = {\n    a: b,\n    b: x,\n    bias: v,\n    preluActivationWeights: I\n  },\n      k = {\n    transposeA: n,\n    transposeB: r,\n    activation: s,\n    leakyreluAlpha: i\n  };\n\n  return null == a ? customGrad$1((e, t, n) => {\n    var r = ENGINE$1.runKernel(_FusedMatMul$1, S, k);\n    return n([e, t, r]), {\n      value: reshape$6(r, y),\n      gradFunc: C\n    };\n  })(b, x) : customGrad$1((e, t, n, r) => {\n    var a = ENGINE$1.runKernel(_FusedMatMul$1, S, k);\n    return r([e, t, a, n]), {\n      value: reshape$6(a, y),\n      gradFunc: C\n    };\n  })(b, x, v);\n}\n\nvar matMul$2 = op$1({\n  fusedMatMul_: fusedMatMul_$1\n});\n\nfunction hammingWindow_$1(e) {\n  return cosineWindow$1(e, .54, .46);\n}\n\nvar hammingWindow$1 = op$1({\n  hammingWindow_: hammingWindow_$1\n});\n\nfunction hannWindow_$1(e) {\n  return cosineWindow$1(e, .5, .5);\n}\n\nvar hannWindow$1 = op$1({\n  hannWindow_: hannWindow_$1\n});\n\nfunction frame_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n  var s = 0;\n  var o = [];\n\n  for (; s + t <= e.size;) {\n    o.push(slice$5(e, s, t)), s += n;\n  }\n\n  if (r) for (; s < e.size;) {\n    var _r18 = s + t - e.size,\n        i = concat$5([slice$5(e, s, t - _r18), fill$5([_r18], a)]);\n\n    o.push(i), s += n;\n  }\n  return 0 === o.length ? tensor2d$1([], [0, t]) : reshape$6(concat$5(o), [o.length, t]);\n}\n\nvar frame$1 = op$1({\n  frame_: frame_$1\n});\n\nfunction stft_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : hannWindow$1;\n  null == r && (r = enclosingPowerOfTwo$1(t));\n  var s = frame$1(e, t, n),\n      o = mul$1(s, a(t));\n  return rfft$1(o, r);\n}\n\nvar stft$1 = op$1({\n  stft_: stft_$1\n});\n\nfunction cropAndResize_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"bilinear\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n  var o = convertToTensor$1(e, \"image\", \"cropAndResize\"),\n      i = convertToTensor$1(t, \"boxes\", \"cropAndResize\", \"float32\"),\n      l = convertToTensor$1(n, \"boxInd\", \"cropAndResize\", \"int32\"),\n      u = i.shape[0];\n  return assert$6(4 === o.rank, () => \"Error in cropAndResize: image must be rank 4,but got rank \".concat(o.rank, \".\")), assert$6(2 === i.rank && 4 === i.shape[1], () => \"Error in cropAndResize: boxes must be have size [\".concat(u, \",4] but had shape \").concat(i.shape, \".\")), assert$6(1 === l.rank && l.shape[0] === u, () => \"Error in cropAndResize: boxInd must be have size [\".concat(u, \"] but had shape \").concat(i.shape, \".\")), assert$6(2 === r.length, () => \"Error in cropAndResize: cropSize must be of length 2, but got length \".concat(r.length, \".\")), assert$6(r[0] >= 1 && r[1] >= 1, () => \"cropSize must be atleast [1,1], but was \".concat(r)), assert$6(\"bilinear\" === a || \"nearest\" === a, () => \"method must be bilinear or nearest, but was \".concat(a)), ENGINE$1.runKernel(CropAndResize$1, {\n    image: o,\n    boxes: i,\n    boxInd: l\n  }, {\n    method: a,\n    extrapolationValue: s,\n    cropSize: r\n  });\n}\n\nvar cropAndResize$5 = op$1({\n  cropAndResize_: cropAndResize_$1\n});\n\nfunction flipLeftRight_$1(e) {\n  var t = convertToTensor$1(e, \"image\", \"flipLeftRight\", \"float32\");\n  return assert$6(4 === t.rank, () => \"Error in flipLeftRight: image must be rank 4,but got rank \".concat(t.rank, \".\")), ENGINE$1.runKernel(FlipLeftRight$1, {\n    image: t\n  }, {});\n}\n\nvar flipLeftRight$1 = op$1({\n  flipLeftRight_: flipLeftRight_$1\n});\n\nfunction rotateWithOffset_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = convertToTensor$1(e, \"image\", \"rotateWithOffset\", \"float32\");\n  return assert$6(4 === a.rank, () => \"Error in rotateWithOffset: image must be rank 4,but got rank \".concat(a.rank, \".\")), ENGINE$1.runKernel(RotateWithOffset$1, {\n    image: a\n  }, {\n    radians: t,\n    fillValue: n,\n    center: r\n  });\n}\n\nvar rotateWithOffset$1 = op$1({\n  rotateWithOffset_: rotateWithOffset_$1\n});\n\nfunction nonMaxSuppSanityCheck$1(e, t, n, r, a, s) {\n  null == r && (r = .5), null == a && (a = Number.NEGATIVE_INFINITY), null == s && (s = 0);\n  var o = e.shape[0];\n  return n = Math.min(n, o), assert$6(0 <= r && r <= 1, () => \"iouThreshold must be in [0, 1], but was '\".concat(r, \"'\")), assert$6(2 === e.rank, () => \"boxes must be a 2D tensor, but was of rank '\".concat(e.rank, \"'\")), assert$6(4 === e.shape[1], () => \"boxes must have 4 columns, but 2nd dimension was \".concat(e.shape[1])), assert$6(1 === t.rank, () => \"scores must be a 1D tensor\"), assert$6(t.shape[0] === o, () => \"scores has incompatible shape with boxes. Expected \".concat(o, \", but was \").concat(t.shape[0])), assert$6(0 <= s && s <= 1, () => \"softNmsSigma must be in [0, 1], but was '\".concat(s, \"'\")), {\n    maxOutputSize: n,\n    iouThreshold: r,\n    scoreThreshold: a,\n    softNmsSigma: s\n  };\n}\n\nfunction nonMaxSuppression_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n  var s = convertToTensor$1(e, \"boxes\", \"nonMaxSuppression\"),\n      o = convertToTensor$1(t, \"scores\", \"nonMaxSuppression\"),\n      i = nonMaxSuppSanityCheck$1(s, o, n, r, a);\n  return ENGINE$1.runKernel(NonMaxSuppressionV3$1, {\n    boxes: s,\n    scores: o\n  }, {\n    maxOutputSize: n = i.maxOutputSize,\n    iouThreshold: r = i.iouThreshold,\n    scoreThreshold: a = i.scoreThreshold\n  });\n}\n\nvar nonMaxSuppression$1 = op$1({\n  nonMaxSuppression_: nonMaxSuppression_$1\n});\n\nfunction binaryInsert$1(e, t, n) {\n  var r = binarySearch$1(e, t, n);\n  e.splice(r < 0 ? -(r + 1) : r, 0, t);\n}\n\nfunction binarySearch$1(e, t, n) {\n  return binarySearch_$1(e, t, n || defaultComparator$1);\n}\n\nfunction defaultComparator$1(e, t) {\n  return e > t ? 1 : e < t ? -1 : 0;\n}\n\nfunction binarySearch_$1(e, t, n) {\n  var r = 0,\n      a = e.length,\n      s = 0,\n      o = !1;\n\n  for (; r < a;) {\n    s = r + (a - r >>> 1);\n    var i = n(t, e[s]);\n    i > 0 ? r = s + 1 : (a = s, o = !i);\n  }\n\n  return o ? r : -r - 1;\n}\n\nfunction nonMaxSuppressionV3Impl$5(e, t, n, r, a) {\n  return nonMaxSuppressionImpl_$1(e, t, n, r, a, 0);\n}\n\nfunction nonMaxSuppressionV4Impl$5(e, t, n, r, a, s) {\n  return nonMaxSuppressionImpl_$1(e, t, n, r, a, 0, !1, s, !0);\n}\n\nfunction nonMaxSuppressionV5Impl$5(e, t, n, r, a, s) {\n  return nonMaxSuppressionImpl_$1(e, t, n, r, a, s, !0);\n}\n\nfunction nonMaxSuppressionImpl_$1(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !1;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : !1;\n  var l = arguments.length > 8 && arguments[8] !== undefined ? arguments[8] : !1;\n  var u = [];\n\n  for (var _e61 = 0; _e61 < t.length; _e61++) {\n    t[_e61] > a && u.push({\n      score: t[_e61],\n      boxIndex: _e61,\n      suppressBeginIndex: 0\n    });\n  }\n\n  u.sort(ascendingComparator$1);\n  var c = s > 0 ? -.5 / s : 0,\n      p = [],\n      d = [];\n\n  for (; p.length < n && u.length > 0;) {\n    var _t53 = u.pop(),\n        {\n      score: _n15,\n      boxIndex: _s8,\n      suppressBeginIndex: _o5\n    } = _t53;\n\n    if (_n15 < a) break;\n\n    var _i4 = !1;\n\n    for (var _n16 = p.length - 1; _n16 >= _o5; --_n16) {\n      var _o6 = intersectionOverUnion$1(e, _s8, p[_n16]);\n\n      if (_o6 >= r) {\n        _i4 = !0;\n        break;\n      }\n\n      if (_t53.score = _t53.score * suppressWeight$1(r, c, _o6), _t53.score <= a) break;\n    }\n\n    _t53.suppressBeginIndex = p.length, _i4 || (_t53.score === _n15 ? (p.push(_s8), d.push(_t53.score)) : _t53.score > a && binaryInsert$1(u, _t53, ascendingComparator$1));\n  }\n\n  var h = p.length,\n      m = n - h;\n  i && m > 0 && (p.push(...new Array(m).fill(0)), d.push(...new Array(m).fill(0)));\n  var f = {\n    selectedIndices: p\n  };\n  return o && (f.selectedScores = d), l && (f.validOutputs = h), f;\n}\n\nfunction intersectionOverUnion$1(e, t, n) {\n  var r = e.subarray(4 * t, 4 * t + 4),\n      a = e.subarray(4 * n, 4 * n + 4),\n      s = Math.min(r[0], r[2]),\n      o = Math.min(r[1], r[3]),\n      i = Math.max(r[0], r[2]),\n      l = Math.max(r[1], r[3]),\n      u = Math.min(a[0], a[2]),\n      c = Math.min(a[1], a[3]),\n      p = Math.max(a[0], a[2]),\n      d = Math.max(a[1], a[3]),\n      h = (i - s) * (l - o),\n      m = (p - u) * (d - c);\n  if (h <= 0 || m <= 0) return 0;\n  var f = Math.max(s, u),\n      g = Math.max(o, c),\n      $ = Math.min(i, p),\n      y = Math.min(l, d),\n      b = Math.max($ - f, 0) * Math.max(y - g, 0);\n  return b / (h + m - b);\n}\n\nfunction suppressWeight$1(e, t, n) {\n  var r = Math.exp(t * n * n);\n  return n <= e ? r : 0;\n}\n\nfunction ascendingComparator$1(e, t) {\n  return e.score - t.score || e.score === t.score && t.boxIndex - e.boxIndex;\n}\n\nfunction nonMaxSuppressionAsync_$1(_x7, _x8, _x9) {\n  return _nonMaxSuppressionAsync_$.apply(this, arguments);\n}\n\nfunction _nonMaxSuppressionAsync_$() {\n  _nonMaxSuppressionAsync_$ = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n    var s = convertToTensor$1(e, \"boxes\", \"nonMaxSuppressionAsync\"),\n        o = convertToTensor$1(t, \"scores\", \"nonMaxSuppressionAsync\"),\n        i = nonMaxSuppSanityCheck$1(s, o, n, r, a);\n    n = i.maxOutputSize, r = i.iouThreshold, a = i.scoreThreshold;\n    var l = yield Promise.all([s.data(), o.data()]),\n        u = l[0],\n        c = l[1],\n        {\n      selectedIndices: p\n    } = nonMaxSuppressionV3Impl$5(u, c, n, r, a);\n    return s !== e && s.dispose(), o !== t && o.dispose(), tensor1d$1(p, \"int32\");\n  });\n  return _nonMaxSuppressionAsync_$.apply(this, arguments);\n}\n\nvar nonMaxSuppressionAsync$1 = nonMaxSuppressionAsync_$1;\n\nfunction nonMaxSuppressionWithScore_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n  var o = convertToTensor$1(e, \"boxes\", \"nonMaxSuppression\"),\n      i = convertToTensor$1(t, \"scores\", \"nonMaxSuppression\"),\n      l = nonMaxSuppSanityCheck$1(o, i, n, r, a, s),\n      u = ENGINE$1.runKernel(NonMaxSuppressionV5$1, {\n    boxes: o,\n    scores: i\n  }, {\n    maxOutputSize: n = l.maxOutputSize,\n    iouThreshold: r = l.iouThreshold,\n    scoreThreshold: a = l.scoreThreshold,\n    softNmsSigma: s = l.softNmsSigma\n  });\n  return {\n    selectedIndices: u[0],\n    selectedScores: u[1]\n  };\n}\n\nvar nonMaxSuppressionWithScore$1 = op$1({\n  nonMaxSuppressionWithScore_: nonMaxSuppressionWithScore_$1\n});\n\nfunction nonMaxSuppressionWithScoreAsync_$1(_x10, _x11, _x12) {\n  return _nonMaxSuppressionWithScoreAsync_$.apply(this, arguments);\n}\n\nfunction _nonMaxSuppressionWithScoreAsync_$() {\n  _nonMaxSuppressionWithScoreAsync_$ = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n    var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n    var o = convertToTensor$1(e, \"boxes\", \"nonMaxSuppressionAsync\"),\n        i = convertToTensor$1(t, \"scores\", \"nonMaxSuppressionAsync\"),\n        l = nonMaxSuppSanityCheck$1(o, i, n, r, a, s);\n    n = l.maxOutputSize, r = l.iouThreshold, a = l.scoreThreshold, s = l.softNmsSigma;\n    var u = yield Promise.all([o.data(), i.data()]),\n        c = u[0],\n        p = u[1],\n        {\n      selectedIndices: d,\n      selectedScores: h\n    } = nonMaxSuppressionV5Impl$5(c, p, n, r, a, s);\n    return o !== e && o.dispose(), i !== t && i.dispose(), {\n      selectedIndices: tensor1d$1(d, \"int32\"),\n      selectedScores: tensor1d$1(h)\n    };\n  });\n  return _nonMaxSuppressionWithScoreAsync_$.apply(this, arguments);\n}\n\nvar nonMaxSuppressionWithScoreAsync$1 = nonMaxSuppressionWithScoreAsync_$1;\n\nfunction nonMaxSuppressionPadded_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = convertToTensor$1(e, \"boxes\", \"nonMaxSuppression\"),\n      i = convertToTensor$1(t, \"scores\", \"nonMaxSuppression\"),\n      l = nonMaxSuppSanityCheck$1(o, i, n, r, a, null),\n      u = ENGINE$1.runKernel(NonMaxSuppressionV4$1, {\n    boxes: o,\n    scores: i\n  }, {\n    maxOutputSize: l.maxOutputSize,\n    iouThreshold: l.iouThreshold,\n    scoreThreshold: l.scoreThreshold,\n    padToMaxOutputSize: s\n  });\n  return {\n    selectedIndices: u[0],\n    validOutputs: u[1]\n  };\n}\n\nvar nonMaxSuppressionPadded$1 = op$1({\n  nonMaxSuppressionPadded_: nonMaxSuppressionPadded_$1\n});\n\nfunction nonMaxSuppressionPaddedAsync_$1(_x13, _x14, _x15) {\n  return _nonMaxSuppressionPaddedAsync_$.apply(this, arguments);\n}\n\nfunction _nonMaxSuppressionPaddedAsync_$() {\n  _nonMaxSuppressionPaddedAsync_$ = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n    var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n    var o = convertToTensor$1(e, \"boxes\", \"nonMaxSuppressionAsync\"),\n        i = convertToTensor$1(t, \"scores\", \"nonMaxSuppressionAsync\"),\n        l = nonMaxSuppSanityCheck$1(o, i, n, r, a, null),\n        u = l.maxOutputSize,\n        c = l.iouThreshold,\n        p = l.scoreThreshold,\n        [d, h] = yield Promise.all([o.data(), i.data()]),\n        {\n      selectedIndices: m,\n      validOutputs: f\n    } = nonMaxSuppressionV4Impl$5(d, h, u, c, p, s);\n    return o !== e && o.dispose(), i !== t && i.dispose(), {\n      selectedIndices: tensor1d$1(m, \"int32\"),\n      validOutputs: scalar$1(f, \"int32\")\n    };\n  });\n  return _nonMaxSuppressionPaddedAsync_$.apply(this, arguments);\n}\n\nvar nonMaxSuppressionPaddedAsync$1 = nonMaxSuppressionPaddedAsync_$1;\n\nfunction resizeBilinear_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor$1(e, \"images\", \"resizeBilinear\");\n  assert$6(3 === a.rank || 4 === a.rank, () => \"Error in resizeBilinear: x must be rank 3 or 4, but got rank \".concat(a.rank, \".\")), assert$6(2 === t.length, () => \"Error in resizeBilinear: new shape must 2D, but got shape \".concat(t, \".\")), assert$6(!1 === r || !1 === n, () => \"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false.\");\n  var s = a,\n      o = !1;\n  3 === a.rank && (o = !0, s = reshape$6(a, [1, a.shape[0], a.shape[1], a.shape[2]]));\n  var i = ENGINE$1.runKernel(ResizeBilinear$1, {\n    images: s\n  }, {\n    alignCorners: n,\n    halfPixelCenters: r,\n    size: t\n  });\n  return o ? reshape$6(i, [i.shape[1], i.shape[2], i.shape[3]]) : i;\n}\n\nvar resizeBilinear$5 = op$1({\n  resizeBilinear_: resizeBilinear_$1\n});\n\nfunction resizeNearestNeighbor_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor$1(e, \"images\", \"resizeNearestNeighbor\");\n  assert$6(3 === a.rank || 4 === a.rank, () => \"Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank \".concat(a.rank, \".\")), assert$6(2 === t.length, () => \"Error in resizeNearestNeighbor: new shape must 2D, but got shape \".concat(t, \".\")), assert$6(\"float32\" === a.dtype || \"int32\" === a.dtype, () => \"`images` must have `int32` or `float32` as dtype\"), assert$6(!1 === r || !1 === n, () => \"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false.\");\n  var s = a,\n      o = !1;\n  3 === a.rank && (o = !0, s = reshape$6(a, [1, a.shape[0], a.shape[1], a.shape[2]]));\n  var i = ENGINE$1.runKernel(ResizeNearestNeighbor$1, {\n    images: s\n  }, {\n    alignCorners: n,\n    halfPixelCenters: r,\n    size: t\n  });\n  return o ? reshape$6(i, [i.shape[1], i.shape[2], i.shape[3]]) : i;\n}\n\nvar resizeNearestNeighbor$5 = op$1({\n  resizeNearestNeighbor_: resizeNearestNeighbor_$1\n});\n\nfunction threshold_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"binary\";\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = convertToTensor$1(e, \"image\", \"threshold\"),\n      s = a.shape[0] * a.shape[1];\n  var o,\n      i,\n      l,\n      u,\n      c = mul$1(tensor1d$1([r]), 255);\n\n  if (assert$6(3 === a.rank, () => \"Error in threshold: image must be rank 3,but got rank \".concat(a.rank, \".\")), assert$6(3 === a.shape[2] || 1 === a.shape[2], () => \"Error in threshold: image color channel must be equal to 3 or 1but got \".concat(a.shape[2], \".\")), assert$6(\"int32\" === a.dtype || \"float32\" === a.dtype, () => \"Error in dtype: image dtype must be int32 or float32,but got dtype \".concat(a.dtype, \".\")), assert$6(\"otsu\" === t || \"binary\" === t, () => \"Method must be binary or otsu, but was \".concat(t)), 3 === a.shape[2]) {\n    [o, i, l] = split$4(a, [1, 1, 1], -1);\n\n    var _e62 = mul$1(o, .2989),\n        _t54 = mul$1(i, .587),\n        _n17 = mul$1(l, .114);\n\n    u = add$5(add$5(_e62, _t54), _n17);\n  } else u = e;\n\n  \"otsu\" === t && (c = otsu$1(bincount$5(cast$7(round$6(u), \"int32\"), tensor$1([]), 256), s));\n  var p = n ? lessEqual$5(u, c) : greater$6(u, c);\n  return cast$7(mul$1(p, 255), \"int32\");\n}\n\nfunction otsu$1(e, t) {\n  var n,\n      r,\n      a,\n      s,\n      o,\n      i,\n      l = tensor1d$1([-1]),\n      u = tensor1d$1([0]),\n      c = tensor1d$1([0]);\n\n  for (var _p4 = 0; _p4 < e.size - 1; _p4++) {\n    n = slice$5(e, 0, _p4 + 1), r = slice$5(e, _p4 + 1), o = div$3(sum$6(n), t), i = div$3(sum$6(r), t);\n    var d = sum$6(mul$1(n, range$8(0, n.size)));\n    a = div$3(d, sum$6(n));\n    var h = fill$5(r.shape, n.size),\n        m = add$5(range$8(0, r.size), h),\n        f = mul$1(r, m);\n    s = div$3(sum$6(f), sum$6(r));\n    var g = sub$5(a, s),\n        $ = sub$5(a, s),\n        y = mul$1(o, i);\n    c = mul$1(mul$1(y, g), $);\n    var b = greater$6(c, u);\n    u = where$1(b, c, u), l = where$1(b, tensor1d$1([_p4]), l);\n  }\n\n  return l;\n}\n\nvar threshold$3 = op$1({\n  threshold_: threshold_$1\n});\n\nfunction transform_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"nearest\";\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"constant\";\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = convertToTensor$1(e, \"image\", \"transform\", \"float32\"),\n      i = convertToTensor$1(t, \"transforms\", \"transform\", \"float32\");\n  return assert$6(4 === o.rank, () => \"Error in transform: image must be rank 4,but got rank \".concat(o.rank, \".\")), assert$6(2 === i.rank && (i.shape[0] === o.shape[0] || 1 === i.shape[0]) && 8 === i.shape[1], () => \"Error in transform: Input transform should be batch x 8 or 1 x 8\"), assert$6(null == s || 2 === s.length, () => \"Error in transform: outputShape must be [height, width] or null, but got \".concat(s, \".\")), ENGINE$1.runKernel(Transform$1, {\n    image: o,\n    transforms: i\n  }, {\n    interpolation: n,\n    fillMode: r,\n    fillValue: a,\n    outputShape: s\n  });\n}\n\nvar transform$5 = op$1({\n  transform_: transform_$1\n});\n\nfunction bandPart_$1(e, t, n) {\n  assert$6(t % 1 == 0, () => \"bandPart(): numLower must be an integer, got \".concat(t, \".\")), assert$6(n % 1 == 0, () => \"bandPart(): numUpper must be an integer, got \".concat(n, \".\"));\n  var r = convertToTensor$1(e, \"a\", \"bandPart\");\n  assert$6(r.rank >= 2, () => \"bandPart(): Rank must be at least 2, got \".concat(r.rank, \".\"));\n  var a = r.shape,\n      [s, o] = r.shape.slice(-2);\n  if (!(t <= s)) throw new Error(\"bandPart(): numLower (\".concat(t, \") must not be greater than the number of rows (\").concat(s, \").\"));\n  if (!(n <= o)) throw new Error(\"bandPart(): numUpper (\".concat(n, \") must not be greater than the number of columns (\").concat(o, \").\"));\n  t < 0 && (t = s), n < 0 && (n = o);\n  var i = reshape$6(range$8(0, s, 1, \"int32\"), [-1, 1]),\n      l = range$8(0, o, 1, \"int32\"),\n      u = sub$5(i, l),\n      c = logicalAnd$5(lessEqual$5(u, scalar$1(+t, \"int32\")), greaterEqual$5(u, scalar$1(-n, \"int32\"))),\n      p = zeros$4([s, o], r.dtype);\n  return reshape$6(stack$1(unstack$1(reshape$6(r, [-1, s, o])).map(e => where$1(c, e, p))), a);\n}\n\nvar bandPart$1 = op$1({\n  bandPart_: bandPart_$1\n});\n\nfunction gramSchmidt_$1(e) {\n  var t;\n\n  if (Array.isArray(e)) {\n    (function () {\n      t = !1, assert$6(null != e && e.length > 0, () => \"Gram-Schmidt process: input must not be null, undefined, or empty\");\n      var n = e[0].shape[0];\n\n      var _loop6 = function _loop6(_t55) {\n        assert$6(e[_t55].shape[0] === n, () => \"Gram-Schmidt: Non-unique lengths found in the input vectors: (\".concat(e[_t55].shape[0], \" vs. \").concat(n, \")\"));\n      };\n\n      for (var _t55 = 1; _t55 < e.length; ++_t55) {\n        _loop6(_t55);\n      }\n    })();\n  } else t = !0, e = split$4(e, e.shape[0], 0).map(e => squeeze$1(e, [0]));\n\n  assert$6(e.length <= e[0].shape[0], () => \"Gram-Schmidt: Number of vectors (\".concat(e.length, \") exceeds number of dimensions (\").concat(e[0].shape[0], \").\"));\n  var n = [],\n      r = e;\n\n  var _loop7 = function _loop7(_t56) {\n    n.push(ENGINE$1.tidy(() => {\n      var e = r[_t56];\n      if (_t56 > 0) for (var _r19 = 0; _r19 < _t56; ++_r19) {\n        var _t57 = mul$1(sum$6(mul$1(n[_r19], e)), n[_r19]);\n\n        e = sub$5(e, _t57);\n      }\n      return div$3(e, norm$1(e, \"euclidean\"));\n    }));\n  };\n\n  for (var _t56 = 0; _t56 < e.length; ++_t56) {\n    _loop7(_t56);\n  }\n\n  return t ? stack$1(n, 0) : n;\n}\n\nvar gramSchmidt$1 = op$1({\n  gramSchmidt_: gramSchmidt_$1\n});\n\nfunction qr_$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  if (assert$6(e.rank >= 2, () => \"qr() requires input tensor to have a rank >= 2, but got rank \".concat(e.rank)), 2 === e.rank) return qr2d$1(e, t);\n  {\n    var n = e.shape.slice(0, e.shape.length - 2).reduce((e, t) => e * t),\n        r = unstack$1(reshape$6(e, [n, e.shape[e.shape.length - 2], e.shape[e.shape.length - 1]]), 0),\n        a = [],\n        s = [];\n    return r.forEach(e => {\n      var [n, r] = qr2d$1(e, t);\n      a.push(n), s.push(r);\n    }), [reshape$6(stack$1(a, 0), e.shape), reshape$6(stack$1(s, 0), e.shape)];\n  }\n}\n\nfunction qr2d$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  return ENGINE$1.tidy(() => {\n    assert$6(2 === e.shape.length, () => \"qr2d() requires a 2D Tensor, but got a \".concat(e.shape.length, \"D Tensor.\"));\n    var n = e.shape[0],\n        r = e.shape[1];\n    var a = eye$1(n),\n        s = clone$1(e);\n    var o = tensor2d$1([[1]], [1, 1]);\n    var i = clone$1(o);\n    var l = n >= r ? r : n;\n\n    var _loop8 = function _loop8(_e63) {\n      var t = s,\n          l = i,\n          u = a;\n      [i, s, a] = ENGINE$1.tidy(() => {\n        var t = slice$5(s, [_e63, _e63], [n - _e63, 1]),\n            l = norm$1(t),\n            u = slice$5(s, [_e63, _e63], [1, 1]),\n            c = where$1(greater$6(u, 0), tensor2d$1([[-1]]), tensor2d$1([[1]])),\n            p = sub$5(u, mul$1(c, l)),\n            d = div$3(t, p);\n        i = 1 === d.shape[0] ? clone$1(o) : concat$5([o, slice$5(d, [1, 0], [d.shape[0] - 1, d.shape[1]])], 0);\n        var h = neg$5(div$3(matMul$3(c, p), l)),\n            m = slice$5(s, [_e63, 0], [n - _e63, r]),\n            f = mul$1(h, i),\n            g = transpose$5(i);\n        if (0 === _e63) s = sub$5(m, matMul$3(f, matMul$3(g, m)));else {\n          var _t58 = sub$5(m, matMul$3(f, matMul$3(g, m)));\n\n          s = concat$5([slice$5(s, [0, 0], [_e63, r]), _t58], 0);\n        }\n        var $ = transpose$5(f),\n            y = slice$5(a, [0, _e63], [n, a.shape[1] - _e63]);\n        if (0 === _e63) a = sub$5(y, matMul$3(matMul$3(y, i), $));else {\n          var _t59 = sub$5(y, matMul$3(matMul$3(y, i), $));\n\n          a = concat$5([slice$5(a, [0, 0], [n, _e63]), _t59], 1);\n        }\n        return [i, s, a];\n      }), dispose$1([t, l, u]);\n    };\n\n    for (var _e63 = 0; _e63 < l; ++_e63) {\n      _loop8(_e63);\n    }\n\n    return !t && n > r && (a = slice$5(a, [0, 0], [n, r]), s = slice$5(s, [0, 0], [r, r])), [a, s];\n  });\n}\n\nvar qr$1 = op$1({\n  qr_: qr_$1\n});\nvar Reduction$1;\n\nfunction computeWeightedLoss_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var r = convertToTensor$1(e, \"losses\", \"computeWeightedLoss\");\n  var a = null;\n  null != t && (a = convertToTensor$1(t, \"weights\", \"computeWeightedLoss\"));\n  var s = null == a ? r : mul$1(r, a);\n  if (n === Reduction$1.NONE) return s;\n  if (n === Reduction$1.SUM) return sum$6(s);\n\n  if (n === Reduction$1.MEAN) {\n    if (null == a) return mean$3(s);\n    {\n      var _e64 = r.size / a.size,\n          _t60 = div$3(sum$6(s), sum$6(a));\n\n      return _e64 > 1 ? div$3(_t60, scalar$1(_e64)) : _t60;\n    }\n  }\n\n  if (n === Reduction$1.SUM_BY_NONZERO_WEIGHTS) {\n    if (null == a) return div$3(sum$6(s), scalar$1(r.size));\n    {\n      var _e65 = mul$1(a, ones$3(r.shape)),\n          _t61 = cast$7(sum$6(notEqual$5(_e65, scalar$1(0))), \"float32\");\n\n      return div$3(sum$6(s), _t61);\n    }\n  }\n\n  throw Error(\"Unknown reduction: \".concat(n));\n}\n\n!function (e) {\n  e[e.NONE = 0] = \"NONE\", e[e.MEAN = 1] = \"MEAN\", e[e.SUM = 2] = \"SUM\", e[e.SUM_BY_NONZERO_WEIGHTS = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n}(Reduction$1 || (Reduction$1 = {}));\nvar computeWeightedLoss$3 = op$1({\n  computeWeightedLoss_: computeWeightedLoss_$1\n});\n\nfunction absoluteDifference_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var a = convertToTensor$1(e, \"labels\", \"absoluteDifference\"),\n      s = convertToTensor$1(t, \"predictions\", \"absoluteDifference\");\n  var o = null;\n  null != n && (o = convertToTensor$1(n, \"weights\", \"absoluteDifference\")), assertShapesMatch$1(a.shape, s.shape, \"Error in absoluteDifference: \");\n  var i = abs$5(sub$5(a, s));\n  return computeWeightedLoss$3(i, o, r);\n}\n\nvar absoluteDifference$1 = op$1({\n  absoluteDifference_: absoluteDifference_$1\n});\n\nfunction cosineDistance_$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor$1(e, \"labels\", \"cosineDistance\"),\n      o = convertToTensor$1(t, \"predictions\", \"cosineDistance\");\n  var i = null;\n  null != r && (i = convertToTensor$1(r, \"weights\", \"cosineDistance\")), assertShapesMatch$1(s.shape, o.shape, \"Error in cosineDistance: \");\n  var l = scalar$1(1),\n      u = sub$5(l, sum$6(mul$1(s, o), n, !0));\n  return computeWeightedLoss$3(u, i, a);\n}\n\nvar cosineDistance$1 = op$1({\n  cosineDistance_: cosineDistance_$1\n});\n\nfunction hingeLoss_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var a = convertToTensor$1(e, \"labels\", \"hingeLoss\");\n  var s = convertToTensor$1(t, \"predictions\", \"hingeLoss\");\n  var o = null;\n  null != n && (o = convertToTensor$1(n, \"weights\", \"hingeLoss\")), assertShapesMatch$1(a.shape, s.shape, \"Error in hingeLoss: \");\n  var i = scalar$1(1);\n  a = sub$5(mul$1(scalar$1(2), a), i);\n  var l = relu$6(sub$5(i, mul$1(a, s)));\n  return computeWeightedLoss$3(l, o, r);\n}\n\nvar hingeLoss$1 = op$1({\n  hingeLoss_: hingeLoss_$1\n});\n\nfunction huberLoss_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor$1(e, \"labels\", \"huberLoss\"),\n      o = convertToTensor$1(t, \"predictions\", \"huberLoss\");\n  var i = null;\n  null != n && (i = convertToTensor$1(n, \"weights\", \"huberLoss\")), assertShapesMatch$1(s.shape, o.shape, \"Error in huberLoss: \");\n  var l = scalar$1(r),\n      u = abs$5(sub$5(o, s)),\n      c = minimum$6(u, l),\n      p = sub$5(u, c),\n      d = add$5(mul$1(scalar$1(.5), square$5(c)), mul$1(l, p));\n  return computeWeightedLoss$3(d, i, a);\n}\n\nvar huberLoss$1 = op$1({\n  huberLoss_: huberLoss_$1\n});\n\nfunction logLoss_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1e-7;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor$1(e, \"labels\", \"logLoss\"),\n      o = convertToTensor$1(t, \"predictions\", \"logLoss\");\n  var i = null;\n  null != n && (i = convertToTensor$1(n, \"weights\", \"logLoss\")), assertShapesMatch$1(s.shape, o.shape, \"Error in logLoss: \");\n  var l = scalar$1(1),\n      u = scalar$1(r),\n      c = neg$5(mul$1(s, log$7(add$5(o, u)))),\n      p = mul$1(sub$5(l, s), log$7(add$5(sub$5(l, o), u))),\n      d = sub$5(c, p);\n  return computeWeightedLoss$3(d, i, a);\n}\n\nvar logLoss$1 = op$1({\n  logLoss_: logLoss_$1\n});\n\nfunction meanSquaredError_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var a = convertToTensor$1(e, \"labels\", \"meanSquaredError\"),\n      s = convertToTensor$1(t, \"predictions\", \"meanSquaredError\");\n  var o = null;\n  null != n && (o = convertToTensor$1(n, \"weights\", \"meanSquaredError\")), assertShapesMatch$1(a.shape, s.shape, \"Error in meanSquaredError: \");\n  var i = squaredDifference$5(a, s);\n  return computeWeightedLoss$3(i, o, r);\n}\n\nvar meanSquaredError$4 = op$1({\n  meanSquaredError_: meanSquaredError_$1\n});\n\nfunction sigmoidCrossEntropyWithLogits_$1(e, t) {\n  var n = convertToTensor$1(e, \"labels\", \"sigmoidCrossEntropyWithLogits\"),\n      r = convertToTensor$1(t, \"logits\", \"sigmoidCrossEntropyWithLogits\");\n  assertShapesMatch$1(n.shape, r.shape, \"Error in sigmoidCrossEntropyWithLogits: \");\n  var a = relu$6(r),\n      s = mul$1(r, n),\n      o = log1p$5(exp$5(neg$5(abs$5(r))));\n  return add$5(sub$5(a, s), o);\n}\n\nfunction sigmoidCrossEntropy_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor$1(e, \"multiClassLabels\", \"sigmoidCrossEntropy\");\n  var o = convertToTensor$1(t, \"logits\", \"sigmoidCrossEntropy\");\n  var i = null;\n\n  if (null != n && (i = convertToTensor$1(n, \"weights\", \"sigmoidCrossEntropy\")), assertShapesMatch$1(s.shape, o.shape, \"Error in sigmoidCrossEntropy: \"), r > 0) {\n    var _e66 = scalar$1(r),\n        _t62 = scalar$1(1),\n        _n18 = scalar$1(.5);\n\n    s = add$5(mul$1(s, sub$5(_t62, _e66)), mul$1(_n18, _e66));\n  }\n\n  var l = sigmoidCrossEntropyWithLogits_$1(s, o);\n  return computeWeightedLoss$3(l, i, a);\n}\n\nvar sigmoidCrossEntropy$1 = op$1({\n  sigmoidCrossEntropy_: sigmoidCrossEntropy_$1\n});\n\nfunction softmaxCrossEntropyWithLogits_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : -1;\n  if (-1 === n && (n = t.rank - 1), n !== t.rank - 1) throw Error(\"Softmax cross entropy along a non-last dimension is not yet supported. Labels / logits was rank \".concat(t.rank, \" and dim was \").concat(n));\n  return customGrad$1((e, t, r) => {\n    var a = logSumExp$1(t, [n], !0),\n        s = sub$5(cast$7(t, \"float32\"), a);\n    r([e, s]);\n    var o = neg$5(mul$1(s, e));\n    return {\n      value: sum$6(o, [n]),\n      gradFunc: (e, t) => {\n        var [r, a] = t,\n            s = expandShapeToKeepDim$1(e.shape, [n]);\n        return [mul$1(reshape$6(e, s), sub$5(cast$7(r, \"float32\"), exp$5(a))), mul$1(reshape$6(e, s), sub$5(exp$5(a), cast$7(r, \"float32\")))];\n      }\n    };\n  })(e, t);\n}\n\nfunction softmaxCrossEntropy_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction$1.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor$1(e, \"onehotLabels\", \"softmaxCrossEntropy\");\n  var o = convertToTensor$1(t, \"logits\", \"softmaxCrossEntropy\");\n  var i = null;\n\n  if (null != n && (i = convertToTensor$1(n, \"weights\", \"softmaxCrossEntropy\")), assertShapesMatch$1(s.shape, o.shape, \"Error in softmaxCrossEntropy: \"), r > 0) {\n    var _e67 = scalar$1(r),\n        _t63 = scalar$1(1),\n        _n19 = scalar$1(s.shape[1]);\n\n    s = add$5(mul$1(s, sub$5(_t63, _e67)), div$3(_e67, _n19));\n  }\n\n  var l = softmaxCrossEntropyWithLogits_$1(s, o);\n  return computeWeightedLoss$3(l, i, a);\n}\n\nvar softmaxCrossEntropy$1 = op$1({\n  softmaxCrossEntropy_: softmaxCrossEntropy_$1\n});\n\nfunction sparseFillEmptyRows_$1(e, t, n, r) {\n  var a = convertToTensor$1(e, \"indices\", \"sparseFillEmptyRows\"),\n      s = convertToTensor$1(t, \"values\", \"sparseFillEmptyRows\"),\n      o = convertToTensor$1(n, \"denseShape\", \"sparseFillEmptyRows\"),\n      i = convertToTensor$1(r, \"defaultValue\", \"sparseFillEmptyRows\", s.dtype);\n  if (2 !== a.rank) throw new Error(\"Indices should be Tensor2D but received shape\\n        \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"Values should be Tensor1D but received shape \".concat(s.shape));\n  if (1 !== o.rank) throw new Error(\"Dense shape should be Tensor1D but received shape \".concat(o.shape));\n  if (0 !== i.rank) throw new Error(\"Default value should be a scalar but received shape \".concat(i.shape));\n  var l = ENGINE$1.runKernel(SparseFillEmptyRows$1, {\n    indices: a,\n    values: s,\n    denseShape: o,\n    defaultValue: i\n  });\n  return {\n    outputIndices: l[0],\n    outputValues: l[1],\n    emptyRowIndicator: l[2],\n    reverseIndexMap: l[3]\n  };\n}\n\nvar sparseFillEmptyRows$5 = op$1({\n  sparseFillEmptyRows_: sparseFillEmptyRows_$1\n});\n\nfunction sparseReshape_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"inputIndices\", \"sparseReshape\"),\n      a = convertToTensor$1(t, \"inputShape\", \"sparseReshape\"),\n      s = convertToTensor$1(n, \"newShape\", \"sparseReshape\");\n  if (2 !== r.rank) throw new Error(\"Input indices should be Tensor2D but received shape\\n        \".concat(r.shape));\n  if (1 !== a.rank) throw new Error(\"Input shape should be Tensor1D but received shape \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"New shape should be Tensor1D but received shape \".concat(s.shape));\n  var o = ENGINE$1.runKernel(SparseReshape$1, {\n    inputIndices: r,\n    inputShape: a,\n    newShape: s\n  });\n  return {\n    outputIndices: o[0],\n    outputShape: o[1]\n  };\n}\n\nvar sparseReshape$5 = op$1({\n  sparseReshape_: sparseReshape_$1\n});\n\nfunction sparseSegmentMean_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"data\", \"sparseSegmentMean\"),\n      a = convertToTensor$1(t, \"indices\", \"sparseSegmentMean\"),\n      s = convertToTensor$1(n, \"segmentIds\", \"sparseSegmentMean\");\n  if (r.rank < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.rank) throw new Error(\"Indices should be Tensor1D but received shape\\n          \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"Segment ids should be Tensor1D but received shape\\n          \".concat(s.shape));\n  return ENGINE$1.runKernel(SparseSegmentMean$1, {\n    data: r,\n    indices: a,\n    segmentIds: s\n  });\n}\n\nvar sparseSegmentMean$5 = op$1({\n  sparseSegmentMean_: sparseSegmentMean_$1\n});\n\nfunction sparseSegmentSum_$1(e, t, n) {\n  var r = convertToTensor$1(e, \"data\", \"sparseSegmentSum\"),\n      a = convertToTensor$1(t, \"indices\", \"sparseSegmentSum\"),\n      s = convertToTensor$1(n, \"segmentIds\", \"sparseSegmentSum\");\n  if (r.rank < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.rank) throw new Error(\"Indices should be Tensor1D but received shape\\n         \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"Segment ids should be Tensor1D but received shape\\n         \".concat(s.shape));\n  return ENGINE$1.runKernel(SparseSegmentSum$1, {\n    data: r,\n    indices: a,\n    segmentIds: s\n  });\n}\n\nvar sparseSegmentSum$5 = op$1({\n  sparseSegmentSum_: sparseSegmentSum_$1\n});\n\nfunction stringNGrams_$1(e, t, n, r, a, s, o, i) {\n  var l = convertToTensor$1(e, \"data\", \"stringNGrams\", \"string\");\n  if (\"string\" !== l.dtype) throw new Error(\"Data must be of datatype string\");\n  if (1 !== l.shape.length) throw new Error(\"Data must be a vector, saw: \".concat(l.shape));\n  var u = convertToTensor$1(t, \"dataSplits\", \"stringNGrams\");\n  if (\"int32\" !== u.dtype) throw new Error(\"Data splits must be of datatype int32\");\n  var c = ENGINE$1.runKernel(StringNGrams$1, {\n    data: l,\n    dataSplits: u\n  }, {\n    separator: n,\n    nGramWidths: r,\n    leftPad: a,\n    rightPad: s,\n    padWidth: o,\n    preserveShortSequences: i\n  });\n  return {\n    nGrams: c[0],\n    nGramsSplits: c[1]\n  };\n}\n\nvar stringNGrams$5 = op$1({\n  stringNGrams_: stringNGrams_$1\n});\n\nfunction stringSplit_$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n  var r = convertToTensor$1(e, \"input\", \"stringSplit\", \"string\"),\n      a = convertToTensor$1(t, \"delimiter\", \"stringSplit\", \"string\");\n  if (1 !== r.rank) throw new Error(\"Input should be Tensor1D but received shape \".concat(r.shape));\n  if (0 !== a.rank) throw new Error(\"Delimiter should be a scalar but received shape \".concat(a.shape));\n  var s = ENGINE$1.runKernel(StringSplit$1, {\n    input: r,\n    delimiter: a\n  }, {\n    skipEmpty: n\n  });\n  return {\n    indices: s[0],\n    values: s[1],\n    shape: s[2]\n  };\n}\n\nvar stringSplit$5 = op$1({\n  stringSplit_: stringSplit_$1\n});\n\nfunction stringToHashBucketFast_$1(e, t) {\n  var n = convertToTensor$1(e, \"input\", \"stringToHashBucketFast\", \"string\"),\n      r = {\n    numBuckets: t\n  };\n  if (t <= 0) throw new Error(\"Number of buckets must be at least 1\");\n  return ENGINE$1.runKernel(StringToHashBucketFast$1, {\n    input: n\n  }, r);\n}\n\nvar stringToHashBucketFast$5 = op$1({\n  stringToHashBucketFast_: stringToHashBucketFast_$1\n}),\n    image$2 = {\n  flipLeftRight: flipLeftRight$1,\n  resizeNearestNeighbor: resizeNearestNeighbor$5,\n  resizeBilinear: resizeBilinear$5,\n  rotateWithOffset: rotateWithOffset$1,\n  cropAndResize: cropAndResize$5,\n  nonMaxSuppression: nonMaxSuppression$1,\n  nonMaxSuppressionAsync: nonMaxSuppressionAsync$1,\n  nonMaxSuppressionWithScore: nonMaxSuppressionWithScore$1,\n  nonMaxSuppressionWithScoreAsync: nonMaxSuppressionWithScoreAsync$1,\n  nonMaxSuppressionPadded: nonMaxSuppressionPadded$1,\n  nonMaxSuppressionPaddedAsync: nonMaxSuppressionPaddedAsync$1,\n  threshold: threshold$3,\n  transform: transform$5\n},\n    linalg$1 = {\n  bandPart: bandPart$1,\n  gramSchmidt: gramSchmidt$1,\n  qr: qr$1\n};\n\nclass Optimizer$1 extends Serializable$1 {\n  minimize(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    var {\n      value: r,\n      grads: a\n    } = this.computeGradients(e, n);\n\n    if (null != n) {\n      var _e68 = n.map(e => ({\n        name: e.name,\n        tensor: a[e.name]\n      }));\n\n      this.applyGradients(_e68);\n    } else this.applyGradients(a);\n\n    return dispose$1(a), t ? r : (r.dispose(), null);\n  }\n\n  get iterations() {\n    return null == this.iterations_ && (this.iterations_ = 0), this.iterations_;\n  }\n\n  incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n\n  computeGradients(e, t) {\n    return variableGrads$1(e, t);\n  }\n\n  dispose() {\n    null != this.iterations_ && dispose$1(this.iterations_);\n  }\n\n  saveIterations() {\n    var _this23 = this;\n\n    return _asyncToGenerator(function* () {\n      return null == _this23.iterations_ && (_this23.iterations_ = 0), {\n        name: \"iter\",\n        tensor: scalar$1(_this23.iterations_, \"int32\")\n      };\n    })();\n  }\n\n  getWeights() {\n    return _asyncToGenerator(function* () {\n      throw new Error(\"getWeights() is not implemented for this optimizer yet.\");\n    })();\n  }\n\n  setWeights(e) {\n    var _this24 = this;\n\n    return _asyncToGenerator(function* () {\n      throw new Error(\"setWeights() is not implemented for this optimizer class \".concat(_this24.getClassName()));\n    })();\n  }\n\n  extractIterations(e) {\n    var _this25 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this25.iterations_ = (yield e[0].tensor.data())[0], e.slice(1);\n    })();\n  }\n\n}\n\nObject.defineProperty(Optimizer$1, Symbol.hasInstance, {\n  value: e => null != e.minimize && null != e.computeGradients && null != e.applyGradients\n});\n\nclass AdadeltaOptimizer$1 extends Optimizer$1 {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    super(), this.learningRate = e, this.rho = t, this.epsilon = n, this.accumulatedGrads = [], this.accumulatedUpdates = [], null == n && (this.epsilon = ENGINE$1.backend.epsilon());\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE$1.registeredVariables[t];\n      null == this.accumulatedGrads[n] && (this.accumulatedGrads[n] = {\n        originalName: \"\".concat(t, \"/accum_grad\"),\n        variable: tidy$1(() => zerosLike$5(r).variable(!1))\n      }), null == this.accumulatedUpdates[n] && (this.accumulatedUpdates[n] = {\n        originalName: \"\".concat(t, \"/accum_var\"),\n        variable: tidy$1(() => zerosLike$5(r).variable(!1))\n      });\n      var a = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == a) return;\n      var s = this.accumulatedGrads[n].variable,\n          o = this.accumulatedUpdates[n].variable;\n      tidy$1(() => {\n        var e = add$5(mul$1(s, this.rho), mul$1(square$5(a), 1 - this.rho)),\n            t = mul$1(div$3(sqrt$5(add$5(o, this.epsilon)), sqrt$5(add$5(s, this.epsilon))), a),\n            n = add$5(mul$1(o, this.rho), mul$1(square$5(t), 1 - this.rho));\n        s.assign(e), o.assign(n);\n        var i = add$5(mul$1(t, -this.learningRate), r);\n        r.assign(i);\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    null != this.accumulatedUpdates && (dispose$1(this.accumulatedGrads.map(e => e.variable)), dispose$1(this.accumulatedUpdates.map(e => e.variable)));\n  }\n\n  getWeights() {\n    var _this26 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [..._this26.accumulatedGrads, ..._this26.accumulatedUpdates];\n      return [yield _this26.saveIterations()].concat(e.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this27 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = (e = yield _this27.extractIterations(e)).length / 2;\n      _this27.accumulatedGrads = e.slice(0, t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      })), _this27.accumulatedUpdates = e.slice(t, 2 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      rho: this.rho,\n      epsilon: this.epsilon\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.rho, t.epsilon);\n  }\n\n}\n\nAdadeltaOptimizer$1.className = \"Adadelta\", registerClass$1(AdadeltaOptimizer$1);\n\nclass AdagradOptimizer$1 extends Optimizer$1 {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .1;\n    super(), this.learningRate = e, this.initialAccumulatorValue = t, this.accumulatedGrads = [];\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE$1.registeredVariables[t];\n\n      if (null == this.accumulatedGrads[n]) {\n        var _e69 = !1;\n\n        this.accumulatedGrads[n] = {\n          originalName: \"\".concat(t, \"/accumulator\"),\n          variable: tidy$1(() => fill$5(r.shape, this.initialAccumulatorValue).variable(_e69))\n        };\n      }\n\n      var a = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == a) return;\n      var s = this.accumulatedGrads[n].variable;\n      tidy$1(() => {\n        var e = add$5(s, square$5(a));\n        s.assign(e);\n        var t = add$5(mul$1(div$3(a, sqrt$5(add$5(e, ENGINE$1.backend.epsilon()))), -this.learningRate), r);\n        r.assign(t);\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    null != this.accumulatedGrads && dispose$1(this.accumulatedGrads.map(e => e.variable));\n  }\n\n  getWeights() {\n    var _this28 = this;\n\n    return _asyncToGenerator(function* () {\n      return [yield _this28.saveIterations()].concat(_this28.accumulatedGrads.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this29 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this29.extractIterations(e), _this29.accumulatedGrads = e.map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      initialAccumulatorValue: this.initialAccumulatorValue\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.initialAccumulatorValue);\n  }\n\n}\n\nAdagradOptimizer$1.className = \"Adagrad\", registerClass$1(AdagradOptimizer$1);\n\nclass AdamOptimizer$1 extends Optimizer$1 {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    super(), this.learningRate = e, this.beta1 = t, this.beta2 = n, this.epsilon = r, this.accumulatedFirstMoment = [], this.accumulatedSecondMoment = [], tidy$1(() => {\n      this.accBeta1 = scalar$1(t).variable(), this.accBeta2 = scalar$1(n).variable();\n    }), null == r && (this.epsilon = ENGINE$1.backend.epsilon());\n  }\n\n  applyGradients(e) {\n    var t = Array.isArray(e) ? e.map(e => e.name) : Object.keys(e);\n    tidy$1(() => {\n      var n = sub$5(1, this.accBeta1),\n          r = sub$5(1, this.accBeta2);\n      t.forEach((t, a) => {\n        var s = ENGINE$1.registeredVariables[t];\n        null == this.accumulatedFirstMoment[a] && (this.accumulatedFirstMoment[a] = {\n          originalName: \"\".concat(t, \"/m\"),\n          variable: tidy$1(() => zerosLike$5(s).variable(!1))\n        }), null == this.accumulatedSecondMoment[a] && (this.accumulatedSecondMoment[a] = {\n          originalName: \"\".concat(t, \"/v\"),\n          variable: tidy$1(() => zerosLike$5(s).variable(!1))\n        });\n        var o = Array.isArray(e) ? e[a].tensor : e[t];\n        if (null == o) return;\n        var i = this.accumulatedFirstMoment[a].variable,\n            l = this.accumulatedSecondMoment[a].variable,\n            u = add$5(mul$1(i, this.beta1), mul$1(o, 1 - this.beta1)),\n            c = add$5(mul$1(l, this.beta2), mul$1(square$5(o), 1 - this.beta2)),\n            p = div$3(u, n),\n            d = div$3(c, r);\n        i.assign(u), l.assign(c);\n        var h = add$5(mul$1(div$3(p, add$5(sqrt$5(d), this.epsilon)), -this.learningRate), s);\n        s.assign(h);\n      }), this.accBeta1.assign(mul$1(this.accBeta1, this.beta1)), this.accBeta2.assign(mul$1(this.accBeta2, this.beta2));\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    this.accBeta1.dispose(), this.accBeta2.dispose(), null != this.accumulatedFirstMoment && dispose$1(this.accumulatedFirstMoment.map(e => e.variable)), null != this.accumulatedSecondMoment && dispose$1(this.accumulatedSecondMoment.map(e => e.variable));\n  }\n\n  getWeights() {\n    var _this30 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [..._this30.accumulatedFirstMoment, ..._this30.accumulatedSecondMoment];\n      return [yield _this30.saveIterations()].concat(e.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this31 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this31.extractIterations(e), tidy$1(() => {\n        _this31.accBeta1.assign(pow$5(_this31.beta1, _this31.iterations_ + 1)), _this31.accBeta2.assign(pow$5(_this31.beta2, _this31.iterations_ + 1));\n      });\n      var t = e.length / 2;\n      _this31.accumulatedFirstMoment = e.slice(0, t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      })), _this31.accumulatedSecondMoment = e.slice(t, 2 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      beta1: this.beta1,\n      beta2: this.beta2,\n      epsilon: this.epsilon\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.beta1, t.beta2, t.epsilon);\n  }\n\n}\n\nAdamOptimizer$1.className = \"Adam\", registerClass$1(AdamOptimizer$1);\n\nclass AdamaxOptimizer$1 extends Optimizer$1 {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n    super(), this.learningRate = e, this.beta1 = t, this.beta2 = n, this.epsilon = r, this.decay = a, this.accumulatedFirstMoment = [], this.accumulatedWeightedInfNorm = [], tidy$1(() => {\n      this.iteration = scalar$1(0).variable(), this.accBeta1 = scalar$1(t).variable();\n    }), null == r && (this.epsilon = ENGINE$1.backend.epsilon());\n  }\n\n  applyGradients(e) {\n    var t = Array.isArray(e) ? e.map(e => e.name) : Object.keys(e);\n    tidy$1(() => {\n      var n = sub$5(1, this.accBeta1),\n          r = div$3(-this.learningRate, add$5(mul$1(this.iteration, this.decay), 1));\n      t.forEach((t, a) => {\n        var s = ENGINE$1.registeredVariables[t];\n        null == this.accumulatedFirstMoment[a] && (this.accumulatedFirstMoment[a] = {\n          originalName: \"\".concat(t, \"/m\"),\n          variable: zerosLike$5(s).variable(!1)\n        }), null == this.accumulatedWeightedInfNorm[a] && (this.accumulatedWeightedInfNorm[a] = {\n          originalName: \"\".concat(t, \"/v\"),\n          variable: zerosLike$5(s).variable(!1)\n        });\n        var o = Array.isArray(e) ? e[a].tensor : e[t];\n        if (null == o) return;\n        var i = this.accumulatedFirstMoment[a].variable,\n            l = this.accumulatedWeightedInfNorm[a].variable,\n            u = add$5(mul$1(i, this.beta1), mul$1(o, 1 - this.beta1)),\n            c = mul$1(l, this.beta2),\n            p = abs$5(o),\n            d = maximum$6(c, p);\n        i.assign(u), l.assign(d);\n        var h = add$5(mul$1(div$3(r, n), div$3(u, add$5(d, this.epsilon))), s);\n        s.assign(h);\n      }), this.iteration.assign(add$5(this.iteration, 1)), this.accBeta1.assign(mul$1(this.accBeta1, this.beta1));\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    this.accBeta1.dispose(), this.iteration.dispose(), null != this.accumulatedFirstMoment && dispose$1(this.accumulatedFirstMoment.map(e => e.variable)), null != this.accumulatedWeightedInfNorm && dispose$1(this.accumulatedWeightedInfNorm.map(e => e.variable));\n  }\n\n  getWeights() {\n    return _asyncToGenerator(function* () {\n      throw new Error(\"getWeights() is not implemented for Adamax yet.\");\n    })();\n  }\n\n  setWeights(e) {\n    return _asyncToGenerator(function* () {\n      throw new Error(\"setWeights() is not implemented for Adamax yet.\");\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      beta1: this.beta1,\n      beta2: this.beta2,\n      epsilon: this.epsilon,\n      decay: this.decay\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.beta1, t.beta2, t.epsilon, t.decay);\n  }\n\n}\n\nAdamaxOptimizer$1.className = \"Adamax\", registerClass$1(AdamaxOptimizer$1);\n\nclass SGDOptimizer$1 extends Optimizer$1 {\n  constructor(e) {\n    super(), this.learningRate = e, this.setLearningRate(e);\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == r) return;\n      var a = ENGINE$1.registeredVariables[t];\n      tidy$1(() => {\n        var e = add$5(mul$1(this.c, r), a);\n        a.assign(e);\n      });\n    }), this.incrementIterations();\n  }\n\n  setLearningRate(e) {\n    this.learningRate = e, null != this.c && this.c.dispose(), this.c = keep$1(scalar$1(-e));\n  }\n\n  dispose() {\n    this.c.dispose();\n  }\n\n  getWeights() {\n    var _this32 = this;\n\n    return _asyncToGenerator(function* () {\n      return [yield _this32.saveIterations()];\n    })();\n  }\n\n  setWeights(e) {\n    var _this33 = this;\n\n    return _asyncToGenerator(function* () {\n      if (0 !== (e = yield _this33.extractIterations(e)).length) throw new Error(\"SGD optimizer does not have settable weights.\");\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate);\n  }\n\n}\n\nSGDOptimizer$1.className = \"SGD\", registerClass$1(SGDOptimizer$1);\n\nclass MomentumOptimizer$1 extends SGDOptimizer$1 {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    super(e), this.learningRate = e, this.momentum = t, this.useNesterov = n, this.accumulations = [], this.m = scalar$1(this.momentum);\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE$1.registeredVariables[t];\n\n      if (null == this.accumulations[n]) {\n        var _e70 = !1;\n\n        this.accumulations[n] = {\n          originalName: \"\".concat(t, \"/momentum\"),\n          variable: tidy$1(() => zerosLike$5(r).variable(_e70))\n        };\n      }\n\n      var a = this.accumulations[n].variable,\n          s = Array.isArray(e) ? e[n].tensor : e[t];\n      null != s && tidy$1(() => {\n        var e;\n        var t = add$5(mul$1(this.m, a), s);\n        e = add$5(mul$1(this.c, this.useNesterov ? add$5(s, mul$1(t, this.m)) : t), r), a.assign(t), r.assign(e);\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    this.m.dispose(), null != this.accumulations && dispose$1(this.accumulations.map(e => e.variable));\n  }\n\n  setMomentum(e) {\n    this.momentum = e;\n  }\n\n  getWeights() {\n    var _this34 = this;\n\n    return _asyncToGenerator(function* () {\n      return [yield _this34.saveIterations()].concat(_this34.accumulations.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this35 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this35.extractIterations(e), _this35.accumulations = e.map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      momentum: this.momentum,\n      useNesterov: this.useNesterov\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.momentum, t.useNesterov);\n  }\n\n}\n\nMomentumOptimizer$1.className = \"Momentum\", registerClass$1(MomentumOptimizer$1);\n\nclass RMSPropOptimizer$1 extends Optimizer$1 {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    if (super(), this.learningRate = e, this.decay = t, this.momentum = n, this.epsilon = r, this.accumulatedMeanSquares = [], this.accumulatedMoments = [], this.accumulatedMeanGrads = [], this.centered = a, null == r && (this.epsilon = ENGINE$1.backend.epsilon()), null == e) throw new Error(\"learningRate for RMSPropOptimizer must be defined.\");\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE$1.registeredVariables[t],\n          a = !1;\n      null == this.accumulatedMeanSquares[n] && (this.accumulatedMeanSquares[n] = {\n        originalName: \"\".concat(t, \"/rms\"),\n        variable: tidy$1(() => zerosLike$5(r).variable(a))\n      }), null == this.accumulatedMoments[n] && (this.accumulatedMoments[n] = {\n        originalName: \"\".concat(t, \"/momentum\"),\n        variable: tidy$1(() => zerosLike$5(r).variable(a))\n      }), null == this.accumulatedMeanGrads[n] && this.centered && (this.accumulatedMeanGrads[n] = {\n        originalName: \"\".concat(t, \"/mg\"),\n        variable: tidy$1(() => zerosLike$5(r).variable(a))\n      });\n      var s = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == s) return;\n      var o = this.accumulatedMeanSquares[n].variable,\n          i = this.accumulatedMoments[n].variable;\n      tidy$1(() => {\n        var e = add$5(mul$1(o, this.decay), mul$1(square$5(s), 1 - this.decay));\n\n        if (this.centered) {\n          var _t64 = this.accumulatedMeanGrads[n].variable,\n              _a13 = add$5(mul$1(_t64, this.decay), mul$1(s, 1 - this.decay)),\n              l = div$3(mul$1(s, this.learningRate), sqrt$5(sub$5(e, add$5(square$5(_a13), this.epsilon)))),\n              u = add$5(mul$1(i, this.momentum), l);\n\n          o.assign(e), _t64.assign(_a13), i.assign(u);\n          var c = sub$5(r, u);\n          r.assign(c);\n        } else {\n          var _e71 = add$5(mul$1(o, this.decay), mul$1(square$5(s), 1 - this.decay)),\n              _t65 = add$5(mul$1(i, this.momentum), div$3(mul$1(s, this.learningRate), sqrt$5(add$5(_e71, this.epsilon))));\n\n          o.assign(_e71), i.assign(_t65);\n\n          var _n20 = sub$5(r, _t65);\n\n          r.assign(_n20);\n        }\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    null != this.accumulatedMeanSquares && dispose$1(this.accumulatedMeanSquares.map(e => e.variable)), null != this.accumulatedMeanGrads && this.centered && dispose$1(this.accumulatedMeanGrads.map(e => e.variable)), null != this.accumulatedMoments && dispose$1(this.accumulatedMoments.map(e => e.variable));\n  }\n\n  getWeights() {\n    var _this36 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [..._this36.accumulatedMeanSquares, ..._this36.accumulatedMoments];\n      return _this36.centered && e.push(..._this36.accumulatedMeanGrads), [yield _this36.saveIterations()].concat(e.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this37 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this37.extractIterations(e);\n      var t = _this37.centered ? e.length / 3 : e.length / 2,\n          n = !1;\n      _this37.accumulatedMeanSquares = e.slice(0, t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(n)\n      })), _this37.accumulatedMoments = e.slice(t, 2 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(n)\n      })), _this37.centered && (_this37.accumulatedMeanGrads = e.slice(2 * t, 3 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(n)\n      })));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      decay: this.decay,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      centered: this.centered\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.decay, t.momentum, t.epsilon, t.centered);\n  }\n\n}\n\nRMSPropOptimizer$1.className = \"RMSProp\", registerClass$1(RMSPropOptimizer$1);\n\nclass OptimizerConstructors$1 {\n  static sgd(e) {\n    return new SGDOptimizer$1(e);\n  }\n\n  static momentum(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    return new MomentumOptimizer$1(e, t, n);\n  }\n\n  static rmsprop(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    return new RMSPropOptimizer$1(e, t, n, r, a);\n  }\n\n  static adam() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .001;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : .999;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    return new AdamOptimizer$1(e, t, n, r);\n  }\n\n  static adadelta() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .001;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .95;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    return new AdadeltaOptimizer$1(e, t, n);\n  }\n\n  static adamax() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .002;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : .999;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n    return new AdamaxOptimizer$1(e, t, n, r, a);\n  }\n\n  static adagrad(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .1;\n    return new AdagradOptimizer$1(e, t);\n  }\n\n}\n\nvar train$1 = {\n  sgd: OptimizerConstructors$1.sgd,\n  momentum: OptimizerConstructors$1.momentum,\n  adadelta: OptimizerConstructors$1.adadelta,\n  adagrad: OptimizerConstructors$1.adagrad,\n  rmsprop: OptimizerConstructors$1.rmsprop,\n  adamax: OptimizerConstructors$1.adamax,\n  adam: OptimizerConstructors$1.adam\n},\n    delayCallback$1 = \"undefined\" != typeof requestAnimationFrame ? requestAnimationFrame : \"undefined\" != typeof setImmediate ? setImmediate : e => e();\n\nfunction nextFrame$1() {\n  return new Promise(e => delayCallback$1(() => e()));\n}\n\nfunction assertParamsConsistent$1(e, t) {\n  var n = e[0].length;\n  e.forEach((e, t) => {\n    assert$6(e.length === n, () => \"Error in concat\".concat(n, \"D: rank of tensors[\").concat(t, \"] must be the same as the rank of the rest (\").concat(n, \")\"));\n  }), assert$6(t >= 0 && t < n, () => \"Error in concat\".concat(n, \"D: axis must be between 0 and \").concat(n - 1, \".\"));\n  var r = e[0];\n  e.forEach((e, a) => {\n    for (var s = 0; s < n; s++) {\n      assert$6(s === t || e[s] === r[s], () => \"Error in concat\".concat(n, \"D: Shape of tensors[\").concat(a, \"] (\").concat(e, \") does not match the shape of the rest (\").concat(r, \") along the non-concatenated axis \").concat(a, \".\"));\n    }\n  });\n}\n\nfunction computeOutShape$4(e, t) {\n  var n = e[0].slice();\n\n  for (var r = 1; r < e.length; r++) {\n    n[t] += e[r][t];\n  }\n\n  return n;\n}\n\nvar PARALLELIZE_THRESHOLD$1 = 30;\n\nfunction computeOptimalWindowSize$1(e) {\n  return e <= PARALLELIZE_THRESHOLD$1 ? e : nearestDivisor$1(e, Math.floor(Math.sqrt(e)));\n}\n\nfunction getImageCenter$1(e, t, n) {\n  return [n * (\"number\" == typeof e ? e : e[0]), t * (\"number\" == typeof e ? e : e[1])];\n}\n\nfunction getReshaped$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = [];\n  if (r) a = a.concat(t.slice(0)), a.push(e[0] / n), a = a.concat(e.slice(1));else {\n    a = a.concat(e[0]);\n    var _n21 = t.length;\n\n    for (var _r20 = 0; _r20 < _n21; ++_r20) {\n      a = a.concat([e[_r20 + 1] / t[_r20], t[_r20]]);\n    }\n\n    a = a.concat(e.slice(_n21 + 1));\n  }\n  return a;\n}\n\nfunction getPermuted$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n  var r = [];\n\n  if (n) {\n    r.push(t);\n\n    for (var _n22 = t + 1; _n22 < e; ++_n22) {\n      _n22 <= 2 * t ? (r.push(_n22), r.push(_n22 - (t + 1))) : r.push(_n22);\n    }\n  } else {\n    var _n23 = [],\n        a = [];\n\n    for (var _r21 = 1; _r21 < e; ++_r21) {\n      _r21 >= 2 * t + 1 || _r21 % 2 == 1 ? a.push(_r21) : _n23.push(_r21);\n    }\n\n    r.push(..._n23), r.push(0), r.push(...a);\n  }\n\n  return r;\n}\n\nfunction getReshapedPermuted$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = [];\n  a.push(r ? e[0] / n : e[0] * n);\n\n  for (var _n24 = 1; _n24 < e.length; ++_n24) {\n    a.push(_n24 <= t.length ? r ? t[_n24 - 1] * e[_n24] : e[_n24] / t[_n24 - 1] : e[_n24]);\n  }\n\n  return a;\n}\n\nfunction getSliceBeginCoords$1(e, t) {\n  var n = [0];\n\n  for (var r = 0; r < t; ++r) {\n    n.push(e[r][0]);\n  }\n\n  return n;\n}\n\nfunction getSliceSize$1(e, t, n) {\n  var r = e.slice(0, 1);\n\n  for (var a = 0; a < n; ++a) {\n    r.push(e[a + 1] - t[a][0] - t[a][1]);\n  }\n\n  return r;\n}\n\nvar SELU_SCALEALPHA$1 = 1.7580993408473768,\n    SELU_SCALE$1 = 1.0507009873554805,\n    ERF_P$1 = .3275911,\n    ERF_A1$1 = .254829592,\n    ERF_A2$1 = -.284496736,\n    ERF_A3$1 = 1.421413741,\n    ERF_A4$1 = -1.453152027,\n    ERF_A5$1 = 1.061405429;\n\nfunction warn$1() {\n  env$1().getBool(\"IS_TEST\") || console.warn(...arguments);\n}\n\nfunction log$6() {\n  env$1().getBool(\"IS_TEST\") || console.log(...arguments);\n}\n\nfunction mergeRealAndImagArrays$1(e, t) {\n  if (e.length !== t.length) throw new Error(\"Cannot merge real and imag arrays of different lengths. real:\".concat(e.length, \", imag: \").concat(t.length, \".\"));\n  var n = new Float32Array(2 * e.length);\n\n  for (var r = 0; r < n.length; r += 2) {\n    n[r] = e[r / 2], n[r + 1] = t[r / 2];\n  }\n\n  return n;\n}\n\nfunction splitRealAndImagArrays$1(e) {\n  var t = new Float32Array(e.length / 2),\n      n = new Float32Array(e.length / 2);\n\n  for (var r = 0; r < e.length; r += 2) {\n    t[r / 2] = e[r], n[r / 2] = e[r + 1];\n  }\n\n  return {\n    real: t,\n    imag: n\n  };\n}\n\nfunction complexWithEvenIndex$1(e) {\n  var t = Math.ceil(e.length / 4),\n      n = new Float32Array(t),\n      r = new Float32Array(t);\n\n  for (var _t66 = 0; _t66 < e.length; _t66 += 4) {\n    n[Math.floor(_t66 / 4)] = e[_t66], r[Math.floor(_t66 / 4)] = e[_t66 + 1];\n  }\n\n  return {\n    real: n,\n    imag: r\n  };\n}\n\nfunction complexWithOddIndex$1(e) {\n  var t = Math.floor(e.length / 4),\n      n = new Float32Array(t),\n      r = new Float32Array(t);\n\n  for (var _t67 = 2; _t67 < e.length; _t67 += 4) {\n    n[Math.floor(_t67 / 4)] = e[_t67], r[Math.floor(_t67 / 4)] = e[_t67 + 1];\n  }\n\n  return {\n    real: n,\n    imag: r\n  };\n}\n\nfunction getComplexWithIndex$1(e, t) {\n  return {\n    real: e[2 * t],\n    imag: e[2 * t + 1]\n  };\n}\n\nfunction assignToTypedArray$1(e, t, n, r) {\n  e[2 * r] = t, e[2 * r + 1] = n;\n}\n\nfunction exponents$1(e, t) {\n  var n = new Float32Array(e / 2),\n      r = new Float32Array(e / 2);\n\n  for (var a = 0; a < Math.ceil(e / 2); a++) {\n    var s = (t ? 2 : -2) * Math.PI * (a / e);\n    n[a] = Math.cos(s), r[a] = Math.sin(s);\n  }\n\n  return {\n    real: n,\n    imag: r\n  };\n}\n\nfunction exponent$1(e, t, n) {\n  var r = (n ? 2 : -2) * Math.PI * (e / t);\n  return {\n    real: Math.cos(r),\n    imag: Math.sin(r)\n  };\n}\n\nvar ARROW$1 = \"->\",\n    ARROW_REGEX$1 = /->/g,\n    COMMA$1 = \",\",\n    ELLIPSIS$1 = \"...\";\n\nfunction decodeEinsumEquation$1(e, t) {\n  var n = ((e = e.replace(/\\s/g, \"\")).length - e.replace(ARROW_REGEX$1, \"\").length) / ARROW$1.length;\n  if (n < 1) throw new Error(\"Equations without an arrow are not supported.\");\n  if (n > 1) throw new Error(\"Equation must contain exactly one arrow (\\\"\".concat(ARROW$1, \"\\\").\"));\n  var [r, a] = e.split(ARROW$1);\n  assert$6(-1 === r.indexOf(ELLIPSIS$1), () => \"The ellipsis notation (\\\"\".concat(ELLIPSIS$1, \"\\\") is not supported yet.\"));\n  var s = r.split(COMMA$1),\n      o = s.length;\n  if (t !== o) throw new Error(\"Expected \".concat(o, \" input tensors, received \").concat(t));\n  if (o > 2) throw new Error(\"Support for more than 2 input tensors is not implemented yet.\");\n  var i = [];\n\n  var _loop9 = function _loop9(_e72) {\n    var t = a[_e72];\n    if (!s.some(e => -1 !== e.indexOf(t))) throw new Error(\"Output subscripts contain the label \".concat(t, \" not present in the input subscripts.\"));\n    -1 === i.indexOf(t) && i.push(t);\n  };\n\n  for (var _e72 = 0; _e72 < a.length; ++_e72) {\n    _loop9(_e72);\n  }\n\n  for (var _e73 = 0; _e73 < r.length; ++_e73) {\n    var _t68 = r[_e73];\n    -1 === i.indexOf(_t68) && _t68 !== COMMA$1 && i.push(_t68);\n  }\n\n  var l = new Array(s.length);\n\n  for (var _e74 = 0; _e74 < o; ++_e74) {\n    if (new Set(s[_e74].split(\"\")).size !== s[_e74].length) throw new Error(\"Found duplicate axes in input component \".concat(s[_e74], \". Support for duplicate axes in input is not implemented yet.\"));\n    l[_e74] = [];\n\n    for (var _t69 = 0; _t69 < s[_e74].length; ++_t69) {\n      l[_e74].push(i.indexOf(s[_e74][_t69]));\n    }\n  }\n\n  var u = i.length,\n      c = [];\n\n  for (var _e75 = a.length; _e75 < u; ++_e75) {\n    c.push(_e75);\n  }\n\n  return {\n    allDims: i,\n    summedDims: c,\n    idDims: l\n  };\n}\n\nfunction getEinsumPermutation$1(e, t) {\n  var n = new Array(e);\n  n.fill(-1);\n\n  for (var _e76 = 0; _e76 < t.length; ++_e76) {\n    n[t[_e76]] = _e76;\n  }\n\n  var r = [];\n\n  for (var _t70 = 0; _t70 < e; ++_t70) {\n    -1 === n[_t70] && r.push(_t70);\n  }\n\n  return n = n.filter(e => -1 !== e), {\n    permutationIndices: n,\n    expandDims: r\n  };\n}\n\nfunction checkEinsumDimSizes$1(e, t, n) {\n  var r = new Array(e);\n\n  var _loop10 = function _loop10(_e77) {\n    var a = n[_e77].shape;\n\n    var _loop11 = function _loop11(_n25) {\n      void 0 === r[t[_e77][_n25]] ? r[t[_e77][_n25]] = a[_n25] : assert$6(r[t[_e77][_n25]] === a[_n25], () => \"Expected dimension \".concat(r[t[_e77][_n25]], \" at axis \").concat(_n25, \" of input shaped \").concat(JSON.stringify(a), \", but got dimension \").concat(a[_n25]));\n    };\n\n    for (var _n25 = 0; _n25 < t[_e77].length; ++_n25) {\n      _loop11(_n25);\n    }\n  };\n\n  for (var _e77 = 0; _e77 < n.length; ++_e77) {\n    _loop10(_e77);\n  }\n}\n\nfunction getEinsumComputePath$1(e, t) {\n  var n = e,\n      r = [];\n  var a = 0;\n  0 === e.length && n.push(-1), a = e.length + 1;\n\n  for (var _e78 = 0; _e78 < a; ++_e78) {\n    r.push([]);\n  }\n\n  var s = [];\n\n  for (var _e79 = 0; _e79 < n.length; ++_e79) {\n    var _a14 = findTermsWithDim$1(t, n[_e79]);\n\n    for (var _t71 of _a14) {\n      -1 === s.indexOf(_t71) && (r[_e79].push(_t71), s.push(_t71));\n    }\n  }\n\n  return {\n    path: n,\n    steps: r\n  };\n}\n\nfunction isIdentityPermutation$1(e) {\n  return e.every((e, t) => e === t);\n}\n\nfunction findTermsWithDim$1(e, t) {\n  var n = [];\n\n  for (var r = 0; r < e.length; ++r) {\n    0 !== e[r].length && -1 === e[r].indexOf(t) && -1 !== t || n.push(r);\n  }\n\n  return n;\n}\n\nfunction prepareSplitSize$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = [];\n  if (\"number\" == typeof t) assert$6(e.shape[n] % t == 0, () => \"Number of splits must evenly divide the axis.\"), r = new Array(t).fill(e.shape[n] / t);else {\n    assert$6(t.reduce((e, t) => (-1 === t && (e += 1), e), 0) <= 1, () => \"There should be only one negative value in split array.\");\n    var a = t.indexOf(-1);\n\n    if (-1 !== a) {\n      var _r22 = t.reduce((e, t) => t > 0 ? e + t : e);\n\n      t[a] = e.shape[n] - _r22;\n    }\n\n    assert$6(e.shape[n] === t.reduce((e, t) => e + t), () => \"The sum of sizes must match the size of the axis dimension.\"), r = t;\n  }\n  return r;\n}\n\nfunction segOpComputeOptimalWindowSize$1(e, t) {\n  var n,\n      r = !1;\n\n  for (e <= PARALLELIZE_THRESHOLD$1 ? (n = e, r = !0) : n = nearestDivisor$1(e, Math.floor(Math.sqrt(e))); !r;) {\n    n > t || n === e ? r = !0 : n = nearestDivisor$1(e, n + 1);\n  }\n\n  return n;\n}\n\nfunction computeOutShape$3(e, t, n) {\n  var r = [],\n      a = e.length;\n\n  for (var s = 0; s < a; s++) {\n    r.push(s !== t ? e[s] : n);\n  }\n\n  return r;\n}\n\nfunction collectGatherOpShapeInfo$1(e, t, n, r) {\n  var a = t.shape.length,\n      s = e.shape.length;\n  if (0 !== r && (r < -a || r > a)) throw new Error(\"Expect batchDims in the range of [-\".concat(a, \", \").concat(a, \"], but got \").concat(r));\n  if (r < 0 && (r += a), r > s) throw new Error(\"batchDims (\".concat(r, \") must be less than rank(x) (\\n    \").concat(s, \").\"));\n  if (n < r) throw new Error(\"batchDims (\".concat(r, \") must be less than or equal to axis (\").concat(n, \").\"));\n\n  for (var _n26 = 0; _n26 < r; ++_n26) {\n    if (e.shape[_n26] !== t.shape[_n26]) throw new Error(\"x.shape[\".concat(_n26, \"]: \").concat(e.shape[_n26], \" should be equal to indices.shape[\").concat(_n26, \"]: \").concat(t.shape[_n26], \".\"));\n  }\n\n  var o = e.shape[n],\n      i = [];\n  var l = 1,\n      u = 1,\n      c = 1;\n\n  for (var _t72 = 0; _t72 < r; ++_t72) {\n    i.push(e.shape[_t72]), l *= e.shape[_t72];\n  }\n\n  for (var _t73 = r; _t73 < n; _t73++) {\n    i.push(e.shape[_t73]), u *= e.shape[_t73];\n  }\n\n  for (var _e80 = r; _e80 < a; _e80++) {\n    i.push(t.shape[_e80]);\n  }\n\n  for (var _t74 = n + 1; _t74 < s; _t74++) {\n    i.push(e.shape[_t74]), c *= e.shape[_t74];\n  }\n\n  return {\n    batchSize: l,\n    sliceSize: c,\n    outerSize: u,\n    dimSize: o,\n    outputShape: i\n  };\n}\n\nvar segment_util$1 = {\n  __proto__: null,\n  segOpComputeOptimalWindowSize: segOpComputeOptimalWindowSize$1,\n  computeOutShape: computeOutShape$3,\n  collectGatherOpShapeInfo: collectGatherOpShapeInfo$1\n};\n\nfunction fromUint8ToStringArray$1(e) {\n  try {\n    return e.map(e => decodeString$1(e));\n  } catch (e) {\n    throw new Error(\"Failed to decode encoded string bytes into utf-8, error: \".concat(e));\n  }\n}\n\nfunction fromStringArrayToUint8$1(e) {\n  return e.map(e => encodeString$1(e));\n}\n\nvar backend_util$1 = {\n  __proto__: null,\n  slice_util: slice_util$1,\n  segment_util: segment_util$1,\n  fromUint8ToStringArray: fromUint8ToStringArray$1,\n  fromStringArrayToUint8: fromStringArrayToUint8$1,\n  upcastType: upcastType$1,\n  axesAreInnerMostDims: axesAreInnerMostDims$1,\n  combineLocations: combineLocations$1,\n  computeOutAndReduceShapes: computeOutAndReduceShapes$1,\n  expandShapeToKeepDim: expandShapeToKeepDim$1,\n  assertAxesAreInnerMostDims: assertAxesAreInnerMostDims$1,\n  getAxesPermutation: getAxesPermutation$1,\n  getUndoAxesPermutation: getUndoAxesPermutation$1,\n  getInnerMostAxes: getInnerMostAxes$1,\n  getBroadcastDims: getBroadcastDims$3,\n  getReductionAxes: getReductionAxes$1,\n  assertAndGetBroadcastShape: assertAndGetBroadcastShape$1,\n  assertParamsConsistent: assertParamsConsistent$1,\n  computeOutShape: computeOutShape$4,\n  computeDilation2DInfo: computeDilation2DInfo$1,\n  computePool2DInfo: computePool2DInfo$1,\n  computePool3DInfo: computePool3DInfo$1,\n  computeConv2DInfo: computeConv2DInfo$1,\n  computeConv3DInfo: computeConv3DInfo$1,\n  computeDefaultPad: computeDefaultPad$1,\n  tupleValuesAreOne: tupleValuesAreOne$1,\n  eitherStridesOrDilationsAreOne: eitherStridesOrDilationsAreOne$1,\n  convertConv2DDataFormat: convertConv2DDataFormat$1,\n  getFusedDyActivation: getFusedDyActivation$1,\n  getFusedBiasGradient: getFusedBiasGradient$1,\n  applyActivation: applyActivation$3,\n  shouldFuse: shouldFuse$1,\n  PARALLELIZE_THRESHOLD: PARALLELIZE_THRESHOLD$1,\n  computeOptimalWindowSize: computeOptimalWindowSize$1,\n  getImageCenter: getImageCenter$1,\n  getReshaped: getReshaped$1,\n  getPermuted: getPermuted$1,\n  getReshapedPermuted: getReshapedPermuted$1,\n  getSliceBeginCoords: getSliceBeginCoords$1,\n  getSliceSize: getSliceSize$1,\n  prepareAndValidate: prepareAndValidate$1,\n  validateUpdateShape: validateUpdateShape$1,\n  validateInput: validateInput$2,\n  calculateShapes: calculateShapes$1,\n  SELU_SCALEALPHA: SELU_SCALEALPHA$1,\n  SELU_SCALE: SELU_SCALE$1,\n  ERF_P: ERF_P$1,\n  ERF_A1: ERF_A1$1,\n  ERF_A2: ERF_A2$1,\n  ERF_A3: ERF_A3$1,\n  ERF_A4: ERF_A4$1,\n  ERF_A5: ERF_A5$1,\n  warn: warn$1,\n  log: log$6,\n  mergeRealAndImagArrays: mergeRealAndImagArrays$1,\n  splitRealAndImagArrays: splitRealAndImagArrays$1,\n  complexWithEvenIndex: complexWithEvenIndex$1,\n  complexWithOddIndex: complexWithOddIndex$1,\n  getComplexWithIndex: getComplexWithIndex$1,\n  assignToTypedArray: assignToTypedArray$1,\n  exponents: exponents$1,\n  exponent: exponent$1,\n  decodeEinsumEquation: decodeEinsumEquation$1,\n  getEinsumPermutation: getEinsumPermutation$1,\n  checkEinsumDimSizes: checkEinsumDimSizes$1,\n  getEinsumComputePath: getEinsumComputePath$1,\n  isIdentityPermutation: isIdentityPermutation$1,\n  prepareSplitSize: prepareSplitSize$1\n};\nvar absGradConfig$1 = {\n  kernelName: Abs$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(e, step$5(cast$7(n, \"float32\"), -1))\n    };\n  }\n},\n    acosGradConfig$1 = {\n  kernelName: Acos$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = square$5(cast$7(n, \"float32\")),\n            r = sqrt$5(sub$5(scalar$1(1), t));\n        return neg$5(div$3(e, r));\n      }\n    };\n  }\n},\n    acoshGradConfig$1 = {\n  kernelName: Acosh$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = sqrt$5(sub$5(square$5(cast$7(n, \"float32\")), 1));\n        return div$3(e, t);\n      }\n    };\n  }\n},\n    addGradConfig$1 = {\n  kernelName: Add$3,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a15 = assertAndGetBroadcastShape$1(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = e;\n        var r = getReductionAxes$1(n.shape, _a15);\n        return r.length > 0 && (t = sum$6(t, r)), reshape$6(t, n.shape);\n      },\n      b: () => {\n        var t = e;\n        var n = getReductionAxes$1(r.shape, _a15);\n        return n.length > 0 && (t = sum$6(t, n)), reshape$6(t, r.shape);\n      }\n    };\n  }\n},\n    addNGradConfig$1 = {\n  kernelName: AddN$1,\n  saveAllInputs: !0,\n  gradFunc: (e, t) => {\n    var n = {};\n    return t.forEach((t, r) => {\n      n[r] = () => e.clone();\n    }), n;\n  }\n},\n    argMaxGradConfig$1 = {\n  kernelName: ArgMax$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => zerosLike$5(n)\n    };\n  }\n},\n    argMinGradConfig$1 = {\n  kernelName: ArgMin$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => zerosLike$5(n)\n    };\n  }\n},\n    asinGradConfig$1 = {\n  kernelName: Asin$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, sqrt$5(sub$5(scalar$1(1), square$5(cast$7(n, \"float32\")))))\n    };\n  }\n},\n    asinhGradConfig$1 = {\n  kernelName: Asinh$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = sqrt$5(add$5(scalar$1(1), square$5(cast$7(n, \"float32\"))));\n        return div$3(e, t);\n      }\n    };\n  }\n},\n    atan2GradConfig$1 = {\n  kernelName: Atan2$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a16 = assertAndGetBroadcastShape$1(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = add$5(square$5(n), square$5(r));\n        var s = mul$1(e, div$3(r, t));\n        var o = getReductionAxes$1(n.shape, _a16);\n        return o.length > 0 && (s = sum$6(s, o)), reshape$6(s, n.shape);\n      },\n      b: () => {\n        var t = add$5(square$5(n), square$5(r));\n        var s = neg$5(mul$1(e, div$3(n, t)));\n        var o = getReductionAxes$1(r.shape, _a16);\n        return o.length > 0 && (s = sum$6(s, o)), reshape$6(s, r.shape);\n      }\n    };\n  }\n},\n    atanGradConfig$1 = {\n  kernelName: Atan$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, add$5(square$5(cast$7(n, \"float32\")), 1))\n    };\n  }\n},\n    atanhGradConfig$1 = {\n  kernelName: Atanh$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, sub$5(scalar$1(1), square$5(cast$7(n, \"float32\"))))\n    };\n  }\n};\n\nfunction avgPool3dGrad_$1(e, t, n, r, a, s) {\n  var o = convertToTensor$1(e, \"dy\", \"avgPool3dGrad\"),\n      i = convertToTensor$1(t, \"input\", \"avgPool3dGrad\");\n  var l = o,\n      u = i,\n      c = !1;\n  4 === i.rank && (c = !0, l = reshape$6(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]]), u = reshape$6(i, [1, i.shape[0], i.shape[1], i.shape[2], i.shape[3]])), assert$6(5 === l.rank, () => \"Error in avgPool3dGrad: dy must be rank 5 but got rank \".concat(l.rank, \".\")), assert$6(5 === u.rank, () => \"Error in avgPool3dGrad: input must be rank 5 but got rank \".concat(u.rank, \".\")), null != s && assert$6(isInt$1(a), () => \"Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode \".concat(s, \" but got pad \").concat(a, \".\"));\n  var p = ENGINE$1.runKernel(AvgPool3DGrad$1, {\n    dy: l,\n    input: u\n  }, {\n    filterSize: n,\n    strides: r,\n    pad: a,\n    dimRoundingMode: s\n  });\n  return c ? reshape$6(p, [p.shape[1], p.shape[2], p.shape[3], p.shape[4]]) : p;\n}\n\nvar avgPool3dGrad$1 = op$1({\n  avgPool3dGrad_: avgPool3dGrad_$1\n}),\n    avgPool3DGradConfig$3 = {\n  kernelName: AvgPool3D$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o,\n      dimRoundingMode: i\n    } = n;\n    return {\n      x: () => avgPool3dGrad$1(e, r, a, s, o, i)\n    };\n  }\n};\n\nfunction avgPoolGrad_$1(e, t, n, r, a) {\n  var s = convertToTensor$1(e, \"dy\", \"avgPoolGrad\"),\n      o = convertToTensor$1(t, \"input\", \"avgPoolGrad\");\n  assert$6(o.rank === s.rank, () => \"Rank of input (\".concat(o.rank, \") does not match rank of dy (\").concat(s.rank, \")\"));\n  var i = o,\n      l = s,\n      u = !1;\n  3 === o.rank && (u = !0, i = reshape$6(o, [1, o.shape[0], o.shape[1], o.shape[2]]), l = reshape$6(s, [1, s.shape[0], s.shape[1], s.shape[2]])), assert$6(4 === l.rank, () => \"Error in avgPoolGrad: dy must be rank 4 but got rank \".concat(l.rank, \".\")), assert$6(4 === i.rank, () => \"Error in avgPoolGrad: input must be rank 4 but got rank \".concat(i.rank, \".\"));\n  var c = ENGINE$1.runKernel(AvgPoolGrad$1, {\n    dy: l,\n    input: i\n  }, {\n    filterSize: n,\n    strides: r,\n    pad: a\n  });\n  return u ? reshape$6(c, [c.shape[1], c.shape[2], c.shape[3]]) : c;\n}\n\nvar avgPoolGrad$5 = op$1({\n  avgPoolGrad_: avgPoolGrad_$1\n}),\n    avgPoolGradConfig$5 = {\n  kernelName: AvgPool$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o\n    } = n;\n    return {\n      x: () => avgPoolGrad$5(e, r, a, s, o)\n    };\n  }\n},\n    batchMatMulGradConfig$1 = {\n  kernelName: BatchMatMul$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t, n) => {\n    var [r, _a17] = t,\n        {\n      transposeA: s,\n      transposeB: o\n    } = n;\n    return s || o ? !s && o ? {\n      a: () => matMul$3(e, _a17, !1, !1),\n      b: () => matMul$3(e, r, !0, !1)\n    } : s && !o ? {\n      a: () => matMul$3(_a17, e, !1, !0),\n      b: () => matMul$3(r, e, !1, !1)\n    } : {\n      a: () => matMul$3(_a17, e, !0, !0),\n      b: () => matMul$3(e, r, !0, !0)\n    } : {\n      a: () => matMul$3(e, _a17, !1, !0),\n      b: () => matMul$3(r, e, !0, !1)\n    };\n  }\n},\n    batchToSpaceNDGradConfig$1 = {\n  kernelName: BatchToSpaceND$1,\n  gradFunc: (e, t, n) => {\n    var {\n      blockShape: r,\n      crops: a\n    } = n;\n    return {\n      x: () => spaceToBatchND$5(e, r, a)\n    };\n  }\n},\n    broadcastToGradConfig$1 = {\n  kernelName: BroadcastTo$1,\n  gradFunc: (e, t, n) => {\n    var r = n.inputShape,\n        a = n.shape,\n        s = Array.from(a);\n\n    for (var _e81 = r.length - 1; _e81 >= 0; _e81--) {\n      if (r[_e81] === a[_e81]) s[_e81] = 1;else if (1 !== r[_e81]) throw new Error(\"broadcastTo(): [\".concat(r, \"] cannot be broadcast to [\").concat(a, \"].\"));\n    }\n\n    var o = [];\n\n    for (var _e82 = 0; _e82 < s.length; _e82++) {\n      s[_e82] > 1 && o.push(_e82);\n    }\n\n    return {\n      x: () => sum$6(e, o, !0)\n    };\n  }\n},\n    castGradConfig$1 = {\n  kernelName: Cast$1,\n  gradFunc: e => ({\n    x: () => e.clone()\n  })\n},\n    ceilGradConfig$1 = {\n  kernelName: Ceil$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    clipByValueGradConfig$1 = {\n  kernelName: ClipByValue$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      clipValueMin: a,\n      clipValueMax: s\n    } = n;\n    return {\n      x: () => where$1(logicalAnd$5(greaterEqual$5(r, a), lessEqual$5(r, s)), e, zerosLike$5(e))\n    };\n  }\n},\n    complexAbsGradConfig$1 = {\n  kernelName: ComplexAbs$1,\n  inputsToSave: [\"x\"],\n  gradFunc: absGradConfig$1.gradFunc\n},\n    concatGradConfig$1 = {\n  kernelName: Concat$1,\n  saveAllInputs: !0,\n  gradFunc: (e, t, n) => {\n    var r = t.map(e => e.shape),\n        {\n      axis: a\n    } = n,\n        s = parseAxisParam$1(a, t[0].shape)[0],\n        o = r.map(e => e[s]);\n    return split$4(e, o, s).map(e => () => e);\n  }\n},\n    conv2DGradConfig$1 = {\n  kernelName: Conv2D$3,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      dilations: s,\n      strides: o,\n      pad: i,\n      dataFormat: l\n    } = n;\n    return assert$6(tupleValuesAreOne$1(s), () => \"Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '\".concat(s, \"'\")), {\n      x: () => conv2DBackpropInput$5(r.shape, e, a, o, i, l),\n      filter: () => conv2DBackpropFilter$5(r, e, a.shape, o, i, l)\n    };\n  }\n},\n    conv2DBackpropInputGradConfig$1 = {\n  kernelName: Conv2DBackpropInput$1,\n  inputsToSave: [\"dy\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      strides: s,\n      pad: o,\n      dataFormat: i,\n      dimRoundingMode: l\n    } = n;\n    return {\n      dy: () => conv2d$7(e, a, s, o, i, 1, l),\n      filter: () => conv2DBackpropFilter$5(e, r, a.shape, s, o, i, l)\n    };\n  }\n};\n\nfunction conv3DBackpropFilter_$1(e, t, n, r, a) {\n  var s = e;\n  4 === e.rank && (s = reshape$6(e, [1, e.shape[0], e.shape[1], e.shape[2], e.shape[3]]));\n  var o = t;\n  return 4 === o.rank && (o = reshape$6(t, [1, t.shape[0], t.shape[1], t.shape[2], t.shape[3]])), assert$6(5 === s.rank, () => \"Error in conv3dDerFilter: input must be rank 5, but got shape \".concat(s.shape, \".\")), assert$6(5 === o.rank, () => \"Error in conv3dDerFilter: dy must be rank 5, but got shape \".concat(o.shape, \".\")), assert$6(5 === n.length, () => \"Error in conv3dDerFilter: filterShape must be length 5, but got \".concat(n, \".\")), assert$6(s.shape[4] === n[3], () => \"Error in conv3dDerFilter: depth of input \".concat(s.shape[4], \") must match input depth in filter (\").concat(n[3], \".\")), assert$6(o.shape[4] === n[4], () => \"Error in conv3dDerFilter: depth of dy (\".concat(o.shape[4], \") must match output depth for filter (\").concat(n[4], \").\")), ENGINE$1.runKernel(Conv3DBackpropFilterV2$1, {\n    x: s,\n    dy: o\n  }, {\n    strides: r,\n    pad: a,\n    filterShape: n\n  });\n}\n\nvar conv3DBackpropFilter$1 = op$1({\n  conv3DBackpropFilter_: conv3DBackpropFilter_$1\n}),\n    conv3DGradConfig$1 = {\n  kernelName: Conv3D$3,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var {\n      dilations: r,\n      strides: a,\n      pad: s\n    } = n;\n    assert$6(tupleValuesAreOne$1(r), () => \"Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '\".concat(r, \"'\"));\n    var [o, i] = t;\n    return {\n      x: () => conv3DBackpropInput$3(o.shape, e, i, a, s),\n      filter: () => conv3DBackpropFilter$1(o, e, i.shape, a, s)\n    };\n  }\n},\n    cosGradConfig$1 = {\n  kernelName: Cos$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(neg$5(sin$5(cast$7(n, \"float32\"))), e)\n    };\n  }\n},\n    coshGradConfig$1 = {\n  kernelName: Cosh$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(sinh$5(cast$7(n, \"float32\")), e)\n    };\n  }\n},\n    cumsumGradConfig$1 = {\n  kernelName: Cumsum$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      axis: a,\n      exclusive: s,\n      reverse: o\n    } = n;\n    return {\n      x: () => {\n        var t = getAxesPermutation$1([a], r.rank);\n        var n = cumsum$5(e, a, s, !o);\n        return null != t && (n = transpose$5(n, t)), n;\n      }\n    };\n  }\n},\n    depthwiseConv2dNativeGradConfig$1 = {\n  kernelName: DepthwiseConv2dNative$1,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var {\n      dilations: r,\n      strides: a,\n      pad: s,\n      dimRoundingMode: o\n    } = n,\n        i = null == r ? [1, 1] : r;\n    assert$6(tupleValuesAreOne$1(i), () => \"Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '\".concat(i, \"'\"));\n    var [l, u] = t;\n    return assert$6(4 === l.rank, () => \"Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank \".concat(l.rank, \".\")), assert$6(4 === u.rank, () => \"Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank \".concat(u.rank, \".\")), assert$6(l.shape[3] === u.shape[2], () => \"Error in gradient of depthwiseConv2d: number of input channels (\".concat(l.shape[3], \") must match the inChannels dimension in filter \").concat(u.shape[2], \".\")), assert$6(eitherStridesOrDilationsAreOne$1(a, i), () => \"Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides \".concat(a, \" and dilations '\").concat(i, \"'.\")), null != o && assert$6(isInt$1(s), () => \"Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(s, \".\")), {\n      x: () => depthwiseConv2dNativeBackpropInput$5(l.shape, e, u, a, s, r, o),\n      filter: () => depthwiseConv2dNativeBackpropFilter$5(l, e, u.shape, a, s, r, o)\n    };\n  }\n},\n    dilation2dGradConfig$1 = {\n  kernelName: Dilation2D$1,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        s = {\n      x: r,\n      filter: a,\n      dy: e\n    },\n        o = {\n      x: r,\n      filter: a,\n      dy: e\n    };\n    return {\n      x: () => ENGINE$1.runKernel(Dilation2DBackpropInput$1, s, n),\n      filter: () => ENGINE$1.runKernel(Dilation2DBackpropFilter$1, o, n)\n    };\n  }\n},\n    eluGradConfig$5 = {\n  kernelName: Elu$3,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t,\n        r = {\n      dy: e,\n      y: n\n    };\n    return {\n      x: () => ENGINE$1.runKernel(EluGrad$1, r)\n    };\n  }\n},\n    erfGradConfig$1 = {\n  kernelName: Erf$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t,\n        r = mul$1(exp$5(neg$5(square$5(n))), 2 / Math.sqrt(Math.PI));\n    return {\n      x: () => mul$1(e, r)\n    };\n  }\n},\n    expGradConfig$1 = {\n  kernelName: Exp$1,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(e, n)\n    };\n  }\n},\n    expandDimsGradConfig$1 = {\n  kernelName: ExpandDims$1,\n  inputsToSave: [\"input\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      input: () => reshape$6(e, n.shape)\n    };\n  }\n},\n    expm1GradConfig$1 = {\n  kernelName: Expm1$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(e, exp$5(n))\n    };\n  }\n},\n    floorGradConfig$1 = {\n  kernelName: Floor$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    floorDivGradConfig$1 = {\n  kernelName: FloorDiv$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a18 = assertAndGetBroadcastShape$1(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = div$3(e, cast$7(r, \"float32\")),\n            s = getReductionAxes$1(n.shape, _a18);\n        return s.length > 0 ? reshape$6(sum$6(t, s), n.shape) : t;\n      },\n      b: () => {\n        var t = mul$1(e, cast$7(n, \"float32\"));\n        var s = getReductionAxes$1(r.shape, _a18);\n        s.length > 0 && (t = reshape$6(sum$6(t, s), r.shape));\n        var o = square$5(r);\n        return neg$5(div$3(t, cast$7(o, \"float32\")));\n      }\n    };\n  }\n},\n    fusedBatchNormGradConfig$1 = {\n  kernelName: FusedBatchNorm$1,\n  inputsToSave: [\"x\", \"mean\", \"variance\", \"scale\"],\n  gradFunc: (e, t, n) => {\n    var {\n      varianceEpsilon: r\n    } = n,\n        [a, s, o, i] = t,\n        l = null == i ? scalar$1(1) : i,\n        u = getReductionAxes$1(s.shape, a.shape),\n        c = [];\n\n    if (1 === s.rank) {\n      for (var _e83 = 0; _e83 < a.shape.length - 1; ++_e83) {\n        c.push(a.shape[_e83]);\n      }\n\n      c.push(1);\n    }\n\n    var p = sub$5(a, s),\n        d = mul$1(e, l),\n        h = rsqrt$5(add$5(o, scalar$1(r))),\n        m = mul$1(mul$1(mul$1(h, h), h), scalar$1(-.5));\n    return {\n      x: () => reshape$6(mul$1(mul$1(e, 1 === s.rank ? tile$7(reshape$6(h, [1, 1, 1, s.shape[0]]), c) : h), l), a.shape),\n      mean: () => {\n        var e = mul$1(mul$1(h, scalar$1(-1)), d);\n        return 1 === s.rank && (e = sum$6(e, u)), reshape$6(e, s.shape);\n      },\n      variance: () => {\n        var e = mul$1(mul$1(m, p), d);\n        return 1 === s.rank && (e = sum$6(e, u)), reshape$6(e, s.shape);\n      },\n      scale: () => {\n        var t = mul$1(p, h);\n        var n = mul$1(e, t);\n        return 1 === s.rank && (n = sum$6(n, u)), reshape$6(n, s.shape);\n      },\n      offset: () => {\n        var t = e;\n        return 1 === s.rank && (t = sum$6(t, u)), reshape$6(t, s.shape);\n      }\n    };\n  }\n},\n    gatherGradConfig$1 = {\n  kernelName: GatherV2$1,\n  inputsToSave: [\"x\", \"indices\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      axis: s\n    } = n,\n        o = parseAxisParam$1(s, r.shape)[0];\n    return {\n      x: () => {\n        var t = r.shape,\n            n = a.size,\n            i = t.slice(0, o),\n            l = i.length,\n            u = t.slice(s, t.length).slice(1),\n            c = u.length,\n            p = arrayRange$1(0, l),\n            d = arrayRange$1(l + 1, l + 1 + c),\n            h = arrayConcat$1([i, [n], u]),\n            m = reshape$6(e, h),\n            f = reshape$6(a, [n]),\n            g = arrayConcat$1([[l], p, d]),\n            $ = transpose$5(m, g);\n        var y = unsortedSegmentSum$5($, f, r.shape[o]);\n        var b = getUndoAxesPermutation$1(g);\n        return y = transpose$5(y, b), y;\n      },\n      indices: () => a\n    };\n  }\n};\n\nfunction arrayRange$1(e, t) {\n  var n = [];\n\n  for (var r = e; r < t; ++r) {\n    n.push(r);\n  }\n\n  return n;\n}\n\nfunction arrayConcat$1(e) {\n  var t = [];\n\n  for (var n = 0; n < e.length; ++n) {\n    for (var r = 0; r < e[n].length; ++r) {\n      t.push(e[n][r]);\n    }\n  }\n\n  return t;\n}\n\nvar greaterEqualGradConfig$1 = {\n  kernelName: GreaterEqual$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t;\n    return {\n      a: () => zerosLike$5(n),\n      b: () => zerosLike$5(r)\n    };\n  }\n},\n    identityGradConfig$1 = {\n  kernelName: Identity$3,\n  gradFunc: e => ({\n    x: () => cast$7(e, \"float32\")\n  })\n},\n    isFiniteGradConfig$1 = {\n  kernelName: IsFinite$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    isInfGradConfig$1 = {\n  kernelName: IsInf$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    isNanGradConfig$1 = {\n  kernelName: IsNan$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    leakyReluGradConfig$1 = {\n  kernelName: LeakyRelu$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      alpha: a\n    } = n,\n        s = greater$6(r, 0);\n    return {\n      x: () => where$1(s, e, mul$1(e, a))\n    };\n  }\n},\n    log1pGradConfig$1 = {\n  kernelName: Log1p$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, add$5(n, 1))\n    };\n  }\n},\n    logGradConfig$1 = {\n  kernelName: Log$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, cast$7(n, \"float32\"))\n    };\n  }\n},\n    logSoftmaxGradConfig$1 = {\n  kernelName: LogSoftmax$3,\n  inputsToSave: [],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      axis: a\n    } = n;\n    return {\n      logits: () => {\n        var t = exp$5(r);\n        return sub$5(e, mul$1(sum$6(e, a, !0), t));\n      }\n    };\n  }\n};\n\nfunction localResponseNormalizationBackprop_$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : .5;\n  return ENGINE$1.runKernel(LRNGrad$1, {\n    x: e,\n    y: t,\n    dy: n\n  }, {\n    depthRadius: r,\n    bias: a,\n    alpha: s,\n    beta: o\n  });\n}\n\nvar localResponseNormalizationBackprop$1 = op$1({\n  localResponseNormalizationBackprop_: localResponseNormalizationBackprop_$1\n}),\n    lrnGradConfig$1 = {\n  kernelName: LRN$1,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      depthRadius: s,\n      bias: o,\n      alpha: i,\n      beta: l\n    } = n;\n    return {\n      x: () => localResponseNormalizationBackprop$1(r, a, e, s, o, i, l)\n    };\n  }\n};\n\nfunction gradForMinAndMax$1(e, t, n, r) {\n  return t.rank < n.rank && (t = reshape$6(t, expandShapeToKeepDim$1(t.shape, r))), e.rank < n.rank && (e = reshape$6(e, expandShapeToKeepDim$1(e.shape, r))), {\n    x: () => mul$1(e, cast$7(equal$5(n, t), e.dtype))\n  };\n}\n\nvar maxGradConfig$1 = {\n  kernelName: Max$1,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      reductionIndices: a\n    } = r,\n        s = t[0],\n        o = gradForMinAndMax$1(e, t[1], s, parseAxisParam$1(a, s.shape));\n    return {\n      x: () => o.x()\n    };\n  }\n},\n    maximumGradConfig$1 = {\n  kernelName: Maximum$3,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t;\n    return {\n      a: () => mul$1(e, cast$7(greaterEqual$5(n, r), \"float32\")),\n      b: () => mul$1(e, cast$7(less$6(n, r), \"float32\"))\n    };\n  }\n};\n\nfunction maxPool3dGrad_$1(e, t, n, r, a, s, o) {\n  var i = convertToTensor$1(e, \"dy\", \"maxPool3dGrad\"),\n      l = convertToTensor$1(t, \"input\", \"maxPool3dGrad\"),\n      u = convertToTensor$1(n, \"output\", \"maxPool3dGrad\");\n  var c = i,\n      p = l,\n      d = u,\n      h = !1;\n  4 === l.rank && (h = !0, c = reshape$6(i, [1, i.shape[0], i.shape[1], i.shape[2], i.shape[3]]), p = reshape$6(l, [1, l.shape[0], l.shape[1], l.shape[2], l.shape[3]]), d = reshape$6(u, [1, u.shape[0], u.shape[1], u.shape[2], u.shape[3]])), assert$6(5 === c.rank, () => \"Error in maxPool3dGrad: dy must be rank 5 but got rank \".concat(c.rank, \".\")), assert$6(5 === p.rank, () => \"Error in maxPool3dGrad: input must be rank 5 but got rank \".concat(p.rank, \".\")), assert$6(5 === d.rank, () => \"Error in maxPool3dGrad: output must be rank 5 but got rank \".concat(d.rank, \".\")), null != o && assert$6(isInt$1(s), () => \"Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(s, \".\"));\n  var m = ENGINE$1.runKernel(MaxPool3DGrad$1, {\n    dy: c,\n    input: p,\n    output: d\n  }, {\n    filterSize: r,\n    strides: a,\n    pad: s,\n    dimRoundingMode: o\n  });\n  return h ? reshape$6(m, [m.shape[1], m.shape[2], m.shape[3], m.shape[4]]) : m;\n}\n\nvar maxPool3dGrad$1 = op$1({\n  maxPool3dGrad_: maxPool3dGrad_$1\n}),\n    maxPool3DGradConfig$3 = {\n  kernelName: MaxPool3D$1,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      filterSize: s,\n      strides: o,\n      pad: i,\n      dimRoundingMode: l\n    } = n;\n    return {\n      x: () => maxPool3dGrad$1(e, r, a, s, o, i, l)\n    };\n  }\n};\n\nfunction maxPoolGrad_$1(e, t, n, r, a, s, o) {\n  var i = convertToTensor$1(e, \"dy\", \"maxPoolGrad\"),\n      l = convertToTensor$1(t, \"input\", \"maxPoolGrad\"),\n      u = convertToTensor$1(n, \"output\", \"maxPoolGrad\");\n  return assert$6(l.rank === i.rank, () => \"Rank of input (\".concat(l.rank, \") does not match rank of dy (\").concat(i.rank, \")\")), assert$6(4 === i.rank, () => \"Error in maxPoolGrad: dy must be rank 4 but got rank \".concat(i.rank, \".\")), assert$6(4 === l.rank, () => \"Error in maxPoolGrad: input must be rank 4 but got rank \".concat(l.rank, \".\")), null != o && assert$6(isInt$1(s), () => \"Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(s, \".\")), ENGINE$1.runKernel(MaxPoolGrad$1, {\n    dy: i,\n    input: l,\n    output: u\n  }, {\n    filterSize: r,\n    strides: a,\n    pad: s,\n    dimRoundingMode: o\n  });\n}\n\nvar maxPoolGrad$5 = op$1({\n  maxPoolGrad_: maxPoolGrad_$1\n}),\n    maxPoolGradConfig$5 = {\n  kernelName: MaxPool$1,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      filterSize: s,\n      strides: o,\n      pad: i\n    } = n;\n    return {\n      x: () => maxPoolGrad$5(e, r, a, s, o, i)\n    };\n  }\n},\n    meanGradConfig$1 = {\n  kernelName: Mean$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      axis: a\n    } = n,\n        s = parseAxisParam$1(a, r.shape),\n        o = sizeFromShape$1(computeOutAndReduceShapes$1(r.shape, s)[1]);\n    return {\n      x: () => {\n        var t = r.shape.slice();\n        s.forEach(e => {\n          t[e] = 1;\n        });\n        var n = reshape$6(e, t);\n        return div$3(mul$1(n, ones$3(r.shape, \"float32\")), o);\n      }\n    };\n  }\n},\n    minGradConfig$1 = {\n  kernelName: Min$1,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      axis: a\n    } = r,\n        [s, o] = t,\n        i = gradForMinAndMax$1(e, o, s, parseAxisParam$1(a, s.shape));\n    return {\n      x: () => i.x()\n    };\n  }\n},\n    minimumGradConfig$1 = {\n  kernelName: Minimum$3,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t;\n    return {\n      a: () => mul$1(e, cast$7(lessEqual$5(n, r), \"float32\")),\n      b: () => mul$1(e, cast$7(greater$6(n, r), \"float32\"))\n    };\n  }\n},\n    mirrorPadGradConfig$1 = {\n  kernelName: MirrorPad$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var r = t[0],\n        {\n      paddings: a\n    } = n,\n        s = a.map(e => e[0]);\n    return {\n      x: () => slice$5(e, s, r.shape)\n    };\n  }\n},\n    modGradConfig$1 = {\n  kernelName: Mod$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a19 = assertAndGetBroadcastShape$1(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = getReductionAxes$1(n.shape, _a19);\n        return t.length > 0 ? reshape$6(sum$6(e, t), n.shape) : e;\n      },\n      b: () => {\n        var t = mul$1(e, neg$5(floor$5(div$3(n, r)))),\n            s = getReductionAxes$1(r.shape, _a19);\n        return s.length > 0 ? reshape$6(sum$6(t, s), r.shape) : t;\n      }\n    };\n  }\n},\n    multiplyGradConfig$1 = {\n  kernelName: Multiply$3,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a20 = assertAndGetBroadcastShape$1(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = mul$1(e, cast$7(r, \"float32\")),\n            s = getReductionAxes$1(n.shape, _a20);\n        return s.length > 0 ? reshape$6(sum$6(t, s), n.shape) : t;\n      },\n      b: () => {\n        var t = mul$1(e, cast$7(n, \"float32\")),\n            s = getReductionAxes$1(r.shape, _a20);\n        return s.length > 0 ? reshape$6(sum$6(t, s), r.shape) : t;\n      }\n    };\n  }\n},\n    negGradConfig$1 = {\n  kernelName: Neg$1,\n  gradFunc: e => ({\n    x: () => neg$5(e)\n  })\n},\n    oneHotGradConfig$1 = {\n  kernelName: OneHot$1,\n  inputsToSave: [\"indices\"],\n  gradFunc: (e, t) => {\n    var n = t[0];\n    return {\n      indices: () => zeros$4(n.shape, \"float32\")\n    };\n  }\n},\n    onesLikeGradConfig$1 = {\n  kernelName: OnesLike$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    packGradConfig$1 = {\n  kernelName: Pack$1,\n  saveAllInputs: !0,\n  gradFunc: (e, t, n) => {\n    var {\n      axis: r\n    } = n;\n    return unstack$1(e, r).map(e => () => e);\n  }\n},\n    padV2GradConfig$1 = {\n  kernelName: PadV2$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var r = t[0],\n        {\n      paddings: a\n    } = n,\n        s = a.map(e => e[0]);\n    return {\n      x: () => slice$5(e, s, r.shape)\n    };\n  }\n},\n    powGradConfig$1 = {\n  kernelName: Pow$1,\n  inputsToSave: [\"a\", \"b\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n, r, a] = t,\n        s = n,\n        o = r,\n        i = assertAndGetBroadcastShape$1(s.shape, o.shape);\n    return {\n      a: () => {\n        var t = cast$7(o, \"float32\");\n        var n = mul$1(e, mul$1(t, pow$5(s, sub$5(t, scalar$1(1)))));\n        var r = getReductionAxes$1(s.shape, i);\n        return r.length > 0 && (n = sum$6(n, r)), reshape$6(n, s.shape);\n      },\n      b: () => {\n        var t = greater$6(s, 0),\n            n = where$1(t, log$7(s), zerosLike$5(s));\n        var r = mul$1(e, mul$1(a, n));\n        var l = getReductionAxes$1(o.shape, i);\n        return l.length > 0 && (r = sum$6(r, l)), reshape$6(r, o.shape);\n      }\n    };\n  }\n},\n    preluGradConfig$1 = {\n  kernelName: Prelu$1,\n  inputsToSave: [\"x\", \"alpha\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        a = greater$6(n, 0);\n    return {\n      x: () => where$1(a, e, mul$1(e, r)),\n      alpha: () => {\n        var t = where$1(a, zerosLike$5(e), mul$1(e, n));\n        var s = getReductionAxes$1(r.shape, e.shape);\n        return s.length > 0 && (t = sum$6(t, s)), reshape$6(t, r.shape);\n      }\n    };\n  }\n},\n    divGradConfig$1 = {\n  kernelName: RealDiv$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a21 = assertAndGetBroadcastShape$1(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = div$3(e, cast$7(r, \"float32\")),\n            s = getReductionAxes$1(n.shape, _a21);\n        return s.length > 0 ? reshape$6(sum$6(t, s), n.shape) : t;\n      },\n      b: () => {\n        var t = mul$1(e, cast$7(n, \"float32\"));\n        var s = getReductionAxes$1(r.shape, _a21);\n        s.length > 0 && (t = reshape$6(sum$6(t, s), r.shape));\n        var o = square$5(r);\n        return neg$5(div$3(t, cast$7(o, \"float32\")));\n      }\n    };\n  }\n},\n    reciprocalGradConfig$1 = {\n  kernelName: Reciprocal$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, neg$5(square$5(n)))\n    };\n  }\n},\n    relu6GradConfig$1 = {\n  kernelName: Relu6$3,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t,\n        r = mul$1(lessEqual$5(n, 6), step$5(n));\n    return {\n      x: () => mul$1(e, cast$7(r, \"float32\"))\n    };\n  }\n},\n    reluGradConfig$1 = {\n  kernelName: Relu$3,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(e, cast$7(step$5(n), \"float32\"))\n    };\n  }\n},\n    reshapeGradConfig$1 = {\n  kernelName: Reshape$3,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => reshape$6(e, n.shape)\n    };\n  }\n},\n    resizeBilinearGradConfig$5 = {\n  kernelName: ResizeBilinear$1,\n  inputsToSave: [\"images\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        a = {\n      dy: e,\n      images: r\n    };\n    return {\n      images: () => ENGINE$1.runKernel(ResizeBilinearGrad$1, a, n)\n    };\n  }\n},\n    resizeNearestNeighborGradConfig$5 = {\n  kernelName: ResizeNearestNeighbor$1,\n  inputsToSave: [\"images\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        a = {\n      dy: e,\n      images: r\n    };\n    return {\n      images: () => ENGINE$1.runKernel(ResizeNearestNeighborGrad$1, a, n)\n    };\n  }\n},\n    reverseGradConfig$1 = {\n  kernelName: Reverse$1,\n  gradFunc: (e, t, n) => {\n    var {\n      dims: r\n    } = n,\n        a = parseAxisParam$1(r, e.shape);\n    return {\n      x: () => reverse$5(e, a)\n    };\n  }\n},\n    roundGradConfig$1 = {\n  kernelName: Round$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    rsqrtGradConfig$1 = {\n  kernelName: Rsqrt$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => neg$5(div$3(e, mul$1(pow$5(n, 1.5), 2)))\n    };\n  }\n},\n    selectGradConfig$1 = {\n  kernelName: Select$1,\n  inputsToSave: [\"condition\"],\n  gradFunc: (_e84, t) => {\n    var [n] = t;\n    return {\n      condition: () => cast$7(zerosLike$5(n), \"float32\"),\n      t: () => mul$1(_e84, cast$7(n, _e84.dtype)),\n      e: () => mul$1(_e84, cast$7(logicalNot$5(n), _e84.dtype))\n    };\n  }\n},\n    seluGradConfig$1 = {\n  kernelName: Selu$3,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = greater$6(n, scalar$1(0)),\n            r = scalar$1(SELU_SCALEALPHA$1),\n            a = scalar$1(SELU_SCALE$1),\n            s = mul$1(e, a),\n            o = mul$1(mul$1(e, r), exp$5(cast$7(n, \"float32\")));\n        return where$1(t, s, o);\n      }\n    };\n  }\n},\n    sigmoidGradConfig$1 = {\n  kernelName: Sigmoid$3,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(e, mul$1(n, sub$5(scalar$1(1), n)))\n    };\n  }\n},\n    signGradConfig$1 = {\n  kernelName: Sign$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    sinGradConfig$1 = {\n  kernelName: Sin$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(cos$5(cast$7(n, \"float32\")), e)\n    };\n  }\n},\n    sinhGradConfig$1 = {\n  kernelName: Sinh$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(cosh$5(cast$7(n, \"float32\")), e)\n    };\n  }\n},\n    sliceGradConfig$1 = {\n  kernelName: Slice$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      begin: a,\n      size: s\n    } = n,\n        o = r.shape,\n        [i, l] = parseSliceParams$1(r, a, s),\n        u = [];\n\n    for (var _t75 = 0; _t75 < e.rank; _t75++) {\n      u.push([i[_t75], o[_t75] - i[_t75] - l[_t75]]);\n    }\n\n    return {\n      x: () => pad$1(e, u)\n    };\n  }\n},\n    softmaxGradConfig$1 = {\n  kernelName: Softmax$5,\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      dim: a\n    } = n,\n        s = mul$1(e, r);\n    return {\n      logits: () => sub$5(s, mul$1(sum$6(s, [a], !0), r))\n    };\n  }\n},\n    softplusGradConfig$1 = {\n  kernelName: Softplus$3,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(e, sigmoid$5(n))\n    };\n  }\n},\n    spaceToBatchNDGradConfig$1 = {\n  kernelName: SpaceToBatchND$1,\n  gradFunc: (e, t, n) => {\n    var {\n      blockShape: r,\n      paddings: a\n    } = n;\n    return {\n      x: () => batchToSpaceND$5(e, r, a)\n    };\n  }\n},\n    splitVGradConfig$1 = {\n  kernelName: SplitV$1,\n  gradFunc: (e, t, n) => {\n    var {\n      axis: r\n    } = n;\n    return {\n      x: () => concat$5(e, r)\n    };\n  }\n},\n    sqrtGradConfig$1 = {\n  kernelName: Sqrt$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, mul$1(sqrt$5(cast$7(n, \"float32\")), 2))\n    };\n  }\n},\n    squareGradConfig$1 = {\n  kernelName: Square$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(e, mul$1(cast$7(n, \"float32\"), 2))\n    };\n  }\n},\n    squaredDifferenceGradConfig$1 = {\n  kernelName: SquaredDifference$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a22 = scalar$1(2);\n\n    return {\n      a: () => mul$1(e, mul$1(_a22, sub$5(n, r))),\n      b: () => mul$1(e, mul$1(_a22, sub$5(r, n)))\n    };\n  }\n},\n    stepGradConfig$1 = {\n  kernelName: Step$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    subGradConfig$1 = {\n  kernelName: Sub$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a23 = assertAndGetBroadcastShape$1(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = e;\n        var r = getReductionAxes$1(n.shape, _a23);\n        return r.length > 0 && (t = sum$6(t, r)), reshape$6(t, n.shape);\n      },\n      b: () => {\n        var t = e;\n        var n = getReductionAxes$1(r.shape, _a23);\n        return n.length > 0 && (t = sum$6(t, n)), reshape$6(neg$5(t), r.shape);\n      }\n    };\n  }\n},\n    sumGradConfig$1 = {\n  kernelName: Sum$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        a = r.shape.slice(),\n        {\n      axis: s\n    } = n;\n    parseAxisParam$1(s, r.shape).forEach(e => {\n      a[e] = 1;\n    });\n    var o = reshape$6(e, a),\n        i = mul$1(o, ones$3(r.shape, \"float32\"));\n    return {\n      x: () => i\n    };\n  }\n},\n    tanGradConfig$1 = {\n  kernelName: Tan$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$3(e, square$5(cos$5(n)))\n    };\n  }\n},\n    tanhGradConfig$1 = {\n  kernelName: Tanh$3,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul$1(sub$5(scalar$1(1), square$5(n)), e)\n    };\n  }\n},\n    tileGradConfig$1 = {\n  kernelName: Tile$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      reps: a\n    } = n;\n    return {\n      x: () => {\n        var t = zerosLike$5(r);\n        if (1 === r.rank) for (var _n27 = 0; _n27 < a[0]; ++_n27) {\n          t = add$5(t, slice$5(e, [_n27 * r.shape[0]], [r.shape[0]]));\n        } else if (2 === r.rank) for (var _n28 = 0; _n28 < a[0]; ++_n28) {\n          for (var s = 0; s < a[1]; ++s) {\n            t = add$5(t, slice$5(e, [_n28 * r.shape[0], s * r.shape[1]], [r.shape[0], r.shape[1]]));\n          }\n        } else if (3 === r.rank) for (var _n29 = 0; _n29 < a[0]; ++_n29) {\n          for (var _s9 = 0; _s9 < a[1]; ++_s9) {\n            for (var o = 0; o < a[2]; ++o) {\n              t = add$5(t, slice$5(e, [_n29 * r.shape[0], _s9 * r.shape[1], o * r.shape[2]], [r.shape[0], r.shape[1], r.shape[2]]));\n            }\n          }\n        } else {\n          if (4 !== r.rank) throw new Error(\"Gradient for tile operation is not implemented for rank-\".concat(r.rank, \" tensors yet.\"));\n\n          for (var _n30 = 0; _n30 < a[0]; ++_n30) {\n            for (var _s10 = 0; _s10 < a[1]; ++_s10) {\n              for (var _o7 = 0; _o7 < a[2]; ++_o7) {\n                for (var i = 0; i < a[3]; ++i) {\n                  t = add$5(t, slice$5(e, [_n30 * r.shape[0], _s10 * r.shape[1], _o7 * r.shape[2], i * r.shape[3]], [r.shape[0], r.shape[1], r.shape[2], r.shape[3]]));\n                }\n              }\n            }\n          }\n        }\n        return t;\n      }\n    };\n  }\n},\n    transposeGradConfig$1 = {\n  kernelName: Transpose$1,\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      perm: a\n    } = r,\n        s = getUndoAxesPermutation$1(a);\n    return {\n      x: () => transpose$5(e, s)\n    };\n  }\n},\n    unpackGradConfig$1 = {\n  kernelName: Unpack$1,\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      axis: a\n    } = r;\n    return {\n      value: () => stack$1(e, a)\n    };\n  }\n},\n    unsortedSegmentSumGradConfig$1 = {\n  kernelName: UnsortedSegmentSum$1,\n  inputsToSave: [\"segmentIds\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => gatherDropNegatives$1(e, n)\n    };\n  }\n};\n\nfunction gatherDropNegatives$1(e, t) {\n  var n = maximum$6(t, zerosLike$5(t)),\n      r = gather$3(e, n);\n  var a = greaterEqual$5(t, scalar$1(0, \"int32\"));\n  var s = r.rank - a.rank;\n\n  for (var _e85 = 0; _e85 < s; ++_e85) {\n    a = expandDims$7(a, _e85 + 1);\n  }\n\n  a = logicalAnd$5(a, ones$3(r.shape, \"bool\"));\n  var o = zerosLike$5(r);\n  return where$1(a, r, o);\n}\n\nvar zerosLikeGradConfig$1 = {\n  kernelName: ZerosLike$1,\n  gradFunc: e => ({\n    x: () => zerosLike$5(e)\n  })\n},\n    gradConfigs$1 = [absGradConfig$1, acosGradConfig$1, acoshGradConfig$1, addGradConfig$1, addNGradConfig$1, argMaxGradConfig$1, argMinGradConfig$1, asinGradConfig$1, asinhGradConfig$1, atan2GradConfig$1, atanGradConfig$1, atanhGradConfig$1, avgPool3DGradConfig$3, avgPoolGradConfig$5, batchMatMulGradConfig$1, batchToSpaceNDGradConfig$1, broadcastToGradConfig$1, castGradConfig$1, ceilGradConfig$1, clipByValueGradConfig$1, complexAbsGradConfig$1, concatGradConfig$1, conv2DBackpropInputGradConfig$1, conv2DGradConfig$1, conv3DGradConfig$1, cosGradConfig$1, coshGradConfig$1, cumsumGradConfig$1, depthwiseConv2dNativeGradConfig$1, dilation2dGradConfig$1, divGradConfig$1, eluGradConfig$5, erfGradConfig$1, expGradConfig$1, expandDimsGradConfig$1, expm1GradConfig$1, floorDivGradConfig$1, floorGradConfig$1, fusedBatchNormGradConfig$1, gatherGradConfig$1, greaterEqualGradConfig$1, identityGradConfig$1, isFiniteGradConfig$1, isInfGradConfig$1, isNanGradConfig$1, leakyReluGradConfig$1, log1pGradConfig$1, logGradConfig$1, logSoftmaxGradConfig$1, lrnGradConfig$1, maxGradConfig$1, maxGradConfig$1, maximumGradConfig$1, maxPool3DGradConfig$3, maxPoolGradConfig$5, meanGradConfig$1, minGradConfig$1, minimumGradConfig$1, mirrorPadGradConfig$1, modGradConfig$1, multiplyGradConfig$1, negGradConfig$1, oneHotGradConfig$1, onesLikeGradConfig$1, packGradConfig$1, padV2GradConfig$1, padV2GradConfig$1, powGradConfig$1, preluGradConfig$1, reciprocalGradConfig$1, relu6GradConfig$1, reluGradConfig$1, reshapeGradConfig$1, resizeBilinearGradConfig$5, resizeNearestNeighborGradConfig$5, reverseGradConfig$1, roundGradConfig$1, rsqrtGradConfig$1, selectGradConfig$1, seluGradConfig$1, sigmoidGradConfig$1, signGradConfig$1, sinGradConfig$1, sinhGradConfig$1, sliceGradConfig$1, softmaxGradConfig$1, softplusGradConfig$1, spaceToBatchNDGradConfig$1, spaceToBatchNDGradConfig$1, splitVGradConfig$1, splitVGradConfig$1, sqrtGradConfig$1, squaredDifferenceGradConfig$1, squareGradConfig$1, stepGradConfig$1, subGradConfig$1, sumGradConfig$1, tanGradConfig$1, tanhGradConfig$1, tileGradConfig$1, transposeGradConfig$1, unpackGradConfig$1, unsortedSegmentSumGradConfig$1, zerosLikeGradConfig$1];\n\nfor (var e of gradConfigs$1) {\n  registerGradient$1(e);\n}\n\nvar _epsilon$1;\n\nfunction epsilon$3() {\n  return null == _epsilon$1 && (_epsilon$1 = backend$1().epsilon()), _epsilon$1;\n}\n\nfunction imageDataFormat$1() {\n  return \"channelsLast\";\n}\n\ngetGlobalTensorClass$1().prototype.abs = function () {\n  return this.throwIfDisposed(), abs$5(this);\n}, getGlobalTensorClass$1().prototype.acos = function () {\n  return this.throwIfDisposed(), acos$5(this);\n}, getGlobalTensorClass$1().prototype.acosh = function () {\n  return this.throwIfDisposed(), acosh$5(this);\n}, getGlobalTensorClass$1().prototype.add = function (e) {\n  return this.throwIfDisposed(), add$5(this, e);\n}, getGlobalTensorClass$1().prototype.all = function (e, t) {\n  return this.throwIfDisposed(), all$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.any = function (e, t) {\n  return this.throwIfDisposed(), any$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.argMax = function (e) {\n  return this.throwIfDisposed(), argMax$5(this, e);\n}, getGlobalTensorClass$1().prototype.argMin = function (e) {\n  return this.throwIfDisposed(), argMin$5(this, e);\n}, getGlobalTensorClass$1().prototype.asScalar = function () {\n  return this.throwIfDisposed(), assert$6(1 === this.size, () => \"The array must have only 1 element.\"), reshape$6(this, []);\n}, getGlobalTensorClass$1().prototype.asType = function (e) {\n  return this.throwIfDisposed(), cast$7(this, e);\n}, getGlobalTensorClass$1().prototype.as1D = function () {\n  return this.throwIfDisposed(), reshape$6(this, [this.size]);\n}, getGlobalTensorClass$1().prototype.as2D = function (e, t) {\n  return this.throwIfDisposed(), reshape$6(this, [e, t]);\n}, getGlobalTensorClass$1().prototype.as3D = function (e, t, n) {\n  return this.throwIfDisposed(), reshape$6(this, [e, t, n]);\n}, getGlobalTensorClass$1().prototype.as4D = function (e, t, n, r) {\n  return this.throwIfDisposed(), reshape$6(this, [e, t, n, r]);\n}, getGlobalTensorClass$1().prototype.as5D = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), reshape$6(this, [e, t, n, r, a]);\n}, getGlobalTensorClass$1().prototype.asin = function () {\n  return this.throwIfDisposed(), asin$5(this);\n}, getGlobalTensorClass$1().prototype.asinh = function () {\n  return this.throwIfDisposed(), asinh$5(this);\n}, getGlobalTensorClass$1().prototype.atan = function () {\n  return this.throwIfDisposed(), atan$5(this);\n}, getGlobalTensorClass$1().prototype.atan2 = function (e) {\n  return this.throwIfDisposed(), atan2$5(this, e);\n}, getGlobalTensorClass$1().prototype.atanh = function () {\n  return this.throwIfDisposed(), atanh$5(this);\n}, getGlobalTensorClass$1().prototype.avgPool = function (e, t, n, r) {\n  return this.throwIfDisposed(), avgPool$5(this, e, t, n, r);\n}, getGlobalTensorClass$1().prototype.batchToSpaceND = function (e, t) {\n  return this.throwIfDisposed(), batchToSpaceND$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.batchNorm = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), batchNorm$5(this, e, t, n, r, a);\n}, getGlobalTensorClass$1().prototype.broadcastTo = function (e) {\n  return this.throwIfDisposed(), broadcastTo$1(this, e);\n}, getGlobalTensorClass$1().prototype.cast = function (e) {\n  return this.throwIfDisposed(), cast$7(this, e);\n}, getGlobalTensorClass$1().prototype.ceil = function () {\n  return this.throwIfDisposed(), ceil$5(this);\n}, getGlobalTensorClass$1().prototype.clipByValue = function (e, t) {\n  return this.throwIfDisposed(), clipByValue$3(this, e, t);\n}, getGlobalTensorClass$1().prototype.concat = function (e, t) {\n  return this.throwIfDisposed(), e instanceof Tensor$1 && (e = [e]), concat$5([this, ...e], t);\n}, getGlobalTensorClass$1().prototype.conv1d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), conv1d$3(this, e, t, n, r, a, s);\n}, getGlobalTensorClass$1().prototype.conv2dTranspose = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), conv2dTranspose$2(this, e, t, n, r, a);\n}, getGlobalTensorClass$1().prototype.conv2d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), conv2d$7(this, e, t, n, r, a, s);\n}, getGlobalTensorClass$1().prototype.cos = function () {\n  return this.throwIfDisposed(), cos$5(this);\n}, getGlobalTensorClass$1().prototype.cosh = function () {\n  return this.throwIfDisposed(), cosh$5(this);\n}, getGlobalTensorClass$1().prototype.cumsum = function (e, t, n) {\n  return this.throwIfDisposed(), cumsum$5(this, e, t, n);\n}, getGlobalTensorClass$1().prototype.depthToSpace = function (e, t) {\n  return this.throwIfDisposed(), depthToSpace$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.depthwiseConv2d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), depthwiseConv2d$5(this, e, t, n, r, a, s);\n}, getGlobalTensorClass$1().prototype.dilation2d = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), dilation2d$1(this, e, t, n, r, a);\n}, getGlobalTensorClass$1().prototype.divNoNan = function (e) {\n  return this.throwIfDisposed(), divNoNan$1(this, e);\n}, getGlobalTensorClass$1().prototype.div = function (e) {\n  return this.throwIfDisposed(), div$3(this, e);\n}, getGlobalTensorClass$1().prototype.dot = function (e) {\n  return this.throwIfDisposed(), dot$4(this, e);\n}, getGlobalTensorClass$1().prototype.elu = function () {\n  return this.throwIfDisposed(), elu$8(this);\n}, getGlobalTensorClass$1().prototype.equal = function (e) {\n  return this.throwIfDisposed(), equal$5(this, e);\n}, getGlobalTensorClass$1().prototype.erf = function () {\n  return this.throwIfDisposed(), erf$5(this);\n}, getGlobalTensorClass$1().prototype.exp = function () {\n  return this.throwIfDisposed(), exp$5(this);\n}, getGlobalTensorClass$1().prototype.expandDims = function (e) {\n  return this.throwIfDisposed(), expandDims$7(this, e);\n}, getGlobalTensorClass$1().prototype.expm1 = function () {\n  return this.throwIfDisposed(), expm1$5(this);\n}, getGlobalTensorClass$1().prototype.fft = function () {\n  return this.throwIfDisposed(), fft$5(this);\n}, getGlobalTensorClass$1().prototype.flatten = function () {\n  return this.throwIfDisposed(), reshape$6(this, [this.size]);\n}, getGlobalTensorClass$1().prototype.floor = function () {\n  return this.throwIfDisposed(), floor$5(this);\n}, getGlobalTensorClass$1().prototype.floorDiv = function (e) {\n  return this.throwIfDisposed(), floorDiv$5(this, e);\n}, getGlobalTensorClass$1().prototype.gather = function (e, t) {\n  return this.throwIfDisposed(), gather$3(this, e, t);\n}, getGlobalTensorClass$1().prototype.greaterEqual = function (e) {\n  return this.throwIfDisposed(), greaterEqual$5(this, e);\n}, getGlobalTensorClass$1().prototype.greater = function (e) {\n  return this.throwIfDisposed(), greater$6(this, e);\n}, getGlobalTensorClass$1().prototype.ifft = function () {\n  return this.throwIfDisposed(), ifft$5(this);\n}, getGlobalTensorClass$1().prototype.irfft = function () {\n  return this.throwIfDisposed(), irfft$1(this);\n}, getGlobalTensorClass$1().prototype.isFinite = function () {\n  return this.throwIfDisposed(), isFinite$6(this);\n}, getGlobalTensorClass$1().prototype.isInf = function () {\n  return this.throwIfDisposed(), isInf$5(this);\n}, getGlobalTensorClass$1().prototype.isNaN = function () {\n  return this.throwIfDisposed(), isNaN$6(this);\n}, getGlobalTensorClass$1().prototype.leakyRelu = function (e) {\n  return this.throwIfDisposed(), leakyRelu$5(this, e);\n}, getGlobalTensorClass$1().prototype.lessEqual = function (e) {\n  return this.throwIfDisposed(), lessEqual$5(this, e);\n}, getGlobalTensorClass$1().prototype.less = function (e) {\n  return this.throwIfDisposed(), less$6(this, e);\n}, getGlobalTensorClass$1().prototype.localResponseNormalization = function (e, t, n, r) {\n  return this.throwIfDisposed(), localResponseNormalization$1(this, e, t, n, r);\n}, getGlobalTensorClass$1().prototype.logSigmoid = function () {\n  return this.throwIfDisposed(), logSigmoid$1(this);\n}, getGlobalTensorClass$1().prototype.logSoftmax = function (e) {\n  return this.throwIfDisposed(), logSoftmax$1(this, e);\n}, getGlobalTensorClass$1().prototype.logSumExp = function (e, t) {\n  return this.throwIfDisposed(), logSumExp$1(this, e, t);\n}, getGlobalTensorClass$1().prototype.log = function () {\n  return this.throwIfDisposed(), log$7(this);\n}, getGlobalTensorClass$1().prototype.log1p = function () {\n  return this.throwIfDisposed(), log1p$5(this);\n}, getGlobalTensorClass$1().prototype.logicalAnd = function (e) {\n  return this.throwIfDisposed(), logicalAnd$5(this, e);\n}, getGlobalTensorClass$1().prototype.logicalNot = function () {\n  return this.throwIfDisposed(), logicalNot$5(this);\n}, getGlobalTensorClass$1().prototype.logicalOr = function (e) {\n  return this.throwIfDisposed(), logicalOr$5(this, e);\n}, getGlobalTensorClass$1().prototype.logicalXor = function (e) {\n  return this.throwIfDisposed(), logicalXor$1(this, e);\n}, getGlobalTensorClass$1().prototype.matMul = function (e, t, n) {\n  return this.throwIfDisposed(), matMul$3(this, e, t, n);\n}, getGlobalTensorClass$1().prototype.maxPool = function (e, t, n, r) {\n  return this.throwIfDisposed(), maxPool$5(this, e, t, n, r);\n}, getGlobalTensorClass$1().prototype.max = function (e, t) {\n  return this.throwIfDisposed(), max$7(this, e, t);\n}, getGlobalTensorClass$1().prototype.maximum = function (e) {\n  return this.throwIfDisposed(), maximum$6(this, e);\n}, getGlobalTensorClass$1().prototype.mean = function (e, t) {\n  return this.throwIfDisposed(), mean$3(this, e, t);\n}, getGlobalTensorClass$1().prototype.min = function (e, t) {\n  return this.throwIfDisposed(), min$7(this, e, t);\n}, getGlobalTensorClass$1().prototype.minimum = function (e) {\n  return this.throwIfDisposed(), minimum$6(this, e);\n}, getGlobalTensorClass$1().prototype.mirrorPad = function (e, t) {\n  return this.throwIfDisposed(), mirrorPad$3(this, e, t);\n}, getGlobalTensorClass$1().prototype.mod = function (e) {\n  return this.throwIfDisposed(), mod$5(this, e);\n}, getGlobalTensorClass$1().prototype.mul = function (e) {\n  return this.throwIfDisposed(), mul$1(this, e);\n}, getGlobalTensorClass$1().prototype.neg = function () {\n  return this.throwIfDisposed(), neg$5(this);\n}, getGlobalTensorClass$1().prototype.norm = function (e, t, n) {\n  return this.throwIfDisposed(), norm$1(this, e, t, n);\n}, getGlobalTensorClass$1().prototype.notEqual = function (e) {\n  return this.throwIfDisposed(), notEqual$5(this, e);\n}, getGlobalTensorClass$1().prototype.oneHot = function (e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  return this.throwIfDisposed(), oneHot$5(this, e, t, n);\n}, getGlobalTensorClass$1().prototype.onesLike = function () {\n  return this.throwIfDisposed(), onesLike$5(this);\n}, getGlobalTensorClass$1().prototype.pad = function (e, t) {\n  return this.throwIfDisposed(), pad$1(this, e, t);\n}, getGlobalTensorClass$1().prototype.pool = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), pool$3(this, e, t, n, r, a);\n}, getGlobalTensorClass$1().prototype.pow = function (e) {\n  return this.throwIfDisposed(), pow$5(this, e);\n}, getGlobalTensorClass$1().prototype.prelu = function (e) {\n  return this.throwIfDisposed(), prelu$6(this, e);\n}, getGlobalTensorClass$1().prototype.prod = function (e, t) {\n  return this.throwIfDisposed(), prod$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.reciprocal = function () {\n  return this.throwIfDisposed(), reciprocal$5(this);\n}, getGlobalTensorClass$1().prototype.relu = function () {\n  return this.throwIfDisposed(), relu$6(this);\n}, getGlobalTensorClass$1().prototype.relu6 = function () {\n  return this.throwIfDisposed(), relu6$5(this);\n}, getGlobalTensorClass$1().prototype.reshapeAs = function (e) {\n  return this.throwIfDisposed(), reshape$6(this, e.shape);\n}, getGlobalTensorClass$1().prototype.reshape = function (e) {\n  return this.throwIfDisposed(), reshape$6(this, e);\n}, getGlobalTensorClass$1().prototype.resizeBilinear = function (e, t, n) {\n  return this.throwIfDisposed(), resizeBilinear$5(this, e, t, n);\n}, getGlobalTensorClass$1().prototype.resizeNearestNeighbor = function (e, t, n) {\n  return this.throwIfDisposed(), resizeNearestNeighbor$5(this, e, t, n);\n}, getGlobalTensorClass$1().prototype.reverse = function (e) {\n  return this.throwIfDisposed(), reverse$5(this, e);\n}, getGlobalTensorClass$1().prototype.rfft = function () {\n  return this.throwIfDisposed(), rfft$1(this);\n}, getGlobalTensorClass$1().prototype.round = function () {\n  return this.throwIfDisposed(), round$6(this);\n}, getGlobalTensorClass$1().prototype.rsqrt = function () {\n  return this.throwIfDisposed(), rsqrt$5(this);\n}, getGlobalTensorClass$1().prototype.selu = function () {\n  return this.throwIfDisposed(), selu$5(this);\n}, getGlobalTensorClass$1().prototype.separableConv2d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), separableConv2d$2(this, e, t, n, r, a, s);\n}, getGlobalTensorClass$1().prototype.sigmoid = function () {\n  return this.throwIfDisposed(), sigmoid$5(this);\n}, getGlobalTensorClass$1().prototype.sign = function () {\n  return this.throwIfDisposed(), sign$5(this);\n}, getGlobalTensorClass$1().prototype.sin = function () {\n  return this.throwIfDisposed(), sin$5(this);\n}, getGlobalTensorClass$1().prototype.sinh = function () {\n  return this.throwIfDisposed(), sinh$5(this);\n}, getGlobalTensorClass$1().prototype.slice = function (e, t) {\n  return this.throwIfDisposed(), slice$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.softmax = function (e) {\n  return this.throwIfDisposed(), softmax$6(this, e);\n}, getGlobalTensorClass$1().prototype.softplus = function () {\n  return this.throwIfDisposed(), softplus$5(this);\n}, getGlobalTensorClass$1().prototype.spaceToBatchND = function (e, t) {\n  return this.throwIfDisposed(), spaceToBatchND$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.split = function (e, t) {\n  return this.throwIfDisposed(), split$4(this, e, t);\n}, getGlobalTensorClass$1().prototype.sqrt = function () {\n  return this.throwIfDisposed(), sqrt$5(this);\n}, getGlobalTensorClass$1().prototype.square = function () {\n  return this.throwIfDisposed(), square$5(this);\n}, getGlobalTensorClass$1().prototype.squaredDifference = function (e) {\n  return this.throwIfDisposed(), squaredDifference$5(this, e);\n}, getGlobalTensorClass$1().prototype.squeeze = function (e) {\n  return this.throwIfDisposed(), squeeze$1(this, e);\n}, getGlobalTensorClass$1().prototype.stack = function (e, t) {\n  this.throwIfDisposed();\n  var n = e instanceof Tensor$1 ? [this, e] : [this, ...e];\n  return stack$1(n, t);\n}, getGlobalTensorClass$1().prototype.step = function (e) {\n  return this.throwIfDisposed(), step$5(this, e);\n}, getGlobalTensorClass$1().prototype.stridedSlice = function (e, t, n, r, a, s, o, i) {\n  return this.throwIfDisposed(), stridedSlice$5(this, e, t, n, r, a, s, o, i);\n}, getGlobalTensorClass$1().prototype.sub = function (e) {\n  return this.throwIfDisposed(), sub$5(this, e);\n}, getGlobalTensorClass$1().prototype.sum = function (e, t) {\n  return this.throwIfDisposed(), sum$6(this, e, t);\n}, getGlobalTensorClass$1().prototype.tan = function () {\n  return this.throwIfDisposed(), tan$5(this);\n}, getGlobalTensorClass$1().prototype.tanh = function () {\n  return this.throwIfDisposed(), tanh$6(this);\n}, getGlobalTensorClass$1().prototype.tile = function (e) {\n  return this.throwIfDisposed(), tile$7(this, e);\n}, getGlobalTensorClass$1().prototype.toBool = function () {\n  return this.throwIfDisposed(), cast$7(this, \"bool\");\n}, getGlobalTensorClass$1().prototype.toFloat = function () {\n  return this.throwIfDisposed(), cast$7(this, \"float32\");\n}, getGlobalTensorClass$1().prototype.toInt = function () {\n  return this.throwIfDisposed(), cast$7(this, \"int32\");\n}, getGlobalTensorClass$1().prototype.topk = function (e, t) {\n  return this.throwIfDisposed(), topk$1(this, e, t);\n}, getGlobalTensorClass$1().prototype.transpose = function (e) {\n  return this.throwIfDisposed(), transpose$5(this, e);\n}, getGlobalTensorClass$1().prototype.unique = function (e) {\n  return this.throwIfDisposed(), unique$7(this, e);\n}, getGlobalTensorClass$1().prototype.unsortedSegmentSum = function (e, t) {\n  return this.throwIfDisposed(), unsortedSegmentSum$5(this, e, t);\n}, getGlobalTensorClass$1().prototype.unstack = function (e) {\n  return this.throwIfDisposed(), unstack$1(this, e);\n}, getGlobalTensorClass$1().prototype.where = function (e, t) {\n  return this.throwIfDisposed(), where$1(e, this, t);\n}, getGlobalTensorClass$1().prototype.zerosLike = function () {\n  return this.throwIfDisposed(), zerosLike$5(this);\n};\n\nclass AttributeError$1 extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, AttributeError$1.prototype);\n  }\n\n}\n\nclass RuntimeError$1 extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, RuntimeError$1.prototype);\n  }\n\n}\n\nclass ValueError$1 extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, ValueError$1.prototype);\n  }\n\n}\n\nclass NotImplementedError$1 extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, NotImplementedError$1.prototype);\n  }\n\n}\n\nclass AssertionError$1 extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, AssertionError$1.prototype);\n  }\n\n}\n\nfunction pyListRepeat$1(e, t) {\n  if (Array.isArray(e)) {\n    var n = [];\n\n    for (var r = 0; r < t; r++) {\n      n = n.concat(e);\n    }\n\n    return n;\n  }\n\n  {\n    var _n31 = new Array(t);\n\n    return _n31.fill(e), _n31;\n  }\n}\n\nfunction assert$5(e, t) {\n  if (!e) throw new AssertionError$1(t);\n}\n\nfunction count$1(e, t) {\n  var n = 0;\n\n  for (var r of e) {\n    r === t && n++;\n  }\n\n  return n;\n}\n\nfunction singletonOrArray$1(e) {\n  return 1 === e.length ? e[0] : e;\n}\n\nfunction toList$1(e) {\n  return Array.isArray(e) ? e : [e];\n}\n\nfunction toSnakeCase$1(e) {\n  var t = e.replace(/(.)([A-Z][a-z0-9]+)/g, \"$1_$2\").replace(/([a-z])([A-Z])/g, \"$1_$2\").toLowerCase();\n  return \"_\" !== t[0] ? t : \"private\" + t;\n}\n\nfunction toCamelCase$1(e) {\n  return e.length <= 1 || -1 === e.indexOf(\"_\") ? e : e.replace(/[_]+(\\w|$)/g, (e, t) => t.toUpperCase());\n}\n\nvar _GLOBAL_CUSTOM_OBJECTS$1 = {};\n\nfunction serializeKerasObject$1(e) {\n  if (null == e) return null;\n  var t = {};\n  return t.className = e.getClassName(), t.config = e.getConfig(), t;\n}\n\nfunction convertNDArrayScalarsInConfig$1(e) {\n  if (null != e && \"object\" == typeof e) if (Array.isArray(e)) e.forEach(e => convertNDArrayScalarsInConfig$1(e));else {\n    var t = Object.keys(e);\n\n    for (var n of t) {\n      var _t76 = e[n];\n      null != _t76 && \"object\" == typeof _t76 && (Array.isArray(_t76) || \"ndarray\" !== _t76.type || \"number\" != typeof _t76.value ? convertNDArrayScalarsInConfig$1(_t76) : e[n] = _t76.value);\n    }\n  }\n}\n\nfunction deserializeKerasObject$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"object\";\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n\n  if (\"string\" == typeof e) {\n    var _a24 = e;\n    var s;\n    if (_a24 in n) s = n[_a24];else if (_a24 in _GLOBAL_CUSTOM_OBJECTS$1) s = _GLOBAL_CUSTOM_OBJECTS$1[_a24];else if (s = t[_a24], null == s) throw new ValueError$1(\"Unknown \".concat(r, \": \").concat(e, \". This may be due to one of the following reasons:\\n1. The \").concat(r, \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \").concat(r, \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\"));\n    return s;\n  }\n\n  {\n    var _s11 = e;\n    if (null == _s11.className || null == _s11.config) throw new ValueError$1(\"\".concat(r, \": Improper config format: \").concat(JSON.stringify(_s11), \".\\n'className' and 'config' must set.\"));\n    var o = _s11.className;\n    var i, l;\n    if (o in n ? [i, l] = n[o] : o in _GLOBAL_CUSTOM_OBJECTS$1 ? [i, l] = _GLOBAL_CUSTOM_OBJECTS$1.className : o in t && ([i, l] = t[o]), null == i) throw new ValueError$1(\"Unknown \".concat(r, \": \").concat(o, \". This may be due to one of the following reasons:\\n1. The \").concat(r, \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \").concat(r, \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\"));\n\n    if (null != l) {\n      var _e86 = {};\n\n      for (var _t78 of Object.keys(_GLOBAL_CUSTOM_OBJECTS$1)) {\n        _e86[_t78] = _GLOBAL_CUSTOM_OBJECTS$1[_t78];\n      }\n\n      for (var _t79 of Object.keys(n)) {\n        _e86[_t79] = n[_t79];\n      }\n\n      _s11.config.customObjects = _e86;\n\n      var _t77 = Object.assign({}, _GLOBAL_CUSTOM_OBJECTS$1);\n\n      for (var _e87 of Object.keys(n)) {\n        _GLOBAL_CUSTOM_OBJECTS$1[_e87] = n[_e87];\n      }\n\n      convertNDArrayScalarsInConfig$1(_s11.config);\n\n      var _r23 = l(i, _s11.config, n, a);\n\n      return _GLOBAL_CUSTOM_OBJECTS$1 = Object.assign({}, _t77), _r23;\n    }\n\n    {\n      var _e88 = Object.assign({}, _GLOBAL_CUSTOM_OBJECTS$1);\n\n      for (var _e89 of Object.keys(n)) {\n        _GLOBAL_CUSTOM_OBJECTS$1[_e89] = n[_e89];\n      }\n\n      var _t80 = new i(_s11.config);\n\n      return _GLOBAL_CUSTOM_OBJECTS$1 = Object.assign({}, _e88), _t80;\n    }\n  }\n}\n\nfunction numberCompare$1(e, t) {\n  return e < t ? -1 : e > t ? 1 : 0;\n}\n\nfunction reverseNumberCompare$1(e, t) {\n  return -1 * numberCompare$1(e, t);\n}\n\nfunction unique$6(e) {\n  if (null == e) return e;\n  var t = [];\n\n  for (var n of e) {\n    -1 === t.indexOf(n) && t.push(n);\n  }\n\n  return t;\n}\n\nfunction isObjectEmpty$1(e) {\n  if (null == e) throw new ValueError$1(\"Invalid value in obj: \".concat(JSON.stringify(e)));\n\n  for (var t in e) {\n    if (e.hasOwnProperty(t)) return !1;\n  }\n\n  return !0;\n}\n\nfunction checkStringTypeUnionValue$1(e, t, n) {\n  if (null != n && e.indexOf(n) < 0) throw new ValueError$1(\"\".concat(n, \" is not a valid \").concat(t, \".  Valid values are \").concat(e, \" or null/undefined.\"));\n}\n\nfunction checkArrayTypeAndLength$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Infinity;\n  return assert$5(n >= 0), assert$5(r >= n), Array.isArray(e) && e.length >= n && e.length <= r && e.every(e => typeof e === t);\n}\n\nfunction assertPositiveInteger$1(e, t) {\n  Array.isArray(e) ? (assert$6(e.length > 0, () => \"\".concat(t, \" is unexpectedly an empty array.\")), e.forEach((e, n) => assertPositiveInteger$1(e, \"element \".concat(n + 1, \" of \").concat(t)))) : assert$6(Number.isInteger(e) && e > 0, () => \"Expected \".concat(t, \" to be a positive integer, but got \").concat(formatAsFriendlyString$1(e), \".\"));\n}\n\nfunction formatAsFriendlyString$1(e) {\n  return null === e ? \"null\" : Array.isArray(e) ? \"[\" + e.map(e => formatAsFriendlyString$1(e)).join(\",\") + \"]\" : \"string\" == typeof e ? \"\\\"\".concat(e, \"\\\"\") : \"\".concat(e);\n}\n\nfunction debounce$1(e, t) {\n  var n,\n      r = now$1();\n  return function () {\n    var s = now$1();\n    return s - r < t || (r = s, n = e(...arguments)), n;\n  };\n}\n\nfunction mapActivationToFusedKernel$1(e) {\n  return \"relu\" === e ? \"relu\" : \"linear\" === e ? \"linear\" : \"elu\" === e ? \"elu\" : null;\n}\n\nfunction calcL2Norms$1(e, t) {\n  return tidy$1(() => sqrt$5(sum$6(mul$1(e, e), t, !0)));\n}\n\nclass Constraint$1 extends Serializable$1 {\n  getConfig() {\n    return {};\n  }\n\n}\n\nclass MaxNorm$1 extends Constraint$1 {\n  constructor(e) {\n    super(), this.defaultMaxValue = 2, this.defaultAxis = 0, this.maxValue = null != e.maxValue ? e.maxValue : this.defaultMaxValue, this.axis = null != e.axis ? e.axis : this.defaultAxis;\n  }\n\n  apply(e) {\n    return tidy$1(() => {\n      var t = calcL2Norms$1(e, this.axis),\n          n = clipByValue$3(t, 0, this.maxValue);\n      return mul$1(e, div$3(n, add$5(epsilon$3(), t)));\n    });\n  }\n\n  getConfig() {\n    return {\n      maxValue: this.maxValue,\n      axis: this.axis\n    };\n  }\n\n}\n\nMaxNorm$1.className = \"MaxNorm\", registerClass$1(MaxNorm$1);\n\nclass UnitNorm$1 extends Constraint$1 {\n  constructor(e) {\n    super(), this.defaultAxis = 0, this.axis = null != e.axis ? e.axis : this.defaultAxis;\n  }\n\n  apply(e) {\n    return tidy$1(() => div$3(e, add$5(epsilon$3(), calcL2Norms$1(e, this.axis))));\n  }\n\n  getConfig() {\n    return {\n      axis: this.axis\n    };\n  }\n\n}\n\nUnitNorm$1.className = \"UnitNorm\", registerClass$1(UnitNorm$1);\n\nclass NonNeg$1 extends Constraint$1 {\n  apply(e) {\n    return relu$6(e);\n  }\n\n}\n\nNonNeg$1.className = \"NonNeg\", registerClass$1(NonNeg$1);\n\nclass MinMaxNorm$1 extends Constraint$1 {\n  constructor(e) {\n    super(), this.defaultMinValue = 0, this.defaultMaxValue = 1, this.defaultRate = 1, this.defaultAxis = 0, this.minValue = null != e.minValue ? e.minValue : this.defaultMinValue, this.maxValue = null != e.maxValue ? e.maxValue : this.defaultMaxValue, this.rate = null != e.rate ? e.rate : this.defaultRate, this.axis = null != e.axis ? e.axis : this.defaultAxis;\n  }\n\n  apply(e) {\n    return tidy$1(() => {\n      var t = calcL2Norms$1(e, this.axis),\n          n = add$5(mul$1(this.rate, clipByValue$3(t, this.minValue, this.maxValue)), mul$1(1 - this.rate, t));\n      return mul$1(e, div$3(n, add$5(epsilon$3(), t)));\n    });\n  }\n\n  getConfig() {\n    return {\n      minValue: this.minValue,\n      maxValue: this.maxValue,\n      rate: this.rate,\n      axis: this.axis\n    };\n  }\n\n}\n\nMinMaxNorm$1.className = \"MinMaxNorm\", registerClass$1(MinMaxNorm$1);\nvar CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP$1 = {\n  maxNorm: \"MaxNorm\",\n  minMaxNorm: \"MinMaxNorm\",\n  nonNeg: \"NonNeg\",\n  unitNorm: \"UnitNorm\"\n};\n\nfunction serializeConstraint$1(e) {\n  return serializeKerasObject$1(e);\n}\n\nfunction deserializeConstraint$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject$1(e, SerializationMap$1.getMap().classNameMap, t, \"constraint\");\n}\n\nfunction getConstraint$1(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeConstraint$1({\n    className: e in CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP$1 ? CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP$1[e] : e,\n    config: {}\n  }) : e instanceof Constraint$1 ? e : deserializeConstraint$1(e);\n}\n\nvar VALID_DATA_FORMAT_VALUES$1 = [\"channelsFirst\", \"channelsLast\"],\n    VALID_INTERPOLATION_FORMAT_VALUES$1 = [\"nearest\", \"bilinear\"],\n    VALID_PADDING_MODE_VALUES$1 = [\"valid\", \"same\", \"causal\"],\n    VALID_POOL_MODE_VALUES$1 = [\"max\", \"avg\"],\n    VALID_BIDIRECTIONAL_MERGE_MODES$1 = [\"sum\", \"mul\", \"concat\", \"ave\"],\n    nameMap$1 = new Map();\n\nfunction checkDataFormat$1(e) {\n  checkStringTypeUnionValue$1(VALID_DATA_FORMAT_VALUES$1, \"DataFormat\", e);\n}\n\nfunction checkInterpolationFormat$1(e) {\n  checkStringTypeUnionValue$1(VALID_INTERPOLATION_FORMAT_VALUES$1, \"InterpolationFormat\", e);\n}\n\nfunction checkPaddingMode$1(e) {\n  checkStringTypeUnionValue$1(VALID_PADDING_MODE_VALUES$1, \"PaddingMode\", e);\n}\n\nfunction checkPoolMode$1(e) {\n  checkStringTypeUnionValue$1(VALID_POOL_MODE_VALUES$1, \"PoolMode\", e);\n}\n\nvar _nameScopeStack$1 = [],\n    _nameScopeDivider$1 = \"/\";\n\nfunction nameScope$1(e, t) {\n  _nameScopeStack$1.push(e);\n\n  try {\n    var _e90 = t();\n\n    return _nameScopeStack$1.pop(), _e90;\n  } catch (e) {\n    throw _nameScopeStack$1.pop(), e;\n  }\n}\n\nfunction currentNameScopePrefix$1() {\n  return 0 === _nameScopeStack$1.length ? \"\" : _nameScopeStack$1.join(_nameScopeDivider$1) + _nameScopeDivider$1;\n}\n\nfunction getScopedTensorName$1(e) {\n  if (!isValidTensorName$1(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");\n  return currentNameScopePrefix$1() + e;\n}\n\nfunction getUniqueTensorName$1(e) {\n  if (!isValidTensorName$1(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");\n  nameMap$1.has(e) || nameMap$1.set(e, 0);\n  var t = nameMap$1.get(e);\n\n  if (nameMap$1.set(e, nameMap$1.get(e) + 1), t > 0) {\n    var n = \"\".concat(e, \"_\").concat(t);\n    return nameMap$1.set(n, 1), n;\n  }\n\n  return e;\n}\n\nvar tensorNameRegex$1 = new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\\._\\/]*$/);\n\nfunction isValidTensorName$1(e) {\n  return !!e.match(tensorNameRegex$1);\n}\n\nfunction isInteger$1(e) {\n  return e === parseInt(e.toString(), 10);\n}\n\nfunction arrayProd$1(e, t, n) {\n  null == t && (t = 0), null == n && (n = e.length);\n  var r = 1;\n\n  for (var a = t; a < n; ++a) {\n    r *= e[a];\n  }\n\n  return r;\n}\n\nfunction min$6(e) {\n  if (0 === e.length) return Number.NaN;\n  var t = Number.POSITIVE_INFINITY;\n\n  for (var n = 0; n < e.length; n++) {\n    var r = e[n];\n    r < t && (t = r);\n  }\n\n  return t;\n}\n\nfunction max$6(e) {\n  if (0 === e.length) return Number.NaN;\n  var t = Number.NEGATIVE_INFINITY;\n\n  for (var n = 0; n < e.length; n++) {\n    var r = e[n];\n    r > t && (t = r);\n  }\n\n  return t;\n}\n\nfunction range$7(e, t) {\n  if (t < e) throw new ValueError$1(\"end (\".concat(t, \") < begin (\").concat(e, \") is forbidden.\"));\n  var n = [];\n\n  for (var r = e; r < t; ++r) {\n    n.push(r);\n  }\n\n  return n;\n}\n\nfunction cast$6(e, t) {\n  return cast$7(e, t);\n}\n\nfunction expandDims$6(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n = e.shape.slice();\n  return t < 0 && (t = n.length + t + 1), n.splice(t, 0, 1), reshape$6(e, n);\n}\n\nfunction repeat$2(e, t) {\n  return tidy$1(() => {\n    if (2 !== e.shape.length) throw new ValueError$1(\"repeat() expects a rank-2 tensor, but received a rank-\".concat(e.shape.length, \" tensor.\"));\n    return tile$6(expandDims$6(e, 1), [1, t, 1]);\n  });\n}\n\nfunction flatten$5(e) {\n  var t = [arrayProd$1(e.shape)];\n  return reshape$6(e, t);\n}\n\nfunction batchFlatten$1(e) {\n  if (e.rank <= 1) throw new ValueError$1(\"batchFlatten requires a minimum rank of 2. Got rank: \".concat(e.rank, \".\"));\n  var t = [e.shape[0], arrayProd$1(e.shape, 1)];\n  return reshape$6(e, t);\n}\n\nfunction sliceAlongFirstAxis$1(e, t, n) {\n  return tidy$1(() => {\n    switch (e.rank) {\n      case 1:\n        return slice1d$1(e, t, n);\n\n      case 2:\n        return slice2d$1(e, [t, 0], [n, e.shape[1]]);\n\n      case 3:\n        return slice3d$1(e, [t, 0, 0], [n, e.shape[1], e.shape[2]]);\n\n      case 4:\n        return slice4d$1(e, [t, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3]]);\n\n      case 5:\n        return slice$5(e, [t, 0, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3], e.shape[4]]);\n\n      case 6:\n        return slice$5(e, [t, 0, 0, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3], e.shape[4], e.shape[5]]);\n\n      default:\n        throw new ValueError$1(\"sliceAlongFirstAxis() received an unsupported tensor rank: \".concat(e.rank));\n    }\n  });\n}\n\nfunction sliceAlongLastAxis$1(e, t, n) {\n  return tidy$1(() => {\n    switch (e.rank) {\n      case 1:\n        return slice1d$1(e, t, n);\n\n      case 2:\n        return slice2d$1(e, [0, t], [e.shape[0], n]);\n\n      case 3:\n        return slice3d$1(e, [0, 0, t], [e.shape[0], e.shape[1], n]);\n\n      case 4:\n        return slice4d$1(e, [0, 0, 0, t], [e.shape[0], e.shape[1], e.shape[2], n]);\n\n      default:\n        throw new ValueError$1(\"sliceAlongLastAxis() received an unsupported tensor rank: \".concat(e.rank));\n    }\n  });\n}\n\nfunction sliceAlongAxis$1(e, t, n, r) {\n  return tidy$1(() => {\n    switch (e.rank) {\n      case 1:\n        return slice1d$1(e, t, n);\n\n      case 2:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis$1(e, t, n);\n\n          case 2:\n            return sliceAlongLastAxis$1(e, t, n);\n\n          default:\n            throw new ValueError$1(\"The axis is not within the rank of the tensor \".concat(r));\n        }\n\n      case 3:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis$1(e, t, n);\n\n          case 2:\n            return slice3d$1(e, [0, t, 0], [e.shape[0], n, e.shape[2]]);\n\n          case 3:\n            return sliceAlongLastAxis$1(e, t, n);\n\n          default:\n            throw new ValueError$1(\"The axis is not within the rank of the tensor \".concat(r));\n        }\n\n      case 4:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis$1(e, t, n);\n\n          case 2:\n            return slice4d$1(e, [0, t, 0, 0], [e.shape[0], n, e.shape[2], e.shape[3]]);\n\n          case 3:\n            return slice4d$1(e, [0, 0, t, 0], [e.shape[0], e.shape[1], n, e.shape[3]]);\n\n          case 4:\n            return sliceAlongLastAxis$1(e, t, n);\n\n          default:\n            throw new ValueError$1(\"The axis is not within the rank of the tensor \".concat(r));\n        }\n\n      default:\n        throw new ValueError$1(\"sliceAlongLastAxis() received an unsupported tensor rank: \".concat(e.rank));\n    }\n  });\n}\n\nfunction concatenate$2(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n;\n  return t < 0 && (n = e[0].rank, t = 0 !== n ? n : 0), t === e[0].rank && (t = -1), concat$5(e, t);\n}\n\nfunction concatAlongFirstAxis$1(e, t) {\n  switch (e.rank) {\n    case 1:\n      return concat1d$1([e, t]);\n\n    case 2:\n      return concat2d$1([e, t], 0);\n\n    case 3:\n      return concat3d$1([e, t], 0);\n\n    case 4:\n      return concat4d$1([e, t], 0);\n\n    default:\n      throw new ValueError$1(\"concatAlongFirstAxis() received an unsupported tensor rank: \".concat(e.rank));\n  }\n}\n\nfunction tile$6(e, t) {\n  if (Array.isArray(t) || (t = [t]), e.rank !== t.length) throw new ValueError$1(\"The length of input n (\".concat(t.length, \") does not match the number of dimensions in input x (\").concat(e.rank, \")\"));\n  return tile$7(e, t);\n}\n\nfunction randomNormal$3(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  return randomNormal$4(e, t, n, r, a);\n}\n\nfunction dot$3(e, t, n, r) {\n  if (e.rank < 2 || t.rank < 2) throw new NotImplementedError$1(\"dot requires both inputs to be rank >= 2 but got x shape = \".concat(e.shape, \" and y shape = \").concat(t.shape));\n  if (t.rank >= 3 && e.shape.slice(-1)[0] !== t.shape.slice(-2)[0]) throw new NotImplementedError$1(\"If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = \".concat(e.shape, \" and  y shape = \").concat(t.shape));\n  if (2 === e.rank && 2 === t.rank) return matMul$2({\n    a: e,\n    b: t,\n    transposeA: !1,\n    transposeB: !1,\n    bias: r ? reshapeBias$1(e.rank, r, imageDataFormat$1()) : null,\n    activation: n\n  });\n  {\n    var a = e.shape.slice(),\n        s = a.pop();\n    e = reshape$6(e, [-1, s]);\n    var o = t.shape.slice(),\n        i = o.pop(),\n        l = o.pop(),\n        u = [...o, i],\n        c = Array.from({\n      length: t.rank\n    }, (e, n) => 0 === n ? t.rank - 2 : n <= t.rank - 2 ? n - 1 : n);\n    t = reshape$6(transpose$5(t, c), [l, -1]);\n    var _p5 = [...a, ...u];\n    return reshape$6(matMul$2({\n      a: e,\n      b: t,\n      transposeA: !1,\n      transposeB: !1,\n      bias: r ? reshapeBias$1(e.rank, r, imageDataFormat$1()) : null,\n      activation: n\n    }), _p5);\n  }\n}\n\nfunction gather$2(e, t, n) {\n  return tidy$1(() => (t = Array.isArray(t) ? tensor1d$1(t, \"int32\") : cast$7(t, \"int32\"), gather$3(e, t, n)));\n}\n\nfunction square$4(e) {\n  return mul$1(e, e);\n}\n\nfunction reshapeBias$1(e, t, n) {\n  var r = t.shape;\n  if (1 !== t.rank && t.rank !== e) throw new ValueError$1(\"Unexpected bias dimensions: \".concat(t.rank, \"; expected it to be 1 or \").concat(e));\n\n  if (5 === e) {\n    if (\"channelsFirst\" === n) return reshape$6(t, 1 === r.length ? [1, r[0], 1, 1, 1] : [1, r[3], r[0], r[1], r[2]]);\n    if (\"channelsLast\" === n) return reshape$6(t, 1 === r.length ? [1, 1, 1, 1, r[0]] : [1].concat(r));\n  } else if (4 === e) {\n    if (\"channelsFirst\" === n) return reshape$6(t, 1 === r.length ? [1, r[0], 1, 1] : [1, r[2], r[0], r[1]]);\n    if (\"channelsLast\" === n) return reshape$6(t, 1 === r.length ? [1, 1, 1, r[0]] : [1].concat(r));\n  } else if (3 === e) {\n    if (\"channelsFirst\" === n) return reshape$6(t, 1 === r.length ? [1, r[0], 1] : [1, r[1], r[0]]);\n    if (\"channelsLast\" === n) return reshape$6(t, 1 === r.length ? [1, 1, r[0]] : [1].concat(r));\n  } else if (e < 3) return t;\n\n  throw new ValueError$1(\"Unsupported input rank by biasAdd: \".concat(t.rank));\n}\n\nfunction biasAdd$1(e, t, n) {\n  return tidy$1(() => (null == n && (n = imageDataFormat$1()), checkDataFormat$1(n), add$5(e, reshapeBias$1(e.rank, t, n))));\n}\n\nfunction elu$7(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  if (1 !== t) throw new NotImplementedError$1(\"Support for alpha values other than 1 (\".concat(t, \") is not implemented yet.\"));\n  return elu$8(e);\n}\n\nfunction softsign$1(e) {\n  return tidy$1(() => div$3(e, add$5(abs$5(e), 1)));\n}\n\nfunction dropout$4(e, t, n, r) {\n  return tidy$1(() => dropout$5(e, t, n, r));\n}\n\nfunction hardSigmoid$1(e) {\n  return tidy$1(() => {\n    var t = add$5(.5, mul$1(.2, e));\n    return clipByValue$3(t, 0, 1);\n  });\n}\n\nfunction inTrainPhase$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return n ? e() : t();\n}\n\nvar VALID_FAN_MODE_VALUES$1 = [\"fanIn\", \"fanOut\", \"fanAvg\"],\n    VALID_DISTRIBUTION_VALUES$1 = [\"normal\", \"uniform\", \"truncatedNormal\"];\n\nfunction checkFanMode$1(e) {\n  checkStringTypeUnionValue$1(VALID_FAN_MODE_VALUES$1, \"FanMode\", e);\n}\n\nfunction checkDistribution$1(e) {\n  checkStringTypeUnionValue$1(VALID_DISTRIBUTION_VALUES$1, \"Distribution\", e);\n}\n\nclass Initializer$1 extends Serializable$1 {\n  fromConfigUsesCustomObjects() {\n    return !1;\n  }\n\n  getConfig() {\n    return {};\n  }\n\n}\n\nclass Zeros$1 extends Initializer$1 {\n  apply(e, t) {\n    return zeros$4(e, t);\n  }\n\n}\n\nZeros$1.className = \"Zeros\", registerClass$1(Zeros$1);\n\nclass Ones$1 extends Initializer$1 {\n  apply(e, t) {\n    return ones$3(e, t);\n  }\n\n}\n\nOnes$1.className = \"Ones\", registerClass$1(Ones$1);\n\nclass Constant$1 extends Initializer$1 {\n  constructor(e) {\n    if (super(), \"object\" != typeof e) throw new ValueError$1(\"Expected argument of type ConstantConfig but got \".concat(e));\n    if (void 0 === e.value) throw new ValueError$1(\"config must have value set but got \".concat(e));\n    this.value = e.value;\n  }\n\n  apply(e, t) {\n    return tidy$1(() => mul$1(scalar$1(this.value), ones$3(e, t)));\n  }\n\n  getConfig() {\n    return {\n      value: this.value\n    };\n  }\n\n}\n\nConstant$1.className = \"Constant\", registerClass$1(Constant$1);\n\nclass RandomUniform$1 extends Initializer$1 {\n  constructor(e) {\n    super(), this.DEFAULT_MINVAL = -.05, this.DEFAULT_MAXVAL = .05, this.minval = e.minval || this.DEFAULT_MINVAL, this.maxval = e.maxval || this.DEFAULT_MAXVAL, this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    return randomUniform$2(e, this.minval, this.maxval, t);\n  }\n\n  getConfig() {\n    return {\n      minval: this.minval,\n      maxval: this.maxval,\n      seed: this.seed\n    };\n  }\n\n}\n\nRandomUniform$1.className = \"RandomUniform\", registerClass$1(RandomUniform$1);\n\nclass RandomNormal$1 extends Initializer$1 {\n  constructor(e) {\n    super(), this.DEFAULT_MEAN = 0, this.DEFAULT_STDDEV = .05, this.mean = e.mean || this.DEFAULT_MEAN, this.stddev = e.stddev || this.DEFAULT_STDDEV, this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError$1(\"randomNormal does not support dType \".concat(t, \".\"));\n    return randomNormal$3(e, this.mean, this.stddev, t, this.seed);\n  }\n\n  getConfig() {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }\n\n}\n\nRandomNormal$1.className = \"RandomNormal\", registerClass$1(RandomNormal$1);\n\nclass TruncatedNormal$1 extends Initializer$1 {\n  constructor(e) {\n    super(), this.DEFAULT_MEAN = 0, this.DEFAULT_STDDEV = .05, this.mean = e.mean || this.DEFAULT_MEAN, this.stddev = e.stddev || this.DEFAULT_STDDEV, this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError$1(\"truncatedNormal does not support dType \".concat(t, \".\"));\n    return truncatedNormal$2(e, this.mean, this.stddev, t, this.seed);\n  }\n\n  getConfig() {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }\n\n}\n\nTruncatedNormal$1.className = \"TruncatedNormal\", registerClass$1(TruncatedNormal$1);\n\nclass Identity$2 extends Initializer$1 {\n  constructor(e) {\n    super(), this.gain = null != e.gain ? e.gain : 1;\n  }\n\n  apply(e, t) {\n    return tidy$1(() => {\n      if (2 !== e.length || e[0] !== e[1]) throw new ValueError$1(\"Identity matrix initializer can only be used for 2D square matrices.\");\n      return mul$1(this.gain, eye$1(e[0]));\n    });\n  }\n\n  getConfig() {\n    return {\n      gain: this.gain\n    };\n  }\n\n}\n\nfunction computeFans$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"channelsLast\";\n  var n, r;\n  if (checkDataFormat$1(t), 2 === e.length) n = e[0], r = e[1];else if (-1 !== [3, 4, 5].indexOf(e.length)) {\n    if (\"channelsFirst\" === t) {\n      var _t81 = arrayProd$1(e, 2);\n\n      n = e[1] * _t81, r = e[0] * _t81;\n    } else if (\"channelsLast\" === t) {\n      var _t82 = arrayProd$1(e, 0, e.length - 2);\n\n      n = e[e.length - 2] * _t82, r = e[e.length - 1] * _t82;\n    }\n  } else {\n    var _t83 = arrayProd$1(e);\n\n    n = Math.sqrt(_t83), r = Math.sqrt(_t83);\n  }\n  return [n, r];\n}\n\nIdentity$2.className = \"Identity\", registerClass$1(Identity$2);\n\nclass VarianceScaling$1 extends Initializer$1 {\n  constructor(e) {\n    if (super(), e.scale < 0) throw new ValueError$1(\"scale must be a positive float. Got: \".concat(e.scale));\n    this.scale = null == e.scale ? 1 : e.scale, this.mode = null == e.mode ? \"fanIn\" : e.mode, checkFanMode$1(this.mode), this.distribution = null == e.distribution ? \"normal\" : e.distribution, checkDistribution$1(this.distribution), this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    var n = computeFans$1(e),\n        r = n[0],\n        a = n[1];\n    var s = this.scale;\n\n    if (s /= \"fanIn\" === this.mode ? Math.max(1, r) : \"fanOut\" === this.mode ? Math.max(1, a) : Math.max(1, (r + a) / 2), \"normal\" === this.distribution) {\n      var _n32 = Math.sqrt(s);\n\n      if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError$1(\"\".concat(this.getClassName(), \" does not support dType \").concat(t, \".\"));\n      return truncatedNormal$2(e, 0, _n32, t, this.seed);\n    }\n\n    {\n      var _n33 = Math.sqrt(3 * s);\n\n      return randomUniform$2(e, -_n33, _n33, t);\n    }\n  }\n\n  getConfig() {\n    return {\n      scale: this.scale,\n      mode: this.mode,\n      distribution: this.distribution,\n      seed: this.seed\n    };\n  }\n\n}\n\nVarianceScaling$1.className = \"VarianceScaling\", registerClass$1(VarianceScaling$1);\n\nclass GlorotUniform$1 extends VarianceScaling$1 {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanAvg\",\n      distribution: \"uniform\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling$1.className;\n  }\n\n}\n\nGlorotUniform$1.className = \"GlorotUniform\", registerClass$1(GlorotUniform$1);\n\nclass GlorotNormal$1 extends VarianceScaling$1 {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanAvg\",\n      distribution: \"normal\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling$1.className;\n  }\n\n}\n\nGlorotNormal$1.className = \"GlorotNormal\", registerClass$1(GlorotNormal$1);\n\nclass HeNormal$1 extends VarianceScaling$1 {\n  constructor(e) {\n    super({\n      scale: 2,\n      mode: \"fanIn\",\n      distribution: \"normal\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling$1.className;\n  }\n\n}\n\nHeNormal$1.className = \"HeNormal\", registerClass$1(HeNormal$1);\n\nclass HeUniform$1 extends VarianceScaling$1 {\n  constructor(e) {\n    super({\n      scale: 2,\n      mode: \"fanIn\",\n      distribution: \"uniform\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling$1.className;\n  }\n\n}\n\nHeUniform$1.className = \"HeUniform\", registerClass$1(HeUniform$1);\n\nclass LeCunNormal$1 extends VarianceScaling$1 {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanIn\",\n      distribution: \"normal\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling$1.className;\n  }\n\n}\n\nLeCunNormal$1.className = \"LeCunNormal\", registerClass$1(LeCunNormal$1);\n\nclass LeCunUniform$1 extends VarianceScaling$1 {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanIn\",\n      distribution: \"uniform\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling$1.className;\n  }\n\n}\n\nLeCunUniform$1.className = \"LeCunNormal\", registerClass$1(LeCunUniform$1);\n\nclass Orthogonal$1 extends Initializer$1 {\n  constructor(e) {\n    if (super(), this.DEFAULT_GAIN = 1, this.gain = null == e.gain ? this.DEFAULT_GAIN : e.gain, this.seed = e.seed, null != this.seed) throw new NotImplementedError$1(\"Random seed is not implemented for Orthogonal Initializer yet.\");\n  }\n\n  apply(e, t) {\n    return tidy$1(() => {\n      if (e.length < 2) throw new NotImplementedError$1(\"Shape must be at least 2D.\");\n      e[0] * e[1] > 2e3 && console.warn(\"Orthogonal initializer is being called on a matrix with more than 2000 (\".concat(e[0] * e[1], \") elements: Slowness may result.\"));\n      var t = randomNormal$3(e[0] > e[1] ? [e[1], e[0]] : e, 0, 1, \"float32\");\n      var n = linalg$1.gramSchmidt(t);\n      return e[0] > e[1] && (n = transpose$5(n)), mul$1(this.gain, n);\n    });\n  }\n\n  getConfig() {\n    return {\n      gain: this.gain,\n      seed: this.seed\n    };\n  }\n\n}\n\nOrthogonal$1.className = \"Orthogonal\", registerClass$1(Orthogonal$1);\nvar INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP$1 = {\n  constant: \"Constant\",\n  glorotNormal: \"GlorotNormal\",\n  glorotUniform: \"GlorotUniform\",\n  heNormal: \"HeNormal\",\n  heUniform: \"HeUniform\",\n  identity: \"Identity\",\n  leCunNormal: \"LeCunNormal\",\n  leCunUniform: \"LeCunUniform\",\n  ones: \"Ones\",\n  orthogonal: \"Orthogonal\",\n  randomNormal: \"RandomNormal\",\n  randomUniform: \"RandomUniform\",\n  truncatedNormal: \"TruncatedNormal\",\n  varianceScaling: \"VarianceScaling\",\n  zeros: \"Zeros\"\n};\n\nfunction deserializeInitializer$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject$1(e, SerializationMap$1.getMap().classNameMap, t, \"initializer\");\n}\n\nfunction serializeInitializer$1(e) {\n  return serializeKerasObject$1(e);\n}\n\nfunction getInitializer$1(e) {\n  if (\"string\" == typeof e) {\n    var t = e in INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP$1 ? INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP$1[e] : e;\n    if (\"GlorotNormal\" === t) return new GlorotNormal$1();\n    if (\"GlorotUniform\" === t) return new GlorotUniform$1();\n    if (\"HeNormal\" === t) return new HeNormal$1();\n    if (\"HeUniform\" === t) return new HeUniform$1();\n    if (\"LeCunNormal\" === t) return new LeCunNormal$1();\n    if (\"LeCunUniform\" === t) return new LeCunUniform$1();\n    {\n      var _e91 = {};\n      return _e91.className = t, _e91.config = {}, deserializeInitializer$1(_e91);\n    }\n  }\n\n  return e instanceof Initializer$1 ? e : deserializeInitializer$1(e);\n}\n\nvar _nextUniqueTensorId$1 = 0;\n\nfunction getNextUniqueTensorId$1() {\n  return _nextUniqueTensorId$1++;\n}\n\nvar _uidPrefixes$1 = {};\n\nfunction getUid$1() {\n  var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : \"\";\n  return e in _uidPrefixes$1 || (_uidPrefixes$1[e] = 0), _uidPrefixes$1[e] += 1, e + _uidPrefixes$1[e].toString();\n}\n\nfunction isArrayOfShapes$1(e) {\n  return Array.isArray(e) && Array.isArray(e[0]);\n}\n\nfunction normalizeShapeList$1(e) {\n  return 0 === e.length ? [] : Array.isArray(e[0]) ? e : [e];\n}\n\nfunction getExactlyOneTensor$1(e) {\n  var t;\n\n  if (Array.isArray(e)) {\n    if (1 !== e.length) throw new ValueError$1(\"Expected Tensor length to be 1; got \".concat(e.length));\n    t = e[0];\n  } else t = e;\n\n  return t;\n}\n\nfunction getExactlyOneShape$1(e) {\n  if (Array.isArray(e) && Array.isArray(e[0])) {\n    if (1 === e.length) return (e = e)[0];\n    throw new ValueError$1(\"Expected exactly 1 Shape; got \".concat(e.length));\n  }\n\n  return e;\n}\n\nfunction countParamsInWeights$1(e) {\n  var t = 0;\n\n  for (var n of e) {\n    t += 0 === n.shape.length ? 1 : n.shape.reduce((e, t) => e * t);\n  }\n\n  return t;\n}\n\nvar DEFAULT_VARIABLE_NAME_PREFIX$1 = \"Variable\";\n\nclass LayerVariable$1 {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : DEFAULT_VARIABLE_NAME_PREFIX$1;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : null;\n    this.dtype = null == t ? \"float32\" : t, this.shape = e.shape, this.id = getNextUniqueTensorId$1(), this.originalName = getScopedTensorName$1(n = null == n ? DEFAULT_VARIABLE_NAME_PREFIX$1 : n), this.name = getUniqueTensorName$1(this.originalName), this.trainable_ = r, this.constraint = a, this.val = variable$1(e, this.trainable_, this.name, this.dtype);\n  }\n\n  read() {\n    return this.assertNotDisposed(), this.val;\n  }\n\n  write(e) {\n    return this.assertNotDisposed(), checkShapesMatch$1(this.val, e), this.val.id !== e.id && (this.val.assign(e), null != this.constraint && this.val.assign(this.constraint.apply(this.val))), this;\n  }\n\n  dispose() {\n    this.assertNotDisposed(), this.val.dispose();\n  }\n\n  assertNotDisposed() {\n    if (this.val.isDisposed) throw new Error(\"LayersVariable \".concat(this.name, \" is already disposed.\"));\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(e) {\n    this.trainable_ = e, this.val.trainable = e;\n  }\n\n}\n\nfunction checkShapesMatch$1(e, t) {\n  if (e.shape.toString() !== t.shape.toString()) throw new Error(\"Shape mismatch: \" + JSON.stringify(e.shape) + \" vs. \" + JSON.stringify(t.shape));\n}\n\nfunction batchGetValue$1(e) {\n  return e.map(e => e.read());\n}\n\nfunction batchSetValue$1(e) {\n  e.forEach(e => {\n    e[0].write(e[1]);\n  });\n}\n\nclass InputSpec$1 {\n  constructor(e) {\n    this.dtype = e.dtype, this.shape = e.shape, this.ndim = null != e.shape ? e.shape.length : e.ndim, this.maxNDim = e.maxNDim, this.minNDim = e.minNDim, this.axes = e.axes || {};\n  }\n\n}\n\nclass SymbolicTensor$1 {\n  constructor(e, t, n, r, a, s, o) {\n    this.dtype = e, this.shape = t, this.sourceLayer = n, this.inputs = r, this.callArgs = a, this.outputTensorIndex = o, this.id = getNextUniqueTensorId$1(), null != s && (this.originalName = getScopedTensorName$1(s), this.name = getUniqueTensorName$1(this.originalName)), this.rank = t.length;\n  }\n\n}\n\nvar _nextNodeID$1 = 0;\n\nclass Node$1 {\n  constructor(e, t) {\n    this.callArgs = t, this.id = _nextNodeID$1++, this.outboundLayer = e.outboundLayer, this.inboundLayers = e.inboundLayers, this.nodeIndices = e.nodeIndices, this.tensorIndices = e.tensorIndices, this.inputTensors = e.inputTensors, this.outputTensors = e.outputTensors, this.inputMasks = e.inputMasks, this.outputMasks = e.outputMasks, this.inputShapes = e.inputShapes, this.outputShapes = e.outputShapes;\n\n    for (var _t84 of e.inboundLayers) {\n      null != _t84 && _t84.outboundNodes.push(this);\n    }\n\n    e.outboundLayer.inboundNodes.push(this);\n  }\n\n  getConfig() {\n    var e = [];\n\n    for (var t of this.inboundLayers) {\n      e.push(null != t ? t.name : null);\n    }\n\n    return {\n      outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,\n      inboundLayers: e,\n      nodeIndices: this.nodeIndices,\n      tensorIndices: this.tensorIndices\n    };\n  }\n\n}\n\nvar _nextLayerID$1 = 0;\n\nclass Layer$1 extends Serializable$1 {\n  constructor() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    super(), this._callHook = null, this._addedWeightNames = [], this._stateful = !1, this.id = _nextLayerID$1++, this.activityRegularizer = null, this.inputSpec = null, this.supportsMasking = !1, this._trainableWeights = [], this._nonTrainableWeights = [], this._losses = [], this._updates = [], this._built = !1, this.inboundNodes = [], this.outboundNodes = [];\n    var t = e.name;\n\n    if (!t) {\n      var _e92 = this.getClassName();\n\n      t = toSnakeCase$1(_e92) + \"_\" + getUid$1(_e92);\n    }\n\n    if (this.name = t, this.trainable_ = null == e.trainable || e.trainable, null != e.inputShape || null != e.batchInputShape) {\n      var _t85;\n\n      if (null != e.batchInputShape) _t85 = e.batchInputShape;else if (null != e.inputShape) {\n        var _n34 = null;\n        null != e.batchSize && (_n34 = e.batchSize), _t85 = [_n34].concat(e.inputShape);\n      }\n      this.batchInputShape = _t85;\n      var n = e.dtype;\n      null == n && (n = e.inputDType), null == n && (n = \"float32\"), this.dtype = n;\n    }\n\n    this.initialWeights = null != e.weights ? e.weights : null, this._refCount = null, this.fastWeightInitDuringBuild = !1;\n  }\n\n  static nodeKey(e, t) {\n    return e.name + \"_ib-\" + t.toString();\n  }\n\n  getNodeAtIndex(e, t) {\n    if (0 === this.inboundNodes.length) throw new RuntimeError$1(\"The layer has never been called and thus has no defined \".concat(t, \".\"));\n    if (this.inboundNodes.length <= e) throw new ValueError$1(\"Asked to get \".concat(t, \" at node \").concat(e, \", but the layer has only \").concat(this.inboundNodes.length, \" inbound nodes.\"));\n    return this.inboundNodes[e];\n  }\n\n  getInputAt(e) {\n    return singletonOrArray$1(this.getNodeAtIndex(e, \"input\").inputTensors);\n  }\n\n  getOutputAt(e) {\n    return singletonOrArray$1(this.getNodeAtIndex(e, \"output\").outputTensors);\n  }\n\n  get input() {\n    if (this.inboundNodes.length > 1) throw new AttributeError$1(\"Layer \".concat(this.name, \" has multiple inbound nodes, hence the notion of \\\"layer input\\\" is ill-defined. Use `getInputAt(nodeIndex)` instead.\"));\n    if (0 === this.inboundNodes.length) throw new AttributeError$1(\"Layer \".concat(this.name, \" is not connected, no input to return.\"));\n    return singletonOrArray$1(this.getNodeAtIndex(0, \"input\").inputTensors);\n  }\n\n  get output() {\n    if (0 === this.inboundNodes.length) throw new AttributeError$1(\"Layer \".concat(this.name, \" has no inbound nodes.\"));\n    if (this.inboundNodes.length > 1) throw new AttributeError$1(\"Layer \".concat(this.name, \" has multiple inbound nodes, hence the notion of \\\"layer output\\\" is ill-defined. Use `getOutputAt(nodeIndex)` instead.\"));\n    return singletonOrArray$1(this.getNodeAtIndex(0, \"output\").outputTensors);\n  }\n\n  get losses() {\n    return this._losses;\n  }\n\n  calculateLosses() {\n    return this.losses.map(e => e());\n  }\n\n  get updates() {\n    return this._updates;\n  }\n\n  get built() {\n    return this._built;\n  }\n\n  set built(e) {\n    this._built = e;\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(e) {\n    this._trainableWeights.forEach(t => t.trainable = e), this.trainable_ = e;\n  }\n\n  get trainableWeights() {\n    return this.trainable_ ? this._trainableWeights.filter(e => e.trainable) : [];\n  }\n\n  set trainableWeights(e) {\n    this._trainableWeights = e;\n  }\n\n  get nonTrainableWeights() {\n    return this.trainable ? this._trainableWeights.filter(e => !e.trainable).concat(this._nonTrainableWeights) : this._trainableWeights.concat(this._nonTrainableWeights);\n  }\n\n  set nonTrainableWeights(e) {\n    this._nonTrainableWeights = e;\n  }\n\n  get weights() {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  get stateful() {\n    return this._stateful;\n  }\n\n  resetStates() {\n    if (!this.stateful) throw new Error(\"Cannot call the resetStates() method of a non-stateful Layer object.\");\n  }\n\n  assertInputCompatibility(e) {\n    if (e = toList$1(e), null == this.inputSpec || 0 === this.inputSpec.length) return;\n    var t = toList$1(this.inputSpec);\n    if (e.length !== t.length) throw new ValueError$1(\"Layer \".concat(this.name, \" expects \").concat(t.length, \" inputs, but it received \").concat(e.length, \" input tensors. Input received: \").concat(e));\n\n    for (var n = 0; n < e.length; n++) {\n      var r = e[n],\n          a = t[n];\n      if (null == a) continue;\n      var s = r.rank;\n      if (null != a.ndim && s !== a.ndim) throw new ValueError$1(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected ndim=\").concat(a.ndim, \", found ndim=\").concat(s));\n      if (null != a.maxNDim && s > a.maxNDim) throw new ValueError$1(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected max_ndim=\").concat(a.maxNDim, \", found ndim=\").concat(s));\n      if (null != a.minNDim && s < a.minNDim) throw new ValueError$1(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected min_ndim=\").concat(a.minNDim, \", found ndim=\").concat(s, \".\"));\n      if (null != a.dtype && r.dtype !== a.dtype) throw new ValueError$1(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \" : expected dtype=\").concat(a.dtype, \", found dtype=\").concat(r.dtype, \".\"));\n\n      if (a.axes) {\n        var _e93 = r.shape;\n\n        for (var _t86 in a.axes) {\n          var _r24 = Number(_t86),\n              _s12 = a.axes[_t86],\n              o = _r24 >= 0 ? _e93[_r24] : _e93[_e93.length + _r24];\n\n          if (null != _s12 && -1 === [_s12, null].indexOf(o)) throw new ValueError$1(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected axis \").concat(_r24, \" of input shape to have value \").concat(_s12, \" but got shape \").concat(_e93, \".\"));\n        }\n      }\n\n      if (null != a.shape) for (var _e94 = 0; _e94 < a.shape.length; ++_e94) {\n        var _t87 = a.shape[_e94],\n            _s13 = r.shape[_e94];\n        if (null != _t87 && null != _s13 && _t87 !== _s13) throw new ValueError$1(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected shape=\").concat(a.shape, \", found shape=\").concat(r.shape, \".\"));\n      }\n    }\n  }\n\n  call(e, t) {\n    return e;\n  }\n\n  invokeCallHook(e, t) {\n    null != this._callHook && this._callHook(e, t);\n  }\n\n  setCallHook(e) {\n    this._callHook = e;\n  }\n\n  clearCallHook() {\n    this._callHook = null;\n  }\n\n  apply(e, t) {\n    t = t || {}, this.assertNotDisposed();\n    var n = toList$1(e);\n    var r = !0;\n\n    for (var _e95 of n) {\n      if (!(_e95 instanceof SymbolicTensor$1)) {\n        r = !1;\n        break;\n      }\n    }\n\n    var a = !0;\n\n    for (var _e96 of n) {\n      if (_e96 instanceof SymbolicTensor$1) {\n        a = !1;\n        break;\n      }\n    }\n\n    if (r === a) throw new ValueError$1(\"Arguments to apply() must be all SymbolicTensors or all Tensors\");\n    return nameScope$1(this.name, () => {\n      if (!this.built) {\n        this.assertInputCompatibility(e);\n        var _t88 = [];\n\n        for (var _n35 of toList$1(e)) {\n          _t88.push(_n35.shape);\n        }\n\n        this.build(singletonOrArray$1(_t88)), this.built = !0, this.initialWeights && this.setWeights(this.initialWeights), null === this._refCount && a && (this._refCount = 1);\n      }\n\n      if (this.assertInputCompatibility(e), a) {\n        var _r25 = this.call(e, t);\n\n        var _a25 = toList$1(_r25),\n            s = [];\n\n        for (var _e97 of _a25) {\n          -1 !== n.indexOf(_e97) && (_e97 = _e97.clone()), s.push(_e97);\n        }\n\n        if (_r25 = singletonOrArray$1(s), null != this.activityRegularizer) throw new NotImplementedError$1(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");\n        return _r25;\n      }\n\n      {\n        var _n36 = collectInputShape$1(e),\n            _r26 = this.computeOutputShape(_n36);\n\n        var _a26;\n\n        var _s14 = guessOutputDType$1(e);\n\n        if (this.warnOnIncompatibleInputShape(Array.isArray(e) ? _n36[0] : _n36), _a26 = null != _r26 && _r26.length > 0 && Array.isArray(_r26[0]) ? _r26.map((n, r) => new SymbolicTensor$1(_s14, n, this, toList$1(e), t, this.name, r)) : new SymbolicTensor$1(_s14, _r26, this, toList$1(e), t, this.name), this.addInboundNode(e, _a26, null, null, _n36, _r26, t), this._refCount++, null != this.activityRegularizer) throw new NotImplementedError$1(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");\n        return _a26;\n      }\n    });\n  }\n\n  warnOnIncompatibleInputShape(e) {\n    if (null != this.batchInputShape) if (e.length !== this.batchInputShape.length) console.warn(\"The rank of the input tensor provided (shape: \".concat(JSON.stringify(e), \") does not match that of the batchInputShape (\").concat(JSON.stringify(this.batchInputShape), \") of the layer \").concat(this.name));else {\n      var t = !1;\n      this.batchInputShape.forEach((n, r) => {\n        null != n && null != e[r] && e[r] !== n && (t = !0);\n      }), t && console.warn(\"The shape of the input tensor (\".concat(JSON.stringify(e), \") does not match the expectation of layer \").concat(this.name, \": \").concat(JSON.stringify(this.batchInputShape)));\n    }\n  }\n\n  get outputShape() {\n    if (null == this.inboundNodes || 0 === this.inboundNodes.length) throw new AttributeError$1(\"The layer \".concat(this.name, \" has never been called and thus has no defined output shape.\"));\n    var e = [];\n\n    for (var t of this.inboundNodes) {\n      var n = JSON.stringify(t.outputShapes);\n      -1 === e.indexOf(n) && e.push(n);\n    }\n\n    if (1 === e.length) {\n      var _e98 = this.inboundNodes[0].outputShapes;\n      return Array.isArray(_e98) && Array.isArray(_e98[0]) && 1 === _e98.length ? _e98[0] : _e98;\n    }\n\n    throw new AttributeError$1(\"The layer \".concat(this.name, \" has multiple inbound nodes with different output shapes. Hence the notion of \\\"output shape\\\" is ill-defined for the layer.\"));\n  }\n\n  countParams() {\n    if (!this.built) throw new RuntimeError$1(\"You tried to call countParams() on \".concat(this.name, \", but the layer is not built yet. Build it first by calling build(batchInputShape).\"));\n    return countParamsInWeights$1(this.weights);\n  }\n\n  build(e) {\n    this.built = !0;\n  }\n\n  getWeights() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !1;\n    return batchGetValue$1(e ? this.trainableWeights : this.weights);\n  }\n\n  setWeights(e) {\n    tidy$1(() => {\n      var t = this.weights;\n      if (t.length !== e.length) throw new ValueError$1(\"You called setWeights(weights) on layer \\\"\".concat(this.name, \"\\\" with a weight list of length \").concat(e.length, \", but the layer was expecting \").concat(t.length, \" weights. Provided weights: \").concat(e, \"...\"));\n      if (0 === t.length) return;\n      var n = [],\n          r = batchGetValue$1(t);\n\n      for (var a = 0; a < r.length; ++a) {\n        var s = r[a],\n            o = t[a],\n            i = e[a];\n        if (!arraysEqual$1(s.shape, i.shape)) throw new ValueError$1(\"Layer weight shape \".concat(s.shape, \" not compatible with provided weight shape \").concat(i.shape));\n        n.push([o, i]);\n      }\n\n      batchSetValue$1(n);\n    });\n  }\n\n  addWeight(e, t, n, r, a, s, o) {\n    if (-1 !== this._addedWeightNames.indexOf(e)) throw new ValueError$1(\"Duplicate weight name \".concat(e, \" for layer \").concat(this.name));\n    this._addedWeightNames.push(e), null == n && (n = \"float32\"), this.fastWeightInitDuringBuild && (r = getInitializer$1(\"zeros\"));\n    var i = r.apply(t, n),\n        l = new LayerVariable$1(i, n, e, s, o);\n    return i.dispose(), null != a && this.addLoss(() => a.apply(l.read())), null == s && (s = !0), s ? this._trainableWeights.push(l) : this._nonTrainableWeights.push(l), l;\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    this.fastWeightInitDuringBuild = e;\n  }\n\n  addLoss(e) {\n    null == e || Array.isArray(e) && 0 === e.length || (e = toList$1(e), null != this._losses && this.losses.push(...e));\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  computeMask(e, t) {\n    if (!this.supportsMasking) {\n      if (null != t) {\n        if (!Array.isArray(t)) throw new TypeError(\"Layer \".concat(this.name, \" does not support masking, but was passed an inputMask.\"));\n        t.forEach(e => {\n          if (null != e) throw new TypeError(\"Layer \".concat(this.name, \" does not support masking, but was passed an inputMask.\"));\n        });\n      }\n\n      return null;\n    }\n\n    return t;\n  }\n\n  addInboundNode(e, t, n, r, a, s) {\n    var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : null;\n    var i = toList$1(e);\n    t = toList$1(t), n = toList$1(n), r = toList$1(r), a = normalizeShapeList$1(a), s = normalizeShapeList$1(s);\n    var l = [],\n        u = [],\n        c = [];\n\n    for (var _e99 of i) {\n      l.push(_e99.sourceLayer), u.push(_e99.nodeIndex), c.push(_e99.tensorIndex);\n    }\n\n    new Node$1({\n      outboundLayer: this,\n      inboundLayers: l,\n      nodeIndices: u,\n      tensorIndices: c,\n      inputTensors: i,\n      outputTensors: t,\n      inputMasks: n,\n      outputMasks: r,\n      inputShapes: a,\n      outputShapes: s\n    }, o);\n\n    for (var _e100 = 0; _e100 < t.length; _e100++) {\n      t[_e100].sourceLayer = this, t[_e100].nodeIndex = this.inboundNodes.length - 1, t[_e100].tensorIndex = _e100;\n    }\n  }\n\n  getConfig() {\n    var e = {\n      name: this.name,\n      trainable: this.trainable\n    };\n    return null != this.batchInputShape && (e.batchInputShape = this.batchInputShape), null != this.dtype && (e.dtype = this.dtype), e;\n  }\n\n  disposeWeights() {\n    return this.weights.forEach(e => e.dispose()), this.weights.length;\n  }\n\n  assertNotDisposed() {\n    if (0 === this._refCount) throw new Error(\"Layer '\".concat(this.name, \"' is already disposed.\"));\n  }\n\n  dispose() {\n    if (!this.built) throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been built yet.\"));\n    if (null === this._refCount) throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been used yet.\"));\n    this.assertNotDisposed();\n    var e = 0;\n    return 0 == --this._refCount && (e = this.disposeWeights()), {\n      refCountAfterDispose: this._refCount,\n      numDisposedVariables: e\n    };\n  }\n\n}\n\nfunction collectInputShape$1(e) {\n  e = toList$1(e);\n  var t = [];\n\n  for (var n of e) {\n    t.push(n.shape);\n  }\n\n  return singletonOrArray$1(t);\n}\n\nfunction guessOutputDType$1(e) {\n  return \"float32\";\n}\n\nfunction getSourceInputs$1(e, t, n) {\n  if ((null == t || null != n && n > 0) && (t = e.sourceLayer, n = e.nodeIndex), 0 === t.inboundNodes.length) return [e];\n  {\n    var _e101 = t.inboundNodes[n];\n    if (0 === _e101.inboundLayers.length) return _e101.inputTensors;\n    {\n      var _t89 = [];\n\n      for (var _n37 = 0; _n37 < _e101.inboundLayers.length; _n37++) {\n        var r = getSourceInputs$1(_e101.inputTensors[_n37], _e101.inboundLayers[_n37], _e101.nodeIndices[_n37]);\n\n        for (var _e102 of r) {\n          -1 === _t89.indexOf(_e102) && _t89.push(_e102);\n        }\n      }\n\n      return _t89;\n    }\n  }\n}\n\nclass InputLayer$1 extends Layer$1 {\n  constructor(e) {\n    if (super({\n      dtype: e.dtype,\n      name: null != e.name ? e.name : getUid$1(\"input\").toString()\n    }), null == e.batchSize && (e.batchSize = null), null == e.sparse && (e.sparse = !1), this.trainable = !1, this.built = !0, this.sparse = e.sparse, null != e.inputShape && null != e.batchInputShape) throw new ValueError$1(\"Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.\");\n    var t = e.batchInputShape;\n\n    if (null == t) {\n      if (null == e.inputShape) throw new ValueError$1(\"An InputLayer should be passed either a `batchInputShape` or an `inputShape`.\");\n      t = [e.batchSize].concat(e.inputShape);\n    } else if (null != e.batchSize) throw new ValueError$1(\"Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.\");\n\n    var n = e.dtype || \"float32\";\n    this.batchInputShape = t, this.dtype = n, this.inputSpec = [{\n      shape: t\n    }];\n    var r = new SymbolicTensor$1(this.dtype, this.batchInputShape, this, [], {}, this.name);\n    r.nodeIndex = 0, r.tensorIndex = 0, new Node$1({\n      outboundLayer: this,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: [r],\n      outputTensors: [r],\n      inputMasks: [null],\n      outputMasks: [null],\n      inputShapes: [t],\n      outputShapes: [t]\n    });\n  }\n\n  apply(e, t) {\n    throw new ValueError$1(\"Cannot pass any input to an InputLayer's apply() method. InputLayer name: \".concat(this.name));\n  }\n\n  dispose() {\n    return {\n      refCountAfterDispose: this._refCount,\n      numDisposedVariables: 0\n    };\n  }\n\n  getConfig() {\n    return {\n      batchInputShape: this.batchInputShape,\n      dtype: this.dtype,\n      sparse: this.sparse,\n      name: this.name\n    };\n  }\n\n}\n\nfunction Input$1(e) {\n  if (null == e.batchShape && null == e.shape) throw new Error(\"Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.\");\n  if (null != e.batchShape && null != e.shape) throw new ValueError$1(\"Please provide either a `shape` or `batchShape` argument to Input, but not both.\");\n  var t = e.batchShape;\n  null != e.shape && null == t && (t = [null].concat(e.shape));\n  var n = e.dtype;\n  return null == n && (n = \"float32\"), new InputLayer$1({\n    batchInputShape: t,\n    name: e.name,\n    dtype: n,\n    sparse: e.sparse\n  }).inboundNodes[0].outputTensors[0];\n}\n\nfunction resolveScalarsInLogs$1(_x16) {\n  return _resolveScalarsInLogs$.apply(this, arguments);\n}\n\nfunction _resolveScalarsInLogs$() {\n  _resolveScalarsInLogs$ = _asyncToGenerator(function* (e) {\n    if (null == e) return;\n    var t = [],\n        n = [],\n        r = [];\n\n    for (var a in e) {\n      var s = e[a];\n\n      if (\"number\" != typeof s) {\n        var _e1129 = s;\n        t.push(_e1129.data()), n.push(a), r.push(_e1129);\n      }\n    }\n\n    if (t.length > 0) {\n      var _a316 = yield Promise.all(t);\n\n      for (var _t760 = 0; _t760 < _a316.length; ++_t760) {\n        e[n[_t760]] = _a316[_t760][0];\n      }\n\n      dispose$1(r);\n    }\n  });\n  return _resolveScalarsInLogs$.apply(this, arguments);\n}\n\nfunction disposeTensorsInLogs$1(e) {\n  if (null != e) for (var t in e) {\n    var n = e[t];\n    \"number\" != typeof n && n.dispose();\n  }\n}\n\nvar ModelLoggingVerbosity$1;\nInputLayer$1.className = \"InputLayer\", registerClass$1(InputLayer$1), function (e) {\n  e[e.SILENT = 0] = \"SILENT\", e[e.VERBOSE = 1] = \"VERBOSE\";\n}(ModelLoggingVerbosity$1 || (ModelLoggingVerbosity$1 = {}));\nvar DEFAULT_YIELD_EVERY_MS$1 = 125;\n\nclass BaseCallback$1 {\n  constructor() {\n    this.validationData = null;\n  }\n\n  setParams(e) {\n    this.params = e;\n  }\n\n  onEpochBegin(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onEpochEnd(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onBatchBegin(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onBatchEnd(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onTrainBegin(e) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onTrainEnd(e) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  setModel(e) {}\n\n}\n\nclass CallbackList$1 {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 10;\n    null == e && (e = []), this.callbacks = e, this.queueLength = t;\n  }\n\n  append(e) {\n    this.callbacks.push(e);\n  }\n\n  setParams(e) {\n    for (var t of this.callbacks) {\n      t.setParams(e);\n    }\n  }\n\n  setModel(e) {\n    for (var t of this.callbacks) {\n      t.setModel(e);\n    }\n  }\n\n  onEpochBegin(e, t) {\n    var _this38 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this38.callbacks) {\n        yield n.onEpochBegin(e, t);\n      }\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this39 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this39.callbacks) {\n        yield n.onEpochEnd(e, t);\n      }\n    })();\n  }\n\n  onBatchBegin(e, t) {\n    var _this40 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this40.callbacks) {\n        yield n.onBatchBegin(e, t);\n      }\n    })();\n  }\n\n  onBatchEnd(e, t) {\n    var _this41 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this41.callbacks) {\n        yield n.onBatchEnd(e, t);\n      }\n    })();\n  }\n\n  onTrainBegin(e) {\n    var _this42 = this;\n\n    return _asyncToGenerator(function* () {\n      null == e && (e = {});\n\n      for (var t of _this42.callbacks) {\n        yield t.onTrainBegin(e);\n      }\n    })();\n  }\n\n  onTrainEnd(e) {\n    var _this43 = this;\n\n    return _asyncToGenerator(function* () {\n      null == e && (e = {});\n\n      for (var t of _this43.callbacks) {\n        yield t.onTrainEnd(e);\n      }\n    })();\n  }\n\n}\n\nclass BaseLogger$1 extends BaseCallback$1 {\n  constructor() {\n    super();\n  }\n\n  onEpochBegin(e) {\n    var _this44 = this;\n\n    return _asyncToGenerator(function* () {\n      _this44.seen = 0, _this44.totals = {};\n    })();\n  }\n\n  onBatchEnd(e, t) {\n    var _this45 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n      var n = null == t.size ? 0 : t.size;\n      _this45.seen += n;\n\n      var _loop12 = function _loop12(_e103) {\n        var r = t[_e103];\n        if (\"number\" == typeof r) _this45.totals.hasOwnProperty(_e103) || (_this45.totals[_e103] = 0), _this45.totals[_e103] = _this45.totals[_e103] + r * n;else {\n          var _t90;\n\n          _e103 in _this45.totals ? _t90 = _this45.totals[_e103] : _this45.totals[_e103] = 0;\n          var a = tidy$1(() => add$5(_this45.totals[_e103], mul$1(r, n)));\n          _this45.totals[_e103] = a, null != _t90 && _t90.dispose();\n        }\n      };\n\n      for (var _e103 in t) {\n        _loop12(_e103);\n      }\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this46 = this;\n\n    return _asyncToGenerator(function* () {\n      if (null != t) {\n        var _loop13 = function _loop13(_e104) {\n          null != _this46.totals[_e104] && (\"number\" == typeof _this46.totals[_e104] ? t[_e104] = _this46.totals[_e104] / _this46.seen : tidy$1(() => {\n            var n = mul$1(div$3(1, _this46.seen), _this46.totals[_e104]);\n            t[_e104] = n, _this46.totals[_e104].dispose(), keep$1(t[_e104]);\n          }));\n        };\n\n        for (var _e104 of _this46.params.metrics) {\n          _loop13(_e104);\n        }\n      }\n    })();\n  }\n\n}\n\nclass History$1 extends BaseCallback$1 {\n  onTrainBegin(e) {\n    var _this47 = this;\n\n    return _asyncToGenerator(function* () {\n      _this47.epoch = [], _this47.history = {};\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this48 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {}), _this48.epoch.push(e);\n\n      for (var _e105 in t) {\n        null == _this48.history[_e105] && (_this48.history[_e105] = []), _this48.history[_e105].push(t[_e105]);\n      }\n    })();\n  }\n\n  syncData() {\n    var _this49 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [],\n          t = [],\n          n = [];\n\n      for (var _r27 in _this49.history) {\n        var a = _this49.history[_r27];\n\n        for (var s = 0; s < a.length; ++s) {\n          \"number\" != typeof a[s] && (e.push(a[s].data()), t.push(_r27), n.push(s));\n        }\n      }\n\n      var r = yield Promise.all(e);\n\n      for (var _e106 = 0; _e106 < r.length; ++_e106) {\n        _this49.history[t[_e106]][n[_e106]].dispose(), _this49.history[t[_e106]][n[_e106]] = r[_e106][0];\n      }\n    })();\n  }\n\n}\n\nclass CustomCallback$1 extends BaseCallback$1 {\n  constructor(e, t) {\n    if (super(), this.currentEpoch = 0, this.yieldEvery = t || \"auto\", \"auto\" === this.yieldEvery && (this.yieldEvery = DEFAULT_YIELD_EVERY_MS$1), \"never\" === this.yieldEvery && null != e.onYield) throw new Error(\"yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback\");\n    isNumber$1(this.yieldEvery) && (this.maybeWait = debounce$1(this.maybeWait.bind(this), this.yieldEvery)), this.trainBegin = e.onTrainBegin, this.trainEnd = e.onTrainEnd, this.epochBegin = e.onEpochBegin, this.epochEnd = e.onEpochEnd, this.batchBegin = e.onBatchBegin, this.batchEnd = e.onBatchEnd, this.yield = e.onYield;\n  }\n\n  maybeWait(e, t, n) {\n    var _this50 = this;\n\n    return _asyncToGenerator(function* () {\n      var r = [];\n      null != _this50.yield && (yield resolveScalarsInLogs$1(n), r.push(_this50.yield(e, t, n))), r.push(nextFrame$1()), yield Promise.all(r);\n    })();\n  }\n\n  onEpochBegin(e, t) {\n    var _this51 = this;\n\n    return _asyncToGenerator(function* () {\n      _this51.currentEpoch = e, null != _this51.epochBegin && (yield resolveScalarsInLogs$1(t), yield _this51.epochBegin(e, t));\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this52 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = [];\n      null != _this52.epochEnd && (yield resolveScalarsInLogs$1(t), n.push(_this52.epochEnd(e, t))), \"epoch\" === _this52.yieldEvery && n.push(nextFrame$1()), yield Promise.all(n);\n    })();\n  }\n\n  onBatchBegin(e, t) {\n    var _this53 = this;\n\n    return _asyncToGenerator(function* () {\n      null != _this53.batchBegin && (yield resolveScalarsInLogs$1(t), yield _this53.batchBegin(e, t));\n    })();\n  }\n\n  onBatchEnd(e, t) {\n    var _this54 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = [];\n      null != _this54.batchEnd && (yield resolveScalarsInLogs$1(t), n.push(_this54.batchEnd(e, t))), \"batch\" === _this54.yieldEvery ? n.push(nextFrame$1()) : isNumber$1(_this54.yieldEvery) && n.push(_this54.maybeWait(_this54.currentEpoch, e, t)), yield Promise.all(n);\n    })();\n  }\n\n  onTrainBegin(e) {\n    var _this55 = this;\n\n    return _asyncToGenerator(function* () {\n      null != _this55.trainBegin && (yield resolveScalarsInLogs$1(e), yield _this55.trainBegin(e));\n    })();\n  }\n\n  onTrainEnd(e) {\n    var _this56 = this;\n\n    return _asyncToGenerator(function* () {\n      null != _this56.trainEnd && (yield resolveScalarsInLogs$1(e), yield _this56.trainEnd(e));\n    })();\n  }\n\n}\n\nfunction standardizeCallbacks$1(e, t) {\n  return null == e && (e = {}), e instanceof BaseCallback$1 ? [e] : Array.isArray(e) && e[0] instanceof BaseCallback$1 ? e : toList$1(e).map(e => new CustomCallback$1(e, t));\n}\n\nclass CallbackConstructorRegistry$1 {\n  constructor() {}\n\n  static registerCallbackConstructor(e, t) {\n    assert$6(e >= 0 && Number.isInteger(e), () => \"Verbosity level is expected to be an integer >= 0, but got \".concat(e)), CallbackConstructorRegistry$1.checkForDuplicate(t), null == CallbackConstructorRegistry$1.constructors[e] && (CallbackConstructorRegistry$1.constructors[e] = []), CallbackConstructorRegistry$1.constructors[e].push(t);\n  }\n\n  static checkForDuplicate(e) {\n    for (var t in CallbackConstructorRegistry$1.constructors) {\n      CallbackConstructorRegistry$1.constructors[+t].forEach(t => {\n        if (t === e) throw new ValueError$1(\"Duplicate callback constructor.\");\n      });\n    }\n  }\n\n  static clear() {\n    CallbackConstructorRegistry$1.constructors = {};\n  }\n\n  static createCallbacks(e) {\n    var t = [];\n\n    for (var n in CallbackConstructorRegistry$1.constructors) {\n      var r = +n;\n      e >= r && t.push(...CallbackConstructorRegistry$1.constructors[r]);\n    }\n\n    return t.map(e => new e());\n  }\n\n}\n\nfunction configureCallbacks$1(e, t, n, r, a, s, o, i, l) {\n  var u = new History$1(),\n      c = [new BaseLogger$1(), ...CallbackConstructorRegistry$1.createCallbacks(t)];\n  null != e && c.push(...e), c.push(u);\n  var p = new CallbackList$1(c);\n  return p.setParams({\n    epochs: n,\n    initialEpoch: r,\n    samples: a,\n    steps: s,\n    batchSize: o,\n    verbose: t,\n    doValidation: i,\n    metrics: l\n  }), {\n    callbackList: p,\n    history: u\n  };\n}\n\nfunction deserialize$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return deserializeKerasObject$1(e, SerializationMap$1.getMap().classNameMap, t, \"layer\", n);\n}\n\nfunction l2Normalize$1(e, t) {\n  return tidy$1(() => {\n    \"float32\" !== e.dtype && (e = cast$7(e, \"float32\"));\n    var n = sum$6(square$4(e), t, !0),\n        r = fill$5(n.shape, epsilon$3()),\n        a = sqrt$5(maximum$6(n, r));\n    return div$3(e, a);\n  });\n}\n\nfunction meanSquaredError$3(e, t) {\n  return tidy$1(() => mean$3(square$4(sub$5(t, e)), -1));\n}\n\nfunction meanAbsoluteError$2(e, t) {\n  return tidy$1(() => mean$3(abs$5(sub$5(t, e)), -1));\n}\n\nfunction meanAbsolutePercentageError$2(e, t) {\n  return tidy$1(() => {\n    var n = sub$5(e, t),\n        r = clipByValue$3(abs$5(e), epsilon$3(), Number.MAX_VALUE),\n        a = abs$5(div$3(n, r));\n    return mul$1(100, mean$3(a, -1));\n  });\n}\n\nfunction meanSquaredLogarithmicError$1(e, t) {\n  return tidy$1(() => {\n    var n = clipByValue$3(t, epsilon$3(), Number.MAX_VALUE),\n        r = log$7(add$5(1, n)),\n        a = clipByValue$3(e, epsilon$3(), Number.MAX_VALUE),\n        s = log$7(add$5(1, a));\n    return mean$3(square$4(sub$5(r, s)), -1);\n  });\n}\n\nfunction squaredHinge$1(e, t) {\n  return tidy$1(() => {\n    var n = maximum$6(0, sub$5(1, mul$1(e, t)));\n    return mean$3(square$4(n), -1);\n  });\n}\n\nfunction hinge$1(e, t) {\n  return tidy$1(() => {\n    var n = maximum$6(0, sub$5(1, mul$1(e, t)));\n    return mean$3(n, -1);\n  });\n}\n\nfunction categoricalHinge$1(e, t) {\n  return tidy$1(() => {\n    var n = sum$6(mul$1(e, t), -1),\n        r = max$7(mul$1(sub$5(1, e), t), -1);\n    return maximum$6(0, add$5(1, sub$5(r, n)));\n  });\n}\n\nfunction logcosh$1(e, t) {\n  return tidy$1(() => {\n    var n = Math.log(2),\n        r = sub$5(t, e),\n        a = sub$5(add$5(r, softplus$5(mul$1(-2, r))), n);\n    return mean$3(a, -1);\n  });\n}\n\nfunction categoricalCrossentropy$4(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return tidy$1(() => {\n    if (n) t = softmax$6(t);else {\n      var _e107 = sum$6(t, t.shape.length - 1, !0);\n\n      t = div$3(t, _e107);\n    }\n    return t = clipByValue$3(t, epsilon$3(), 1 - epsilon$3()), neg$5(sum$6(mul$1(cast$7(e, \"float32\"), log$7(t)), t.shape.length - 1));\n  });\n}\n\nfunction sparseCategoricalCrossentropy$3(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return tidy$1(() => {\n    var r = cast$7(floor$5(flatten$5(e)), \"int32\"),\n        a = (t = clipByValue$3(t, epsilon$3(), 1 - epsilon$3())).shape;\n    return categoricalCrossentropy$4(reshape$6(oneHot$5(r, a[a.length - 1]), a), t, n);\n  });\n}\n\nfunction sigmoidCrossEntropyWithLogits$1(e, t) {\n  if (!arraysEqual$1(e.shape, t.shape)) throw new ValueError$1(\"logits and labels must have the same shape, but got shapes \".concat(JSON.stringify(e.shape), \" and \").concat(JSON.stringify(t.shape)));\n  return tidy$1(() => {\n    var n = relu$6(t),\n        r = neg$5(abs$5(t));\n    return add$5(sub$5(n, mul$1(t, e)), log1p$5(exp$5(r)));\n  });\n}\n\nfunction binaryCrossentropy$4(e, t) {\n  return tidy$1(() => {\n    var n;\n    return n = clipByValue$3(t, epsilon$3(), 1 - epsilon$3()), n = log$7(div$3(n, sub$5(1, n))), mean$3(sigmoidCrossEntropyWithLogits$1(e, n), -1);\n  });\n}\n\nfunction kullbackLeiblerDivergence$1(e, t) {\n  return tidy$1(() => {\n    var n = clipByValue$3(e, epsilon$3(), 1),\n        r = clipByValue$3(t, epsilon$3(), 1);\n    return sum$6(mul$1(e, log$7(div$3(n, r))), -1);\n  });\n}\n\nfunction poisson$1(e, t) {\n  return tidy$1(() => {\n    var n = log$7(add$5(epsilon$3(), t));\n    return mean$3(sub$5(t, mul$1(e, n)), -1);\n  });\n}\n\nfunction cosineProximity$2(e, t) {\n  return tidy$1(() => {\n    var n = l2Normalize$1(e, -1),\n        r = l2Normalize$1(t, -1),\n        a = mul$1(n, r);\n    return neg$5(sum$6(a, -1));\n  });\n}\n\nCallbackConstructorRegistry$1.constructors = {};\nvar lossesMap$1 = {\n  meanSquaredError: meanSquaredError$3,\n  meanAbsoluteError: meanAbsoluteError$2,\n  meanAbsolutePercentageError: meanAbsolutePercentageError$2,\n  meanSquaredLogarithmicError: meanSquaredLogarithmicError$1,\n  squaredHinge: squaredHinge$1,\n  hinge: hinge$1,\n  categoricalHinge: categoricalHinge$1,\n  logcosh: logcosh$1,\n  categoricalCrossentropy: categoricalCrossentropy$4,\n  sparseCategoricalCrossentropy: sparseCategoricalCrossentropy$3,\n  binaryCrossentropy: binaryCrossentropy$4,\n  kullbackLeiblerDivergence: kullbackLeiblerDivergence$1,\n  poisson: poisson$1,\n  cosineProximity: cosineProximity$2\n};\n\nfunction get$3(e) {\n  if (\"string\" == typeof e) {\n    if (e in lossesMap$1) return lossesMap$1[e];\n    var t = \"Unknown loss \".concat(e);\n    throw e.toLowerCase().includes(\"softmaxcrossentropy\") && (t = \"Unknown loss \".concat(e, \". Use \\\"categoricalCrossentropy\\\" as the string name for tf.losses.softmaxCrossEntropy\")), new ValueError$1(t);\n  }\n\n  return e;\n}\n\nfunction binaryAccuracy$2(e, t) {\n  return tidy$1(() => {\n    var n = mul$1(.5, onesLike$5(t)),\n        r = cast$6(greater$6(t, n), e.dtype);\n    return mean$3(equal$5(e, r), -1);\n  });\n}\n\nfunction categoricalAccuracy$2(e, t) {\n  return tidy$1(() => cast$6(equal$5(argMax$5(e, -1), argMax$5(t, -1)), \"float32\"));\n}\n\nfunction truePositives$1(e, t) {\n  return tidy$1(() => cast$7(sum$6(logicalAnd$5(equal$5(e, 1), equal$5(t, 1))), \"float32\"));\n}\n\nfunction falsePositives$1(e, t) {\n  return tidy$1(() => cast$7(sum$6(logicalAnd$5(equal$5(e, 0), equal$5(t, 1))), \"float32\"));\n}\n\nfunction precision$2(e, t) {\n  return tidy$1(() => {\n    var n = truePositives$1(e, t),\n        r = falsePositives$1(e, t),\n        a = add$5(n, r);\n    return cast$7(where$1(greater$6(a, 0), div$3(n, a), 0), \"float32\");\n  });\n}\n\nfunction binaryCrossentropy$3(e, t) {\n  return binaryCrossentropy$4(e, t);\n}\n\nfunction sparseCategoricalAccuracy$2(e, t) {\n  return e.rank === t.rank && (e = squeeze$1(e, [e.rank - 1])), (t = argMax$5(t, -1)).dtype !== e.dtype && (t = cast$7(t, e.dtype)), cast$7(equal$5(e, t), \"float32\");\n}\n\nvar mse$2 = meanSquaredError$3,\n    MSE$2 = meanSquaredError$3,\n    mae$1 = meanAbsoluteError$2,\n    MAE$1 = meanAbsoluteError$2,\n    mape$2 = meanAbsolutePercentageError$2,\n    MAPE$2 = meanAbsolutePercentageError$2,\n    categoricalCrossentropy$3 = categoricalCrossentropy$4,\n    cosine$1 = cosineProximity$2,\n    sparseCategoricalCrossentropy$2 = sparseCategoricalCrossentropy$3,\n    metricsMap$1 = {\n  binaryAccuracy: binaryAccuracy$2,\n  categoricalAccuracy: categoricalAccuracy$2,\n  precision: precision$2,\n  categoricalCrossentropy: categoricalCrossentropy$3,\n  sparseCategoricalCrossentropy: sparseCategoricalCrossentropy$2,\n  mse: mse$2,\n  MSE: MSE$2,\n  mae: mae$1,\n  MAE: MAE$1,\n  mape: mape$2,\n  MAPE: MAPE$2,\n  cosine: cosine$1\n};\n\nfunction get$2(e) {\n  if (\"string\" == typeof e && e in metricsMap$1) return metricsMap$1[e];\n  if (\"string\" != typeof e && null != e) return e;\n  throw new ValueError$1(\"Unknown metric \".concat(e));\n}\n\nfunction getLossOrMetricName$1(e) {\n  if (assert$5(null !== e, \"Unknown LossOrMetricFn \".concat(e)), \"string\" == typeof e) return e;\n  {\n    var t;\n\n    for (var n of Object.keys(lossesMap$1)) {\n      if (lossesMap$1[n] === e) {\n        t = n;\n        break;\n      }\n    }\n\n    if (void 0 !== t) return t;\n\n    for (var _n38 of Object.keys(metricsMap$1)) {\n      if (metricsMap$1[_n38] === e) {\n        t = _n38;\n        break;\n      }\n    }\n\n    return void 0 !== t ? t : e.name;\n  }\n}\n\nfunction getOptimizer$1(e) {\n  var t = {\n    Adagrad: () => train$1.adagrad(.01),\n    Adadelta: () => train$1.adadelta(1, .95, epsilon$3()),\n    Adam: () => train$1.adam(.001, .9, .999, epsilon$3()),\n    Adamax: () => train$1.adamax(.002, .9, .999, epsilon$3(), 0),\n    RMSProp: () => train$1.rmsprop(.001, .9, 0, epsilon$3()),\n    SGD: () => train$1.sgd(.01)\n  };\n  if (t.adagrad = t.Adagrad, t.adadelta = t.Adadelta, t.adam = t.Adam, t.adamax = t.Adamax, t.rmsprop = t.RMSProp, t.sgd = t.SGD, e in t) return t[e]();\n  throw new ValueError$1(\"Unknown Optimizer \".concat(e));\n}\n\nvar MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH$1 = 1048576;\n\nfunction checkUserDefinedMetadata$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  if (null == e || \"object\" != typeof e || Object.getPrototypeOf(e) !== Object.prototype || !plainObjectCheck$1(e)) throw new Error(\"User-defined metadata is expected to be a JSON object, but is not.\");\n\n  if (n) {\n    var _n39 = JSON.stringify(e);\n\n    _n39.length > MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH$1 && console.warn(\"User-defined metadata of model \\\"\".concat(t, \"\\\" is too large in size (length=\").concat(_n39.length, \" when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= \").concat(MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH$1, \".\"));\n  }\n}\n\nfunction plainObjectCheck$1(e) {\n  if (null === e) return !0;\n\n  if (\"object\" == typeof e) {\n    if (Object.getPrototypeOf(e) === Object.prototype) {\n      var t = Object.keys(e);\n\n      for (var n of t) {\n        if (\"string\" != typeof n) return !1;\n        if (!plainObjectCheck$1(e[n])) return !1;\n      }\n\n      return !0;\n    }\n\n    if (Array.isArray(e)) {\n      for (var _t91 of e) {\n        if (!plainObjectCheck$1(_t91)) return !1;\n      }\n\n      return !0;\n    }\n\n    return !1;\n  }\n\n  {\n    var _t92 = typeof e;\n\n    return \"string\" === _t92 || \"number\" === _t92 || \"boolean\" === _t92;\n  }\n}\n\nfunction printSummary$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : console.log;\n  var a = isModelSequentialLike$1(e),\n      s = [\"Layer (type)\", \"Output shape\", \"Param #\"];\n  var o;\n\n  if (a ? (t = t || 65, n = n || [.45, .85, 1]) : (t = t || 98, n = n || [.33, .55, .67, 1]), n[n.length - 1] <= 1 && (n = n.map(e => Math.floor(t * e))), !a) {\n    s.push(\"Receives inputs\"), o = [];\n\n    for (var _t93 in e.nodesByDepth) {\n      o.push(...e.nodesByDepth[_t93]);\n    }\n  }\n\n  r(\"_\".repeat(t)), printRow$1(s, n, r), r(\"=\".repeat(t));\n  var i = e.layers;\n\n  for (var _e108 = 0; _e108 < i.length; ++_e108) {\n    a ? printLayerSummary$1(i[_e108], n, r) : printLayerSummaryWithConnections$1(i[_e108], n, o, r), r((_e108 === i.length - 1 ? \"=\" : \"_\").repeat(t));\n  }\n\n  e.checkTrainableWeightsConsistency();\n  var l = countTrainableParams$1(e),\n      u = countParamsInWeights$1(e.nonTrainableWeights);\n  r(\"Total params: \".concat(l + u)), r(\"Trainable params: \".concat(l)), r(\"Non-trainable params: \".concat(u)), r(\"_\".repeat(t));\n}\n\nfunction countTrainableParams$1(e) {\n  var t;\n  return t = countParamsInWeights$1(null != e.collectedTrainableWeights ? e.collectedTrainableWeights : e.trainableWeights), t;\n}\n\nfunction isModelSequentialLike$1(e) {\n  var t = !0;\n  var n = [],\n      r = [];\n\n  for (var _t94 in e.nodesByDepth) {\n    n.push(e.nodesByDepth[_t94]);\n  }\n\n  for (var _e109 of n) {\n    if (_e109.length > 1 || 1 === _e109.length && _e109[0].inboundLayers.length > 1) {\n      t = !1;\n      break;\n    }\n\n    r.push(..._e109);\n  }\n\n  if (t) for (var _n40 of e.layers) {\n    var _e110 = !1;\n\n    for (var a of _n40.inboundNodes) {\n      if (-1 !== r.indexOf(a)) {\n        if (_e110) {\n          t = !1;\n          break;\n        }\n\n        _e110 = !0;\n      }\n    }\n\n    if (!t) break;\n  }\n  return t;\n}\n\nfunction printRow$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : console.log;\n  var r = \"\";\n\n  for (var _n41 = 0; _n41 < e.length; ++_n41) {\n    _n41 > 0 && (r = r.slice(0, r.length - 1) + \" \"), r += e[_n41], r = r.slice(0, t[_n41]), r += \" \".repeat(t[_n41] - r.length);\n  }\n\n  n(r);\n}\n\nfunction printLayerSummary$1(e, t, n) {\n  var r;\n\n  try {\n    r = JSON.stringify(e.outputShape);\n  } catch (e) {\n    r = \"multiple\";\n  }\n\n  printRow$1([\"\".concat(e.name, \" (\").concat(e.getClassName(), \")\"), r, e.countParams().toString()], t, n);\n}\n\nfunction printLayerSummaryWithConnections$1(e, t, n, r) {\n  var a;\n\n  try {\n    a = JSON.stringify(e.outputShape);\n  } catch (e) {\n    a = \"multiple\";\n  }\n\n  var s = [];\n\n  for (var _t95 of e.inboundNodes) {\n    if (!(null != n && n.length > 0 && -1 === n.indexOf(_t95))) for (var _e111 = 0; _e111 < _t95.inboundLayers.length; ++_e111) {\n      s.push(\"\".concat(_t95.inboundLayers[_e111].name, \"[\").concat(_t95.nodeIndices[_e111], \"][\").concat(_t95.tensorIndices[_e111], \"]\"));\n    }\n  }\n\n  var o = e.name,\n      i = e.getClassName(),\n      l = 0 === s.length ? \"\" : s[0];\n  printRow$1([\"\".concat(o, \" (\").concat(i, \")\"), a, e.countParams().toString(), l], t, r);\n\n  for (var _e112 = 1; _e112 < s.length; ++_e112) {\n    printRow$1([\"\", \"\", \"\", s[_e112]], t, r);\n  }\n}\n\nfunction isArrayItemInputOrOutputName$1(e, t, n) {\n  return (\"inboundNodes\" === e || \"outputLayers\" === e || \"inputLayers\" === e) && 0 === t && \"string\" == typeof n;\n}\n\nfunction convertPythonicToTs$1(e, t) {\n  if (null === e) return null;\n  if (\"string\" == typeof e) return toCamelCase$1(e);\n  if (\"number\" == typeof e || \"boolean\" == typeof e) return e;\n\n  if (e instanceof Array) {\n    var n = [],\n        r = e.length;\n\n    for (var a = 0; a < r; ++a) {\n      var _r28 = e[a];\n      isArrayItemInputOrOutputName$1(t, a, _r28) ? n.push(_r28) : n.push(convertPythonicToTs$1(_r28, t));\n    }\n\n    return n;\n  }\n\n  {\n    var _t96 = {};\n\n    for (var _n42 of Object.keys(e)) {\n      var _r29 = e[_n42];\n      if (\"name\" === _n42 && \"string\" == typeof _r29) _t96[_n42] = _r29;else {\n        var _e113 = toCamelCase$1(_n42);\n\n        _t96[_e113] = convertPythonicToTs$1(_r29, _e113);\n      }\n    }\n\n    return _t96;\n  }\n}\n\nfunction convertTsToPythonic$1(e, t) {\n  if (null == e) return null;\n  if (\"string\" == typeof e) return toSnakeCase$1(e);\n  if (\"number\" == typeof e || \"boolean\" == typeof e) return e;\n\n  if (e instanceof Array) {\n    var n = [],\n        r = e.length;\n\n    for (var a = 0; a < r; ++a) {\n      var _r30 = e[a];\n      isArrayItemInputOrOutputName$1(t, a, _r30) ? n.push(_r30) : n.push(convertTsToPythonic$1(_r30, t));\n    }\n\n    return n;\n  }\n\n  {\n    var _t97 = {};\n\n    for (var _n43 of Object.keys(e)) {\n      var _r31 = e[_n43];\n      _t97[toSnakeCase$1(_n43)] = \"name\" !== _n43 && \"className\" !== _n43 || \"string\" != typeof _r31 ? convertTsToPythonic$1(_r31, _n43) : _r31;\n    }\n\n    return _t97;\n  }\n}\n\nvar version$d = \"3.8.0\";\n\nfunction assertFeedCompatibility$1(e, t) {\n  if (null == e.dtype || e.dtype === t.dtype) return t;\n\n  try {\n    return cast$7(t, e.dtype);\n  } catch (n) {\n    throw new ValueError$1(\"The dtype of the feed (\".concat(t.dtype, \") can not be cast to the dtype of the key '\").concat(e.name, \"' (\").concat(e.dtype, \").\"));\n  }\n}\n\nclass FeedDict$1 {\n  constructor(e) {\n    if (this.id2Value = {}, this.id2Mask = {}, this.name2Id = {}, e instanceof FeedDict$1) for (var t in e.id2Value) {\n      this.id2Value[t] = e.id2Value[t], t in e.id2Mask && (this.id2Mask[t] = e.id2Mask[t]);\n    } else {\n      if (null == e) return;\n\n      for (var _t98 of e) {\n        this.add(_t98.key, _t98.value);\n      }\n    }\n  }\n\n  add(e, t, n) {\n    if (null != this.id2Value[e.id]) throw new ValueError$1(\"Duplicate key: name=\".concat(e.name, \", id=\").concat(e.id));\n    return this.id2Value[e.id] = assertFeedCompatibility$1(e, t), this.name2Id[e.name] = e.id, null != n && (this.id2Mask[e.id] = n), this;\n  }\n\n  addFeed(e) {\n    this.add(e.key, e.value);\n  }\n\n  hasKey(e) {\n    return null != this.id2Value[e.id];\n  }\n\n  names() {\n    return Object.keys(this.name2Id);\n  }\n\n  getValue(e) {\n    if (e instanceof SymbolicTensor$1) {\n      if (null == this.id2Value[e.id]) throw new ValueError$1(\"Nonexistent key: \".concat(e.name));\n      return this.id2Value[e.id];\n    }\n\n    {\n      var t = this.name2Id[e];\n      if (null == t) throw new ValueError$1(\"Feed dict has no SymbolicTensor name: \".concat(e));\n      return this.id2Value[t];\n    }\n  }\n\n  getMask(e) {\n    if (e instanceof SymbolicTensor$1) {\n      if (null == this.id2Value[e.id]) throw new ValueError$1(\"Nonexistent key: \".concat(e.name));\n      return this.id2Mask[e.id];\n    }\n\n    {\n      var t = this.name2Id[e];\n      if (null == t) throw new ValueError$1(\"Feed dict has no SymbolicTensor name: \".concat(e));\n      return this.id2Mask[t];\n    }\n  }\n\n  disposeMasks() {\n    null != this.id2Mask && dispose$1(this.id2Mask);\n  }\n\n}\n\nvar cachedSorted$1 = {},\n    cachedRecipientCounts$1 = {};\n\nfunction execute$1(e, t, n, r) {\n  var a = null != n && n.training,\n      s = Array.isArray(e),\n      o = s ? e : [e],\n      i = o.map(e => e.name),\n      l = [],\n      u = t.names();\n\n  for (var _e114 of i) {\n    -1 !== u.indexOf(_e114) ? l.push(t.getValue(_e114)) : l.push(null);\n  }\n\n  null != r && (r.maxNumTensors = -Infinity, r.minNumTensors = Infinity);\n  var c = i.join(\",\") + \"|\" + t.names().join(\",\");\n  var p, d;\n\n  if (null == cachedSorted$1[c]) {\n    var _e115 = getTopologicalSortAndRecipientCounts$1(o, t);\n\n    p = _e115.sorted, d = _e115.recipientCounts, cachedSorted$1[c] = p, cachedRecipientCounts$1[c] = d;\n  }\n\n  p = cachedSorted$1[c], d = {}, a || Object.assign(d, cachedRecipientCounts$1[c]);\n  var h = new FeedDict$1(t);\n\n  for (var _e116 = 0; _e116 < p.length; ++_e116) {\n    if (null != r) {\n      var _e117 = memory$1().numTensors;\n      _e117 > r.maxNumTensors && (r.maxNumTensors = _e117), _e117 < r.minNumTensors && (r.minNumTensors = _e117);\n    }\n\n    var _s15 = p[_e116],\n        _o8 = _s15.sourceLayer;\n    if (_o8 instanceof InputLayer$1) continue;\n    var _u2 = [],\n        _c2 = [],\n        m = [];\n    var f = !1;\n\n    for (var _e118 of _s15.inputs) {\n      var _n44 = h.getValue(_e118),\n          _r32 = h.getMask(_e118);\n\n      _u2.push(_n44), _c2.push(_r32), null != _r32 && (f = !0), a || (d[_e118.name]--, 0 !== d[_e118.name] || t.hasKey(_e118) || -1 !== i.indexOf(_e118.name) || _n44.isDisposed || !0 === _e118.sourceLayer.stateful || m.push(_n44));\n    }\n\n    f && ((n = n || {}).mask = _c2[0]);\n    var g = toList$1(_o8.apply(_u2, n));\n    var $ = null;\n    _o8.supportsMasking && ($ = _o8.computeMask(_u2, _c2));\n    var y = getNodeOutputs$1(_s15),\n        b = Array.isArray(y) ? y : [y];\n\n    for (var _e119 = 0; _e119 < b.length; ++_e119) {\n      h.hasKey(b[_e119]) || h.add(b[_e119], g[_e119], Array.isArray($) ? $[0] : $);\n\n      var _t99 = i.indexOf(b[_e119].name);\n\n      -1 !== _t99 && (l[_t99] = g[_e119]);\n    }\n\n    a || dispose$1(m);\n  }\n\n  return h.disposeMasks(), s ? l : l[0];\n}\n\nfunction getTopologicalSortAndRecipientCounts$1(e, t) {\n  assert$6(null != e && e.length > 0, () => \"Expected at least one fetch, got none\");\n  var n = [],\n      r = {};\n\n  if (1 === e.length) {\n    var a = getTopologicalSortAndRecipientCountsForOneFetch$1(e[0], t);\n    n = a.sorted, r = a.recipientMap;\n  } else {\n    var _a27 = new Set();\n\n    for (var s of e) {\n      var {\n        sorted: _e120,\n        recipientMap: o\n      } = getTopologicalSortAndRecipientCountsForOneFetch$1(s, t);\n\n      for (var _t100 of _e120) {\n        _a27.has(_t100.name) || (n.push(_t100), _a27.add(_t100.name));\n      }\n\n      var _loop14 = function _loop14(_e121) {\n        null == r[_e121] && (r[_e121] = new Set()), o[_e121].forEach(t => r[_e121].add(t));\n      };\n\n      for (var _e121 in o) {\n        _loop14(_e121);\n      }\n    }\n  }\n\n  return {\n    sorted: n,\n    recipientCounts: recipientMap2Counts$1(r)\n  };\n}\n\nfunction recipientMap2Counts$1(e) {\n  var t = {};\n\n  for (var n in e) {\n    t[n] = e[n].size;\n  }\n\n  return t;\n}\n\nfunction getTopologicalSortAndRecipientCountsForOneFetch$1(e, t) {\n  var n = new Set(),\n      r = [],\n      a = {};\n\n  for (var _e122 of t.names()) {\n    n.add(_e122);\n  }\n\n  var s = [],\n      o = [];\n\n  for (s.push(e); s.length > 0;) {\n    var _e123 = s[s.length - 1];\n\n    if (n.has(_e123.name)) {\n      s.pop();\n      continue;\n    }\n\n    var _t101 = o[o.length - 1] === s.length - 1;\n\n    if (0 === _e123.inputs.length || _t101) s.pop(), r.push(_e123), n.add(_e123.name), _t101 && o.pop();else {\n      o.push(s.length - 1);\n\n      for (var _t102 of _e123.inputs) {\n        null == a[_t102.name] && (a[_t102.name] = new Set()), a[_t102.name].add(_e123.name), n.has(_t102.name) || s.push(_t102);\n      }\n    }\n  }\n\n  return {\n    sorted: r,\n    recipientMap: a\n  };\n}\n\nfunction getNodeOutputs$1(e) {\n  var t;\n  if (1 === e.sourceLayer.inboundNodes.length) t = e.sourceLayer.output;else {\n    var n = null;\n\n    for (var _t103 = 0; _t103 < e.sourceLayer.inboundNodes.length; ++_t103) {\n      for (var r of e.sourceLayer.inboundNodes[_t103].outputTensors) {\n        if (r.id === e.id) {\n          n = _t103;\n          break;\n        }\n      }\n    }\n\n    t = e.sourceLayer.getOutputAt(n);\n  }\n  return t;\n}\n\nclass Container$1 extends Layer$1 {\n  constructor(e) {\n    if (super({}), this.containerNodes = new Set(), this.name = e.name, null == this.name) {\n      var _e124 = this.getClassName().toLowerCase();\n\n      this.name = getUid$1(_e124);\n    }\n\n    if (this.supportsMasking = !1, this.trainable_ = !0, this.inputs = Array.isArray(e.inputs) ? e.inputs.slice() : [e.inputs], this.outputs = Array.isArray(e.outputs) ? e.outputs.slice() : [e.outputs], unique$6(this.inputs).length !== this.inputs.length) throw new ValueError$1(\"The list of inputs passed to the model is redundant. All inputs should only appear once. Found: \".concat(this.inputs.map(e => e.name)));\n    unique$6(this.outputs).length !== this.outputs.length && console.warn(\"The list of outputs passed to the model is redundant. All outputs should only appear once. Found: \".concat(this.outputs.map(e => e.name))), this.inputLayers = [], this.inputLayersNodeIndices = [], this.inputLayersTensorIndices = [], this.outputLayers = [], this.outputLayersNodeIndices = [], this.outputLayersTensorIndices = [], this.layers = [], this.internalContainerRefs = [];\n\n    for (var _e125 of this.outputs) {\n      var _t104 = _e125.nodeIndex,\n          _n45 = _e125.tensorIndex;\n      this.outputLayers.push(_e125.sourceLayer), this.outputLayersNodeIndices.push(_t104), this.outputLayersTensorIndices.push(_n45);\n    }\n\n    for (var _e126 of this.inputs) {\n      var _t105 = _e126.sourceLayer,\n          _n46 = _e126.nodeIndex,\n          _r33 = _e126.tensorIndex;\n      assert$5(0 === _n46, \"input layer has >1 nodes\"), assert$5(0 === _r33, \"input layer has >1 tensors\"), this.inputLayers.push(_t105), this.inputLayersNodeIndices.push(_n46), this.inputLayersTensorIndices.push(_r33);\n    }\n\n    this.inputNames = [], this.outputNames = [], this.feedInputShapes = [], this.feedInputNames = [], this.feedOutputNames = [];\n\n    for (var _t106 = 0; _t106 < this.inputLayers.length; _t106++) {\n      var _n47 = this.inputLayers[_t106];\n      if (!(_n47 instanceof InputLayer$1)) throw new TypeError(\"Input layers to a LayersModel must be InputLayer objects. Received inputs: \".concat(e.inputs, \". Input \").concat(_t106, \" (0-based) originates from layer type \").concat(_n47.getClassName(), \".\"));\n      this.inputNames.push(_n47.name), this.feedInputShapes.push(_n47.batchInputShape), this.feedInputNames.push(_n47.name);\n    }\n\n    for (var _e127 of this.outputLayers) {\n      this.outputNames.push(_e127.name);\n    }\n\n    this.internalInputShapes = this.inputs.map(e => e.shape), this.internalOutputShapes = this.outputs.map(e => e.shape);\n\n    var t = {},\n        n = {},\n        r = {},\n        a = {},\n        s = {},\n        o = [],\n        i = (e, t, n, r, a, l) => {\n      null != r && null != a && null != l || (r = e.sourceLayer, a = e.nodeIndex, l = e.tensorIndex);\n      var u = r.inboundNodes[a];\n      if (-1 !== n.indexOf(u)) throw new RuntimeError$1(\"The tensor \".concat(e.name, \" at layer \\\"\").concat(r.name, \"\\\" is part of a cycle.\"));\n      if (-1 !== t.indexOf(u)) return;\n      this.containerNodes.add(Container$1.nodeKey(r, a)), r.id in s || (s[r.id] = Object.keys(s).length), -1 === n.indexOf(u) && n.push(u);\n      var c = u.inboundLayers.length;\n\n      for (var _e128 = 0; _e128 < c; _e128++) {\n        i(u.inputTensors[_e128], t, n, u.inboundLayers[_e128], u.nodeIndices[_e128], u.tensorIndices[_e128]);\n      }\n\n      for (t.push(u); n.indexOf(u) >= 0;) {\n        n.splice(n.indexOf(u), 1);\n      }\n\n      o.push(u);\n    },\n        l = [],\n        u = [];\n\n    for (var _e129 of this.outputs) {\n      i(_e129, l, u);\n    }\n\n    var c = o.slice().reverse();\n\n    for (var _e130 of c) {\n      n[_e130.id] = _e130, _e130.id in t || (t[_e130.id] = 0);\n      var _s16 = t[_e130.id];\n      _s16 = Math.max(_s16, null == r[_e130.outboundLayer.id] ? 0 : r[_e130.outboundLayer.id]), r[_e130.outboundLayer.id] = _s16, a[_e130.outboundLayer.id] = _e130.outboundLayer, t[_e130.id] = _s16;\n\n      for (var _r34 = 0; _r34 < _e130.inboundLayers.length; _r34++) {\n        var _a28 = _e130.inboundLayers[_r34].inboundNodes[_e130.nodeIndices[_r34]];\n        t[_a28.id] = Math.max(_s16 + 1, null == t[_a28.id] ? 0 : t[_a28.id]), n[_a28.id] = _a28;\n      }\n    }\n\n    var p = {};\n\n    for (var _e131 in t) {\n      var _r35 = t[_e131];\n      _r35 in p || (p[_r35] = []), p[_r35].push(n[_e131]);\n    }\n\n    var d = {};\n\n    for (var _e132 in r) {\n      var _t107 = r[_e132];\n      _t107 in d || (d[_t107] = []), d[_t107].push(a[_e132]);\n    }\n\n    var h = Object.keys(d).map(e => parseInt(e, 10)).sort(reverseNumberCompare$1);\n    this.layers = [];\n\n    for (var _e133 of h) {\n      var _t108 = d[_e133];\n\n      _t108.sort((e, t) => {\n        var n = s[e.id],\n            r = s[t.id];\n        return n < r ? -1 : n > r ? 1 : 0;\n      });\n\n      for (var _e134 of _t108) {\n        _e134 instanceof Container$1 && this.internalContainerRefs.push(_e134), this.layers.push(_e134);\n      }\n    }\n\n    this.layersByDepth = d, h = Object.keys(p).map(e => parseInt(e, 10)).sort(reverseNumberCompare$1);\n    var m = this.inputs.slice(),\n        f = [];\n\n    for (var _e135 of h) {\n      for (var _t109 of p[_e135]) {\n        var _e136 = _t109.outboundLayer;\n\n        if (null != _e136) {\n          for (var _n48 of _t109.inputTensors) {\n            if (-1 === m.indexOf(_n48)) throw new RuntimeError$1(\"Graph disconnected: cannot obtain value for tensor \".concat(_n48, \" at layer \\\"\").concat(_e136.name, \"\\\". The following previous layers were accessed without issue: \").concat(f));\n          }\n\n          for (var _e137 of _t109.outputTensors) {\n            m.push(_e137);\n          }\n\n          f.push(_e136.name);\n        }\n      }\n    }\n\n    this.nodesByDepth = p;\n    var g = this.layers.map(e => e.name);\n\n    var _loop15 = function _loop15(_e138) {\n      var t = g.filter(t => t === _e138).length;\n      if (1 !== t) throw new RuntimeError$1(\"The name \\\"\".concat(_e138, \"\\\" is used \").concat(t, \" times in the model. All layer names should be unique. Layer names: \") + JSON.stringify(g));\n    };\n\n    for (var _e138 of g) {\n      _loop15(_e138);\n    }\n\n    this.outboundNodes = [], this.inboundNodes = [], new Node$1({\n      outboundLayer: this,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: this.inputs,\n      outputTensors: this.outputs,\n      inputMasks: this.inputs.map(e => null),\n      outputMasks: this.outputs.map(e => null),\n      inputShapes: this.inputs.map(e => e.shape),\n      outputShapes: this.outputs.map(e => e.shape)\n    }), this.built = !0, this._refCount = 1;\n  }\n\n  assertNotDisposed() {\n    if (0 === this._refCount) throw new Error(\"Container '\".concat(this.name, \"' is already disposed.\"));\n  }\n\n  dispose() {\n    this.assertNotDisposed();\n    var e = {\n      refCountAfterDispose: null,\n      numDisposedVariables: 0\n    };\n\n    if (0 == --this._refCount) {\n      for (var t of this.layers) {\n        e.numDisposedVariables += t.dispose().numDisposedVariables;\n      }\n\n      for (var _t110 of this.internalContainerRefs) {\n        e.numDisposedVariables += _t110.dispose().numDisposedVariables;\n      }\n    }\n\n    return e.refCountAfterDispose = this._refCount, e;\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(e) {\n    this.layers.forEach(t => {\n      t._trainableWeights.forEach(t => t.trainable = e);\n    }), this.trainable_ = e;\n  }\n\n  get trainableWeights() {\n    if (this._trainableWeights.length > 0) throw new ValueError$1(\"Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.\");\n    if (!this.trainable) return [];\n    var e = [];\n\n    for (var t of this.layers) {\n      e = e.concat(t.trainableWeights);\n    }\n\n    return e;\n  }\n\n  get nonTrainableWeights() {\n    var e = [];\n\n    for (var t of this.layers) {\n      e.push(...t.nonTrainableWeights);\n    }\n\n    if (!this.trainable) {\n      var _t111 = [];\n\n      for (var _e139 of this.layers) {\n        _t111.push(..._e139.trainableWeights);\n      }\n\n      return _t111.concat(e);\n    }\n\n    return e;\n  }\n\n  get weights() {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  loadWeights(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = {};\n    var r = 0;\n\n    for (var _e140 of this.layers) {\n      for (var _t112 of _e140.weights) {\n        if (null != n[_t112.originalName]) throw new ValueError$1(\"Duplicate weight name: \".concat(_t112.originalName));\n        n[_t112.originalName] = _t112, r++;\n      }\n    }\n\n    var a = [];\n\n    for (var _r36 in e) {\n      var s = _r36;\n\n      if (null == n[_r36]) {\n        var _e141 = _r36.split(\"/\");\n\n        s = _e141.slice(0, -2).concat([_e141[_e141.length - 1]]).join(\"/\");\n      }\n\n      if (null != n[s]) a.push([n[s], e[_r36]]);else if (t) throw new ValueError$1(\"Provided weight data has no target variable: \".concat(_r36));\n      delete n[s];\n    }\n\n    if (t) {\n      var _e142 = [];\n\n      for (var _t113 in n) {\n        _e142.push(_t113);\n      }\n\n      if (_e142.length > 0) throw new ValueError$1(\"\".concat(_e142.length, \" of \").concat(r, \" weights are not set: \").concat(_e142));\n    }\n\n    batchSetValue$1(a);\n  }\n\n  updatedConfig() {\n    var e = this.getConfig(),\n        t = {};\n    return t.className = this.getClassName(), t.config = e, t.kerasVersion = \"tfjs-layers \".concat(version$d), t.backend = \"TensorFlow.js\", t;\n  }\n\n  toJSON(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = convertTsToPythonic$1(this.updatedConfig());\n    return t ? JSON.stringify(n) : n;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      e = toList$1(e);\n      var n = new FeedDict$1();\n\n      for (var _t114 = 0; _t114 < this.inputs.length; ++_t114) {\n        n.add(this.inputs[_t114], e[_t114]);\n      }\n\n      return execute$1(this.outputs, n, t);\n    });\n  }\n\n  computeMask(e, t) {\n    return tidy$1(() => {\n      var n;\n      return e = toList$1(e), n = null == t ? pyListRepeat$1(null, e.length) : toList$1(t), this.runInternalGraph(e, n)[1];\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = normalizeShapeList$1(e);\n    if (t.length !== this.inputLayers.length) throw new ValueError$1(\"Invalid inputShape argument \".concat(e, \": model has \").concat(this.inputLayers.length, \" tensor inputs.\"));\n    var n = {};\n\n    for (var _e143 = 0; _e143 < t.length; _e143++) {\n      n[this.inputLayers[_e143].name + \"_0_0\"] = t[_e143];\n    }\n\n    var r = Object.keys(this.nodesByDepth).map(e => parseInt(e, 10)).sort(reverseNumberCompare$1);\n    if (r.length > 1) for (var _e144 of r) {\n      var _t115 = this.nodesByDepth[_e144];\n\n      for (var _e145 of _t115) {\n        var _t116 = _e145.outboundLayer;\n        if (-1 !== this.inputLayers.map(e => e.id).indexOf(_t116.id)) continue;\n        var _r37 = [];\n\n        for (var _t117 = 0; _t117 < _e145.inboundLayers.length; _t117++) {\n          _r37.push(n[\"\".concat(_e145.inboundLayers[_t117].name, \"_\").concat(_e145.nodeIndices[_t117], \"_\").concat(_e145.tensorIndices[_t117])]);\n        }\n\n        var _a29 = normalizeShapeList$1(_t116.computeOutputShape(singletonOrArray$1(_r37))),\n            _s17 = _t116.inboundNodes.indexOf(_e145);\n\n        for (var _e146 = 0; _e146 < _a29.length; _e146++) {\n          n[\"\".concat(_t116.name, \"_\").concat(_s17, \"_\").concat(_e146)] = _a29[_e146];\n        }\n      }\n    }\n    var a = [],\n        s = [];\n\n    for (var _e147 = 0; _e147 < this.outputLayers.length; _e147++) {\n      s.push(\"\".concat(this.outputLayers[_e147].name, \"_\").concat(this.outputLayersNodeIndices[_e147], \"_\").concat(this.outputLayersTensorIndices[_e147]));\n    }\n\n    for (var _e148 = 0; _e148 < s.length; _e148++) {\n      var _t118 = s[_e148];\n      assert$5(_t118 in n), a.push(n[_t118]);\n    }\n\n    return singletonOrArray$1(a);\n  }\n\n  runInternalGraph(e, t) {\n    null == t && (t = pyListRepeat$1(null, e.length));\n    var n = {};\n\n    for (var _r38 = 0; _r38 < this.inputs.length; ++_r38) {\n      n[this.inputs[_r38].id] = [e[_r38], t[_r38]];\n    }\n\n    var r = Object.keys(this.nodesByDepth).map(e => parseInt(e, 10)).sort(reverseNumberCompare$1);\n\n    for (var _e149 of r) {\n      var _t119 = this.nodesByDepth[_e149];\n\n      for (var _e150 of _t119) {\n        var _t120 = _e150.outboundLayer,\n            _r39 = _e150.inputTensors,\n            _a30 = _e150.outputTensors,\n            _s18 = new Array();\n\n        for (var _e151 of _r39) {\n          _e151.id in n && _s18.push(n[_e151.id]);\n        }\n\n        if (_s18.length === _r39.length) {\n          var _r40 = void 0,\n              _o9 = void 0,\n              i = void 0,\n              l = void 0,\n              u = {};\n\n          if (null != _e150.callArgs && (u = _e150.callArgs), 1 === _s18.length) {\n            var [_e152, _n49] = _s18[0];\n            null == u.mask && (u.mask = _n49), i = toList$1(_t120.call(_e152, u)), l = toList$1(_t120.computeMask(_e152, _n49)), _r40 = [_e152], _o9 = [_n49];\n          } else _r40 = _s18.map(e => e[0]), _o9 = _s18.map(e => e[1]), null == u.mask && (u.mask = _o9), i = toList$1(_t120.call(_r40, u)), l = toList$1(_t120.computeMask(_r40, _o9));\n\n          if (_t120.activityRegularizer) throw new NotImplementedError$1(\"LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.\");\n\n          for (var _e153 = 0; _e153 < _a30.length; ++_e153) {\n            n[_a30[_e153].id] = [i[_e153], l[_e153]];\n          }\n        }\n      }\n    }\n\n    var a = [],\n        s = [],\n        o = [];\n\n    for (var _e154 of this.outputs) {\n      assert$5(_e154.id in n, \"Could not compute output \".concat(_e154.name, \" : \").concat(_e154.id));\n      var [_t121, _r41] = n[_e154.id];\n      o.push(_t121.shape), a.push(_t121), s.push(_r41);\n    }\n\n    return [a, s, o];\n  }\n\n  buildNodeConversionMap(e) {\n    var t = {};\n    var n;\n\n    for (var _e155 of this.layers) {\n      n = _e155 instanceof Container$1 ? 1 : 0;\n\n      for (var r = 0; r < _e155.inboundNodes.length; r++) {\n        var a = Container$1.nodeKey(_e155, r);\n        this.containerNodes.has(a) && (t[a] = n, n += 1);\n      }\n    }\n\n    return t;\n  }\n\n  getLayer(e, t) {\n    if (null != t) {\n      if (this.layers.length <= t) throw new ValueError$1(\"Was asked to retrieve layer at index \".concat(t, \", but model only has \").concat(this.layers.length, \" layer(s).\"));\n      return this.layers[t];\n    }\n\n    if (null == e) throw new ValueError$1(\"Provide either a layer name or layer index\");\n\n    for (var _t122 of this.layers) {\n      if (_t122.name === e) return _t122;\n    }\n\n    throw new ValueError$1(\"No such layer: \".concat(e));\n  }\n\n  calculateLosses() {\n    return tidy$1(() => {\n      var e = [];\n\n      for (var t of this.layers) {\n        for (var n = 0; n < t.inboundNodes.length; ++n) {\n          var r = Container$1.nodeKey(t, n);\n          this.containerNodes.has(r) && e.push(...t.calculateLosses());\n        }\n      }\n\n      return e;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      name: this.name\n    },\n        t = this.buildNodeConversionMap(this.layers),\n        n = [];\n\n    for (var _e156 of this.layers) {\n      var _r42 = _e156.getClassName(),\n          _a31 = _e156.getConfig(),\n          s = [];\n\n      for (var _n50 = 0; _n50 < _e156.inboundNodes.length; _n50++) {\n        var _r43 = _e156.inboundNodes[_n50],\n            _a32 = Container$1.nodeKey(_e156, _n50);\n\n        var _o10 = {};\n\n        if (this.containerNodes.has(_a32)) {\n          if (_r43.callArgs) try {\n            JSON.stringify(_r43.callArgs), _o10 = _r43.callArgs;\n          } catch (t) {\n            console.warn(\"Layer \".concat(_e156.name, \" was passed non-serializable keyword arguments: \").concat(_r43.callArgs, \". They will not be included in the serialized model (and thus will be missing at deserialization time).\")), _o10 = {};\n          }\n\n          if (_r43.inboundLayers.length > 0) {\n            var _e157 = [];\n\n            for (var _n51 = 0; _n51 < _r43.inboundLayers.length; _n51++) {\n              var _a33 = _r43.inboundLayers[_n51],\n                  _s19 = _r43.tensorIndices[_n51];\n              var i = t[Container$1.nodeKey(_a33, _r43.nodeIndices[_n51])];\n              null == i && (i = 0), _e157.push([_a33.name, i, _s19, _o10]);\n            }\n\n            s.push(_e157);\n          }\n        }\n      }\n\n      var o = {};\n      o.name = _e156.name, o.className = _r42, o.config = _a31, o.inboundNodes = s, n.push(o);\n    }\n\n    e.layers = n;\n    var r = [];\n\n    for (var _e158 = 0; _e158 < this.inputLayers.length; _e158++) {\n      var _n52 = this.inputLayers[_e158],\n          _a34 = Container$1.nodeKey(_n52, this.inputLayersNodeIndices[_e158]);\n\n      if (!this.containerNodes.has(_a34)) continue;\n      var _s20 = t[_a34];\n      null == _s20 && (_s20 = 0), r.push([_n52.name, _s20, this.inputLayersTensorIndices[_e158]]);\n    }\n\n    e.inputLayers = r;\n    var a = [];\n\n    for (var _e159 = 0; _e159 < this.outputLayers.length; _e159++) {\n      var _n53 = this.outputLayers[_e159],\n          _r44 = Container$1.nodeKey(_n53, this.outputLayersNodeIndices[_e159]);\n\n      if (!this.containerNodes.has(_r44)) continue;\n      var _s21 = t[_r44];\n      null == _s21 && (_s21 = 0), a.push([_n53.name, _s21, this.outputLayersTensorIndices[_e159]]);\n    }\n\n    return e.outputLayers = a, e;\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = {},\n        s = {};\n\n    function o(e, t) {\n      e.name in s ? s[e.name].push(t) : s[e.name] = [t];\n    }\n\n    function i(e, t) {\n      var n = [];\n      var r;\n\n      for (var _s22 of t) {\n        var _i5 = _s22[0],\n            _l5 = _s22[1],\n            _u3 = _s22[2];\n        if (r = null == _s22[3] ? {} : _s22[3], !(_i5 in a)) return void o(e, t);\n        var _c3 = a[_i5];\n        if (_c3.inboundNodes.length <= _l5) return void o(e, t);\n        n.push(_c3.inboundNodes[_l5].outputTensors[_u3]);\n      }\n\n      n.length > 0 && e.apply(singletonOrArray$1(n), r);\n    }\n\n    function l(e) {\n      var n = e.name,\n          s = deserialize$1(e, null != t.customObjects ? t.customObjects : {});\n      s.setFastWeightInitDuringBuild(r), a[n] = s, e.inboundNodes.forEach(e => {\n        if (!(e instanceof Array)) throw new ValueError$1(\"Corrupted configuration, expected array for nodeData: \".concat(e));\n        o(s, e);\n      });\n    }\n\n    var u = t.name,\n        c = t.layers;\n\n    for (var _e160 of c) {\n      l(_e160);\n    }\n\n    for (; !isObjectEmpty$1(s);) {\n      for (var _e161 of c) {\n        var _t123 = a[_e161.name];\n\n        if (_t123.name in s) {\n          var _e162 = s[_t123.name];\n          delete s[_t123.name];\n\n          for (var _n54 of _e162) {\n            i(_t123, _n54);\n          }\n        }\n      }\n    }\n\n    var p = [],\n        d = [],\n        h = t.inputLayers;\n\n    for (var _e163 of h) {\n      var _t124 = _e163[0],\n          _n55 = _e163[1],\n          _r45 = _e163[2];\n      assert$5(_t124 in a), p.push(a[_t124].inboundNodes[_n55].outputTensors[_r45]);\n    }\n\n    var m = t.outputLayers;\n\n    for (var _e164 of m) {\n      var _t125 = _e164[0],\n          _n56 = _e164[1],\n          _r46 = _e164[2];\n      assert$5(_t125 in a), d.push(a[_t125].inboundNodes[_n56].outputTensors[_r46]);\n    }\n\n    return new e({\n      inputs: p,\n      outputs: d,\n      name: u\n    });\n  }\n\n  get stateful() {\n    if (this._stateful) throw new ValueError$1(\"Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.\");\n\n    for (var _e165 of this.layers) {\n      if (_e165.stateful) return !0;\n    }\n\n    return !1;\n  }\n\n  resetStates() {\n    tidy$1(() => {\n      this.layers.forEach(e => {\n        e.stateful && e.resetStates();\n      });\n    });\n  }\n\n}\n\nfunction standardizeSampleOrClassWeights$1(e, t, n) {\n  var r = t.length;\n  if (null == e || Array.isArray(e) && 0 === e.length) return t.map(e => null);\n  if (1 === r) return Array.isArray(e) && 1 === e.length ? e : \"object\" == typeof e && t[0] in e ? [e[t[0]]] : [e];\n\n  if (Array.isArray(e)) {\n    if (e.length !== r) throw new Error(\"Provided \".concat(n, \" is an array of \").concat(e.length, \" element(s), but the model has \").concat(r, \" outputs. Make sure a set of weights is provided for each model output.\"));\n    return e;\n  }\n\n  if (\"object\" == typeof e && Object.keys(e).length > 0 && \"object\" == typeof e[Object.keys(e)[0]]) {\n    var _n57 = [];\n    return t.forEach(t => {\n      _n57.push(t in e ? e[t] : null);\n    }), _n57;\n  }\n\n  throw new Error(\"The model has multiple (\".concat(r, \") outputs, so \").concat(n, \" must be either an array with \").concat(r, \" elements or an object with \").concat(t, \" keys. Provided \").concat(n, \" not understood: \").concat(JSON.stringify(e)));\n}\n\nfunction standardizeClassWeights$1(e, t) {\n  return standardizeSampleOrClassWeights$1(e, t, \"classWeight\");\n}\n\nfunction standardizeWeights$1(_x17, _x18, _x19, _x20) {\n  return _standardizeWeights$.apply(this, arguments);\n}\n\nfunction _standardizeWeights$() {\n  _standardizeWeights$ = _asyncToGenerator(function* (e, t, n, r) {\n    if (null != t || null != r) throw new Error(\"Support sampleWeight is not implemented yet\");\n\n    if (null != n) {\n      var _t761 = tidy$1(() => {\n        if (1 === e.shape.length) return clone$1(e);\n\n        if (2 === e.shape.length) {\n          if (e.shape[1] > 1) return argMax$5(e, 1);\n          if (1 === e.shape[1]) return reshape$6(e, [e.shape[0]]);\n          throw new Error(\"Encountered unexpected last-dimension size (\".concat(e.shape[1], \") during handling of class weights. The size is expected to be >= 1.\"));\n        }\n\n        throw new Error(\"Unexpected rank of target (y) tensor (\".concat(e.rank, \") during handling of class weights. The rank is expected to be 1 or 2.\"));\n      }),\n          _r445 = Array.from(yield _t761.data());\n\n      dispose$1(_t761);\n      var a = [];\n      return _r445.forEach(e => {\n        if (null == n[e]) throw new Error(\"classWeight must contain all classes in the training data. The class \".concat(e, \" exists in the data but not in classWeight\"));\n        a.push(n[e]);\n      }), tensor1d$1(a, \"float32\");\n    }\n\n    return null;\n  });\n  return _standardizeWeights$.apply(this, arguments);\n}\n\nfunction computeWeightedLoss$2(e, t) {\n  return mul$1(e, t);\n}\n\nvar DEFAULT_VALIDATION_BATCH_SIZE$1 = 32;\n\nfunction standardizeDataIteratorOutput$1(e, t) {\n  var n, r;\n  n = t.xs, r = t.ys, assert$6(null != n && null != r, () => \"A Dataset iterator for fitDataset() is expected to generate objects of the form `{xs: xVal, ys: yVal}`, where the two values may be `tf.Tensor`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates \".concat(t));\n  var a = flattenTensorOrArrayOrMap$1(\"input\", e.inputNames, n),\n      s = flattenTensorOrArrayOrMap$1(\"output\", e.outputNames, r),\n      o = a[0].shape[0];\n  assert$6(a.length === e.inputs.length, () => \"LayersModel has \".concat(e.inputs.length, \" inputs, but the dataset provides \").concat(a.length, \" inputs.  (Expected input keys: \").concat(JSON.stringify(e.inputNames), \")\")), assert$6(s.length === e.outputs.length, () => \"LayersModel has \".concat(e.outputs.length, \" outputs, but the dataset provides \").concat(s.length, \" outputs.  (Expected output keys: \").concat(JSON.stringify(e.outputNames), \")\"));\n\n  var _loop16 = function _loop16(_t126) {\n    assert$6(a[_t126].shape[0] === o, () => \"Batch size mismatch: input \".concat(e.inputNames[_t126], \" has \").concat(a[_t126].shape[0], \"; expected  \").concat(o, \" based on input \").concat(e.inputNames[0], \".\"));\n  };\n\n  for (var _t126 = 0; _t126 < a.length; _t126++) {\n    _loop16(_t126);\n  }\n\n  var _loop17 = function _loop17(_t127) {\n    assert$6(s[_t127].shape[0] === o, () => \"Batch size mismatch: output \".concat(e.outputNames[_t127], \" has \").concat(s[_t127].shape[0], \"; expected  \").concat(o, \" based on input \").concat(e.inputNames[0], \".\"));\n  };\n\n  for (var _t127 = 0; _t127 < s.length; _t127++) {\n    _loop17(_t127);\n  }\n\n  return {\n    xs: a,\n    ys: s\n  };\n}\n\nfunction flattenTensorOrArrayOrMap$1(e, t, n) {\n  if (n instanceof Tensor$1) return [n];\n  if (Array.isArray(n)) return assert$6(n.length === t.length, () => \"Received an array of \".concat(n.length, \" Tensors, but expected \").concat(t.length, \" to match the \").concat(e, \" keys \").concat(t, \".\")), n;\n  {\n    var r = [];\n\n    for (var a of t) {\n      if (null == n[a]) throw new ValueError$1(\"The feature data generated by the dataset lacks the required \".concat(e, \" key '\").concat(a, \"'.\"));\n      r.push(n[a]);\n    }\n\n    return r;\n  }\n}\n\nfunction standardizeTensorValidationData$1(e) {\n  if (3 === e.length) throw new NotImplementedError$1(\"Validation with sample weights is not implemented yet.\");\n  return {\n    xs: e[0],\n    ys: e[1]\n  };\n}\n\nfunction fitDataset$1(_x21, _x22, _x23) {\n  return _fitDataset$.apply(this, arguments);\n}\n\nfunction _fitDataset$() {\n  _fitDataset$ = _asyncToGenerator(function* (e, t, n) {\n    var r = null != n.batchesPerEpoch;\n    if (assert$6(null != e.optimizer, () => \"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).\"), assert$6(null != n, () => \"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.\"), assert$6(null != n.epochs && n.epochs > 0 && Number.isInteger(n.epochs), () => \"For fitDataset(), config.epochs is expected to be a positive integer, but got \".concat(n.epochs)), assert$6(!r || n.batchesPerEpoch > 0 && Number.isInteger(n.batchesPerEpoch), () => \"For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got \".concat(n.batchesPerEpoch)), assert$6(null == n.validationSplit, () => \"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.\"), e.isTraining) throw new Error(\"Cannot start training because another fit() call is ongoing.\");\n    e.isTraining = !0;\n\n    try {\n      var a = null != n.validationData;\n      var s, o;\n      if (a) if (isDatasetObject$1(n.validationData)) assert$6(null == n.validationBatches || n.validationBatches > 0 && Number.isInteger(n.validationBatches), () => \"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got \".concat(n.validationBatches));else {\n        var _e1130 = standardizeTensorValidationData$1(n.validationData);\n\n        s = _e1130.xs, o = _e1130.ys;\n      }\n      var i = e.makeTrainFunction(),\n          l = e.getDedupedMetricsNames();\n      var u;\n      u = a ? l.slice().concat(l.map(e => \"val_\" + e)) : l.slice();\n\n      var c = standardizeCallbacks$1(n.callbacks, n.yieldEvery),\n          _p40 = null == n.verbose ? 1 : n.verbose,\n          {\n        callbackList: d,\n        history: h\n      } = configureCallbacks$1(c, _p40, n.epochs, null, null, getStepsPerEpoch$1(t, n), null, a, u);\n\n      d.setModel(e), e.history = h, yield d.onTrainBegin(), e.stopTraining_ = !1;\n      var m = null == n.initialEpoch ? 0 : n.initialEpoch,\n          f = yield t.iterator();\n\n      for (; m < n.epochs;) {\n        var _u64 = {};\n        yield d.onEpochBegin(m);\n        var _c42 = 0,\n            _p41 = 0;\n\n        for (r || (f = yield t.iterator()); !r || _c42 < n.batchesPerEpoch;) {\n          var _t762 = yield f.next();\n\n          if (r && _t762.done) {\n            console.warn(\"You provided `batchesPerEpoch` as \".concat(n.batchesPerEpoch, \", but your dataset iterator ran out of data after \").concat(_c42, \" batches; interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, \") + n.batchesPerEpoch * n.epochs + \" batches). You may need to use the repeat() function when building your dataset.\");\n            break;\n          }\n\n          if (null != _t762.value) {\n            var {\n              xs: _r446,\n              ys: _a317\n            } = standardizeDataIteratorOutput$1(e, _t762.value),\n                _s224 = {};\n            _s224.batch = _p41, _s224.size = _r446[0].shape[0], yield d.onBatchBegin(_p41, _s224);\n            var _o159 = [];\n\n            if (null != n.classWeight) {\n              var _t763 = standardizeClassWeights$1(n.classWeight, e.outputNames);\n\n              for (var _e1131 = 0; _e1131 < _t763.length; ++_e1131) {\n                _o159.push(yield standardizeWeights$1(_a317[_e1131], null, _t763[_e1131]));\n              }\n            }\n\n            var _u65 = _r446.concat(_a317).concat(_o159),\n                _h25 = i(_u65);\n\n            dispose$1(_u65);\n\n            for (var _e1132 = 0; _e1132 < l.length; ++_e1132) {\n              var _t764 = _h25[_e1132];\n              _s224[l[_e1132]] = _t764, keep$1(_t764);\n            }\n\n            yield d.onBatchEnd(_p41, _s224), disposeTensorsInLogs$1(_s224), _p41++, _c42++;\n          }\n\n          if (r ? _c42 >= n.batchesPerEpoch : _t762.done) {\n            if (a) {\n              var _t765 = void 0;\n\n              _t765 = isDatasetObject$1(n.validationData) ? toList$1(yield e.evaluateDataset(n.validationData, {\n                batches: n.validationBatches\n              })) : toList$1(e.evaluate(s, o, {\n                batchSize: null == n.validationBatchSize ? DEFAULT_VALIDATION_BATCH_SIZE$1 : n.validationBatchSize,\n                verbose: 0\n              }));\n\n              for (var _n451 = 0; _n451 < e.metricsNames.length; ++_n451) {\n                _u64[\"val_\".concat(e.metricsNames[_n451])] = _t765[_n451];\n              }\n            }\n\n            break;\n          }\n\n          if (e.stopTraining_) break;\n        }\n\n        if (yield d.onEpochEnd(m, _u64), m++, e.stopTraining_) break;\n      }\n\n      return yield d.onTrainEnd(), yield e.history.syncData(), e.history;\n    } finally {\n      e.isTraining = !1;\n    }\n  });\n  return _fitDataset$.apply(this, arguments);\n}\n\nfunction getStepsPerEpoch$1(e, t) {\n  var n = null;\n  return null != t.batchesPerEpoch ? n = t.batchesPerEpoch : Number.isFinite(e.size) && (n = e.size), n;\n}\n\nfunction isDatasetObject$1(e) {\n  return \"function\" == typeof e.iterator;\n}\n\nfunction isLazyIteratorObject$1(e) {\n  return \"function\" == typeof e.next;\n}\n\nfunction evaluateDataset$1(_x24, _x25, _x26) {\n  return _evaluateDataset$.apply(this, arguments);\n}\n\nfunction _evaluateDataset$() {\n  _evaluateDataset$ = _asyncToGenerator(function* (e, t, n) {\n    var r = null != (n = n || {}).batches,\n        a = e.testFunction;\n    var s = [];\n    if (n.verbose > 0) throw new NotImplementedError$1(\"Verbose mode is not implemented yet.\");\n    assert$6(!r || n.batches > 0 && Number.isInteger(n.batches), () => \"Test loop expects `batches` to be a positive integer, but received \".concat(JSON.stringify(n.batches)));\n    var o = isLazyIteratorObject$1(t) ? t : yield t.iterator();\n    var i = 0,\n        l = 0;\n\n    var _loop61 = function* _loop61() {\n      var t = yield o.next();\n\n      if (s = tidy$1(() => {\n        if (t.value) {\n          (function () {\n            var {\n              xs: n,\n              ys: r\n            } = standardizeDataIteratorOutput$1(e, t.value),\n                o = n.concat(r),\n                u = tidy$1(() => a(o));\n            if (dispose$1(o), 0 === l) for (var _e1134 = 0; _e1134 < u.length; ++_e1134) {\n              s.push(scalar$1(0));\n            }\n            var c = o[0].shape[0];\n\n            var _loop62 = function _loop62(_e1135) {\n              var t = u[_e1135],\n                  n = s[_e1135];\n              s[_e1135] = tidy$1(() => add$5(s[_e1135], mul$1(c, t))), l > 0 && dispose$1(n);\n            };\n\n            for (var _e1135 = 0; _e1135 < u.length; ++_e1135) {\n              _loop62(_e1135);\n            }\n\n            dispose$1(u), i += c, ++l;\n          })();\n        }\n\n        return s;\n      }), t.done) {\n        r && console.warn(\"Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least `batches` batches (in this case, \".concat(n.batches, \" batches). You may need to use the repeat() function when building your dataset.\"));\n        return \"break\";\n      }\n    };\n\n    for (; !r || l < n.batches;) {\n      var _ret5 = yield* _loop61();\n\n      if (_ret5 === \"break\") break;\n    }\n\n    for (var _e1133 = 0; _e1133 < s.length; ++_e1133) {\n      var _t766 = s[_e1133];\n      s[_e1133] = div$3(s[_e1133], i), dispose$1(_t766);\n    }\n\n    return singletonOrArray$1(s);\n  });\n  return _evaluateDataset$.apply(this, arguments);\n}\n\nfunction checkBatchSize$1(e) {\n  assert$6(e > 0 && Number.isInteger(e), () => \"batchSize is required to be a positive integer, but got \".concat(e));\n}\n\nfunction sliceArrays$1(e, t, n) {\n  return null == e ? [null] : Array.isArray(e) ? e.map(e => sliceAlongFirstAxis$1(e, t, n - t)) : sliceAlongFirstAxis$1(e, t, n - t);\n}\n\nfunction sliceArraysByIndices$1(e, t) {\n  return tidy$1(() => null == e ? null : Array.isArray(e) ? e.map(e => sliceArraysByIndices$1(e, t)) : gather$2(e, \"int32\" === t.dtype ? t : cast$7(t, \"int32\")));\n}\n\nfunction makeBatches$1(e, t) {\n  var n = [];\n  var r = 0,\n      a = null;\n\n  for (; r < e;) {\n    a = r + t, a >= e && (a = e), n.push([r, a]), r = a;\n  }\n\n  return n;\n}\n\nfunction fitLoop$1(_x27, _x28, _x29, _x30, _x31, _x32, _x33, _x34, _x35, _x36, _x37, _x38, _x39, _x40, _x41) {\n  return _fitLoop$.apply(this, arguments);\n}\n\nfunction _fitLoop$() {\n  _fitLoop$ = _asyncToGenerator(function* (e, t, n, r, a, s, o, i, l, u, c, p, d, h, m) {\n    null == a && (a = 32), null == s && (s = 1), null == c && (c = !0), null == d && (d = 0);\n    var f = !1;\n    if (null != l && null != u && (f = !0), null != m && (f = !0, null == h)) throw new ValueError$1(\"Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.\");\n    var g = e.checkNumSamples(n, a, h, \"steps_per_epoch\");\n    var $;\n    null != g && ($ = range$7(0, g)), null == o && (o = 1);\n    var {\n      callbackList: y,\n      history: b\n    } = configureCallbacks$1(i, o, s, d, g, h, a, f, p);\n    y.setModel(e), e.history = b, yield y.onTrainBegin(), e.stopTraining_ = !1;\n\n    var _loop63 = function* _loop63(_o160) {\n      yield y.onEpochBegin(_o160);\n      var s = {};\n      if (null != h) throw new NotImplementedError$1(\"stepsPerEpoch mode is not implemented yet.\");\n      {\n        yield* function* () {\n          if (\"batch\" === c) throw new NotImplementedError$1(\"batch shuffling is not implemneted yet\");\n          c && shuffle$1($);\n          var o = tensor1d$1($),\n              i = makeBatches$1(g, a);\n\n          var _loop64 = function* _loop64(_c43) {\n            var p = {};\n            if (yield y.onBatchBegin(_c43, p), tidy$1(() => {\n              var d = i[_c43][0],\n                  h = i[_c43][1],\n                  m = sliceAlongFirstAxis$1(o, d, h - d);\n              p.batch = _c43, p.size = h - d;\n              var g = sliceArraysByIndices$1(n, m),\n                  $ = t(g);\n\n              for (var _e1136 = 0; _e1136 < r.length; ++_e1136) {\n                var _t767 = $[_e1136];\n                p[r[_e1136]] = _t767, keep$1(_t767);\n              }\n\n              if (_c43 === i.length - 1 && f) {\n                var _t768 = e.testLoop(l, u, a);\n\n                for (var _e1137 = 0; _e1137 < r.length; ++_e1137) {\n                  var _n452 = r[_e1137],\n                      _a318 = _t768[_e1137];\n                  keep$1(_a318), s[\"val_\" + _n452] = _a318;\n                }\n              }\n            }), yield y.onBatchEnd(_c43, p), disposeTensorsInLogs$1(p), e.stopTraining_) return \"break\";\n          };\n\n          for (var _c43 = 0; _c43 < i.length; ++_c43) {\n            var _ret7 = yield* _loop64(_c43);\n\n            if (_ret7 === \"break\") break;\n          }\n\n          o.dispose();\n        }();\n      }\n      if (yield y.onEpochEnd(_o160, s), e.stopTraining_) return \"break\";\n    };\n\n    for (var _o160 = d; _o160 < s; ++_o160) {\n      var _ret6 = yield* _loop63(_o160);\n\n      if (_ret6 === \"break\") break;\n    }\n\n    return yield y.onTrainEnd(), yield e.history.syncData(), e.history;\n  });\n  return _fitLoop$.apply(this, arguments);\n}\n\nfunction fitTensors$1(_x42, _x43, _x44) {\n  return _fitTensors$.apply(this, arguments);\n}\n\nfunction _fitTensors$() {\n  _fitTensors$ = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : {};\n    if (e.isTraining) throw new Error(\"Cannot start training because another fit() call is ongoing.\");\n    var a, s, o, i, l, u, c;\n    e.isTraining = !0;\n\n    try {\n      var _p42 = null == r.batchSize ? 32 : r.batchSize;\n\n      checkBatchSize$1(_p42);\n      var d = !1,\n          h = yield e.standardizeUserData(t, n, r.sampleWeight, r.classWeight, d, _p42);\n      a = h[0], s = h[1], c = h[2];\n      var m,\n          f = !1;\n\n      if (null != r.validationData && r.validationData.length > 0) {\n        if (f = !0, 2 !== r.validationData.length) throw 3 === r.validationData.length ? new NotImplementedError$1(\"validationData including sample weights is not supported yet.\") : new ValueError$1(\"When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; \".concat(r.validationData, \" is invalid.\"));\n        o = r.validationData[0], i = r.validationData[1];\n\n        var _t769 = !0,\n            _n453 = yield e.standardizeUserData(o, i, null, null, _t769, _p42);\n\n        l = _n453[0], u = _n453[1], m = l.concat(u);\n      } else if (null != r.validationSplit && r.validationSplit > 0 && r.validationSplit < 1) {\n        f = !0;\n\n        var _e1138 = Math.floor(a[0].shape[0] * (1 - r.validationSplit)),\n            _t770 = a[0].shape[0];\n\n        l = sliceArrays$1(a, _e1138, _t770), a = sliceArrays$1(a, 0, _e1138), u = sliceArrays$1(s, _e1138, _t770), s = sliceArrays$1(s, 0, _e1138), m = l.concat(u);\n      } else null != r.validationSteps && (f = !0);\n\n      var g = a.concat(s).concat(c);\n      e.checkTrainableWeightsConsistency();\n      var $ = e.makeTrainFunction(),\n          y = e.getDedupedMetricsNames();\n      var b, x;\n      f ? (e.makeTestFunction(), b = e.testFunction, x = y.slice().concat(y.map(e => \"val_\" + e))) : (b = null, m = [], x = y.slice());\n      var v = standardizeCallbacks$1(r.callbacks, r.yieldEvery);\n      return yield fitLoop$1(e, $, g, y, _p42, r.epochs, r.verbose, v, b, m, r.shuffle, x, r.initialEpoch, null, null);\n    } finally {\n      e.isTraining = !1, disposeNewTensors$1(a, t), disposeNewTensors$1(s, n), disposeNewTensors$1(l, o), disposeNewTensors$1(u, i), null != c && dispose$1(c);\n    }\n  });\n  return _fitTensors$.apply(this, arguments);\n}\n\nfunction ensureTensorsRank2OrHigher$1(e) {\n  var t = [];\n  e instanceof Tensor$1 && (e = [e]);\n\n  for (var n = 0; n < e.length; ++n) {\n    var r = e[n];\n    if (1 === r.rank) t.push(expandDims$6(r, 1));else {\n      if (0 === r.rank) throw new Error(\"Expected tensor to be at least 1D, but received a 0D tensor (scalar).\");\n      t.push(r);\n    }\n  }\n\n  return t;\n}\n\nfunction disposeNewTensors$1(e, t) {\n  if (null == e) return;\n  var n = [];\n  if (t instanceof Tensor$1) n.push(t.id);else if (Array.isArray(t)) t.forEach(e => n.push(e.id));else if (null != t) for (var _e166 in t) {\n    n.push(t[_e166].id);\n  }\n  var r = [];\n  if (e instanceof Tensor$1) -1 === n.indexOf(e.id) && r.push(e);else if (Array.isArray(e)) e.forEach(e => {\n    -1 === n.indexOf(e.id) && r.push(e);\n  });else if (null != e) for (var _t128 in e) {\n    var a = e[_t128];\n    -1 === n.indexOf(a.id) && r.push(a);\n  }\n  r.forEach(e => {\n    e.isDisposed || e.dispose();\n  });\n}\n\nfunction isDataTensor$1(e) {\n  return e instanceof Tensor$1;\n}\n\nfunction isDataArray$1(e) {\n  return Array.isArray(e);\n}\n\nfunction isDataDict$1(e) {\n  return !isDataTensor$1(e) && !isDataArray$1(e);\n}\n\nfunction standardizeInputData$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"\";\n\n  if (null == t || 0 === t.length) {\n    if (null != e) {\n      var _t129 = !1;\n\n      if (isDataArray$1(e) && e.length > 0) _t129 = !0;else if (isDataDict$1(e)) {\n        for (var _n58 in e) {\n          if (e.hasOwnProperty(_n58)) {\n            _t129 = !0;\n            break;\n          }\n        }\n      } else _t129 = !0;\n      if (_t129) throw new ValueError$1(\"Error when checking model \".concat(a, \" expected no data, but got \").concat(e));\n    }\n\n    return [];\n  }\n\n  if (null == e) return t.map(e => null);\n  var s;\n\n  if (isDataDict$1(e)) {\n    e = e, s = [];\n\n    for (var _n59 of t) {\n      if (null == e[_n59]) throw new ValueError$1(\"No data provided for \\\"\".concat(_n59, \"\\\". Need data for each key in: \").concat(t));\n      s.push(e[_n59]);\n    }\n  } else if (isDataArray$1(e)) {\n    if ((e = e).length !== t.length) throw new ValueError$1(\"Error when checking model \".concat(a, \": the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see \").concat(t.length, \" Tensor(s), but instead got the following list of Tensor(s): \").concat(e));\n    s = e;\n  } else {\n    if (e = e, t.length > 1) throw new ValueError$1(\"The model \".concat(a, \" expects \").concat(t.length, \" Tensor(s), but only received one Tensor. Found: Tensor with shape \").concat(e.shape));\n    s = [e];\n  }\n\n  if (s = ensureTensorsRank2OrHigher$1(s), null != n) for (var _e167 = 0; _e167 < t.length; ++_e167) {\n    if (null == n[_e167]) continue;\n    var o = s[_e167];\n    if (o.shape.length !== n[_e167].length) throw new ValueError$1(\"Error when checking \".concat(a, \": expected \").concat(t[_e167], \" to have \").concat(n[_e167].length, \" dimension(s). but got array with shape \").concat(o.shape));\n\n    for (var _s23 = 0; _s23 < n[_e167].length; ++_s23) {\n      if (0 === _s23 && !r) continue;\n      var i = o.shape[_s23],\n          l = n[_e167][_s23];\n      if (null != l && l >= 0 && i !== l) throw new ValueError$1(\"Error when checking \".concat(a, \": expected \").concat(t[_e167], \" to have shape [\").concat(n[_e167], \"], but got array with shape [\").concat(o.shape, \"].\"));\n    }\n  }\n  return s;\n}\n\nfunction checkArrayLengths$1(e, t, n) {\n  var r = unique$6(e.map(e => e.shape[0]));\n  r.sort();\n  var a = unique$6(t.map(e => e.shape[0]));\n  if (a.sort(), r.length > 1) throw new ValueError$1(\"All input Tensors (x) should have the same number of samples. Got array shapes: \".concat(JSON.stringify(e.map(e => e.shape))));\n  if (a.length > 1) throw new ValueError$1(\"All target Tensors (y) should have the same number of samples. Got array shapes: \".concat(JSON.stringify(t.map(e => e.shape))));\n  if (r.length > 0 && a.length > 0 && !arraysEqual$1(r, a)) throw new ValueError$1(\"Input Tensors should have the same number of samples as target Tensors. Found \".concat(r[0], \" input sample(s) and \").concat(a[0], \" target sample(s).\"));\n}\n\nfunction checkLossAndTargetCompatibility$1(e, t, n) {\n  var r = [meanSquaredError$3, binaryCrossentropy$4, categoricalCrossentropy$4];\n\n  for (var a = 0; a < e.length; ++a) {\n    var s = e[a],\n        o = t[a],\n        i = n[a];\n\n    if (null != o) {\n      if (o === categoricalCrossentropy$4 && 1 === s.shape[s.shape.length - 1]) throw new ValueError$1(\"You are passing a target array of shape \".concat(s.shape, \" while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].\"));\n\n      if (-1 !== r.indexOf(o)) {\n        var _e168 = s.shape.slice(1),\n            _t130 = i.slice(1);\n\n        for (var _n60 = 0; _n60 < _e168.length; ++_n60) {\n          var _r47 = _e168[_n60],\n              _a35 = _t130[_n60];\n          if (null != _a35 && _r47 !== _a35) throw new ValueError$1(\"A target Tensor with shape \".concat(s.shape, \" was passed for an output of shape \").concat(i, \", while using a loss function that expects targets to have the same shape as the output.\"));\n        }\n      }\n    }\n  }\n}\n\nfunction checkInputData$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"\";\n  var s;\n\n  if (Array.isArray(e)) {\n    if (e.length !== t.length) throw new ValueError$1(\"Error when checking model \".concat(a, \": the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see \").concat(t.length, \" Tensor(s), but instead got \").concat(e.length, \" Tensors(s).\"));\n    s = e;\n  } else {\n    if (t.length > 1) throw new ValueError$1(\"The model expects \".concat(t.length, \" \").concat(a, \" Tensors, but only received one Tensor. Found: array with shape \").concat(JSON.stringify(e.shape), \".\"));\n    s = [e];\n  }\n\n  if (null != n) for (var _e169 = 0; _e169 < t.length; ++_e169) {\n    if (null == n[_e169]) continue;\n    var o = s[_e169];\n    if (o.shape.length !== n[_e169].length) throw new ValueError$1(\"Error when checking \".concat(a, \": expected \").concat(t[_e169], \" to have \").concat(n[_e169].length, \" dimension(s), but got array with shape \").concat(JSON.stringify(o.shape)));\n\n    for (var _s24 = 0; _s24 < n[_e169].length; ++_s24) {\n      if (0 === _s24 && !r) continue;\n      var i = o.shape[_s24],\n          l = n[_e169][_s24];\n      if (null != l && l !== i) throw new ValueError$1(\"Error when checking \".concat(a, \": expected \").concat(t[_e169], \" to have shape \").concat(JSON.stringify(n[_e169]), \" but got array with shape \").concat(JSON.stringify(o.shape), \".\"));\n    }\n  }\n}\n\nfunction collectMetrics$1(e, t) {\n  if (null == e || Array.isArray(e) && 0 === e.length) return t.map(e => []);\n  var n;\n  if (\"string\" == typeof e || \"function\" == typeof e) n = [e];else {\n    if (!Array.isArray(e) && \"object\" != typeof e) throw new TypeError(\"Type of metrics argument not understood. Expected an string,function, Array, or Object, found: \".concat(e));\n    n = e;\n  }\n  if (Array.isArray(n)) return t.map(e => n);\n  {\n    var _e170 = [];\n\n    for (var r of t) {\n      var _t131 = n.hasOwnProperty(r) ? n[r] : [];\n\n      Array.isArray(_t131) || (_t131 = [_t131]), _e170.push(_t131);\n    }\n\n    return _e170;\n  }\n}\n\nvar LAYERS_MODEL_FORMAT_NAME$1 = \"layers-model\";\n\nclass LayersModel$1 extends Container$1 {\n  constructor(e) {\n    super(e), this.isTraining = !1;\n  }\n\n  summary(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : console.log;\n    if (!this.built) throw new ValueError$1(\"This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).\");\n    printSummary$1(this, e, t, n);\n  }\n\n  compile(e) {\n    var _this57 = this;\n\n    if (null == e.loss && (e.loss = []), this.loss = e.loss, \"string\" == typeof e.optimizer) this.optimizer_ = getOptimizer$1(e.optimizer), this.isOptimizerOwned = !0;else {\n      if (!(e.optimizer instanceof Optimizer$1)) throw new ValueError$1(\"User-defined optimizer must be an instance of tf.Optimizer.\");\n      this.optimizer_ = e.optimizer, this.isOptimizerOwned = !1;\n    }\n    var t = [];\n    if (Array.isArray(e.loss) || \"string\" == typeof e.loss || \"function\" == typeof e.loss) {\n      if (Array.isArray(e.loss)) {\n        if (e.loss.length !== this.outputs.length) throw new ValueError$1(\"When passing an Array as loss, it should have one entry per model output. The model has \".concat(this.outputs.length, \" output(s), but you passed loss=\").concat(e.loss, \".\"));\n        t = e.loss.map(e => get$3(e));\n      } else {\n        var _n61 = get$3(e.loss);\n\n        this.outputs.forEach(e => {\n          t.push(_n61);\n        });\n      }\n    } else {\n      e.loss = e.loss;\n\n      for (var _t132 in e.loss) {\n        if (-1 === this.outputNames.indexOf(_t132)) throw new ValueError$1(\"Unknown entry in loss dictionary: \\\"\".concat(_t132, \"\\\". Only expected the following keys: \").concat(this.outputNames));\n      }\n\n      for (var _n62 of this.outputNames) {\n        null == e.loss[_n62] && console.warn(\"Output \\\"\".concat(_n62, \"\\\" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to \").concat(_n62, \" during training\")), t.push(get$3(e.loss[_n62]));\n      }\n    }\n    this.lossFunctions = t, this.feedOutputNames = [], this.feedOutputShapes = [], this.feedLossFns = [];\n\n    for (var _e171 = 0; _e171 < this.outputs.length; ++_e171) {\n      var _t133 = this.internalOutputShapes[_e171];\n      this.feedOutputNames.push(this.outputNames[_e171]), this.feedOutputShapes.push(_t133), this.feedLossFns.push(this.lossFunctions[_e171]);\n    }\n\n    var n = [];\n    this.metrics = e.metrics, this.metricsNames = [\"loss\"], this.metricsTensors = [], nameScope$1(\"loss\", () => {\n      for (var _e172 = 0; _e172 < this.outputs.length; ++_e172) {\n        if (-1 !== n.indexOf(_e172)) continue;\n        var _t134 = this.lossFunctions[_e172];\n        this.outputs.length > 1 && (this.metricsTensors.push([_t134, _e172]), this.metricsNames.push(this.outputNames[_e172] + \"_loss\"));\n      }\n    });\n\n    var r = collectMetrics$1(e.metrics, this.outputNames),\n        a = (e, t, n) => {\n      this.outputNames.length > 1 && (t = this.outputNames[e] + \"_\" + t), this.metricsNames.push(t), this.metricsTensors.push([n, e]);\n    };\n\n    nameScope$1(\"metric\", () => {\n      var _loop18 = function _loop18(_e173) {\n        -1 === n.indexOf(_e173) && (t => {\n          var n, r, s;\n\n          for (var o of t) {\n            if (\"string\" == typeof o && -1 !== [\"accuracy\", \"acc\", \"crossentropy\", \"ce\"].indexOf(o)) {\n              var _t136 = _this57.internalOutputShapes[_e173];\n\n              var _a36 = void 0;\n\n              1 === _t136[_t136.length - 1] || _this57.lossFunctions[_e173] === binaryCrossentropy$4 ? -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? r = binaryAccuracy$2 : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (r = binaryCrossentropy$3) : _this57.lossFunctions[_e173] === sparseCategoricalCrossentropy$3 ? -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? r = sparseCategoricalAccuracy$2 : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (r = sparseCategoricalCrossentropy$2) : -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? r = categoricalAccuracy$2 : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (r = categoricalCrossentropy$3), -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? _a36 = \"acc\" : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (_a36 = \"ce\"), s = r, n = \"\" + _a36;\n            } else {\n              var _e174 = get$2(o);\n\n              s = _e174, n = \"\" + getLossOrMetricName$1(o);\n            }\n\n            var _t135 = void 0;\n\n            nameScope$1(n, () => {\n              _t135 = s;\n            }), a(_e173, n, _t135);\n          }\n        })(r[_e173]);\n      };\n\n      for (var _e173 = 0; _e173 < this.outputs.length; ++_e173) {\n        _loop18(_e173);\n      }\n    }), this.collectedTrainableWeights = this.trainableWeights;\n  }\n\n  checkTrainableWeightsConsistency() {\n    null != this.collectedTrainableWeights && this.trainableWeights.length !== this.collectedTrainableWeights.length && console.warn(\"Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?\");\n  }\n\n  evaluate(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = null == n.batchSize ? 32 : n.batchSize;\n    checkBatchSize$1(r);\n    var a = this.standardizeUserDataXY(e, t, !0, r);\n\n    try {\n      var s = a[0].concat(a[1]);\n      return this.makeTestFunction(), singletonOrArray$1(this.testLoop(this.testFunction, s, r, n.verbose, n.steps));\n    } finally {\n      disposeNewTensors$1(a[0], e), disposeNewTensors$1(a[1], t);\n    }\n  }\n\n  evaluateDataset(e, t) {\n    var _this58 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this58.makeTestFunction(), evaluateDataset$1(_this58, e, t);\n    })();\n  }\n\n  checkNumSamples(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"steps\";\n    var a;\n\n    if (null != n) {\n      if (a = null, null != t) throw new ValueError$1(\"If \".concat(r, \" is set, batchSize must be null or undefined.Got batchSize = \").concat(t));\n    } else {\n      if (null == e) throw new ValueError$1(\"Either the input data should have a defined shape, or \".concat(r, \" shoud be specified.\"));\n      a = Array.isArray(e) ? e[0].shape[0] : e.shape[0];\n    }\n\n    return a;\n  }\n\n  execute(e, t) {\n    if (Array.isArray(t) && 0 === t.length) throw new ValueError$1(\"`outputs` is an empty Array, which is not allowed.\");\n    var n = Array.isArray(t),\n        r = this.retrieveSymbolicTensors(n ? t : [t]),\n        a = new FeedDict$1();\n\n    if (e instanceof Tensor$1 && (e = [e]), Array.isArray(e)) {\n      if (e.length !== this.inputs.length) throw new ValueError$1(\"The number of inputs provided (\".concat(e.length, \") does not match the number of inputs of this model (\").concat(this.inputs.length, \").\"));\n\n      for (var _t137 = 0; _t137 < this.inputs.length; ++_t137) {\n        a.add(this.inputs[_t137], e[_t137]);\n      }\n    } else for (var _t138 of this.inputs) {\n      var _n63 = e[_t138.name];\n      if (null == _n63) throw new ValueError$1(\"No value is provided for the model's input \".concat(_t138.name));\n      a.add(_t138, _n63);\n    }\n\n    var s = execute$1(r, a);\n    return n ? s : s[0];\n  }\n\n  retrieveSymbolicTensors(e) {\n    var t = pyListRepeat$1(null, e.length);\n    var n = e.length;\n\n    for (var r of this.layers) {\n      var a = Array.isArray(r.output) ? r.output : [r.output],\n          s = a.map(e => e.name);\n\n      for (var _r48 = 0; _r48 < e.length; ++_r48) {\n        var o = s.indexOf(e[_r48]);\n        if (-1 !== o && (t[_r48] = a[o], n--), 0 === n) break;\n      }\n\n      if (0 === n) break;\n    }\n\n    if (n > 0) {\n      var _n64 = [];\n      throw t.forEach((t, r) => {\n        null == t && _n64.push(e[r]);\n      }), new ValueError$1(\"Cannot find SymbolicTensors for output name(s): \".concat(JSON.stringify(_n64)));\n    }\n\n    return t;\n  }\n\n  predictLoop(e) {\n    var _this59 = this;\n\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 32;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    return tidy$1(() => {\n      var r = this.checkNumSamples(e);\n      if (n) throw new NotImplementedError$1(\"Verbose predictLoop() is not implemented yet.\");\n      var a = makeBatches$1(r, t),\n          s = this.outputs.map(e => []);\n\n      var _loop19 = function _loop19(_t139) {\n        tidy$1(() => {\n          var n = sliceArrays$1(e, a[_t139][0], a[_t139][1]),\n              r = [];\n          if (Array.isArray(n)) for (var _e175 = 0; _e175 < n.length; ++_e175) {\n            r.push({\n              key: _this59.inputs[_e175],\n              value: n[_e175]\n            });\n          } else r.push({\n            key: _this59.inputs[0],\n            value: n\n          });\n          var s = new FeedDict$1(r);\n          return execute$1(_this59.outputs, s);\n        }).forEach((e, t) => s[t].push(e));\n      };\n\n      for (var _t139 = 0; _t139 < a.length; ++_t139) {\n        _loop19(_t139);\n      }\n\n      return singletonOrArray$1(s.map(e => concat$5(e, 0)));\n    });\n  }\n\n  predict(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    var n = ensureTensorsRank2OrHigher$1(e);\n    checkInputData$1(n, this.inputNames, this.feedInputShapes, !1);\n\n    try {\n      var r = null == t.batchSize ? 32 : t.batchSize;\n      return checkBatchSize$1(r), this.predictLoop(n, r);\n    } finally {\n      disposeNewTensors$1(n, e);\n    }\n  }\n\n  predictOnBatch(e) {\n    checkInputData$1(e, this.inputNames, this.feedInputShapes, !0);\n    var t = (Array.isArray(e) ? e[0] : e).shape[0];\n    return this.predictLoop(e, t);\n  }\n\n  standardizeUserDataXY(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n    var r = arguments.length > 3 ? arguments[3] : undefined;\n    if (null == this.optimizer_) throw new RuntimeError$1(\"You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).\");\n    var a = [];\n\n    for (var _e176 = 0; _e176 < this.feedOutputShapes.length; ++_e176) {\n      var _t140 = this.feedOutputShapes[_e176];\n      a.push(this.feedLossFns[_e176] === sparseCategoricalCrossentropy$3 ? _t140.slice(0, _t140.length - 1).concat([1]) : _t140);\n    }\n\n    if (checkArrayLengths$1(e = standardizeInputData$1(e, this.feedInputNames, this.feedInputShapes, !1, \"input\"), t = standardizeInputData$1(t, this.feedOutputNames, a, !1, \"target\")), checkLossAndTargetCompatibility$1(t, this.feedLossFns, this.feedOutputShapes), this.stateful && null != r && r > 0 && e[0].shape[0] % r != 0) throw new ValueError$1(\"In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size \".concat(r, \". Found: \").concat(e[0].shape[0], \" sample(s).\"));\n    return [e, t];\n  }\n\n  standardizeUserData(e, t, n, r) {\n    var _arguments = arguments,\n        _this60 = this;\n\n    return _asyncToGenerator(function* () {\n      var a = _arguments.length > 4 && _arguments[4] !== undefined ? _arguments[4] : !0;\n      var s = _arguments.length > 5 ? _arguments[5] : undefined;\n\n      var [o, i] = _this60.standardizeUserDataXY(e, t, a, s);\n\n      if (null != n) throw new Error(\"sample weight is not supported yet.\");\n      var l = null;\n\n      if (null != r) {\n        var _e177 = standardizeClassWeights$1(r, _this60.outputNames);\n\n        l = [];\n\n        for (var _t141 = 0; _t141 < _e177.length; ++_t141) {\n          l.push(yield standardizeWeights$1(i[_t141], null, _e177[_t141]));\n        }\n      }\n\n      return [o, i, l];\n    })();\n  }\n\n  testLoop(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n    var a = arguments.length > 4 ? arguments[4] : undefined;\n    return tidy$1(() => {\n      var s = this.checkNumSamples(t, n, a, \"steps\"),\n          o = [];\n      if (r > 0) throw new NotImplementedError$1(\"Verbose mode is not implemented yet.\");\n      if (null != a) throw new NotImplementedError$1(\"steps mode in testLoop() is not implemented yet\");\n      {\n        var _r49 = makeBatches$1(s, n),\n            _a37 = tensor1d$1(range$7(0, s));\n\n        for (var _n65 = 0; _n65 < _r49.length; ++_n65) {\n          var _s25 = _r49[_n65][0],\n              i = _r49[_n65][1],\n              l = sliceAlongFirstAxis$1(_a37, _s25, i - _s25),\n              u = sliceArraysByIndices$1(t, l),\n              c = e(u);\n          if (0 === _n65) for (var _e178 = 0; _e178 < c.length; ++_e178) {\n            o.push(scalar$1(0));\n          }\n\n          for (var _e179 = 0; _e179 < c.length; ++_e179) {\n            o[_e179] = add$5(o[_e179], mul$1(i - _s25, c[_e179]));\n          }\n        }\n\n        for (var _e180 = 0; _e180 < o.length; ++_e180) {\n          o[_e180] = div$3(o[_e180], s);\n        }\n      }\n      return o;\n    });\n  }\n\n  getDedupedMetricsNames() {\n    var e = this.metricsNames,\n        t = [];\n\n    for (var n = 0; n < e.length; ++n) {\n      var r = e[n];\n      var a = r;\n      count$1(e, r) > 1 && (a += \"_\".concat(count$1(e.slice(0, n), r))), t.push(a);\n    }\n\n    return t;\n  }\n\n  makeTrainFunction() {\n    return e => {\n      var t = [],\n          n = e.slice(0, this.inputs.length),\n          r = e.slice(this.inputs.length, this.inputs.length + this.outputs.length),\n          a = e.slice(this.inputs.length + this.outputs.length, this.inputs.length + 2 * this.outputs.length),\n          s = [],\n          o = this.collectedTrainableWeights.map(e => e.read());\n      return [this.optimizer_.minimize(() => {\n        var e = [];\n\n        for (var _t142 = 0; _t142 < this.inputs.length; ++_t142) {\n          e.push({\n            key: this.inputs[_t142],\n            value: n[_t142]\n          });\n        }\n\n        var o = new FeedDict$1(e),\n            i = execute$1(this.outputs, o, {\n          training: !0\n        });\n        var l;\n\n        for (var _e181 = 0; _e181 < this.lossFunctions.length; ++_e181) {\n          var _n66 = (0, this.lossFunctions[_e181])(r[_e181], i[_e181]);\n\n          null != a[_e181] && (_n66 = computeWeightedLoss$2(_n66, a[_e181]));\n\n          var _s26 = mean$3(_n66);\n\n          t.push(_s26), l = 0 === _e181 ? _n66 : add$5(l, _n66);\n        }\n\n        for (var _e182 = 0; _e182 < this.metricsTensors.length; ++_e182) {\n          var _n67 = void 0;\n\n          if (this.outputs.length > 1 && _e182 < this.outputs.length) _n67 = t[_e182];else {\n            var _t143 = this.metricsTensors[_e182][1];\n            _n67 = mean$3((0, this.metricsTensors[_e182][0])(r[_t143], i[_t143]));\n          }\n          keep$1(_n67), s.push(_n67);\n        }\n\n        return l = mean$3(l), this.calculateLosses().forEach(e => {\n          l = add$5(l, e);\n        }), l;\n      }, !0, o)].concat(s);\n    };\n  }\n\n  makeTestFunction() {\n    this.testFunction = e => tidy$1(() => {\n      var t = [];\n      var n;\n      var r = e.slice(0, this.inputs.length),\n          a = e.slice(this.inputs.length, this.inputs.length + this.outputs.length),\n          s = [];\n\n      for (var _e183 = 0; _e183 < this.inputs.length; ++_e183) {\n        s.push({\n          key: this.inputs[_e183],\n          value: r[_e183]\n        });\n      }\n\n      var o = new FeedDict$1(s),\n          i = execute$1(this.outputs, o);\n\n      for (var _e184 = 0; _e184 < this.lossFunctions.length; ++_e184) {\n        var _r50 = mean$3((0, this.lossFunctions[_e184])(a[_e184], i[_e184]));\n\n        n = 0 === _e184 ? _r50 : add$5(n, _r50), t.push(n);\n      }\n\n      for (var _e185 = 0; _e185 < this.metricsTensors.length; ++_e185) {\n        var _n68 = this.metricsTensors[_e185][1],\n            _r51 = mean$3((0, this.metricsTensors[_e185][0])(a[_n68], i[_n68]));\n\n        t.push(_r51);\n      }\n\n      return t;\n    });\n  }\n\n  fit(e, t) {\n    var _arguments2 = arguments,\n        _this61 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = _arguments2.length > 2 && _arguments2[2] !== undefined ? _arguments2[2] : {};\n      return fitTensors$1(_this61, e, t, n);\n    })();\n  }\n\n  fitDataset(e, t) {\n    var _this62 = this;\n\n    return _asyncToGenerator(function* () {\n      return fitDataset$1(_this62, e, t);\n    })();\n  }\n\n  trainOnBatch(e, t) {\n    var _this63 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = yield _this63.standardizeUserData(e, t),\n          r = n[0],\n          a = n[1],\n          s = _this63.makeTrainFunction()(r.concat(a)),\n          o = [];\n\n      for (var _e186 of s) {\n        var _t144 = yield _e186.data();\n\n        o.push(_t144[0]);\n      }\n\n      return dispose$1(s), singletonOrArray$1(o);\n    })();\n  }\n\n  getNamedWeights(e) {\n    var t = [],\n        n = null != e && e.trainableOnly,\n        r = n ? this.trainableWeights : this.weights,\n        a = this.getWeights(n);\n\n    for (var _e187 = 0; _e187 < r.length; ++_e187) {\n      n && !r[_e187].trainable || t.push({\n        name: r[_e187].originalName,\n        tensor: a[_e187]\n      });\n    }\n\n    return t;\n  }\n\n  set stopTraining(e) {\n    this.stopTraining_ = e;\n  }\n\n  get stopTraining() {\n    return this.stopTraining_;\n  }\n\n  get optimizer() {\n    return this.optimizer_;\n  }\n\n  set optimizer(e) {\n    this.optimizer_ !== e && (this.optimizer_ = e, this.isOptimizerOwned = !1);\n  }\n\n  dispose() {\n    var e = super.dispose();\n\n    if (0 === e.refCountAfterDispose && null != this.optimizer && this.isOptimizerOwned) {\n      var t = memory$1().numTensors;\n      this.optimizer_.dispose(), e.numDisposedVariables += t - memory$1().numTensors;\n    }\n\n    return e;\n  }\n\n  getLossIdentifiers() {\n    var e;\n    if (\"string\" == typeof this.loss) e = toSnakeCase$1(this.loss);else if (Array.isArray(this.loss)) {\n      for (var _e188 of this.loss) {\n        if (\"string\" != typeof _e188) throw new Error(\"Serialization of non-string loss is not supported.\");\n      }\n\n      e = this.loss.map(e => toSnakeCase$1(e));\n    } else {\n      var t = Object.keys(this.loss);\n      e = {};\n      var n = this.loss;\n\n      for (var r of t) {\n        if (\"string\" != typeof n[r]) throw new Error(\"Serialization of non-string loss is not supported.\");\n        e[r] = toSnakeCase$1(n[r]);\n      }\n    }\n    return e;\n  }\n\n  getMetricIdentifiers() {\n    if (\"string\" == typeof this.metrics || \"function\" == typeof this.metrics) return [toSnakeCase$1(getLossOrMetricName$1(this.metrics))];\n    if (Array.isArray(this.metrics)) return this.metrics.map(e => toSnakeCase$1(getLossOrMetricName$1(e)));\n    {\n      var _e189 = {};\n\n      for (var t in this.metrics) {\n        _e189[t] = toSnakeCase$1(getLossOrMetricName$1(this.metrics[t]));\n      }\n\n      return _e189;\n    }\n  }\n\n  getTrainingConfig() {\n    return {\n      loss: this.getLossIdentifiers(),\n      metrics: this.getMetricIdentifiers(),\n      optimizer_config: {\n        class_name: this.optimizer.getClassName(),\n        config: this.optimizer.getConfig()\n      }\n    };\n  }\n\n  loadTrainingConfig(e) {\n    if (null != e.weighted_metrics) throw new Error(\"Loading weight_metrics is not supported yet.\");\n    if (null != e.loss_weights) throw new Error(\"Loading loss_weights is not supported yet.\");\n    if (null != e.sample_weight_mode) throw new Error(\"Loading sample_weight_mode is not supported yet.\");\n    var t = deserialize$1(convertPythonicToTs$1(e.optimizer_config));\n    var n, r;\n    if (\"string\" == typeof e.loss) n = toCamelCase$1(e.loss);else if (Array.isArray(e.loss)) n = e.loss.map(e => toCamelCase$1(e));else if (null != e.loss) {\n      n = {};\n\n      for (var _t145 in e.loss) {\n        n[_t145] = toCamelCase$1(e.loss[_t145]);\n      }\n    }\n    if (Array.isArray(e.metrics)) r = e.metrics.map(e => toCamelCase$1(e));else if (null != e.metrics) {\n      r = {};\n\n      for (var _t146 in e.metrics) {\n        r[_t146] = toCamelCase$1(e.metrics[_t146]);\n      }\n    }\n    this.compile({\n      loss: n,\n      metrics: r,\n      optimizer: t\n    });\n  }\n\n  save(e, t) {\n    var _this64 = this;\n\n    return _asyncToGenerator(function* () {\n      if (\"string\" == typeof e) {\n        var _t147 = getSaveHandlers$1(e);\n\n        if (0 === _t147.length) throw new ValueError$1(\"Cannot find any save handlers for URL '\".concat(e, \"'\"));\n        if (_t147.length > 1) throw new ValueError$1(\"Found more than one (\".concat(_t147.length, \") save handlers for URL '\").concat(e, \"'\"));\n        e = _t147[0];\n      }\n\n      if (null == e.save) throw new ValueError$1(\"LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.\");\n      var n = yield encodeWeights$1(_this64.getNamedWeights(t)),\n          r = {\n        modelTopology: _this64.toJSON(null, !1),\n        format: LAYERS_MODEL_FORMAT_NAME$1,\n        generatedBy: \"TensorFlow.js tfjs-layers v\".concat(version$d),\n        convertedBy: null\n      };\n\n      if (null != t && t.includeOptimizer && null != _this64.optimizer) {\n        r.trainingConfig = _this64.getTrainingConfig();\n        var _e190 = \"optimizer\",\n            {\n          data: _t148,\n          specs: a\n        } = yield encodeWeights$1(yield _this64.optimizer.getWeights(), _e190);\n        n.specs.push(...a), n.data = concatenateArrayBuffers$1([n.data, _t148]);\n      }\n\n      return null != _this64.userDefinedMetadata && (checkUserDefinedMetadata$1(_this64.userDefinedMetadata, _this64.name, !0), r.userDefinedMetadata = _this64.userDefinedMetadata), r.weightData = n.data, r.weightSpecs = n.specs, e.save(r);\n    })();\n  }\n\n  setUserDefinedMetadata(e) {\n    checkUserDefinedMetadata$1(e, this.name), this.userDefinedMetadata = e;\n  }\n\n  getUserDefinedMetadata() {\n    return this.userDefinedMetadata;\n  }\n\n}\n\nLayersModel$1.className = \"Model\", registerClass$1(LayersModel$1);\n\nclass Functional$1 extends LayersModel$1 {}\n\nfunction loadLayersModelInternal$1(_x45, _x46) {\n  return _loadLayersModelInternal$.apply(this, arguments);\n}\n\nfunction _loadLayersModelInternal$() {\n  _loadLayersModelInternal$ = _asyncToGenerator(function* (e, t) {\n    if (null == t && (t = {}), \"string\" == typeof e) {\n      var n = getLoadHandlers$1(e, t);\n      if (0 === n.length) n.push(browserHTTPRequest$1(e, t));else if (n.length > 1) throw new ValueError$1(\"Found more than one (\".concat(n.length, \") load handlers for URL '\").concat(e, \"'\"));\n      e = n[0];\n    }\n\n    return loadLayersModelFromIOHandler$1(e, void 0, t);\n  });\n  return _loadLayersModelInternal$.apply(this, arguments);\n}\n\nfunction loadLayersModelFromIOHandler$1(_x47, _x48, _x49) {\n  return _loadLayersModelFromIOHandler$.apply(this, arguments);\n}\n\nfunction _loadLayersModelFromIOHandler$() {\n  _loadLayersModelFromIOHandler$ = _asyncToGenerator(function* (e, t, n) {\n    if (null == n && (n = {}), null == e.load) throw new ValueError$1(\"Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.\");\n    var r = yield e.load();\n    var a = r.modelTopology;\n    null != a.model_config && (a = a.model_config);\n    var s = null == n.strict || n.strict,\n        o = null != r.weightData && null != r.weightSpecs && s,\n        i = deserialize$1(convertPythonicToTs$1(a), t, o),\n        l = r.trainingConfig;\n\n    if (null != l && i.loadTrainingConfig(l), null != r.userDefinedMetadata && i.setUserDefinedMetadata(r.userDefinedMetadata), null != r.weightData) {\n      if (null == r.weightSpecs) throw new ValueError$1(\"LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.\");\n      var {\n        modelWeights: _e1139,\n        optimizerWeights: _t771\n      } = decodeModelAndOptimizerWeights$1(r.weightData, r.weightSpecs);\n      i.loadWeights(_e1139, s), null != i.optimizer && _t771.length > 0 && (yield i.optimizer.setWeights(_t771)), dispose$1(_e1139), dispose$1(_t771.map(e => e.tensor));\n    }\n\n    return i;\n  });\n  return _loadLayersModelFromIOHandler$.apply(this, arguments);\n}\n\nfunction decodeModelAndOptimizerWeights$1(e, t) {\n  var n = decodeWeights$1(e, t),\n      r = {},\n      a = [];\n  return t.forEach(e => {\n    \"optimizer\" === e.group ? a.push({\n      name: e.name,\n      tensor: n[e.name]\n    }) : r[e.name] = n[e.name];\n  }), {\n    modelWeights: r,\n    optimizerWeights: a\n  };\n}\n\nFunctional$1.className = \"Functional\", registerClass$1(Functional$1);\n\nclass Sequential$1 extends LayersModel$1 {\n  constructor(e) {\n    if (super({\n      inputs: [],\n      outputs: []\n    }), e = e || {}, this.trainable = !0, this.built = !1, this.name = null != e.name ? e.name : getUid$1(\"sequential_\"), null != e.layers) for (var t of e.layers) {\n      this.add(t);\n    }\n  }\n\n  checkShape(e) {\n    if (e.inboundNodes[0].outputTensors[0].shape.some(e => e < 0)) throw new ValueError$1(\"Negative dimension size caused by adding layer \".concat(e.name, \" with input shape [\").concat(e.inboundNodes[0].inputTensors[0].shape, \"]\"));\n  }\n\n  add(e) {\n    var t = e instanceof Sequential$1 || e instanceof LayersModel$1;\n    var n;\n\n    if (t) {\n      if (n = e, 1 !== n.outputs.length) throw new ValueError$1(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n      if (1 !== n.inputs.length) throw new ValueError$1(\"All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.\");\n    }\n\n    if (0 === this.outputs.length) {\n      if (0 === e.inboundNodes.length) {\n        if (null == e.batchInputShape) throw new ValueError$1(\"The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.\");\n\n        var _t149 = Input$1({\n          batchShape: e.batchInputShape,\n          dtype: e.dtype,\n          name: e.name + \"_input\"\n        });\n\n        e.apply(_t149);\n      }\n\n      if (t) this.outputs = n.outputs, this.inputs = n.inputs;else {\n        if (1 !== e.inboundNodes.length) throw new ValueError$1(\"A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer \".concat(e.name, \" which has \").concat(e.inboundNodes.length, \" pre-existing inbound connections.\"));\n        if (1 !== e.inboundNodes[0].outputTensors.length) throw new ValueError$1(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n        this.checkShape(e), this.outputs = [e.inboundNodes[0].outputTensors[0]], this.inputs = getSourceInputs$1(this.outputs[0]);\n      }\n      this.inboundNodes = [], new Node$1({\n        outboundLayer: this,\n        inboundLayers: [],\n        nodeIndices: [],\n        tensorIndices: [],\n        inputTensors: this.inputs,\n        outputTensors: this.outputs,\n        inputMasks: pyListRepeat$1(null, this.inputs.length),\n        outputMasks: [null],\n        inputShapes: this.inputs.map(e => e.shape),\n        outputShapes: this.outputs[0].shape\n      });\n    } else {\n      var _t150 = e.apply(this.outputs[0]);\n\n      if (Array.isArray(_t150)) throw new TypeError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n      this.checkShape(e), this.outputs = [_t150], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n\n    this.layers.push(e), this.built = !1;\n  }\n\n  pop() {\n    if (0 === this.layers.length) throw new TypeError(\"There are no layers in the model.\");\n    if (this.layers.pop(), 0 === this.layers.length) this.outputs = [], this.inboundNodes = [], this.outboundNodes = [];else {\n      var _e191 = this.layers.length - 1;\n\n      this.layers[_e191].outboundNodes = [], this.outputs = [this.layers[_e191].output], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n  }\n\n  call(e, t) {\n    return null == this.model && this.build(), this.model.call(e, t);\n  }\n\n  build(e) {\n    if (getExactlyOneShape$1(e), 0 === this.inputs.length || 0 === this.outputs.length) throw new TypeError(\"Sequential model cannot be built: model is empty. Add some layers first.\");\n    this.model = new LayersModel$1({\n      inputs: this.inputs,\n      outputs: this.outputs[0],\n      name: this.name + \"_model\"\n    }), this.model.trainable = this.trainable, this.supportsMasking = this.model.supportsMasking, this.inputLayers = this.model.inputLayers, this.inputLayersNodeIndices = this.model.inputLayersNodeIndices, this.inputLayersTensorIndices = this.model.inputLayersTensorIndices, this.outputLayers = this.model.outputLayers, this.outputLayersNodeIndices = this.model.outputLayersNodeIndices, this.outputLayersTensorIndices = this.model.outputLayersTensorIndices, this.nodesByDepth = this.model.nodesByDepth, this.containerNodes = this.model.containerNodes, this.outputNames = this.model.outputNames, this.inputNames = this.model.inputNames, this.built = !0;\n  }\n\n  countParams() {\n    return this.built || this.build(), super.countParams();\n  }\n\n  summary(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : console.log;\n    this.built || this.build(), super.summary(e, t, n);\n  }\n\n  setWeights(e) {\n    null == this.model && this.build(), this.model.setWeights(e);\n  }\n\n  evaluate(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    if (!this.built) throw new RuntimeError$1(\"The model needs to be compiled before being used.\");\n    return this.model.evaluate(e, t, n);\n  }\n\n  evaluateDataset(e, t) {\n    var _this65 = this;\n\n    return _asyncToGenerator(function* () {\n      if (!_this65.built) throw new RuntimeError$1(\"The model needs to be compiled before being used.\");\n      return _this65.model.evaluateDataset(e, t);\n    })();\n  }\n\n  predict(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    return null == this.model && this.build(), this.model.predict(e, t);\n  }\n\n  predictOnBatch(e) {\n    return null == this.model && this.build(), this.model.predictOnBatch(e);\n  }\n\n  compile(e) {\n    this.build(), this.model.compile(e), this.optimizer_ = this.model.optimizer, this.isOptimizerOwned = this.model.isOptimizerOwned, this.loss = this.model.loss, this.metrics = this.model.metrics, this.metricsTensors = this.model.metricsTensors, this.metricsNames = this.model.metricsNames;\n  }\n\n  get optimizer() {\n    return null == this.model ? void 0 : this.model.optimizer;\n  }\n\n  set optimizer(e) {\n    this.model.optimizer = e;\n  }\n\n  fit(e, t) {\n    var _arguments3 = arguments,\n        _this66 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = _arguments3.length > 2 && _arguments3[2] !== undefined ? _arguments3[2] : {};\n      if (!_this66.built) throw new RuntimeError$1(\"The model needs to be compiled before being used.\");\n      return _this66.model.fit(e, t, n);\n    })();\n  }\n\n  fitDataset(e, t) {\n    var _this67 = this;\n\n    return _asyncToGenerator(function* () {\n      if (!_this67.built) throw new RuntimeError$1(\"The model needs to be compiled before being used.\");\n      return _this67.model.fitDataset(e, t);\n    })();\n  }\n\n  trainOnBatch(e, t) {\n    var _this68 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this68.model.trainOnBatch(e, t);\n    })();\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a,\n        s = {};\n\n    if (t instanceof Array) {\n      if (null == t[0].className || \"Merge\" === t[0].className) throw new ValueError$1(\"Legacy serialization format not supported yet.\");\n      a = t;\n    } else assert$6(null != t.layers, () => \"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.\"), a = t.layers, delete t.layers, s = t;\n\n    var o = new e(s);\n    if (!(o instanceof Sequential$1)) throw new NotImplementedError$1(\"Sequential.fromConfig called on non-Sequential input: \".concat(o));\n\n    for (var _e192 of a) {\n      var _t151 = deserialize$1(_e192, void 0, r);\n\n      r && _t151.setFastWeightInitDuringBuild(!0), o.add(_t151);\n    }\n\n    return o;\n  }\n\n  set stopTraining(e) {\n    if (null == this.model) throw new ValueError$1(\"Cannot set the stopTraining property of a sequential model before it is compiled.\");\n    this.model.stopTraining = e;\n  }\n\n  get stopTraining() {\n    if (null == this.model) throw new ValueError$1(\"Cannot get the stopTraining property of a sequential model before it is compiled.\");\n    return this.model.stopTraining;\n  }\n\n  getConfig() {\n    var e = [];\n\n    for (var t of this.layers) {\n      var n = {};\n      n.className = t.getClassName(), n.config = t.getConfig(), e.push(n);\n    }\n\n    return {\n      name: this.name,\n      layers: e\n    };\n  }\n\n}\n\nfunction sequential$1(e) {\n  return new Sequential$1(e);\n}\n\nfunction loadLayersModel$1(e, t) {\n  return null == t && (t = {}), loadLayersModelInternal$1(e, t);\n}\n\nSequential$1.className = \"Sequential\", registerClass$1(Sequential$1);\n\nclass Activation$3 extends Serializable$1 {\n  getConfig() {\n    return {};\n  }\n\n}\n\nclass Elu$2 extends Activation$3 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    return elu$7(e, t);\n  }\n\n}\n\nElu$2.className = \"elu\", registerClass$1(Elu$2);\n\nclass Selu$2 extends Activation$3 {\n  apply(e) {\n    return selu$5(e);\n  }\n\n}\n\nSelu$2.className = \"selu\", registerClass$1(Selu$2);\n\nclass Relu$2 extends Activation$3 {\n  apply(e) {\n    return relu$6(e);\n  }\n\n}\n\nRelu$2.className = \"relu\", registerClass$1(Relu$2);\n\nclass Relu6$2 extends Activation$3 {\n  apply(e) {\n    return tidy$1(() => minimum$6(6, relu$6(e)));\n  }\n\n}\n\nRelu6$2.className = \"relu6\", registerClass$1(Relu6$2);\n\nclass Linear$1 extends Activation$3 {\n  apply(e) {\n    return e;\n  }\n\n}\n\nLinear$1.className = \"linear\", registerClass$1(Linear$1);\n\nclass Sigmoid$2 extends Activation$3 {\n  apply(e) {\n    return sigmoid$5(e);\n  }\n\n}\n\nSigmoid$2.className = \"sigmoid\", registerClass$1(Sigmoid$2);\n\nclass HardSigmoid$1 extends Activation$3 {\n  apply(e) {\n    return hardSigmoid$1(e);\n  }\n\n}\n\nHardSigmoid$1.className = \"hardSigmoid\", registerClass$1(HardSigmoid$1);\n\nclass Softplus$2 extends Activation$3 {\n  apply(e) {\n    return softplus$5(e);\n  }\n\n}\n\nSoftplus$2.className = \"softplus\", registerClass$1(Softplus$2);\n\nclass Softsign$1 extends Activation$3 {\n  apply(e) {\n    return softsign$1(e);\n  }\n\n}\n\nSoftsign$1.className = \"softsign\", registerClass$1(Softsign$1);\n\nclass Tanh$2 extends Activation$3 {\n  apply(e) {\n    return tanh$6(e);\n  }\n\n}\n\nTanh$2.className = \"tanh\", registerClass$1(Tanh$2);\n\nclass Softmax$4 extends Activation$3 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n    return softmax$6(e, t);\n  }\n\n}\n\nSoftmax$4.className = \"softmax\", registerClass$1(Softmax$4);\n\nclass LogSoftmax$2 extends Activation$3 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n    return logSoftmax$1(e, t);\n  }\n\n}\n\nLogSoftmax$2.className = \"logSoftmax\", registerClass$1(LogSoftmax$2);\n\nclass Swish$1 extends Activation$3 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    return tidy$1(() => mul$1(sigmoid$5(mul$1(e, t)), e));\n  }\n\n}\n\nSwish$1.className = \"swish\", registerClass$1(Swish$1);\n\nclass Mish$1 extends Activation$3 {\n  apply(e) {\n    return tidy$1(() => mul$1(e, tanh$6(softplus$5(e))));\n  }\n\n}\n\nfunction serializeActivation$1(e) {\n  return e.getClassName();\n}\n\nfunction deserializeActivation$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject$1(e, SerializationMap$1.getMap().classNameMap, t, \"activation\");\n}\n\nfunction getActivation$1(e) {\n  if (null == e) return deserializeActivation$1({\n    className: \"linear\",\n    config: {}\n  });\n\n  if (\"string\" == typeof e) {\n    var t = {};\n    return t.className = e, t.config = {}, deserializeActivation$1(t);\n  }\n\n  return e instanceof Activation$3 ? e : deserializeActivation$1(e);\n}\n\nfunction assertObjectArgs$1(e) {\n  if (null != e && \"object\" != typeof e) throw new Error(\"Argument to L1L2 regularizer's constructor is expected to be an object, but received: \".concat(e));\n}\n\nMish$1.className = \"mish\", registerClass$1(Mish$1);\n\nclass Regularizer$1 extends Serializable$1 {}\n\nclass L1L2$1 extends Regularizer$1 {\n  constructor(e) {\n    super(), assertObjectArgs$1(e), this.l1 = null == e || null == e.l1 ? .01 : e.l1, this.l2 = null == e || null == e.l2 ? .01 : e.l2, this.hasL1 = 0 !== this.l1, this.hasL2 = 0 !== this.l2;\n  }\n\n  apply(e) {\n    return tidy$1(() => {\n      var t = zeros$4([1]);\n      return this.hasL1 && (t = add$5(t, sum$6(mul$1(this.l1, abs$5(e))))), this.hasL2 && (t = add$5(t, sum$6(mul$1(this.l2, square$4(e))))), reshape$6(t, []);\n    });\n  }\n\n  getConfig() {\n    return {\n      l1: this.l1,\n      l2: this.l2\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e({\n      l1: t.l1,\n      l2: t.l2\n    });\n  }\n\n}\n\nL1L2$1.className = \"L1L2\", registerClass$1(L1L2$1);\nvar REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP$1 = {\n  l1l2: \"L1L2\"\n};\n\nfunction serializeRegularizer$1(e) {\n  return serializeKerasObject$1(e);\n}\n\nfunction deserializeRegularizer$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject$1(e, SerializationMap$1.getMap().classNameMap, t, \"regularizer\");\n}\n\nfunction getRegularizer$1(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeRegularizer$1({\n    className: e in REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP$1 ? REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP$1[e] : e,\n    config: {}\n  }) : e instanceof Regularizer$1 ? e : deserializeRegularizer$1(e);\n}\n\nclass ReLU$1 extends Layer$1 {\n  constructor(e) {\n    super(null == e ? {} : e), this.supportsMasking = !0, null != e && (this.maxValue = e.maxValue);\n  }\n\n  call(e, t) {\n    e = getExactlyOneTensor$1(e);\n    var n = relu$6(e);\n    return null != this.maxValue && (n = clipByValue$3(n, 0, this.maxValue)), n;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      maxValue: this.maxValue\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nReLU$1.className = \"ReLU\", registerClass$1(ReLU$1);\n\nclass LeakyReLU$1 extends Layer$1 {\n  constructor(e) {\n    super(null == e ? {} : e), this.DEFAULT_ALPHA = .3, null == e && (e = {}), this.alpha = null == e.alpha ? this.DEFAULT_ALPHA : e.alpha;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor$1(e);\n    return leakyRelu$5(n, this.alpha);\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      alpha: this.alpha\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nLeakyReLU$1.className = \"LeakyReLU\", registerClass$1(LeakyReLU$1);\n\nclass PReLU$1 extends Layer$1 {\n  constructor(e) {\n    if (super(null == e ? {} : e), this.DEFAULT_ALPHA_INITIALIZER = \"zeros\", null == e && (e = {}), this.supportsMasking = !0, this.alphaInitializer = getInitializer$1(e.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER), this.alphaRegularizer = getRegularizer$1(e.alphaRegularizer), this.alphaConstraint = getConstraint$1(e.alphaConstraint), null == e.sharedAxes) this.sharedAxes = null;else if (Array.isArray(e.sharedAxes)) this.sharedAxes = e.sharedAxes;else {\n      if (\"number\" != typeof e.sharedAxes) throw new ValueError$1(\"Expected sharedAxes to be a number or an array of numbers, but got \".concat(e.sharedAxes));\n      this.sharedAxes = [e.sharedAxes];\n    }\n  }\n\n  build(e) {\n    var t = (e = getExactlyOneShape$1(e)).slice(1);\n    if (null != this.sharedAxes) for (var _e193 of this.sharedAxes) {\n      t[_e193 - 1] = 1;\n    }\n    this.alpha = this.addWeight(\"alpha\", t, \"float32\", this.alphaInitializer, this.alphaRegularizer, !0, this.alphaConstraint);\n    var n = {};\n    if (null != this.sharedAxes) for (var _t152 = 1; _t152 < e.length; ++_t152) {\n      n[_t152] = e[_t152];\n    }\n    this.inputSpec = [new InputSpec$1({\n      ndim: e.length,\n      axes: n\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return e = getExactlyOneTensor$1(e), prelu$6(e, this.alpha.read());\n  }\n\n  getConfig() {\n    var e = {\n      alphaInitializer: serializeInitializer$1(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer$1(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint$1(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nPReLU$1.className = \"PReLU\", registerClass$1(PReLU$1);\n\nclass ELU$7 extends Layer$1 {\n  constructor(e) {\n    if (super(null == e ? {} : e), this.DEFAULT_ALPHA = 1, null == e && (e = {}), null != e.alpha && e.alpha !== this.DEFAULT_ALPHA) throw new NotImplementedError$1(\"Non-default alpha value (\".concat(e.alpha, \") is not supported by the ELU layer yet.\"));\n    this.alpha = null == e.alpha ? this.DEFAULT_ALPHA : e.alpha;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor$1(e);\n    return elu$8(n);\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      alpha: this.alpha\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nELU$7.className = \"ELU\", registerClass$1(ELU$7);\n\nclass ThresholdedReLU$1 extends Layer$1 {\n  constructor(e) {\n    super(null == e ? {} : e), this.DEFAULT_THETA = 1, null == e && (e = {}), this.theta = null == e.theta ? this.DEFAULT_THETA : e.theta;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor$1(e);\n    return mul$1(n, cast$7(greater$6(n, this.theta), \"float32\"));\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      theta: this.theta\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nThresholdedReLU$1.className = \"ThresholdedReLU\", registerClass$1(ThresholdedReLU$1);\n\nclass Softmax$3 extends Layer$1 {\n  constructor(e) {\n    super(null == e ? {} : e), this.DEFAULT_AXIS = 1, null == e && (e = {}), this.softmax = new Softmax$4().apply, this.axis = null == e.axis ? this.DEFAULT_AXIS : e.axis;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor$1(e);\n    return this.softmax(n, this.axis);\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction normalizeArray$1(e, t, n) {\n  if (\"number\" == typeof e) return pyListRepeat$1(e, t);\n  if (e.length !== t) throw new ValueError$1(\"The \".concat(n, \" argument must be an integer or tuple of \").concat(t, \" integers. Received: \").concat(e.length, \" elements.\"));\n\n  for (var r = 0; r < t; ++r) {\n    var a = e[r];\n    if (!isInteger$1(a)) throw new ValueError$1(\"The \".concat(n, \" argument must be an integer or tuple of \").concat(t, \" integers. Received: \").concat(JSON.stringify(e), \" including a non-integer number \").concat(a));\n  }\n\n  return e;\n}\n\nfunction convOutputLength$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1;\n  if (null == e) return e;\n  var s;\n  return s = \"same\" === n ? e : e - (t + (t - 1) * (a - 1)) + 1, Math.floor((s + r - 1) / r);\n}\n\nfunction deconvLength$1(e, t, n, r) {\n  if (null == e) return null;\n  if (\"valid\" === r) e = e * t + max$6([n - t, 0]);else {\n    if (\"same\" !== r) throw new ValueError$1(\"Unsupport padding mode: \".concat(r, \".\"));\n    e *= t;\n  }\n  return e;\n}\n\nfunction preprocessConv2DInput$1(e, t) {\n  return tidy$1(() => (checkDataFormat$1(t), \"channelsFirst\" === t ? transpose$5(e, [0, 2, 3, 1]) : e));\n}\n\nfunction preprocessConv3DInput$1(e, t) {\n  return tidy$1(() => (checkDataFormat$1(t), \"channelsFirst\" === t ? transpose$5(e, [0, 2, 3, 4, 1]) : e));\n}\n\nfunction conv1dWithBias$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"valid\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : 1;\n  return tidy$1(() => {\n    if (null == s && (s = imageDataFormat$1()), checkDataFormat$1(s), 3 !== e.shape.length) throw new ValueError$1(\"The input of a conv1dWithBias operation should be 3, but is \".concat(e.shape.length, \" instead.\"));\n    if (3 !== t.shape.length) throw new ValueError$1(\"The kernel for a conv1dWithBias operation should be 3, but is \".concat(t.shape.length, \" instead\"));\n    if (null != n && 1 !== n.shape.length) throw new ValueError$1(\"The bias for a conv1dWithBias operation should be 1, but is \".concat(t.shape.length, \" instead\"));\n    if (\"channelsFirst\" === s && (e = transpose$5(e, [0, 2, 1])), \"causal\" === a) throw new NotImplementedError$1(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");\n    var i = conv1d$3(e, t, r, \"same\" === a ? \"same\" : \"valid\", \"NWC\", o);\n    return null != n && (i = biasAdd$1(i, n)), i;\n  });\n}\n\nfunction conv2dWithBiasActivation$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : [1, 1];\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"valid\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : null;\n  return tidy$1(() => {\n    if (null == s && (s = imageDataFormat$1()), checkDataFormat$1(s), 3 !== e.rank && 4 !== e.rank) throw new ValueError$1(\"conv2dWithBiasActivation expects input to be of rank 3 or 4, but received \".concat(e.rank, \".\"));\n    if (3 !== t.rank && 4 !== t.rank) throw new ValueError$1(\"conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received \".concat(e.rank, \".\"));\n    var l = preprocessConv2DInput$1(e, s);\n    if (\"causal\" === a) throw new NotImplementedError$1(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");\n    return l = conv2d$6({\n      x: l,\n      filter: t,\n      strides: r,\n      pad: \"same\" === a ? \"same\" : \"valid\",\n      dilations: o,\n      dataFormat: \"NHWC\",\n      bias: n,\n      activation: i\n    }), \"channelsFirst\" === s && (l = transpose$5(l, [0, 3, 1, 2])), l;\n  });\n}\n\nfunction conv3dWithBias$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : [1, 1, 1];\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"valid\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  return tidy$1(() => {\n    if (null == s && (s = imageDataFormat$1()), checkDataFormat$1(s), 4 !== e.rank && 5 !== e.rank) throw new ValueError$1(\"conv3dWithBias expects input to be of rank 4 or 5, but received \".concat(e.rank, \".\"));\n    if (4 !== t.rank && 5 !== t.rank) throw new ValueError$1(\"conv3dWithBias expects kernel to be of rank 4 or 5, but received \".concat(e.rank, \".\"));\n    var i = preprocessConv3DInput$1(e, s);\n    if (\"causal\" === a) throw new NotImplementedError$1(\"The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.\");\n    return i = conv3d$2(i, t, r, \"same\" === a ? \"same\" : \"valid\", \"NDHWC\", o), null != n && (i = biasAdd$1(i, n)), \"channelsFirst\" === s && (i = transpose$5(i, [0, 4, 1, 2, 3])), i;\n  });\n}\n\nSoftmax$3.className = \"Softmax\", registerClass$1(Softmax$3);\n\nclass BaseConv$1 extends Layer$1 {\n  constructor(e, t) {\n    if (super(t), this.bias = null, this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", BaseConv$1.verifyArgs(t), this.rank = e, assertPositiveInteger$1(this.rank, \"rank\"), 1 !== this.rank && 2 !== this.rank && 3 !== this.rank) throw new NotImplementedError$1(\"Convolution layer for rank other than 1, 2, or 3 (\".concat(this.rank, \") is not implemented yet.\"));\n    if (this.kernelSize = normalizeArray$1(t.kernelSize, e, \"kernelSize\"), this.strides = normalizeArray$1(null == t.strides ? 1 : t.strides, e, \"strides\"), this.padding = null == t.padding ? \"valid\" : t.padding, checkPaddingMode$1(this.padding), this.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, checkDataFormat$1(this.dataFormat), this.activation = getActivation$1(t.activation), this.useBias = null == t.useBias || t.useBias, this.biasInitializer = getInitializer$1(t.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.biasConstraint = getConstraint$1(t.biasConstraint), this.biasRegularizer = getRegularizer$1(t.biasRegularizer), this.activityRegularizer = getRegularizer$1(t.activityRegularizer), this.dilationRate = normalizeArray$1(null == t.dilationRate ? 1 : t.dilationRate, e, \"dilationRate\"), 1 === this.rank && Array.isArray(this.dilationRate) && 1 !== this.dilationRate.length) throw new ValueError$1(\"dilationRate must be a number or an array of a single number for 1D convolution, but received \".concat(JSON.stringify(this.dilationRate)));\n\n    if (2 === this.rank) {\n      if (\"number\" == typeof this.dilationRate) this.dilationRate = [this.dilationRate, this.dilationRate];else if (2 !== this.dilationRate.length) throw new ValueError$1(\"dilationRate must be a number or array of two numbers for 2D convolution, but received \".concat(JSON.stringify(this.dilationRate)));\n    } else if (3 === this.rank) if (\"number\" == typeof this.dilationRate) this.dilationRate = [this.dilationRate, this.dilationRate, this.dilationRate];else if (3 !== this.dilationRate.length) throw new ValueError$1(\"dilationRate must be a number or array of three numbers for 3D convolution, but received \".concat(JSON.stringify(this.dilationRate)));\n  }\n\n  static verifyArgs(e) {\n    if (assert$5(\"kernelSize\" in e, \"required key 'kernelSize' not in config\"), \"number\" != typeof e.kernelSize && !checkArrayTypeAndLength$1(e.kernelSize, \"number\", 1, 3)) throw new ValueError$1(\"BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n  getConfig() {\n    var e = {\n      kernelSize: this.kernelSize,\n      strides: this.strides,\n      padding: this.padding,\n      dataFormat: this.dataFormat,\n      dilationRate: this.dilationRate,\n      activation: serializeActivation$1(this.activation),\n      useBias: this.useBias,\n      biasInitializer: serializeInitializer$1(this.biasInitializer),\n      biasRegularizer: serializeRegularizer$1(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer$1(this.activityRegularizer),\n      biasConstraint: serializeConstraint$1(this.biasConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass Conv$1 extends BaseConv$1 {\n  constructor(e, t) {\n    super(e, t), this.kernel = null, Conv$1.verifyArgs(t), this.filters = t.filters, assertPositiveInteger$1(this.filters, \"filters\"), this.kernelInitializer = getInitializer$1(t.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.kernelConstraint = getConstraint$1(t.kernelConstraint), this.kernelRegularizer = getRegularizer$1(t.kernelRegularizer);\n  }\n\n  build(e) {\n    e = getExactlyOneShape$1(e);\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError$1(\"The channel dimension of the input should be defined. Found \".concat(e[t]));\n    var n = e[t],\n        r = this.kernelSize.concat([n, this.filters]);\n    this.kernel = this.addWeight(\"kernel\", r, null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [{\n      ndim: this.rank + 2,\n      axes: {\n        [t]: n\n      }\n    }], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t;\n      e = getExactlyOneTensor$1(e);\n      var n = null == this.bias ? null : this.bias.read(),\n          r = mapActivationToFusedKernel$1(this.activation.getClassName());\n      if (null != r && 2 === this.rank) t = conv2dWithBiasActivation$1(e, this.kernel.read(), n, this.strides, this.padding, this.dataFormat, this.dilationRate, r);else {\n        if (1 === this.rank) t = conv1dWithBias$1(e, this.kernel.read(), n, this.strides[0], this.padding, this.dataFormat, this.dilationRate[0]);else if (2 === this.rank) t = conv2dWithBiasActivation$1(e, this.kernel.read(), n, this.strides, this.padding, this.dataFormat, this.dilationRate);else {\n          if (3 !== this.rank) throw new NotImplementedError$1(\"convolutions greater than 3D are not implemented yet.\");\n          t = conv3dWithBias$1(e, this.kernel.read(), n, this.strides, this.padding, this.dataFormat, this.dilationRate);\n        }\n        null != this.activation && (t = this.activation.apply(t));\n      }\n      return t;\n    });\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape$1(e);\n    var t = [],\n        n = \"channelsLast\" === this.dataFormat ? e.slice(1, e.length - 1) : e.slice(2);\n\n    for (var _e194 = 0; _e194 < n.length; ++_e194) {\n      var _r52 = convOutputLength$1(n[_e194], this.kernelSize[_e194], this.padding, this.strides[_e194], \"number\" == typeof this.dilationRate ? this.dilationRate : this.dilationRate[_e194]);\n\n      t.push(_r52);\n    }\n\n    var r = [e[0]];\n    return \"channelsLast\" === this.dataFormat ? (r = r.concat(t), r.push(this.filters)) : (r.push(this.filters), r = r.concat(t)), r;\n  }\n\n  getConfig() {\n    var e = {\n      filters: this.filters,\n      kernelInitializer: serializeInitializer$1(this.kernelInitializer),\n      kernelRegularizer: serializeRegularizer$1(this.kernelRegularizer),\n      kernelConstraint: serializeConstraint$1(this.kernelConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  static verifyArgs(e) {\n    if (!(\"filters\" in e) || \"number\" != typeof e.filters || e.filters < 1) throw new ValueError$1(\"Convolution layer expected config.filters to be a 'number' > 0 but got \".concat(JSON.stringify(e.filters)));\n  }\n\n}\n\nclass Conv2D$2 extends Conv$1 {\n  constructor(e) {\n    super(2, e), Conv2D$2.verifyArgs(e);\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, e;\n  }\n\n  static verifyArgs(e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength$1(e.kernelSize, \"number\", 1, 2)) throw new ValueError$1(\"Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n}\n\nConv2D$2.className = \"Conv2D\", registerClass$1(Conv2D$2);\n\nclass Conv3D$2 extends Conv$1 {\n  constructor(e) {\n    super(3, e), Conv3D$2.verifyArgs(e);\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, e;\n  }\n\n  static verifyArgs(e) {\n    if (\"number\" != typeof e.kernelSize && (!Array.isArray(e.kernelSize) || 1 !== e.kernelSize.length && 3 !== e.kernelSize.length)) throw new ValueError$1(\"Conv3D expects config.kernelSize to be number or [number, number, number], but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n}\n\nConv3D$2.className = \"Conv3D\", registerClass$1(Conv3D$2);\n\nclass Conv2DTranspose$1 extends Conv2D$2 {\n  constructor(e) {\n    if (super(e), this.inputSpec = [new InputSpec$1({\n      ndim: 4\n    })], \"same\" !== this.padding && \"valid\" !== this.padding) throw new ValueError$1(\"Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode \".concat(this.padding));\n  }\n\n  build(e) {\n    if (4 !== (e = getExactlyOneShape$1(e)).length) throw new ValueError$1(\"Input should have rank 4; Received input shape: \" + JSON.stringify(e));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError$1(\"The channel dimension of the inputs should be defined. Found `None`.\");\n    var n = e[t],\n        r = this.kernelSize.concat([this.filters, n]);\n    this.kernel = this.addWeight(\"kernel\", r, \"float32\", this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [new InputSpec$1({\n      ndim: 4,\n      axes: {\n        [t]: n\n      }\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t = getExactlyOneTensor$1(e);\n      if (4 !== t.shape.length) throw new ValueError$1(\"Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-\".concat(t.shape.length));\n      var n = t.shape;\n      var r, a;\n      \"channelsFirst\" === this.dataFormat ? (r = 2, a = 3) : (r = 1, a = 2);\n      var s = n[a],\n          o = this.kernelSize[1],\n          i = this.strides[1],\n          l = [n[0], deconvLength$1(n[r], this.strides[0], this.kernelSize[0], this.padding), deconvLength$1(s, i, o, this.padding), this.filters];\n      \"channelsLast\" !== this.dataFormat && (t = transpose$5(t, [0, 2, 3, 1]));\n      var u = conv2dTranspose$2(t, this.kernel.read(), l, this.strides, this.padding);\n      return \"channelsLast\" !== this.dataFormat && (u = transpose$5(u, [0, 3, 1, 2])), null != this.bias && (u = biasAdd$1(u, this.bias.read(), this.dataFormat)), null != this.activation && (u = this.activation.apply(u)), u;\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape$1(e)).slice();\n    var n, r, a;\n    \"channelsFirst\" === this.dataFormat ? (n = 1, r = 2, a = 3) : (n = 3, r = 1, a = 2);\n    var s = this.kernelSize[0],\n        o = this.kernelSize[1],\n        i = this.strides[0],\n        l = this.strides[1];\n    return t[n] = this.filters, t[r] = deconvLength$1(t[r], i, s, this.padding), t[a] = deconvLength$1(t[a], l, o, this.padding), t;\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.dilationRate, e;\n  }\n\n}\n\nConv2DTranspose$1.className = \"Conv2DTranspose\", registerClass$1(Conv2DTranspose$1);\n\nclass Conv3DTranspose$1 extends Conv3D$2 {\n  constructor(e) {\n    if (super(e), this.inputSpec = [new InputSpec$1({\n      ndim: 5\n    })], \"same\" !== this.padding && \"valid\" !== this.padding) throw new ValueError$1(\"Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode \".concat(this.padding));\n  }\n\n  build(e) {\n    if (5 !== (e = getExactlyOneShape$1(e)).length) throw new ValueError$1(\"Input should have rank 5; Received input shape: \" + JSON.stringify(e));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError$1(\"The channel dimension of the inputs should be defined. Found `None`.\");\n    var n = e[t],\n        r = this.kernelSize.concat([this.filters, n]);\n    this.kernel = this.addWeight(\"kernel\", r, \"float32\", this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [new InputSpec$1({\n      ndim: 5,\n      axes: {\n        [t]: n\n      }\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t = getExactlyOneTensor$1(e);\n      if (5 !== t.shape.length) throw new ValueError$1(\"Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-\".concat(t.shape.length));\n      var n = t.shape;\n      var r, a, s;\n      \"channelsFirst\" === this.dataFormat ? (s = 2, r = 3, a = 4) : (s = 1, r = 2, a = 3);\n      var o = n[r],\n          i = n[a],\n          l = this.kernelSize[1],\n          u = this.kernelSize[2],\n          c = this.strides[1],\n          p = this.strides[2],\n          d = [n[0], deconvLength$1(n[s], this.strides[0], this.kernelSize[0], this.padding), deconvLength$1(o, c, l, this.padding), deconvLength$1(i, p, u, this.padding), this.filters];\n      \"channelsLast\" !== this.dataFormat && (t = transpose$5(t, [0, 2, 3, 4, 1]));\n      var h = conv3dTranspose$2(t, this.kernel.read(), d, this.strides, this.padding);\n      return \"channelsLast\" !== this.dataFormat && (h = transpose$5(h, [0, 4, 1, 2, 3])), null !== this.bias && (h = biasAdd$1(h, this.bias.read(), this.dataFormat)), null !== this.activation && (h = this.activation.apply(h)), h;\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape$1(e)).slice();\n    var n, r, a, s;\n    \"channelsFirst\" === this.dataFormat ? (n = 1, r = 2, a = 3, s = 4) : (n = 4, r = 1, a = 2, s = 3);\n    var o = this.kernelSize[0],\n        i = this.kernelSize[1],\n        l = this.kernelSize[2],\n        u = this.strides[0],\n        c = this.strides[1],\n        p = this.strides[2];\n    return t[n] = this.filters, t[r] = deconvLength$1(t[r], u, o, this.padding), t[a] = deconvLength$1(t[a], c, i, this.padding), t[s] = deconvLength$1(t[s], p, l, this.padding), t;\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.dilationRate, e;\n  }\n\n}\n\nConv3DTranspose$1.className = \"Conv3DTranspose\", registerClass$1(Conv3DTranspose$1);\n\nclass SeparableConv$1 extends Conv$1 {\n  constructor(e, t) {\n    if (super(e, t), this.DEFAULT_DEPTHWISE_INITIALIZER = \"glorotUniform\", this.DEFAULT_POINTWISE_INITIALIZER = \"glorotUniform\", this.depthwiseKernel = null, this.pointwiseKernel = null, null == t.filters) throw new ValueError$1(\"The `filters` configuration field is required by SeparableConv, but is unspecified.\");\n    if (null != t.kernelInitializer || null != t.kernelRegularizer || null != t.kernelConstraint) throw new ValueError$1(\"Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.\");\n    if (null != t.padding && \"same\" !== t.padding && \"valid\" !== t.padding) throw new ValueError$1(\"SeparableConv\".concat(this.rank, \"D supports only padding modes: 'same' and 'valid', but received \").concat(JSON.stringify(t.padding)));\n    this.depthMultiplier = null == t.depthMultiplier ? 1 : t.depthMultiplier, this.depthwiseInitializer = getInitializer$1(t.depthwiseInitializer || this.DEFAULT_DEPTHWISE_INITIALIZER), this.depthwiseRegularizer = getRegularizer$1(t.depthwiseRegularizer), this.depthwiseConstraint = getConstraint$1(t.depthwiseConstraint), this.pointwiseInitializer = getInitializer$1(t.depthwiseInitializer || this.DEFAULT_POINTWISE_INITIALIZER), this.pointwiseRegularizer = getRegularizer$1(t.pointwiseRegularizer), this.pointwiseConstraint = getConstraint$1(t.pointwiseConstraint);\n  }\n\n  build(e) {\n    if ((e = getExactlyOneShape$1(e)).length < this.rank + 2) throw new ValueError$1(\"Inputs to SeparableConv\".concat(this.rank, \"D should have rank \").concat(this.rank + 2, \", but received input shape: \").concat(JSON.stringify(e)));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t] || e[t] < 0) throw new ValueError$1(\"The channel dimension of the inputs should be defined, but found \".concat(JSON.stringify(e[t])));\n    var n = e[t],\n        r = this.kernelSize.concat([n, this.depthMultiplier]),\n        a = [];\n\n    for (var _e195 = 0; _e195 < this.rank; ++_e195) {\n      a.push(1);\n    }\n\n    a.push(n * this.depthMultiplier, this.filters);\n    var s = !0;\n    this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", r, \"float32\", this.depthwiseInitializer, this.depthwiseRegularizer, s, this.depthwiseConstraint), this.pointwiseKernel = this.addWeight(\"pointwise_kernel\", a, \"float32\", this.pointwiseInitializer, this.pointwiseRegularizer, s, this.pointwiseConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, s, this.biasConstraint) : null, this.inputSpec = [new InputSpec$1({\n      ndim: this.rank + 2,\n      axes: {\n        [t]: n\n      }\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t;\n      if (e = getExactlyOneTensor$1(e), 1 === this.rank) throw new NotImplementedError$1(\"1D separable convolution is not implemented yet.\");\n      return 2 === this.rank && (\"channelsFirst\" === this.dataFormat && (e = transpose$5(e, [0, 2, 3, 1])), t = separableConv2d$2(e, this.depthwiseKernel.read(), this.pointwiseKernel.read(), this.strides, this.padding, this.dilationRate, \"NHWC\")), this.useBias && (t = biasAdd$1(t, this.bias.read(), this.dataFormat)), null != this.activation && (t = this.activation.apply(t)), \"channelsFirst\" === this.dataFormat && (t = transpose$5(t, [0, 3, 1, 2])), t;\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, delete e.kernelInitializer, delete e.kernelRegularizer, delete e.kernelConstraint, e.depthwiseInitializer = serializeInitializer$1(this.depthwiseInitializer), e.pointwiseInitializer = serializeInitializer$1(this.pointwiseInitializer), e.depthwiseRegularizer = serializeRegularizer$1(this.depthwiseRegularizer), e.pointwiseRegularizer = serializeRegularizer$1(this.pointwiseRegularizer), e.depthwiseConstraint = serializeConstraint$1(this.depthwiseConstraint), e.pointwiseConstraint = serializeConstraint$1(this.pointwiseConstraint), e;\n  }\n\n}\n\nSeparableConv$1.className = \"SeparableConv\";\n\nclass SeparableConv2D$1 extends SeparableConv$1 {\n  constructor(e) {\n    super(2, e);\n  }\n\n}\n\nSeparableConv2D$1.className = \"SeparableConv2D\", registerClass$1(SeparableConv2D$1);\n\nclass Conv1D$1 extends Conv$1 {\n  constructor(e) {\n    super(1, e), Conv1D$1.verifyArgs(e), this.inputSpec = [{\n      ndim: 3\n    }];\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, delete e.dataFormat, e;\n  }\n\n  static verifyArgs(e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength$1(e.kernelSize, \"number\", 1, 1)) throw new ValueError$1(\"Conv1D expects config.kernelSize to be number or number[] with length 1, but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n}\n\nConv1D$1.className = \"Conv1D\", registerClass$1(Conv1D$1);\n\nclass Cropping2D$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.cropping = \"number\" == typeof e.cropping ? [[e.cropping, e.cropping], [e.cropping, e.cropping]] : \"number\" == typeof e.cropping[0] ? [[e.cropping[0], e.cropping[0]], [e.cropping[1], e.cropping[1]]] : e.cropping, this.dataFormat = void 0 === e.dataFormat ? \"channelsLast\" : e.dataFormat, this.inputSpec = [{\n      ndim: 4\n    }];\n  }\n\n  computeOutputShape(e) {\n    return \"channelsFirst\" === this.dataFormat ? [e[0], e[1], e[2] - this.cropping[0][0] - this.cropping[0][1], e[3] - this.cropping[1][0] - this.cropping[1][1]] : [e[0], e[1] - this.cropping[0][0] - this.cropping[0][1], e[2] - this.cropping[1][0] - this.cropping[1][1], e[3]];\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      if (e = getExactlyOneTensor$1(e), \"channelsLast\" === this.dataFormat) {\n        var _t153 = sliceAlongAxis$1(e, this.cropping[0][0], e.shape[1] - this.cropping[0][0] - this.cropping[0][1], 2);\n\n        return sliceAlongAxis$1(_t153, this.cropping[1][0], e.shape[2] - this.cropping[1][1] - this.cropping[1][0], 3);\n      }\n\n      {\n        var _t154 = sliceAlongAxis$1(e, this.cropping[0][0], e.shape[2] - this.cropping[0][0] - this.cropping[0][1], 3);\n\n        return sliceAlongAxis$1(_t154, this.cropping[1][0], e.shape[3] - this.cropping[1][1] - this.cropping[1][0], 4);\n      }\n    });\n  }\n\n  getConfig() {\n    var e = {\n      cropping: this.cropping,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nCropping2D$1.className = \"Cropping2D\", registerClass$1(Cropping2D$1);\n\nclass UpSampling2D$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.DEFAULT_SIZE = [2, 2], this.inputSpec = [{\n      ndim: 4\n    }], this.size = null == e.size ? this.DEFAULT_SIZE : e.size, this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat$1(this.dataFormat), this.interpolation = null == e.interpolation ? \"nearest\" : e.interpolation, checkInterpolationFormat$1(this.interpolation);\n  }\n\n  computeOutputShape(e) {\n    return \"channelsFirst\" === this.dataFormat ? [e[0], e[1], null == e[2] ? null : this.size[0] * e[2], null == e[3] ? null : this.size[1] * e[3]] : [e[0], null == e[1] ? null : this.size[0] * e[1], null == e[2] ? null : this.size[1] * e[2], e[3]];\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t = getExactlyOneTensor$1(e);\n      var n = t.shape;\n\n      if (\"channelsFirst\" === this.dataFormat) {\n        t = transpose$5(t, [0, 2, 3, 1]);\n\n        var _e196 = this.size[0] * n[2],\n            r = this.size[1] * n[3],\n            a = \"nearest\" === this.interpolation ? image$2.resizeNearestNeighbor(t, [_e196, r]) : image$2.resizeBilinear(t, [_e196, r]);\n\n        return transpose$5(a, [0, 3, 1, 2]);\n      }\n\n      {\n        var _e197 = this.size[0] * n[1],\n            _r53 = this.size[1] * n[2];\n\n        return \"nearest\" === this.interpolation ? image$2.resizeNearestNeighbor(t, [_e197, _r53]) : image$2.resizeBilinear(t, [_e197, _r53]);\n      }\n    });\n  }\n\n  getConfig() {\n    var e = {\n      size: this.size,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction depthwiseConv2d$4(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : [1, 1];\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"valid\";\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  return tidy$1(() => {\n    null == a && (a = imageDataFormat$1()), checkDataFormat$1(a);\n    var o = preprocessConv2DInput$1(e, a);\n    if (4 !== e.rank) throw new ValueError$1(\"Input for depthwiseConv2d is required to be 4-D, but is instead \".concat(e.rank, \"-D\"));\n    if (4 !== t.rank) throw new ValueError$1(\"depthwiseKernel is required to be 4-D, but is instead \".concat(t.rank, \"-D\"));\n    return o = depthwiseConv2d$5(o, t, n, \"same\" === r ? \"same\" : \"valid\", \"NHWC\", s), \"channelsFirst\" === a && (o = transpose$5(o, [0, 3, 1, 2])), o;\n  });\n}\n\nUpSampling2D$1.className = \"UpSampling2D\", registerClass$1(UpSampling2D$1);\n\nclass DepthwiseConv2D$1 extends BaseConv$1 {\n  constructor(e) {\n    super(2, e), this.depthwiseKernel = null, this.depthMultiplier = null == e.depthMultiplier ? 1 : e.depthMultiplier, this.depthwiseInitializer = getInitializer$1(e.depthwiseInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.depthwiseConstraint = getConstraint$1(e.depthwiseConstraint), this.depthwiseRegularizer = getRegularizer$1(e.depthwiseRegularizer);\n  }\n\n  build(e) {\n    if ((e = getExactlyOneShape$1(e)).length < 4) throw new ValueError$1(\"Inputs to DepthwiseConv2D should have rank 4. Received input shape: \".concat(JSON.stringify(e), \".\"));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : 3;\n    if (null == e[t] || e[t] < 0) throw new ValueError$1(\"The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (\".concat(e[t], \").\"));\n    var n = e[t];\n    this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", [this.kernelSize[0], this.kernelSize[1], n, this.depthMultiplier], null, this.depthwiseInitializer, this.depthwiseRegularizer, !0, this.depthwiseConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [n * this.depthMultiplier], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t = depthwiseConv2d$4(e = getExactlyOneTensor$1(e), this.depthwiseKernel.read(), this.strides, this.padding, this.dataFormat, null);\n      return this.useBias && (t = biasAdd$1(t, this.bias.read(), this.dataFormat)), null != this.activation && (t = this.activation.apply(t)), t;\n    });\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape$1(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[3] : e[2],\n        n = \"channelsFirst\" === this.dataFormat ? e[1] * this.depthMultiplier : e[3] * this.depthMultiplier,\n        r = convOutputLength$1(\"channelsFirst\" === this.dataFormat ? e[2] : e[1], this.kernelSize[0], this.padding, this.strides[0]),\n        a = convOutputLength$1(t, this.kernelSize[1], this.padding, this.strides[1]);\n    return \"channelsFirst\" === this.dataFormat ? [e[0], n, r, a] : [e[0], r, a, n];\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return e.depthMultiplier = this.depthMultiplier, e.depthwiseInitializer = serializeInitializer$1(this.depthwiseInitializer), e.depthwiseRegularizer = serializeRegularizer$1(this.depthwiseRegularizer), e.depthwiseConstraint = serializeConstraint$1(this.depthwiseRegularizer), e;\n  }\n\n}\n\nfunction standardizeArgs$1(e, t, n, r) {\n  if (Array.isArray(e)) {\n    if (null != t || null != n) throw new ValueError$1(\"When inputs is an array, neither initialState or constants should be provided\");\n    null != r && (n = e.slice(e.length - r, e.length), e = e.slice(0, e.length - r)), e.length > 1 && (t = e.slice(1, e.length)), e = e[0];\n  }\n\n  function a(e) {\n    return null == e || Array.isArray(e) ? e : [e];\n  }\n\n  return {\n    inputs: e,\n    initialState: t = a(t),\n    constants: n = a(n)\n  };\n}\n\nfunction rnn$2(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !1;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : !1;\n  return tidy$1(() => {\n    var l = t.shape.length;\n    if (l < 3) throw new ValueError$1(\"Input should be at least 3D, but is \".concat(l, \"D.\"));\n    var u = [1, 0].concat(range$7(2, l));\n    if (t = transpose$5(t, u), null != s) throw new NotImplementedError$1(\"The rnn() functoin of the deeplearn.js backend does not support constants yet.\");\n    o && console.warn(\"Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend.\"), null != a && ((a = cast$7(cast$7(a, \"bool\"), \"float32\")).rank === l - 1 && (a = expandDims$7(a, -1)), a = transpose$5(a, u)), r && (t = reverse$5(t, 0), null != a && (a = reverse$5(a, 0)));\n    var c = [];\n    var p,\n        d = n;\n    var h = t.shape[0],\n        m = unstack$1(t);\n    var f, g;\n    null != a && (f = unstack$1(a));\n\n    var _loop20 = function _loop20(_t155) {\n      var n = m[_t155],\n          r = tidy$1(() => e(n, d));\n      if (null == a) p = r[0], d = r[1];else {\n        var _e198 = tidy$1(() => {\n          var e = f[_t155],\n              n = sub$5(onesLike$5(e), e);\n          return {\n            output: add$5(mul$1(r[0], e), mul$1(d[0], n)),\n            newStates: d.map((t, a) => add$5(mul$1(r[1][a], e), mul$1(t, n)))\n          };\n        });\n\n        p = _e198.output, d = _e198.newStates;\n      }\n      i && c.push(p);\n    };\n\n    for (var _t155 = 0; _t155 < h; ++_t155) {\n      _loop20(_t155);\n    }\n\n    return i && (g = stack$1(c, 1)), [p, g, d];\n  });\n}\n\nDepthwiseConv2D$1.className = \"DepthwiseConv2D\", registerClass$1(DepthwiseConv2D$1);\n\nclass RNN$1 extends Layer$1 {\n  constructor(e) {\n    var t;\n    if (super(e), null == e.cell) throw new ValueError$1(\"cell property is missing for the constructor of RNN.\");\n    if (t = Array.isArray(e.cell) ? new StackedRNNCells$1({\n      cells: e.cell\n    }) : e.cell, null == t.stateSize) throw new ValueError$1(\"The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).\");\n    this.cell = t, this.returnSequences = null != e.returnSequences && e.returnSequences, this.returnState = null != e.returnState && e.returnState, this.goBackwards = null != e.goBackwards && e.goBackwards, this._stateful = null != e.stateful && e.stateful, this.unroll = null != e.unroll && e.unroll, this.supportsMasking = !0, this.inputSpec = [new InputSpec$1({\n      ndim: 3\n    })], this.stateSpec = null, this.states_ = null, this.numConstants = null, this.keptStates = [];\n  }\n\n  getStates() {\n    return null == this.states_ ? range$7(0, Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1).map(e => null) : this.states_;\n  }\n\n  setStates(e) {\n    this.states_ = e;\n  }\n\n  computeOutputShape(e) {\n    isArrayOfShapes$1(e) && (e = e[0]), e = e;\n    var t = this.cell.stateSize;\n    Array.isArray(t) || (t = [t]);\n    var n = t[0];\n    var r;\n\n    if (r = this.returnSequences ? [e[0], e[1], n] : [e[0], n], this.returnState) {\n      var _n69 = [];\n\n      for (var _r54 of t) {\n        _n69.push([e[0], _r54]);\n      }\n\n      return [r].concat(_n69);\n    }\n\n    return r;\n  }\n\n  computeMask(e, t) {\n    return tidy$1(() => {\n      Array.isArray(t) && (t = t[0]);\n      var e = this.returnSequences ? t : null;\n\n      if (this.returnState) {\n        var _t156 = this.states.map(e => null);\n\n        return [e].concat(_t156);\n      }\n\n      return e;\n    });\n  }\n\n  get states() {\n    if (null == this.states_) {\n      var _e199 = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1,\n          t = [];\n\n      for (var n = 0; n < _e199; ++n) {\n        t.push(null);\n      }\n\n      return t;\n    }\n\n    return this.states_;\n  }\n\n  set states(e) {\n    this.states_ = e;\n  }\n\n  build(e) {\n    if (null != this.numConstants) throw new NotImplementedError$1(\"Constants support is not implemented in RNN yet.\");\n    isArrayOfShapes$1(e) && (e = e[0]), e = e;\n    var t = this.stateful ? e[0] : null,\n        n = e.slice(2);\n    this.inputSpec[0] = new InputSpec$1({\n      shape: [t, null, ...n]\n    });\n    var r = [e[0]].concat(e.slice(2));\n    var a;\n\n    if (this.cell.build(r), a = Array.isArray(this.cell.stateSize) ? this.cell.stateSize : [this.cell.stateSize], null != this.stateSpec) {\n      if (!arraysEqual$1(this.stateSpec.map(e => e.shape[e.shape.length - 1]), a)) throw new ValueError$1(\"An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=\".concat(this.stateSpec, \"; However cell.stateSize is \").concat(this.cell.stateSize));\n    } else this.stateSpec = a.map(e => new InputSpec$1({\n      shape: [null, e]\n    }));\n\n    this.stateful && this.resetStates();\n  }\n\n  resetStates(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    tidy$1(() => {\n      if (!this.stateful) throw new AttributeError$1(\"Cannot call resetStates() on an RNN Layer that is not stateful.\");\n      var n = this.inputSpec[0].shape[0];\n      if (null == n) throw new ValueError$1(\"If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.\");\n      if (null == this.states_) this.states_ = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.map(e => zeros$4([n, e])) : [zeros$4([n, this.cell.stateSize])];else if (null == e) dispose$1(this.states_), null != this.keptStates && (dispose$1(this.keptStates), this.keptStates = []), Array.isArray(this.cell.stateSize) ? this.states_ = this.cell.stateSize.map(e => zeros$4([n, e])) : this.states_[0] = zeros$4([n, this.cell.stateSize]);else {\n        if (Array.isArray(e) || (e = [e]), e.length !== this.states_.length) throw new ValueError$1(\"Layer \".concat(this.name, \" expects \").concat(this.states_.length, \" state(s), but it received \").concat(e.length, \" state value(s). Input received: \").concat(e));\n        !0 === t ? this.keptStates.push(this.states_.slice()) : dispose$1(this.states_);\n\n        for (var _t157 = 0; _t157 < this.states_.length; ++_t157) {\n          var r = e[_t157],\n              a = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[_t157] : this.cell.stateSize,\n              s = [n, a];\n          if (!arraysEqual$1(r.shape, s)) throw new ValueError$1(\"State \".concat(_t157, \" is incompatible with layer \").concat(this.name, \": expected shape=\").concat(s, \", received shape=\").concat(r.shape));\n          this.states_[_t157] = r;\n        }\n      }\n      this.states_ = this.states_.map(e => keep$1(e.clone()));\n    });\n  }\n\n  apply(e, t) {\n    var n = null == t ? null : t.initialState,\n        r = null == t ? null : t.constants;\n    null == t && (t = {});\n    var a = standardizeArgs$1(e, n, r, this.numConstants);\n    e = a.inputs, n = a.initialState, r = a.constants;\n    var s = [],\n        o = [];\n\n    if (null != n) {\n      t.initialState = n, s = s.concat(n), this.stateSpec = [];\n\n      for (var _e200 of n) {\n        this.stateSpec.push(new InputSpec$1({\n          shape: _e200.shape\n        }));\n      }\n\n      o = o.concat(this.stateSpec);\n    }\n\n    if (null != r && (t.constants = r, s = s.concat(r), this.numConstants = r.length), s[0] instanceof SymbolicTensor$1) {\n      var _n70 = [e].concat(s),\n          _r55 = this.inputSpec.concat(o),\n          _a38 = this.inputSpec;\n\n      this.inputSpec = _r55;\n      var i = super.apply(_n70, t);\n      return this.inputSpec = _a38, i;\n    }\n\n    return super.apply(e, t);\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var n = null == t ? null : t.mask,\n          r = null == t ? null : t.training;\n      var a = null == t ? null : t.initialState;\n      e = getExactlyOneTensor$1(e), null == a && (a = this.stateful ? this.states_ : this.getInitialState(e));\n      var s = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      if (a.length !== s) throw new ValueError$1(\"RNN Layer has \".concat(s, \" state(s) but was passed \").concat(a.length, \" initial state(s).\"));\n      this.unroll && console.warn(\"Ignoring unroll = true for RNN layer, due to imperative backend.\");\n      var o = {\n        training: r\n      },\n          i = rnn$2((e, t) => {\n        var n = this.cell.call([e].concat(t), o);\n        return [n[0], n.slice(1)];\n      }, e, a, this.goBackwards, n, null, this.unroll, this.returnSequences),\n          l = i[0],\n          u = i[1],\n          c = i[2];\n      this.stateful && this.resetStates(c, r);\n      var p = this.returnSequences ? u : l;\n      return this.returnState ? [p].concat(c) : p;\n    });\n  }\n\n  getInitialState(e) {\n    return tidy$1(() => {\n      var t = zeros$4(e.shape);\n      return t = sum$6(t, [1, 2]), t = expandDims$6(t), Array.isArray(this.cell.stateSize) ? this.cell.stateSize.map(e => e > 1 ? tile$6(t, [1, e]) : t) : this.cell.stateSize > 1 ? [tile$6(t, [1, this.cell.stateSize])] : [t];\n    });\n  }\n\n  get trainableWeights() {\n    return this.trainable ? this.cell.trainableWeights : [];\n  }\n\n  get nonTrainableWeights() {\n    return this.trainable ? this.cell.nonTrainableWeights : this.cell.weights;\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    super.setFastWeightInitDuringBuild(e), null != this.cell && this.cell.setFastWeightInitDuringBuild(e);\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n    null != this.numConstants && (t.numConstants = this.numConstants);\n    var n = this.cell.getConfig();\n    return this.getClassName() === RNN$1.className && (t.cell = {\n      className: this.cell.getClassName(),\n      config: n\n    }), Object.assign({}, n, e, t);\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = deserialize$1(t.cell, n);\n    return new e(Object.assign(t, {\n      cell: r\n    }));\n  }\n\n}\n\nRNN$1.className = \"RNN\", registerClass$1(RNN$1);\n\nclass RNNCell$1 extends Layer$1 {}\n\nclass SimpleRNNCell$1 extends RNNCell$1 {\n  constructor(e) {\n    super(e), this.DEFAULT_ACTIVATION = \"tanh\", this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", this.units = e.units, assertPositiveInteger$1(this.units, \"units\"), this.activation = getActivation$1(null == e.activation ? this.DEFAULT_ACTIVATION : e.activation), this.useBias = null == e.useBias || e.useBias, this.kernelInitializer = getInitializer$1(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.recurrentInitializer = getInitializer$1(e.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER), this.biasInitializer = getInitializer$1(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.kernelRegularizer = getRegularizer$1(e.kernelRegularizer), this.recurrentRegularizer = getRegularizer$1(e.recurrentRegularizer), this.biasRegularizer = getRegularizer$1(e.biasRegularizer), this.kernelConstraint = getConstraint$1(e.kernelConstraint), this.recurrentConstraint = getConstraint$1(e.recurrentConstraint), this.biasConstraint = getConstraint$1(e.biasConstraint), this.dropout = min$6([1, max$6([0, null == e.dropout ? 0 : e.dropout])]), this.recurrentDropout = min$6([1, max$6([0, null == e.recurrentDropout ? 0 : e.recurrentDropout])]), this.stateSize = this.units, this.dropoutMask = null, this.recurrentDropoutMask = null;\n  }\n\n  build(e) {\n    e = getExactlyOneShape$1(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      if (2 !== (e = e).length) throw new ValueError$1(\"SimpleRNNCell expects 2 input Tensors, got \".concat(e.length, \".\"));\n      var n = e[1];\n      e = e[0];\n      var r = null != t.training && t.training;\n      var a;\n      0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(e),\n        rate: this.dropout,\n        training: r\n      })), 0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(n),\n        rate: this.recurrentDropout,\n        training: r\n      }));\n      var s = this.dropoutMask,\n          o = this.recurrentDropoutMask;\n      a = dot$3(null != s ? mul$1(e, s) : e, this.kernel.read()), null != this.bias && (a = biasAdd$1(a, this.bias.read())), null != o && (n = mul$1(n, o));\n      var i = add$5(a, dot$3(n, this.recurrentKernel.read()));\n      return null != this.activation && (i = this.activation.apply(i)), [i, i];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      units: this.units,\n      activation: serializeActivation$1(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer$1(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer$1(this.recurrentInitializer),\n      biasInitializer: serializeInitializer$1(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer$1(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer$1(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer$1(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer$1(this.activityRegularizer),\n      kernelConstraint: serializeConstraint$1(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint$1(this.recurrentConstraint),\n      biasConstraint: serializeConstraint$1(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n    return Object.assign({}, e, t);\n  }\n\n}\n\nSimpleRNNCell$1.className = \"SimpleRNNCell\", registerClass$1(SimpleRNNCell$1);\n\nclass SimpleRNN$1 extends RNN$1 {\n  constructor(e) {\n    e.cell = new SimpleRNNCell$1(e), super(e);\n  }\n\n  call(e, t) {\n    return tidy$1(() => (null != this.cell.dropoutMask && (dispose$1(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose$1(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), super.call(e, {\n      mask: null == t ? null : t.mask,\n      training: null == t ? null : t.training,\n      initialState: null == t ? null : t.initialState\n    })));\n  }\n\n  static fromConfig(e, t) {\n    return new e(t);\n  }\n\n}\n\nSimpleRNN$1.className = \"SimpleRNN\", registerClass$1(SimpleRNN$1);\n\nclass GRUCell$1 extends RNNCell$1 {\n  constructor(e) {\n    if (super(e), this.DEFAULT_ACTIVATION = \"tanh\", this.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", e.resetAfter) throw new ValueError$1(\"GRUCell does not support reset_after parameter set to true.\");\n    this.units = e.units, assertPositiveInteger$1(this.units, \"units\"), this.activation = getActivation$1(void 0 === e.activation ? this.DEFAULT_ACTIVATION : e.activation), this.recurrentActivation = getActivation$1(void 0 === e.recurrentActivation ? this.DEFAULT_RECURRENT_ACTIVATION : e.recurrentActivation), this.useBias = null == e.useBias || e.useBias, this.kernelInitializer = getInitializer$1(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.recurrentInitializer = getInitializer$1(e.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER), this.biasInitializer = getInitializer$1(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.kernelRegularizer = getRegularizer$1(e.kernelRegularizer), this.recurrentRegularizer = getRegularizer$1(e.recurrentRegularizer), this.biasRegularizer = getRegularizer$1(e.biasRegularizer), this.kernelConstraint = getConstraint$1(e.kernelConstraint), this.recurrentConstraint = getConstraint$1(e.recurrentConstraint), this.biasConstraint = getConstraint$1(e.biasConstraint), this.dropout = min$6([1, max$6([0, null == e.dropout ? 0 : e.dropout])]), this.recurrentDropout = min$6([1, max$6([0, null == e.recurrentDropout ? 0 : e.recurrentDropout])]), this.implementation = e.implementation, this.stateSize = this.units, this.dropoutMask = null, this.recurrentDropoutMask = null;\n  }\n\n  build(e) {\n    e = getExactlyOneShape$1(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], 3 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 3 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [3 * this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      if (2 !== (e = e).length) throw new ValueError$1(\"GRUCell expects 2 input Tensors (inputs, h, c), got \".concat(e.length, \".\"));\n      var n = null != t.training && t.training;\n      var r = e[1];\n      e = e[0], 0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(e),\n        rate: this.dropout,\n        training: n,\n        count: 3\n      })), 0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(r),\n        rate: this.recurrentDropout,\n        training: n,\n        count: 3\n      }));\n      var a = this.recurrentDropoutMask;\n      var s, o, i;\n      0 < this.dropout && this.dropout < 1 && (e = mul$1(e, this.dropoutMask[0]));\n      var l = dot$3(e, this.kernel.read());\n      this.useBias && (l = biasAdd$1(l, this.bias.read())), 0 < this.recurrentDropout && this.recurrentDropout < 1 && (r = mul$1(r, a[0]));\n      var u = this.recurrentKernel.read(),\n          [c, p] = split$4(u, [2 * this.units, this.units], u.rank - 1),\n          d = dot$3(r, c),\n          [h, m, f] = split$4(l, 3, l.rank - 1),\n          [g, $] = split$4(d, 2, d.rank - 1);\n      s = this.recurrentActivation.apply(add$5(h, g)), o = this.recurrentActivation.apply(add$5(m, $));\n      var y = dot$3(mul$1(o, r), p);\n      i = this.activation.apply(add$5(f, y));\n      var b = add$5(mul$1(s, r), mul$1(add$5(1, neg$5(s)), i));\n      return [b, b];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      units: this.units,\n      activation: serializeActivation$1(this.activation),\n      recurrentActivation: serializeActivation$1(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer$1(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer$1(this.recurrentInitializer),\n      biasInitializer: serializeInitializer$1(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer$1(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer$1(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer$1(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer$1(this.activityRegularizer),\n      kernelConstraint: serializeConstraint$1(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint$1(this.recurrentConstraint),\n      biasConstraint: serializeConstraint$1(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: !1\n    };\n    return Object.assign({}, e, t);\n  }\n\n}\n\nGRUCell$1.className = \"GRUCell\", registerClass$1(GRUCell$1);\n\nclass GRU$1 extends RNN$1 {\n  constructor(e) {\n    0 === e.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), e.cell = new GRUCell$1(e), super(e);\n  }\n\n  call(e, t) {\n    return tidy$1(() => (null != this.cell.dropoutMask && (dispose$1(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose$1(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), super.call(e, {\n      mask: null == t ? null : t.mask,\n      training: null == t ? null : t.training,\n      initialState: null == t ? null : t.initialState\n    })));\n  }\n\n  static fromConfig(e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }\n\n}\n\nGRU$1.className = \"GRU\", registerClass$1(GRU$1);\n\nclass LSTMCell$1 extends RNNCell$1 {\n  constructor(e) {\n    super(e), this.DEFAULT_ACTIVATION = \"tanh\", this.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", this.units = e.units, assertPositiveInteger$1(this.units, \"units\"), this.activation = getActivation$1(void 0 === e.activation ? this.DEFAULT_ACTIVATION : e.activation), this.recurrentActivation = getActivation$1(void 0 === e.recurrentActivation ? this.DEFAULT_RECURRENT_ACTIVATION : e.recurrentActivation), this.useBias = null == e.useBias || e.useBias, this.kernelInitializer = getInitializer$1(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.recurrentInitializer = getInitializer$1(e.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER), this.biasInitializer = getInitializer$1(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.unitForgetBias = e.unitForgetBias, this.kernelRegularizer = getRegularizer$1(e.kernelRegularizer), this.recurrentRegularizer = getRegularizer$1(e.recurrentRegularizer), this.biasRegularizer = getRegularizer$1(e.biasRegularizer), this.kernelConstraint = getConstraint$1(e.kernelConstraint), this.recurrentConstraint = getConstraint$1(e.recurrentConstraint), this.biasConstraint = getConstraint$1(e.biasConstraint), this.dropout = min$6([1, max$6([0, null == e.dropout ? 0 : e.dropout])]), this.recurrentDropout = min$6([1, max$6([0, null == e.recurrentDropout ? 0 : e.recurrentDropout])]), this.implementation = e.implementation, this.stateSize = [this.units, this.units], this.dropoutMask = null, this.recurrentDropoutMask = null;\n  }\n\n  build(e) {\n    var t;\n    var n;\n\n    if (e = getExactlyOneShape$1(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], 4 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 4 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias) {\n      if (this.unitForgetBias) {\n        var _e201 = this.biasInitializer,\n            r = this.units;\n        n = new ((t = class extends Initializer$1 {\n          apply(t, n) {\n            var a = _e201.apply([r]),\n                s = new Ones$1().apply([r]),\n                o = _e201.apply([2 * r]);\n\n            return concatAlongFirstAxis$1(concatAlongFirstAxis$1(a, s), o);\n          }\n\n        }).className = \"CustomInit\", t)();\n      } else n = this.biasInitializer;\n\n      this.bias = this.addWeight(\"bias\", [4 * this.units], null, n, this.biasRegularizer, !0, this.biasConstraint);\n    } else this.bias = null;\n\n    this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var n = null != t.training && t.training;\n      if (3 !== (e = e).length) throw new ValueError$1(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \".concat(e.length, \".\"));\n      var r = e[1];\n      var a = e[2];\n      e = e[0], 0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(e),\n        rate: this.dropout,\n        training: n,\n        count: 4\n      })), 0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(r),\n        rate: this.recurrentDropout,\n        training: n,\n        count: 4\n      }));\n      var s = this.recurrentDropoutMask;\n      var o, i, l, u;\n      0 < this.dropout && this.dropout < 1 && (e = mul$1(e, this.dropoutMask[0]));\n      var c = dot$3(e, this.kernel.read());\n      0 < this.recurrentDropout && this.recurrentDropout < 1 && (r = mul$1(r, s[0])), c = add$5(c, dot$3(r, this.recurrentKernel.read())), this.useBias && (c = biasAdd$1(c, this.bias.read()));\n      var [p, d, h, m] = split$4(c, 4, c.rank - 1);\n      o = this.recurrentActivation.apply(p), i = this.recurrentActivation.apply(d), l = add$5(mul$1(i, a), mul$1(o, this.activation.apply(h))), u = this.recurrentActivation.apply(m);\n      var f = mul$1(u, this.activation.apply(l));\n      return [f, f, l];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      units: this.units,\n      activation: serializeActivation$1(this.activation),\n      recurrentActivation: serializeActivation$1(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer$1(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer$1(this.recurrentInitializer),\n      biasInitializer: serializeInitializer$1(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer$1(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer$1(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer$1(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer$1(this.activityRegularizer),\n      kernelConstraint: serializeConstraint$1(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint$1(this.recurrentConstraint),\n      biasConstraint: serializeConstraint$1(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n    return Object.assign({}, e, t);\n  }\n\n}\n\nLSTMCell$1.className = \"LSTMCell\", registerClass$1(LSTMCell$1);\n\nclass LSTM$1 extends RNN$1 {\n  constructor(e) {\n    0 === e.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), e.cell = new LSTMCell$1(e), super(e);\n  }\n\n  call(e, t) {\n    return tidy$1(() => (null != this.cell.dropoutMask && (dispose$1(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose$1(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), super.call(e, {\n      mask: null == t ? null : t.mask,\n      training: null == t ? null : t.training,\n      initialState: null == t ? null : t.initialState\n    })));\n  }\n\n  static fromConfig(e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }\n\n}\n\nLSTM$1.className = \"LSTM\", registerClass$1(LSTM$1);\n\nclass StackedRNNCells$1 extends RNNCell$1 {\n  constructor(e) {\n    super(e), this.cells = e.cells;\n  }\n\n  get stateSize() {\n    var e = [];\n\n    for (var t of this.cells.slice().reverse()) {\n      Array.isArray(t.stateSize) ? e.push(...t.stateSize) : e.push(t.stateSize);\n    }\n\n    return e;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var n = (e = e).slice(1);\n      var r = [];\n\n      for (var _e202 of this.cells.slice().reverse()) {\n        Array.isArray(_e202.stateSize) ? r.push(n.splice(0, _e202.stateSize.length)) : r.push(n.splice(0, 1));\n      }\n\n      r.reverse();\n      var a = [];\n      var s;\n\n      for (var o = 0; o < this.cells.length; ++o) {\n        var i = this.cells[o];\n        n = r[o], s = 0 === o ? [e[0]].concat(n) : [s[0]].concat(n), s = i.call(s, t), a.push(s.slice(1));\n      }\n\n      n = [];\n\n      for (var _e203 of a.slice().reverse()) {\n        n.push(..._e203);\n      }\n\n      return [s[0]].concat(n);\n    });\n  }\n\n  build(e) {\n    var t;\n    isArrayOfShapes$1(e) && (e = e[0]), e = e, this.cells.forEach((n, r) => {\n      nameScope$1(\"RNNCell_\".concat(r), () => {\n        n.build(e), t = Array.isArray(n.stateSize) ? n.stateSize[0] : n.stateSize, e = [e[0], t];\n      });\n    }), this.built = !0;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = this.cells.map(e => ({\n      className: e.getClassName(),\n      config: e.getConfig()\n    }));\n    return Object.assign({}, e, {\n      cells: t\n    });\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = [];\n\n    for (var _e204 of t.cells) {\n      r.push(deserialize$1(_e204, n));\n    }\n\n    return new e({\n      cells: r\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) return [];\n    var e = [];\n\n    for (var t of this.cells) {\n      e.push(...t.trainableWeights);\n    }\n\n    return e;\n  }\n\n  get nonTrainableWeights() {\n    var e = [];\n\n    for (var t of this.cells) {\n      e.push(...t.nonTrainableWeights);\n    }\n\n    if (!this.trainable) {\n      var _t158 = [];\n\n      for (var _e205 of this.cells) {\n        _t158.push(..._e205.trainableWeights);\n      }\n\n      return _t158.concat(e);\n    }\n\n    return e;\n  }\n\n  getWeights() {\n    var e = [];\n\n    for (var t of this.cells) {\n      e.push(...t.weights);\n    }\n\n    return batchGetValue$1(e);\n  }\n\n  setWeights(e) {\n    var t = [];\n\n    for (var n of this.cells) {\n      var r = e.splice(n.weights.length);\n\n      for (var _e206 = 0; _e206 < n.weights.length; ++_e206) {\n        t.push([n.weights[_e206], r[_e206]]);\n      }\n    }\n\n    batchSetValue$1(t);\n  }\n\n}\n\nfunction generateDropoutMask$1(e) {\n  var {\n    ones: t,\n    rate: n,\n    training: r = !1,\n    count: a = 1\n  } = e,\n      s = () => dropout$4(t(), n),\n      o = () => inTrainPhase$1(s, t, r);\n\n  return !a || a <= 1 ? keep$1(o().clone()) : Array(a).fill(void 0).map(o).map(e => keep$1(e.clone()));\n}\n\nStackedRNNCells$1.className = \"StackedRNNCells\", registerClass$1(StackedRNNCells$1);\n\nvar __rest$1 = function __rest$1(e, t) {\n  var n = {};\n\n  for (var r in e) {\n    Object.prototype.hasOwnProperty.call(e, r) && t.indexOf(r) < 0 && (n[r] = e[r]);\n  }\n\n  if (null != e && \"function\" == typeof Object.getOwnPropertySymbols) {\n    var a = 0;\n\n    for (r = Object.getOwnPropertySymbols(e); a < r.length; a++) {\n      t.indexOf(r[a]) < 0 && Object.prototype.propertyIsEnumerable.call(e, r[a]) && (n[r[a]] = e[r[a]]);\n    }\n  }\n\n  return n;\n};\n\nclass ConvRNN2D$1 extends RNN$1 {\n  constructor(e) {\n    if (e.unroll) throw new NotImplementedError$1(\"Unrolling is not possible with convolutional RNNs.\");\n    if (Array.isArray(e.cell)) throw new NotImplementedError$1(\"It is not possible at the moment to stack convolutional cells.\");\n    super(e), this.inputSpec = [new InputSpec$1({\n      ndim: 5\n    })];\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      if (null != this.cell.dropoutMask && (dispose$1(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose$1(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), t && t.constants) throw new ValueError$1(\"ConvRNN2D cell does not support constants\");\n      return super.call(e, {\n        mask: null == t ? null : t.mask,\n        training: null == t ? null : t.training,\n        initialState: null == t ? null : t.initialState\n      });\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = this.computeSingleOutputShape(e);\n    return this.returnSequences || (t = [t[0], ...t.slice(2)]), this.returnState && (t = [t, ...Array(2).fill([e[0], ...t.slice(-3)])]), t;\n  }\n\n  getInitialState(e) {\n    return tidy$1(() => {\n      var {\n        stateSize: t\n      } = this.cell,\n          n = this.computeSingleOutputShape(e.shape),\n          r = zeros$4([n[0], ...n.slice(2)]);\n      return Array.isArray(t) ? Array(t.length).fill(r) : [r];\n    });\n  }\n\n  resetStates(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    tidy$1(() => {\n      if (!this.stateful) throw new AttributeError$1(\"Cannot call resetStates() on an RNN Layer that is not stateful.\");\n      var n = this.inputSpec[0].shape,\n          r = this.computeSingleOutputShape(n),\n          a = [r[0], ...r.slice(2)];\n      if (null == n[0]) throw new ValueError$1(\"If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.\");\n      if (null == this.getStates()) this.states_ = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.map(() => zeros$4(a)) : [zeros$4(a)];else if (null == e) dispose$1(this.states_), null != this.keptStates && (dispose$1(this.keptStates), this.keptStates = []), Array.isArray(this.cell.stateSize) ? this.states_ = this.cell.stateSize.map(() => zeros$4(a)) : this.states_[0] = zeros$4(a);else {\n        if (Array.isArray(e) || (e = [e]), e.length !== this.states_.length) throw new ValueError$1(\"Layer \".concat(this.name, \" expects \").concat(this.states_.length, \" state(s), but it received \").concat(e.length, \" state value(s). Input received: \").concat(e));\n        t ? this.keptStates.push(this.states_.slice()) : dispose$1(this.states_);\n\n        for (var _t159 = 0; _t159 < this.states_.length; ++_t159) {\n          var _n71 = e[_t159],\n              _r56 = a;\n          if (!arraysEqual$1(_n71.shape, _r56)) throw new ValueError$1(\"State \".concat(_t159, \" is incompatible with layer \").concat(this.name, \": expected shape=\").concat(_r56, \", received shape=\").concat(_n71.shape));\n          this.states_[_t159] = _n71;\n        }\n      }\n      this.states_ = this.states_.map(e => keep$1(e.clone()));\n    });\n  }\n\n  computeSingleOutputShape(e) {\n    var {\n      dataFormat: t,\n      filters: n,\n      kernelSize: r,\n      padding: a,\n      strides: s,\n      dilationRate: o\n    } = this.cell,\n        i = \"channelsFirst\" === t,\n        l = e[i ? 4 : 3],\n        u = convOutputLength$1(e[i ? 3 : 2], r[0], a, s[0], o[0]),\n        c = convOutputLength$1(l, r[1], a, s[1], o[1]);\n    return [...e.slice(0, 2), ...(i ? [n, u, c] : [u, c, n])];\n  }\n\n}\n\nConvRNN2D$1.className = \"ConvRNN2D\";\n\nclass ConvLSTM2DCell$1 extends LSTMCell$1 {\n  constructor(e) {\n    var {\n      filters: t,\n      kernelSize: n,\n      strides: r,\n      padding: a,\n      dataFormat: s,\n      dilationRate: o\n    } = e;\n    super(Object.assign({}, e, {\n      units: t\n    })), this.filters = t, assertPositiveInteger$1(this.filters, \"filters\"), this.kernelSize = normalizeArray$1(n, 2, \"kernelSize\"), this.kernelSize.forEach(e => assertPositiveInteger$1(e, \"kernelSize\")), this.strides = normalizeArray$1(r || 1, 2, \"strides\"), this.strides.forEach(e => assertPositiveInteger$1(e, \"strides\")), this.padding = a || \"valid\", checkPaddingMode$1(this.padding), this.dataFormat = s || \"channelsLast\", checkDataFormat$1(this.dataFormat), this.dilationRate = normalizeArray$1(o || 1, 2, \"dilationRate\"), this.dilationRate.forEach(e => assertPositiveInteger$1(e, \"dilationRate\"));\n  }\n\n  build(e) {\n    var t;\n    e = getExactlyOneShape$1(e);\n    var n = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[n]) throw new ValueError$1(\"The channel dimension of the input should be defined. Found \".concat(e[n]));\n    var r = this.kernelSize.concat([e[n], 4 * this.filters]);\n    this.kernel = this.addWeight(\"kernel\", r, null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint);\n    var a = this.kernelSize.concat([this.filters, 4 * this.filters]);\n\n    if (this.recurrentKernel = this.addWeight(\"recurrent_kernel\", a, null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias) {\n      var _e207;\n\n      if (this.unitForgetBias) {\n        var _n72 = this.biasInitializer,\n            _r57 = this.filters;\n        _e207 = new ((t = class extends Initializer$1 {\n          apply(e, t) {\n            return concatenate$2([_n72.apply([_r57]), ones$3([_r57]), _n72.apply([2 * _r57])]);\n          }\n\n        }).className = \"CustomInit\", t)();\n      } else _e207 = this.biasInitializer;\n\n      this.bias = this.addWeight(\"bias\", [4 * this.filters], null, _e207, this.biasRegularizer, !0, this.biasConstraint);\n    }\n\n    this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      if (3 !== e.length) throw new ValueError$1(\"ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got \".concat(e.length, \".\"));\n      var n = t.training || !1,\n          r = e[0],\n          a = e[1],\n          s = e[2];\n      0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(r),\n        rate: this.dropout,\n        training: n,\n        count: 4\n      }));\n\n      var o = this.dropoutMask,\n          i = (e, t, n) => t && t[n] ? mul$1(t[n], e) : e;\n\n      var l = i(r, o, 0),\n          u = i(r, o, 1),\n          c = i(r, o, 2),\n          p = i(r, o, 3);\n      0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask$1({\n        ones: () => onesLike$5(a),\n        rate: this.recurrentDropout,\n        training: n,\n        count: 4\n      }));\n      var d = this.recurrentDropoutMask;\n      var h = i(a, d, 0),\n          m = i(a, d, 1),\n          f = i(a, d, 2),\n          g = i(a, d, 3);\n      var [$, y, b, x] = split$4(this.kernel.read(), 4, 3),\n          [v, I, C, S] = this.useBias ? split$4(this.bias.read(), 4) : [null, null, null, null];\n      l = this.inputConv(l, $, v, this.padding), u = this.inputConv(u, y, I, this.padding), c = this.inputConv(c, b, C, this.padding), p = this.inputConv(p, x, S, this.padding);\n      var [k, T, N, w] = split$4(this.recurrentKernel.read(), 4, 3);\n      h = this.recurrentConv(h, k), m = this.recurrentConv(m, T), f = this.recurrentConv(f, N), g = this.recurrentConv(g, w);\n      var E = this.recurrentActivation.apply(add$5(l, h)),\n          A = this.recurrentActivation.apply(add$5(u, m)),\n          D = add$5(mul$1(A, s), mul$1(E, this.activation.apply(add$5(c, f)))),\n          R = mul$1(this.recurrentActivation.apply(add$5(p, g)), this.activation.apply(D));\n      return [R, R, D];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = __rest$1(e, [\"units\"]);\n\n    return Object.assign({}, t, {\n      filters: this.filters,\n      kernelSize: this.kernelSize,\n      padding: this.padding,\n      dataFormat: this.dataFormat,\n      dilationRate: this.dilationRate,\n      strides: this.strides\n    });\n  }\n\n  inputConv(e, t, n, r) {\n    var a = conv2d$7(e, t, this.strides, r || \"valid\", \"channelsFirst\" === this.dataFormat ? \"NCHW\" : \"NHWC\", this.dilationRate);\n    return n ? biasAdd$1(a, n, this.dataFormat) : a;\n  }\n\n  recurrentConv(e, t) {\n    return conv2d$7(e, t, 1, \"same\", \"channelsFirst\" === this.dataFormat ? \"NCHW\" : \"NHWC\");\n  }\n\n}\n\nConvLSTM2DCell$1.className = \"ConvLSTM2DCell\", registerClass$1(ConvLSTM2DCell$1);\n\nclass ConvLSTM2D$1 extends ConvRNN2D$1 {\n  constructor(e) {\n    var t = new ConvLSTM2DCell$1(e);\n    super(Object.assign({}, e, {\n      cell: t\n    }));\n  }\n\n  static fromConfig(e, t) {\n    return new e(t);\n  }\n\n}\n\nConvLSTM2D$1.className = \"ConvLSTM2D\", registerClass$1(ConvLSTM2D$1);\n\nclass Dropout$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.rate = Math.max(Math.min(e.rate, 1), 0), this.noiseShape = e.noiseShape, this.seed = e.seed, this.supportsMasking = !0;\n  }\n\n  getNoiseShape(e) {\n    if (null == this.noiseShape) return this.noiseShape;\n    var t = e.shape,\n        n = [];\n\n    for (var _e208 = 0; _e208 < this.noiseShape.length; ++_e208) {\n      n.push(null == this.noiseShape[_e208] ? t[_e208] : this.noiseShape[_e208]);\n    }\n\n    return n;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e);\n\n      if (0 < this.rate && this.rate < 1) {\n        var _e209 = null != t.training && t.training,\n            r = this.getNoiseShape(n);\n\n        return inTrainPhase$1(() => dropout$4(n, this.rate, r, this.seed), () => n, _e209);\n      }\n\n      return e;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      rate: this.rate,\n      noiseShape: this.noiseShape,\n      seed: this.seed\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  dispose() {\n    return super.dispose();\n  }\n\n}\n\nDropout$1.className = \"Dropout\", registerClass$1(Dropout$1);\n\nclass SpatialDropout1D$1 extends Dropout$1 {\n  constructor(e) {\n    super(e), this.inputSpec = [{\n      ndim: 3\n    }];\n  }\n\n  getNoiseShape(e) {\n    var t = e.shape;\n    return [t[0], 1, t[2]];\n  }\n\n}\n\nSpatialDropout1D$1.className = \"SpatialDropout1D\", registerClass$1(SpatialDropout1D$1);\n\nclass Dense$1 extends Layer$1 {\n  constructor(e) {\n    if (super(e), this.activation = null, this.useBias = !0, this.kernel = null, this.bias = null, this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", null == e.batchInputShape && null == e.inputShape && null != e.inputDim) {\n      var t = null;\n      null != e.batchSize && (t = e.batchSize), this.batchInputShape = [t, e.inputDim];\n    }\n\n    this.units = e.units, assertPositiveInteger$1(this.units, \"units\"), this.activation = getActivation$1(e.activation), null != e.useBias && (this.useBias = e.useBias), this.kernelInitializer = getInitializer$1(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.biasInitializer = getInitializer$1(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.kernelConstraint = getConstraint$1(e.kernelConstraint), this.biasConstraint = getConstraint$1(e.biasConstraint), this.kernelRegularizer = getRegularizer$1(e.kernelRegularizer), this.biasRegularizer = getRegularizer$1(e.biasRegularizer), this.activityRegularizer = getRegularizer$1(e.activityRegularizer), this.supportsMasking = !0, this.inputSpec = [{\n      minNDim: 2\n    }];\n  }\n\n  build(e) {\n    var t = (e = getExactlyOneShape$1(e))[e.length - 1];\n    null == this.kernel && (this.kernel = this.addWeight(\"kernel\", [t, this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint))), this.inputSpec = [{\n      minNDim: 2,\n      axes: {\n        [-1]: t\n      }\n    }], this.built = !0;\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape$1(e)).slice();\n    return t[t.length - 1] = this.units, t;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e),\n          r = mapActivationToFusedKernel$1(this.activation.getClassName());\n      var a;\n      return null != r ? a = dot$3(n, this.kernel.read(), r, this.bias ? this.bias.read() : null) : (a = dot$3(n, this.kernel.read()), null != this.bias && (a = biasAdd$1(a, this.bias.read())), null != this.activation && (a = this.activation.apply(a))), a;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      units: this.units,\n      activation: serializeActivation$1(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer$1(this.kernelInitializer),\n      biasInitializer: serializeInitializer$1(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer$1(this.kernelRegularizer),\n      biasRegularizer: serializeRegularizer$1(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer$1(this.activityRegularizer),\n      kernelConstraint: serializeConstraint$1(this.kernelConstraint),\n      biasConstraint: serializeConstraint$1(this.biasConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nDense$1.className = \"Dense\", registerClass$1(Dense$1);\n\nclass Flatten$1 extends Layer$1 {\n  constructor(e) {\n    super(e = e || {}), this.inputSpec = [{\n      minNDim: 3\n    }], this.dataFormat = e.dataFormat;\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape$1(e);\n\n    for (var t of e.slice(1)) {\n      if (null == t) throw new ValueError$1(\"The shape of the input to \\\"Flatten\\\" is not fully defined (got \".concat(e.slice(1), \"). Make sure to pass a complete \\\"input_shape\\\" or \\\"batch_input_shape\\\" argument to the first layer in your model.\"));\n    }\n\n    return [e[0], arrayProd$1(e, 1)];\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e);\n\n      if (\"channelsFirst\" === this.dataFormat && n.rank > 1) {\n        var _e210 = [0];\n\n        for (var _t160 = 2; _t160 < n.rank; ++_t160) {\n          _e210.push(_t160);\n        }\n\n        _e210.push(1), n = transpose$5(n, _e210);\n      }\n\n      return batchFlatten$1(n);\n    });\n  }\n\n  getConfig() {\n    var e = {};\n    null != this.dataFormat && (e.dataFormat = this.dataFormat);\n    var t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nFlatten$1.className = \"Flatten\", registerClass$1(Flatten$1);\n\nclass Activation$2 extends Layer$1 {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.activation = getActivation$1(e.activation);\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e);\n      return this.activation.apply(n);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      activation: serializeActivation$1(this.activation)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nActivation$2.className = \"Activation\", registerClass$1(Activation$2);\n\nclass RepeatVector$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.n = e.n, this.inputSpec = [{\n      ndim: 2\n    }];\n  }\n\n  computeOutputShape(e) {\n    return [e[0], this.n, e[1]];\n  }\n\n  call(e, t) {\n    return tidy$1(() => repeat$2(e = getExactlyOneTensor$1(e), this.n));\n  }\n\n  getConfig() {\n    var e = {\n      n: this.n\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nRepeatVector$1.className = \"RepeatVector\", registerClass$1(RepeatVector$1);\n\nclass Reshape$2 extends Layer$1 {\n  constructor(e) {\n    super(e), this.targetShape = e.targetShape;\n\n    for (var _e211 = 0; _e211 < this.targetShape.length; ++_e211) {\n      this.isUnknown(this.targetShape[_e211]) && (this.targetShape[_e211] = null);\n    }\n  }\n\n  isUnknown(e) {\n    return e < 0 || null == e;\n  }\n\n  fixUnknownDimension(e, t) {\n    var n = \"Total size of new array must be unchanged.\",\n        r = t.slice();\n    var a = 1,\n        s = null;\n\n    for (var _e212 = 0; _e212 < r.length; ++_e212) {\n      var _t161 = r[_e212];\n\n      if (this.isUnknown(_t161)) {\n        if (null !== s) throw new ValueError$1(\"Can only specifiy one unknown dimension.\");\n        s = _e212;\n      } else a *= _t161;\n    }\n\n    var o = arrayProd$1(e);\n\n    if (null !== s) {\n      if (0 === a || o % a != 0) throw new ValueError$1(n);\n      r[s] = o / a;\n    } else if (o !== a) throw new ValueError$1(n);\n\n    return r;\n  }\n\n  computeOutputShape(e) {\n    var t = !1;\n\n    for (var n = 0; n < e.length; ++n) {\n      if (this.isUnknown(e[n])) {\n        t = !0;\n        break;\n      }\n    }\n\n    return t ? e.slice(0, 1).concat(this.targetShape) : e.slice(0, 1).concat(this.fixUnknownDimension(e.slice(1), this.targetShape));\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e),\n          r = n.shape,\n          a = r.slice(0, 1).concat(this.fixUnknownDimension(r.slice(1), this.targetShape));\n      return reshape$6(n, a);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      targetShape: this.targetShape\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nReshape$2.className = \"Reshape\", registerClass$1(Reshape$2);\n\nclass Permute$1 extends Layer$1 {\n  constructor(e) {\n    if (super(e), null == e.dims) throw new Error(\"Required configuration field `dims` is missing during Permute constructor call.\");\n    if (!Array.isArray(e.dims)) throw new Error(\"Permute constructor requires `dims` to be an Array, but received \".concat(e.dims, \" instead.\"));\n    var t = range$7(1, e.dims.length + 1);\n    if (!arraysEqual$1(e.dims.slice().sort(), t)) throw new Error(\"Invalid permutation `dims`: \" + JSON.stringify(e.dims) + \" `dims` must contain consecutive integers starting from 1.\");\n    this.dims = e.dims, this.dimsIncludingBatch = [0].concat(this.dims), this.inputSpec = [new InputSpec$1({\n      ndim: this.dims.length + 1\n    })];\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape$1(e)).slice();\n    return this.dims.forEach((n, r) => {\n      t[r + 1] = e[n];\n    }), t;\n  }\n\n  call(e, t) {\n    return transpose$5(getExactlyOneTensor$1(e), this.dimsIncludingBatch);\n  }\n\n  getConfig() {\n    var e = {\n      dims: this.dims\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nPermute$1.className = \"Permute\", registerClass$1(Permute$1);\n\nclass Masking$1 extends Layer$1 {\n  constructor(e) {\n    super(null == e ? {} : e), this.supportsMasking = !0, this.maskValue = null != e ? null == e.maskValue ? 0 : e.maskValue : 0;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      maskValue: this.maskValue\n    };\n    return Object.assign(t, e), t;\n  }\n\n  computeMask(e, t) {\n    var n = getExactlyOneTensor$1(e);\n    return any$5(notEqual$5(n, this.maskValue), -1);\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e),\n          r = any$5(notEqual$5(n, this.maskValue), -1, !0);\n      return mul$1(n, cast$7(r, n.dtype));\n    });\n  }\n\n}\n\nMasking$1.className = \"Masking\", registerClass$1(Masking$1);\n\nclass Embedding$1 extends Layer$1 {\n  constructor(e) {\n    if (super(e), this.embeddings = null, this.DEFAULT_EMBEDDINGS_INITIALIZER = \"randomUniform\", null == e.batchInputShape && null == e.inputShape) {\n      var t = null;\n      null != e.batchSize && (t = e.batchSize), this.batchInputShape = null == e.inputLength ? [t, null] : [t].concat(toList$1(e.inputLength));\n    }\n\n    this.inputDim = e.inputDim, assertPositiveInteger$1(this.inputDim, \"inputDim\"), this.outputDim = e.outputDim, assertPositiveInteger$1(this.outputDim, \"outputDim\"), this.embeddingsInitializer = getInitializer$1(e.embeddingsInitializer || this.DEFAULT_EMBEDDINGS_INITIALIZER), this.embeddingsRegularizer = getRegularizer$1(e.embeddingsRegularizer), this.activityRegularizer = getRegularizer$1(e.activityRegularizer), this.embeddingsConstraint = getConstraint$1(e.embeddingsConstraint), this.maskZero = e.maskZero, this.supportsMasking = e.maskZero, this.inputLength = e.inputLength;\n  }\n\n  build(e) {\n    this.embeddings = this.addWeight(\"embeddings\", [this.inputDim, this.outputDim], this.dtype, this.embeddingsInitializer, this.embeddingsRegularizer, !0, this.embeddingsConstraint), this.built = !0;\n  }\n\n  warnOnIncompatibleInputShape(e) {}\n\n  computeMask(e, t) {\n    return tidy$1(() => this.maskZero ? (e = getExactlyOneTensor$1(e), notEqual$5(e, zerosLike$5(e))) : null);\n  }\n\n  computeOutputShape(e) {\n    if (e = getExactlyOneShape$1(e), null == this.inputLength) return [...e, this.outputDim];\n    var t = toList$1(this.inputLength);\n    if (t.length !== e.length - 1) throw new ValueError$1(\"\\\"inputLength\\\" is \".concat(this.inputLength, \", but received input shape has shape \").concat(e));\n    {\n      var n = 0;\n\n      for (var r = 0; r < t.length; ++r) {\n        var a = t[r],\n            s = e[r + 1];\n        if (null != a && null != s && a !== s) throw new ValueError$1(\"\\\"inputLength\\\" is \".concat(this.inputLength, \", but received input shape has shape \").concat(e));\n        null == a && (t[n] = s), n++;\n      }\n    }\n    return [e[0], ...t, this.outputDim];\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e);\n      \"int32\" !== n.dtype && (n = cast$6(n, \"int32\"));\n      var r = gather$2(this.embeddings.read(), reshape$6(n, [n.size]));\n      return reshape$6(r, getExactlyOneShape$1(this.computeOutputShape(n.shape)));\n    });\n  }\n\n  getConfig() {\n    var e = {\n      inputDim: this.inputDim,\n      outputDim: this.outputDim,\n      embeddingsInitializer: serializeInitializer$1(this.embeddingsInitializer),\n      embeddingsRegularizer: serializeRegularizer$1(this.embeddingsRegularizer),\n      activityRegularizer: serializeRegularizer$1(this.activityRegularizer),\n      embeddingsConstraint: serializeConstraint$1(this.embeddingsConstraint),\n      maskZero: this.maskZero,\n      inputLength: this.inputLength\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nEmbedding$1.className = \"Embedding\", registerClass$1(Embedding$1);\n\nclass Merge$1 extends Layer$1 {\n  constructor(e) {\n    super(e || {}), this.supportsMasking = !0;\n  }\n\n  mergeFunction(e) {\n    throw new NotImplementedError$1();\n  }\n\n  computeElementwiseOpOutputShape(e, t) {\n    if (null == e || null == t) return null;\n    if (e.length < t.length) return this.computeElementwiseOpOutputShape(t, e);\n    if (0 === t.length) return e;\n    var n = e.slice(0, e.length - t.length);\n\n    for (var r = 0; r < t.length; ++r) {\n      var a = e[e.length - t.length + r],\n          s = t[r];\n      if (null == a || null == s || a < 0 || s < 0) n.push(null);else if (1 === a) n.push(s);else if (1 === s) n.push(a);else {\n        if (a !== s) throw new ValueError$1(\"Operands could not be broadcast together with shapes \" + JSON.stringify(e) + \" \" + JSON.stringify(t));\n        n.push(a);\n      }\n    }\n\n    return n;\n  }\n\n  build(e) {\n    if (Array.isArray(e) && !Array.isArray(e[0]) && (e = [getExactlyOneShape$1(e)]), (e = e).length < 2) throw new ValueError$1(\"A merge layer should be called on an Array of at least 2 inputs. Got \".concat(e.length, \" input(s).\"));\n    var t = [];\n\n    for (var _n73 of e) {\n      null != _n73 && null !== _n73[0] && t.push(_n73[0]);\n    }\n\n    if (t = unique$6(t), t.length > 1) throw new ValueError$1(\"Can not merge tensors with different batch sizes. Got tensors with shapes: \".concat(JSON.stringify(e), \".\"));\n    var n = null == e[0] ? null : e[0].slice(1);\n\n    for (var _t162 = 1; _t162 < e.length; ++_t162) {\n      var _r58 = null == e[_t162] ? null : e[_t162].slice(1);\n\n      n = this.computeElementwiseOpOutputShape(n, _r58);\n    }\n\n    var r = e.map(e => e.length);\n    this.reshapeRequired = -1 !== e.indexOf(null) || 1 !== unique$6(r).length;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      if (e = e, this.reshapeRequired) {\n        var _t163 = [],\n            n = e.map(e => e.rank);\n\n        if (-1 === n.indexOf(null)) {\n          var r = max$6(n);\n\n          for (var _n74 of e) {\n            var _e213 = _n74.rank;\n\n            for (var _t164 = 0; _t164 < r - _e213; ++_t164) {\n              _n74 = expandDims$6(_n74, 1);\n            }\n\n            _t163.push(_n74);\n          }\n\n          return this.mergeFunction(_t163);\n        }\n\n        {\n          var _n75 = !1;\n\n          for (var _r60 of e) {\n            var _e214 = _r60.rank;\n\n            if (null == _e214) {\n              var _e215 = _r60.shape,\n                  _a39 = _e215[0],\n                  s = _e215.slice(1).concat([_a39]);\n\n              var o = reshape$6(_r60, [_a39].concat(arrayProd$1(_e215.slice(1))));\n              o = transpose$5(o, [1, 0]), o = reshape$6(o, s), _t163.push(o), _n75 = !0;\n            } else if (_e214 > 1) {\n              var _a40 = range$7(1, _e214).concat([0]);\n\n              _t163.push(transpose$5(_r60, _a40)), _n75 = !0;\n            } else _t163.push(_r60);\n          }\n\n          var _r59 = this.mergeFunction(_t163);\n\n          var a = _r59.rank;\n          if (_n75) if (null == a) {\n            var _e216 = _r59.shape,\n                _t165 = _e216[_e216.length - 1],\n                _n76 = [_t165].concat(_e216.slice(0, _e216.length - 1));\n\n            _r59 = reshape$6(transpose$5(reshape$6(_r59, [-1, _t165]), [1, 0]), _n76);\n          } else if (a > 1) {\n            var _e217 = [a - 1].concat(range$7(0, a - 1));\n\n            _r59 = transpose$5(_r59, _e217);\n          }\n          return _r59;\n        }\n      }\n\n      return this.mergeFunction(e);\n    });\n  }\n\n  computeOutputShape(e) {\n    var t;\n    t = null == (e = e)[0] ? null : e[0].slice(1);\n\n    for (var _n77 = 1; _n77 < e.length; ++_n77) {\n      var r = null == e[_n77] ? null : e[_n77].slice(1);\n      t = this.computeElementwiseOpOutputShape(t, r);\n    }\n\n    var n = [];\n\n    for (var _t166 of e) {\n      null != _t166 && null !== _t166[0] && n.push(_t166[0]);\n    }\n\n    return n = unique$6(n), t = 1 === n.length ? n.concat(t) : [null].concat(t), t;\n  }\n\n  computeMask(e, t) {\n    return tidy$1(() => {\n      if (null == t) return null;\n      if (!Array.isArray(t)) throw new ValueError$1(\"`mask` should be an Array\");\n      if (!Array.isArray(e)) throw new ValueError$1(\"`inputs` should be an Array\");\n      if (t.length !== e.length) throw new ValueError$1(\"The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (\".concat(e.length, \" vs \").concat(t.length, \")\"));\n      if (t.every(e => null == e)) return null;\n      var n = (t = t.map(e => null == e ? e : expandDims$7(e, 0)))[0];\n\n      for (var _e218 = 1; _e218 < t.length - 1; ++_e218) {\n        n = logicalAnd$5(n, t[_e218]);\n      }\n\n      return n;\n    });\n  }\n\n}\n\nclass Add$2 extends Merge$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy$1(() => {\n      var t = e[0].clone();\n\n      for (var n = 1; n < e.length; ++n) {\n        t = add$5(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nAdd$2.className = \"Add\", registerClass$1(Add$2);\n\nclass Multiply$2 extends Merge$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy$1(() => {\n      var t = e[0].clone();\n\n      for (var n = 1; n < e.length; ++n) {\n        t = mul$1(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nMultiply$2.className = \"Multiply\", registerClass$1(Multiply$2);\n\nclass Average$1 extends Merge$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy$1(() => {\n      var t = e[0].clone();\n\n      for (var n = 1; n < e.length; ++n) {\n        t = add$5(t, e[n]);\n      }\n\n      return mul$1(1 / e.length, t);\n    });\n  }\n\n}\n\nAverage$1.className = \"Average\", registerClass$1(Average$1);\n\nclass Maximum$2 extends Merge$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy$1(() => {\n      var t = e[0];\n\n      for (var n = 1; n < e.length; ++n) {\n        t = maximum$6(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nMaximum$2.className = \"Maximum\", registerClass$1(Maximum$2);\n\nclass Minimum$2 extends Merge$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy$1(() => {\n      var t = e[0];\n\n      for (var n = 1; n < e.length; ++n) {\n        t = minimum$6(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nMinimum$2.className = \"Minimum\", registerClass$1(Minimum$2);\n\nclass Concatenate$1 extends Merge$1 {\n  constructor(e) {\n    super(e), this.DEFAULT_AXIS = -1, null == e && (e = {}), this.axis = null == e.axis ? this.DEFAULT_AXIS : e.axis, this.supportsMasking = !0, this.reshapeRequired = !1;\n  }\n\n  build(e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0]) || 1 === e.length) throw new ValueError$1(\"A `Concatenate` layer should be called on a list of at least 2 inputs\");\n    e = e;\n    var t = !0;\n\n    for (var _n78 of e) {\n      if (null != _n78) {\n        t = !1;\n        break;\n      }\n    }\n\n    if (t) return;\n    var n = [];\n\n    for (var _t167 = 0; _t167 < e.length; ++_t167) {\n      var r = e[_t167].slice();\n\n      r.splice(this.axis, 1);\n      var a = !1;\n\n      for (var _e219 of n) {\n        if (arraysEqual$1(_e219, r)) {\n          a = !0;\n          break;\n        }\n      }\n\n      a || n.push(r);\n    }\n\n    if (n.length > 1) throw new ValueError$1(\"A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: \" + JSON.stringify(e));\n  }\n\n  mergeFunction(e) {\n    return tidy$1(() => concatenate$2(e, this.axis));\n  }\n\n  computeOutputShape(e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0])) throw new ValueError$1(\"A `Concatenate` layer should be called on a list of inputs.\");\n    var t = e,\n        n = t[0].slice(),\n        r = this.axis < 0 ? n.length + this.axis : this.axis;\n\n    for (var _e220 of t.slice(1)) {\n      if (null == n[r] || null == _e220[r]) {\n        n[r] = null;\n        break;\n      }\n\n      n[r] += _e220[r];\n    }\n\n    return n;\n  }\n\n  computeMask(e, t) {\n    if (null == t) return null;\n    if (!Array.isArray(t)) throw new ValueError$1(\"`mask` should be an array for Concatenate\");\n    if (!Array.isArray(e)) throw new ValueError$1(\"`inputs` should be an array for Concatenate\");\n    if (t.length !== e.length) throw new ValueError$1(\"Mismatch in the length of mask (\".concat(t.length, \") and the legnth of inputs (\").concat(e.length, \")\"));\n    return tidy$1(() => {\n      var n = !0;\n      if (t.forEach(e => {\n        null == e || (n = !1);\n      }), n) return null;\n      var r = [];\n\n      for (var _n79 = 0; _n79 < e.length; ++_n79) {\n        r.push(null == t[_n79] ? cast$7(onesLike$5(e[_n79]), \"bool\") : t[_n79].rank < e[_n79].rank ? expandDims$7(t[_n79], -1) : t[_n79]);\n      }\n\n      var a = concat$5(r, this.axis);\n      return all$5(a, -1, !1);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction interpretAxis$1(e, t) {\n  for (; e < 0;) {\n    e += t;\n  }\n\n  return e;\n}\n\nfunction batchDot$1(e, t, n) {\n  if (e.shape.length > 3 || t.shape.length > 3) throw new NotImplementedError$1(\"batchDot is not implemented for tensors of 4D or higher rank yet\");\n  if (assert$6(e.shape.length >= 2, () => \"batchDot requires the rank of x to be >= 2, but got \".concat(e.shape.length)), assert$6(e.shape.length >= 2, () => \"batchDot requires the rank of y to be >= 2, but got \".concat(t.shape.length)), \"number\" == typeof n && (n = [n, n]), \"complex64\" === e.dtype || \"complex64\" === t.dtype) throw new NotImplementedError$1(\"batchDot is not implemented for complex64-type Tensors yet.\");\n  var r = e.shape.length,\n      a = t.shape.length;\n  null == n && (n = [r - 1, a - 2]);\n  var s = n;\n  return tidy$1(() => {\n    var n, o;\n\n    if (r > a) {\n      n = r - a;\n      var _e221 = [];\n\n      for (var _t168 = 0; _t168 < n; ++_t168) {\n        _e221.push(1);\n      }\n\n      t = reshape$6(t, t.shape.concat(_e221));\n    } else if (a > r) {\n      n = a - r;\n      var _t169 = [];\n\n      for (var _e222 = 0; _e222 < n; ++_e222) {\n        _t169.push(1);\n      }\n\n      e = reshape$6(e, e.shape.concat(_t169));\n    } else n = 0;\n\n    if (o = 2 === e.shape.length && 2 === t.shape.length ? s[0] === s[1] ? sum$6(mul$1(e, t), s[0]) : sum$6(mul$1(transpose$5(e, [1, 0]), t), s[1]) : matMul$3(e, t, s[0] !== e.shape.length - 1, s[1] === t.shape.length - 1), n > 0) {\n      var _e223;\n\n      _e223 = r > a ? r + a - 3 : r - 1;\n      var _t170 = [];\n\n      for (var _r61 = _e223; _r61 < _e223 + n; ++_r61) {\n        _t170.push(_r61);\n      }\n\n      o = squeeze$1(o, _t170);\n    }\n\n    return 1 === o.shape.length && (o = expandDims$7(o, 1)), o;\n  });\n}\n\nConcatenate$1.className = \"Concatenate\", registerClass$1(Concatenate$1);\n\nclass Dot$1 extends Merge$1 {\n  constructor(e) {\n    super(e), this.axes = e.axes, this.normalize = null != e.normalize && e.normalize, this.supportsMasking = !0, this.reshapeRequired = !1;\n  }\n\n  build(e) {\n    assert$6(Array.isArray(e) && 2 === e.length && Array.isArray(e[0]) && Array.isArray(e[1]), () => \"A `Dot` layer should be called on a list of exactly 2 inputs.\");\n    var t = e[0],\n        n = e[1];\n    if (t.length > 3 || n.length > 3) throw new NotImplementedError$1(\"Dot layer does not support tensors of 4D or higher rank yet.\");\n    var r = this.interpretAxes(t, n);\n    if (t[r[0]] !== n[r[1]]) throw new ValueError$1(\"Dimension incompatibility: \".concat(t[r[0]], \" !== \").concat(n[r[1]]));\n  }\n\n  mergeFunction(e) {\n    if (2 !== e.length) throw new ValueError$1(\"A `Dot` layer must be called on exactly 2 inputs, but received \".concat(e.length, \" input(s).\"));\n    var t,\n        n = e[0],\n        r = e[1];\n    return t = Array.isArray(this.axes) ? this.axes.map((t, n) => interpretAxis$1(t, e[n].shape.length)) : [interpretAxis$1(this.axes, n.shape.length), interpretAxis$1(this.axes, r.shape.length)], this.normalize && (n = l2Normalize$1(n, t[0]), r = l2Normalize$1(r, t[1])), batchDot$1(n, r, t);\n  }\n\n  interpretAxes(e, t) {\n    var n;\n    return n = Array.isArray(this.axes) ? this.axes : [interpretAxis$1(this.axes, e.length), interpretAxis$1(this.axes, t.length)], n;\n  }\n\n  computeOutputShape(e) {\n    assert$6(Array.isArray(e) && 2 === e.length && Array.isArray(e[0]) && Array.isArray(e[1]), () => \"A `Dot` layer should be called on a list of exactly 2 inputs.\");\n    var t = e[0].slice(),\n        n = e[1].slice();\n    if (t.length > 3 || n.length > 3) throw new NotImplementedError$1(\"Dot layer does not support tensors of 4D or higher rank yet.\");\n    var r = this.interpretAxes(t, n);\n    t.splice(r[0], 1), n.splice(r[1], 1), n.splice(0, 1);\n    var a = t.concat(n);\n    return 1 === a.length && a.push(1), a;\n  }\n\n  computeMask(e, t) {\n    return null;\n  }\n\n  getConfig() {\n    var e = {\n      axes: this.axes,\n      normalize: this.normalize\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nDot$1.className = \"Dot\", registerClass$1(Dot$1);\n\nclass GaussianNoise$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.stddev = e.stddev;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      stddev: this.stddev\n    };\n    return Object.assign(t, e), t;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e);\n      return inTrainPhase$1(() => add$5(randomNormal$3(n.shape, 0, this.stddev), n), () => n, t.training || !1);\n    });\n  }\n\n}\n\nGaussianNoise$1.className = \"GaussianNoise\", registerClass$1(GaussianNoise$1);\n\nclass GaussianDropout$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.rate = e.rate;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      rate: this.rate\n    };\n    return Object.assign(t, e), t;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor$1(e);\n      return this.rate > 0 && this.rate < 1 ? inTrainPhase$1(() => {\n        var e = Math.sqrt(this.rate / (1 - this.rate));\n        return mul$1(n, randomNormal$3(n.shape, 1, e));\n      }, () => n, t.training || !1) : n;\n    });\n  }\n\n}\n\nGaussianDropout$1.className = \"GaussianDropout\", registerClass$1(GaussianDropout$1);\n\nclass AlphaDropout$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.rate = e.rate, this.noiseShape = e.noiseShape;\n  }\n\n  _getNoiseShape(e) {\n    return this.noiseShape || getExactlyOneTensor$1(e).shape;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      rate: this.rate\n    };\n    return Object.assign(t, e), t;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        var n = this._getNoiseShape(e);\n\n        return inTrainPhase$1(() => {\n          var t = getExactlyOneTensor$1(e),\n              r = -1.7580993408473766;\n          var a = greaterEqual$5(randomUniform$2(n), this.rate);\n          a = cast$6(a, \"float32\");\n          var s = ((1 - this.rate) * (1 + this.rate * r ** 2)) ** -.5,\n              o = -s * r * this.rate,\n              i = add$5(mul$1(t, a), mul$1(add$5(a, -1), r));\n          return add$5(mul$1(i, s), o);\n        }, () => getExactlyOneTensor$1(e), t.training || !1);\n      }\n\n      return e;\n    });\n  }\n\n}\n\nfunction batchNormalization$2(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : .001;\n  var o;\n  if (2 === e.rank) o = batchNorm2d$1(e, t, n, r, a, s);else if (3 === e.rank) o = batchNorm3d$1(e, t, n, r, a, s);else {\n    if (4 !== e.rank) throw new NotImplementedError$1(\"batchNormalization is not implemented for array of rank \".concat(e.rank, \" yet\"));\n    o = batchNorm4d$1(e, t, n, r, a, s);\n  }\n  return o;\n}\n\nfunction regularNormalizeBatchInTraining$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .001;\n  return tidy$1(() => {\n    var s = moments$1(e, r),\n        o = s.mean,\n        i = s.variance;\n    return [batchNormalization$2(e, o, i, n, t, a), o, i];\n  });\n}\n\nfunction broadcastNormalizeBatchInTraining$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .001;\n  return tidy$1(() => {\n    var s = moments$1(e, r),\n        o = s.mean,\n        i = s.variance,\n        l = [];\n\n    for (var _t171 of range$7(0, e.rank)) {\n      -1 !== r.indexOf(_t171) ? l.push(1) : l.push(e.shape[_t171]);\n    }\n\n    var u = reshape$6(o, l),\n        c = reshape$6(i, l),\n        p = null == t ? null : reshape$6(t, l),\n        d = null == n ? null : reshape$6(n, l);\n    return [batchNormalization$2(e, u, c, d, p, a), o, i];\n  });\n}\n\nfunction normalizeBatchInTraining$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .001;\n  return arraysEqual$1(r.slice().sort(), range$7(0, e.rank - 1)) ? regularNormalizeBatchInTraining$1(e, t, n, r, a) : broadcastNormalizeBatchInTraining$1(e, t, n, r, a);\n}\n\nAlphaDropout$1.className = \"AlphaDropout\", registerClass$1(AlphaDropout$1);\n\nclass BatchNormalization$1 extends Layer$1 {\n  constructor(e) {\n    null == e && (e = {}), super(e), this.supportsMasking = !0, this.axis = null == e.axis ? -1 : e.axis, this.momentum = null == e.momentum ? .99 : e.momentum, this.epsilon = null == e.epsilon ? .001 : e.epsilon, this.center = null == e.center || e.center, this.scale = null == e.scale || e.scale, this.betaInitializer = getInitializer$1(e.betaInitializer || \"zeros\"), this.gammaInitializer = getInitializer$1(e.gammaInitializer || \"ones\"), this.movingMeanInitializer = getInitializer$1(e.movingMeanInitializer || \"zeros\"), this.movingVarianceInitializer = getInitializer$1(e.movingVarianceInitializer || \"ones\"), this.betaConstraint = getConstraint$1(e.betaConstraint), this.gammaConstraint = getConstraint$1(e.gammaConstraint), this.betaRegularizer = getRegularizer$1(e.betaRegularizer), this.gammaRegularizer = getRegularizer$1(e.gammaRegularizer);\n  }\n\n  build(e) {\n    e = getExactlyOneShape$1(e);\n    var t = this.axis >= 0 ? this.axis : this.axis + e.length,\n        n = e[t];\n    if (null == n) throw new ValueError$1(\"Axis \".concat(t, \" of input tensor should have a defined dimension but the layer received an input with shape \").concat(JSON.stringify(e), \".\"));\n    this.inputSpec = [new InputSpec$1({\n      ndim: e.length,\n      axes: {\n        [t]: n\n      }\n    })];\n    var r = [n];\n    this.scale && (this.gamma = this.addWeight(\"gamma\", r, null, this.gammaInitializer, this.gammaRegularizer, !0, this.gammaConstraint)), this.center && (this.beta = this.addWeight(\"beta\", r, null, this.betaInitializer, this.betaRegularizer, !0, this.betaConstraint)), this.movingMean = this.addWeight(\"moving_mean\", r, null, this.movingMeanInitializer, null, !1), this.movingVariance = this.addWeight(\"moving_variance\", r, null, this.movingVarianceInitializer, null, !1), this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var n = null != t.training && t.training,\n          r = getExactlyOneTensor$1(e),\n          a = r.shape,\n          s = a.length,\n          o = range$7(0, s),\n          i = this.axis >= 0 ? this.axis : this.axis + s;\n      o.splice(i, 1);\n      var l = pyListRepeat$1(1, s);\n      l[i] = a[i];\n      var u = o.slice();\n      u.sort();\n      var c = !arraysEqual$1(u, range$7(0, s).slice(0, s - 1));\n      if (!n) return (() => {\n        if (c) {\n          var _e224 = reshape$6(this.movingMean.read(), l),\n              _t172 = reshape$6(this.movingVariance.read(), l),\n              _n80 = this.center ? reshape$6(this.beta.read(), l) : null,\n              _a41 = this.scale ? reshape$6(this.gamma.read(), l) : null;\n\n          return batchNormalization$2(r, _e224, _t172, _n80, _a41, this.epsilon);\n        }\n\n        return batchNormalization$2(r, this.movingMean.read(), this.movingVariance.read(), null == this.beta ? null : this.beta.read(), null == this.gamma ? null : this.gamma.read(), this.epsilon);\n      })();\n\n      var [p, d, h] = normalizeBatchInTraining$1(r, this.gamma.read(), this.beta.read(), o, this.epsilon),\n          m = (e, t, n) => {\n        tidy$1(() => {\n          var r = 1 - n,\n              a = e.read(),\n              s = mul$1(sub$5(a, t), r);\n          e.write(sub$5(a, s));\n        });\n      };\n\n      return (() => {\n        m(this.movingMean, d, this.momentum), m(this.movingVariance, h, this.momentum);\n      })(), p;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer$1(this.betaInitializer),\n      gammaInitializer: serializeInitializer$1(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer$1(this.movingMeanInitializer),\n      movingVarianceInitializer: serializeInitializer$1(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer$1(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer$1(this.gammaRegularizer),\n      betaConstraint: serializeConstraint$1(this.betaConstraint),\n      gammaConstraint: serializeConstraint$1(this.gammaConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nBatchNormalization$1.className = \"BatchNormalization\", registerClass$1(BatchNormalization$1);\n\nclass LayerNormalization$1 extends Layer$1 {\n  constructor(e) {\n    if (null == e && (e = {}), super(e), this.axis = null == e.axis ? -1 : e.axis, \"number\" == typeof this.axis) {\n      if (!Number.isInteger(this.axis)) throw new Error(\"Expected axis to be an integer, but received \".concat(this.axis));\n    } else {\n      if (!Array.isArray(this.axis)) throw new Error(\"Expected axis to be an integer or an array of integers, but received \".concat(JSON.stringify(this.axis)));\n\n      for (var _e225 of this.axis) {\n        if (!Number.isInteger(_e225)) throw new Error(\"Expected axis to be an array of integers, but received \".concat(JSON.stringify(this.axis)));\n      }\n    }\n\n    this.epsilon = null == e.epsilon ? .001 : e.epsilon, this.center = null == e.center || e.center, this.scale = null == e.scale || e.scale, this.betaInitializer = getInitializer$1(e.betaInitializer || \"zeros\"), this.gammaInitializer = getInitializer$1(e.gammaInitializer || \"ones\"), this.betaRegularizer = getRegularizer$1(e.betaRegularizer), this.gammaRegularizer = getRegularizer$1(e.gammaRegularizer), this.supportsMasking = !0;\n  }\n\n  build(e) {\n    var t = (e = getExactlyOneShape$1(e)).length;\n    \"number\" == typeof this.axis && (this.axis = [this.axis]);\n\n    for (var _e226 = 0; _e226 < this.axis.length; ++_e226) {\n      this.axis[_e226] < 0 && (this.axis[_e226] += t);\n    }\n\n    for (var _e227 of this.axis) {\n      if (_e227 < 0 || _e227 >= t) throw new Error(\"Invalid axis: \".concat(_e227));\n    }\n\n    if (this.axis.length !== unique$6(this.axis).length) throw new Error(\"Found duplicate axes in: \".concat(this.axis));\n    var n = this.axis.map(t => e[t]);\n    this.gamma = this.scale ? this.addWeight(\"gamma\", n, \"float32\", this.gammaInitializer, this.gammaRegularizer, !0) : null, this.beta = this.center ? this.addWeight(\"beta\", n, \"float32\", this.betaInitializer, this.betaRegularizer, !0) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor$1(e),\n        r = n.shape,\n        a = r.length;\n    return tidy$1(() => {\n      var {\n        mean: e,\n        variance: t\n      } = moments$1(n, this.axis, !0);\n      var s = pyListRepeat$1(1, a);\n\n      for (var _e228 of this.axis) {\n        s[_e228] = r[_e228];\n      }\n\n      var o = e => null != e && e.shape.length !== a && this.axis !== [a - 1] ? reshape$6(e, s) : e;\n\n      var i = o(this.gamma.read()),\n          l = o(this.beta.read());\n      var u = [],\n          c = [];\n\n      for (var _e229 = 0; _e229 < a; ++_e229) {\n        -1 !== this.axis.indexOf(_e229) ? (u.push(r[_e229]), c.push(1)) : (u.push(1), c.push(r[_e229]));\n      }\n\n      return e = tile$7(e, u), t = tile$7(t, u), i = tile$7(i, c), l = tile$7(l, c), batchNormalization$2(n, e, t, l, i, this.epsilon);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer$1(this.betaInitializer),\n      gammaInitializer: serializeInitializer$1(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer$1(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer$1(this.gammaRegularizer)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction spatial2dPadding$1(e, t, n) {\n  return tidy$1(() => {\n    if (4 !== e.rank) throw new ValueError$1(\"temporalPadding expects input tensor to be 4-D, but received a \".concat(e.rank, \"-D tensor.\"));\n    if (null == t && (t = [[1, 1], [1, 1]]), 2 !== t.length || 2 !== t[0].length || 2 !== t[1].length) throw new ValueError$1(\"spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.\");\n    if (null == n && (n = imageDataFormat$1()), \"channelsLast\" !== n && \"channelsFirst\" !== n) throw new ValueError$1(\"Unknown data format: \".concat(n, \". Supported data formats are 'channelsLast' and 'channelsFirst.\"));\n    var r;\n    return r = \"channelsFirst\" === n ? [[0, 0], [0, 0], t[0], t[1]] : [[0, 0], t[0], t[1], [0, 0]], pad$1(e, r);\n  });\n}\n\nLayerNormalization$1.className = \"LayerNormalization\", registerClass$1(LayerNormalization$1);\n\nclass ZeroPadding2D$1 extends Layer$1 {\n  constructor(e) {\n    if (null == e && (e = {}), super(e), this.dataFormat = null == e.dataFormat ? imageDataFormat$1() : e.dataFormat, null == e.padding) this.padding = [[1, 1], [1, 1]];else if (\"number\" == typeof e.padding) this.padding = [[e.padding, e.padding], [e.padding, e.padding]];else {\n      if (e.padding = e.padding, 2 !== e.padding.length) throw new ValueError$1(\"ZeroPadding2D expects padding to be a length-2 array, but received a length-\".concat(e.padding.length, \" array.\"));\n      var t, n;\n      if (\"number\" == typeof e.padding[0]) t = [e.padding[0], e.padding[0]], n = [e.padding[1], e.padding[1]];else {\n        if (e.padding = e.padding, 2 !== e.padding[0].length) throw new ValueError$1(\"ZeroPadding2D expects height padding to be a length-2 array, but received a length-\".concat(e.padding[0].length, \" array.\"));\n        if (t = e.padding[0], 2 !== e.padding[1].length) throw new ValueError$1(\"ZeroPadding2D expects width padding to be a length-2 array, but received a length-\".concat(e.padding[1].length, \" array.\"));\n        n = e.padding[1];\n      }\n      this.padding = [t, n];\n    }\n    this.inputSpec = [new InputSpec$1({\n      ndim: 4\n    })];\n  }\n\n  computeOutputShape(e) {\n    var t, n;\n    return e = getExactlyOneShape$1(e), \"channelsFirst\" === this.dataFormat ? (t = null != e[2] && e[2] >= 0 ? e[2] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[3] && e[3] >= 0 ? e[3] + this.padding[1][0] + this.padding[1][1] : null, [e[0], e[1], t, n]) : (t = null != e[1] && e[1] >= 0 ? e[1] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[2] && e[2] >= 0 ? e[2] + this.padding[1][0] + this.padding[1][1] : null, [e[0], t, n, e[3]]);\n  }\n\n  call(e, t) {\n    return tidy$1(() => spatial2dPadding$1(getExactlyOneTensor$1(e), this.padding, this.dataFormat));\n  }\n\n  getConfig() {\n    var e = {\n      padding: this.padding,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction pool2d$1(e, t, n, r, a, s) {\n  return tidy$1(() => {\n    var o;\n    checkDataFormat$1(a), checkPoolMode$1(s), checkPaddingMode$1(r), null == n && (n = [1, 1]), null == r && (r = \"valid\"), null == a && (a = imageDataFormat$1()), null == s && (s = \"max\"), e = preprocessConv2DInput$1(e, a);\n    var i = \"same\" === r ? \"same\" : \"valid\";\n    return o = \"max\" === s ? maxPool$5(e, t, n, i) : avgPool$5(e, t, n, i), \"channelsFirst\" === a && (o = transpose$5(o, [0, 3, 1, 2])), o;\n  });\n}\n\nfunction pool3d$3(e, t, n, r, a, s) {\n  return tidy$1(() => {\n    var o;\n    checkDataFormat$1(a), checkPoolMode$1(s), checkPaddingMode$1(r), null == n && (n = [1, 1, 1]), null == r && (r = \"valid\"), null == a && (a = imageDataFormat$1()), null == s && (s = \"max\"), e = preprocessConv3DInput$1(e, a);\n    var i = \"same\" === r ? \"same\" : \"valid\";\n    return o = \"max\" === s ? maxPool3d$3(e, t, n, i) : avgPool3d$2(e, t, n, i), \"channelsFirst\" === a && (o = transpose$5(o, [0, 4, 1, 2, 3])), o;\n  });\n}\n\nZeroPadding2D$1.className = \"ZeroPadding2D\", registerClass$1(ZeroPadding2D$1);\n\nclass Pooling1D$1 extends Layer$1 {\n  constructor(e) {\n    if (null == e.poolSize && (e.poolSize = 2), super(e), \"number\" == typeof e.poolSize) this.poolSize = [e.poolSize];else {\n      if (!Array.isArray(e.poolSize) || 1 !== e.poolSize.length || \"number\" != typeof e.poolSize[0]) throw new ValueError$1(\"poolSize for 1D convolutional layer must be a number or an Array of a single number, but received \".concat(JSON.stringify(e.poolSize)));\n      this.poolSize = e.poolSize;\n    }\n    if (assertPositiveInteger$1(this.poolSize, \"poolSize\"), null == e.strides) this.strides = this.poolSize;else if (\"number\" == typeof e.strides) this.strides = [e.strides];else {\n      if (!Array.isArray(e.strides) || 1 !== e.strides.length || \"number\" != typeof e.strides[0]) throw new ValueError$1(\"strides for 1D convolutional layer must be a number or an Array of a single number, but received \".concat(JSON.stringify(e.strides)));\n      this.strides = e.strides;\n    }\n    assertPositiveInteger$1(this.strides, \"strides\"), this.padding = null == e.padding ? \"valid\" : e.padding, checkPaddingMode$1(this.padding), this.inputSpec = [new InputSpec$1({\n      ndim: 3\n    })];\n  }\n\n  computeOutputShape(e) {\n    var t = convOutputLength$1((e = getExactlyOneShape$1(e))[1], this.poolSize[0], this.padding, this.strides[0]);\n    return [e[0], t, e[2]];\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      this.invokeCallHook(e, t), e = expandDims$6(getExactlyOneTensor$1(e), 2);\n      var n = this.poolingFunction(getExactlyOneTensor$1(e), [this.poolSize[0], 1], [this.strides[0], 1], this.padding, \"channelsLast\");\n      return squeeze$1(n, [2]);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass MaxPooling1D$1 extends Pooling1D$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat$1(a), checkPaddingMode$1(r), pool2d$1(e, t, n, r, a, \"max\");\n  }\n\n}\n\nMaxPooling1D$1.className = \"MaxPooling1D\", registerClass$1(MaxPooling1D$1);\n\nclass AveragePooling1D$1 extends Pooling1D$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat$1(a), checkPaddingMode$1(r), pool2d$1(e, t, n, r, a, \"avg\");\n  }\n\n}\n\nAveragePooling1D$1.className = \"AveragePooling1D\", registerClass$1(AveragePooling1D$1);\n\nclass Pooling2D$1 extends Layer$1 {\n  constructor(e) {\n    if (null == e.poolSize && (e.poolSize = [2, 2]), super(e), this.poolSize = Array.isArray(e.poolSize) ? e.poolSize : [e.poolSize, e.poolSize], null == e.strides) this.strides = this.poolSize;else if (Array.isArray(e.strides)) {\n      if (2 !== e.strides.length) throw new ValueError$1(\"If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length \".concat(e.strides.length, \".\"));\n      this.strides = e.strides;\n    } else this.strides = [e.strides, e.strides];\n    assertPositiveInteger$1(this.poolSize, \"poolSize\"), assertPositiveInteger$1(this.strides, \"strides\"), this.padding = null == e.padding ? \"valid\" : e.padding, this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat$1(this.dataFormat), checkPaddingMode$1(this.padding), this.inputSpec = [new InputSpec$1({\n      ndim: 4\n    })];\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape$1(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2];\n    return t = convOutputLength$1(t, this.poolSize[0], this.padding, this.strides[0]), n = convOutputLength$1(n, this.poolSize[1], this.padding, this.strides[1]), \"channelsFirst\" === this.dataFormat ? [e[0], e[1], t, n] : [e[0], t, n, e[3]];\n  }\n\n  call(e, t) {\n    return tidy$1(() => (this.invokeCallHook(e, t), this.poolingFunction(getExactlyOneTensor$1(e), this.poolSize, this.strides, this.padding, this.dataFormat)));\n  }\n\n  getConfig() {\n    var e = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass MaxPooling2D$1 extends Pooling2D$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat$1(a), checkPaddingMode$1(r), pool2d$1(e, t, n, r, a, \"max\");\n  }\n\n}\n\nMaxPooling2D$1.className = \"MaxPooling2D\", registerClass$1(MaxPooling2D$1);\n\nclass AveragePooling2D$1 extends Pooling2D$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat$1(a), checkPaddingMode$1(r), pool2d$1(e, t, n, r, a, \"avg\");\n  }\n\n}\n\nAveragePooling2D$1.className = \"AveragePooling2D\", registerClass$1(AveragePooling2D$1);\n\nclass Pooling3D$1 extends Layer$1 {\n  constructor(e) {\n    if (null == e.poolSize && (e.poolSize = [2, 2, 2]), super(e), this.poolSize = Array.isArray(e.poolSize) ? e.poolSize : [e.poolSize, e.poolSize, e.poolSize], null == e.strides) this.strides = this.poolSize;else if (Array.isArray(e.strides)) {\n      if (3 !== e.strides.length) throw new ValueError$1(\"If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length \".concat(e.strides.length, \".\"));\n      this.strides = e.strides;\n    } else this.strides = [e.strides, e.strides, e.strides];\n    assertPositiveInteger$1(this.poolSize, \"poolSize\"), assertPositiveInteger$1(this.strides, \"strides\"), this.padding = null == e.padding ? \"valid\" : e.padding, this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat$1(this.dataFormat), checkPaddingMode$1(this.padding), this.inputSpec = [new InputSpec$1({\n      ndim: 5\n    })];\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape$1(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2],\n        r = \"channelsFirst\" === this.dataFormat ? e[4] : e[3];\n    return t = convOutputLength$1(t, this.poolSize[0], this.padding, this.strides[0]), n = convOutputLength$1(n, this.poolSize[1], this.padding, this.strides[1]), r = convOutputLength$1(r, this.poolSize[2], this.padding, this.strides[2]), \"channelsFirst\" === this.dataFormat ? [e[0], e[1], t, n, r] : [e[0], t, n, r, e[4]];\n  }\n\n  call(e, t) {\n    return tidy$1(() => (this.invokeCallHook(e, t), this.poolingFunction(getExactlyOneTensor$1(e), this.poolSize, this.strides, this.padding, this.dataFormat)));\n  }\n\n  getConfig() {\n    var e = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass MaxPooling3D$1 extends Pooling3D$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat$1(a), checkPaddingMode$1(r), pool3d$3(e, t, n, r, a, \"max\");\n  }\n\n}\n\nMaxPooling3D$1.className = \"MaxPooling3D\", registerClass$1(MaxPooling3D$1);\n\nclass AveragePooling3D$1 extends Pooling3D$1 {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat$1(a), checkPaddingMode$1(r), pool3d$3(e, t, n, r, a, \"avg\");\n  }\n\n}\n\nAveragePooling3D$1.className = \"AveragePooling3D\", registerClass$1(AveragePooling3D$1);\n\nclass GlobalPooling1D$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.inputSpec = [new InputSpec$1({\n      ndim: 3\n    })];\n  }\n\n  computeOutputShape(e) {\n    return [e[0], e[2]];\n  }\n\n  call(e, t) {\n    throw new NotImplementedError$1();\n  }\n\n}\n\nclass GlobalAveragePooling1D$1 extends GlobalPooling1D$1 {\n  constructor(e) {\n    super(e || {});\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t = getExactlyOneTensor$1(e);\n      return mean$3(t, 1);\n    });\n  }\n\n}\n\nGlobalAveragePooling1D$1.className = \"GlobalAveragePooling1D\", registerClass$1(GlobalAveragePooling1D$1);\n\nclass GlobalMaxPooling1D$1 extends GlobalPooling1D$1 {\n  constructor(e) {\n    super(e || {});\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var t = getExactlyOneTensor$1(e);\n      return max$7(t, 1);\n    });\n  }\n\n}\n\nGlobalMaxPooling1D$1.className = \"GlobalMaxPooling1D\", registerClass$1(GlobalMaxPooling1D$1);\n\nclass GlobalPooling2D$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat$1(this.dataFormat), this.inputSpec = [new InputSpec$1({\n      ndim: 4\n    })];\n  }\n\n  computeOutputShape(e) {\n    return e = e, \"channelsLast\" === this.dataFormat ? [e[0], e[3]] : [e[0], e[1]];\n  }\n\n  call(e, t) {\n    throw new NotImplementedError$1();\n  }\n\n  getConfig() {\n    var e = {\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass GlobalAveragePooling2D$1 extends GlobalPooling2D$1 {\n  call(e, t) {\n    return tidy$1(() => {\n      var t = getExactlyOneTensor$1(e);\n      return mean$3(t, \"channelsLast\" === this.dataFormat ? [1, 2] : [2, 3]);\n    });\n  }\n\n}\n\nGlobalAveragePooling2D$1.className = \"GlobalAveragePooling2D\", registerClass$1(GlobalAveragePooling2D$1);\n\nclass GlobalMaxPooling2D$1 extends GlobalPooling2D$1 {\n  call(e, t) {\n    return tidy$1(() => {\n      var t = getExactlyOneTensor$1(e);\n      return max$7(t, \"channelsLast\" === this.dataFormat ? [1, 2] : [2, 3]);\n    });\n  }\n\n}\n\nGlobalMaxPooling2D$1.className = \"GlobalMaxPooling2D\", registerClass$1(GlobalMaxPooling2D$1);\n\nclass Wrapper$1 extends Layer$1 {\n  constructor(e) {\n    super(e), this.layer = e.layer;\n  }\n\n  build(e) {\n    this.built = !0;\n  }\n\n  get trainable() {\n    return null != this.layer && this.layer.trainable;\n  }\n\n  set trainable(e) {\n    null != this.layer && (this.layer.trainable = e);\n  }\n\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  }\n\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  }\n\n  get updates() {\n    return this.layer._updates;\n  }\n\n  get losses() {\n    return this.layer.losses;\n  }\n\n  getWeights() {\n    return this.layer.getWeights();\n  }\n\n  setWeights(e) {\n    this.layer.setWeights(e);\n  }\n\n  getConfig() {\n    var e = {\n      layer: {\n        className: this.layer.getClassName(),\n        config: this.layer.getConfig()\n      }\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    super.setFastWeightInitDuringBuild(e), null != this.layer && this.layer.setFastWeightInitDuringBuild(e);\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = deserialize$1(t.layer, n);\n    delete t.layer;\n    var a = {\n      layer: r\n    };\n    return Object.assign(a, t), new e(a);\n  }\n\n}\n\nclass TimeDistributed$1 extends Wrapper$1 {\n  constructor(e) {\n    super(e), this.supportsMasking = !0;\n  }\n\n  build(e) {\n    if ((e = getExactlyOneShape$1(e)).length < 3) throw new ValueError$1(\"TimeDistributed layer expects an input shape >= 3D, but received input shape \".concat(JSON.stringify(e)));\n    this.inputSpec = [{\n      shape: e\n    }];\n    var t = [e[0]].concat(e.slice(2));\n    this.layer.built || (this.layer.build(t), this.layer.built = !0), super.build(e);\n  }\n\n  computeOutputShape(e) {\n    var t = [(e = getExactlyOneShape$1(e))[0]].concat(e.slice(2)),\n        n = this.layer.computeOutputShape(t);\n    return [n[0], e[1]].concat(n.slice(1));\n  }\n\n  call(e, t) {\n    return tidy$1(() => rnn$2((e, n) => [getExactlyOneTensor$1(this.layer.call(e, t)), []], e = getExactlyOneTensor$1(e), [], !1, null, null, !1, !0)[1]);\n  }\n\n}\n\nfunction checkBidirectionalMergeMode$1(e) {\n  checkStringTypeUnionValue$1(VALID_BIDIRECTIONAL_MERGE_MODES$1, \"BidirectionalMergeMode\", e);\n}\n\nTimeDistributed$1.className = \"TimeDistributed\", registerClass$1(TimeDistributed$1);\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE$1 = \"concat\";\n\nclass Bidirectional$1 extends Wrapper$1 {\n  constructor(e) {\n    super(e);\n    var t = e.layer.getConfig(),\n        n = {};\n    n.className = e.layer.getClassName(), n.config = t, this.forwardLayer = deserialize$1(n), t.goBackwards = !0 !== t.goBackwards;\n    var r = {};\n    if (r.className = e.layer.getClassName(), r.config = t, this.backwardLayer = deserialize$1(r), this.forwardLayer.name = \"forward_\" + this.forwardLayer.name, this.backwardLayer.name = \"backward_\" + this.backwardLayer.name, this.mergeMode = void 0 === e.mergeMode ? DEFAULT_BIDIRECTIONAL_MERGE_MODE$1 : e.mergeMode, checkBidirectionalMergeMode$1(this.mergeMode), e.weights) throw new NotImplementedError$1(\"weights support is not implemented for Bidirectional layer yet.\");\n    this._stateful = e.layer.stateful, this.returnSequences = e.layer.returnSequences, this.returnState = e.layer.returnState, this.supportsMasking = !0, this._trainable = !0, this.inputSpec = e.layer.inputSpec, this.numConstants = null;\n  }\n\n  get trainable() {\n    return this._trainable;\n  }\n\n  set trainable(e) {\n    this._trainable = e, null != this.forwardLayer && (this.forwardLayer.trainable = e), null != this.backwardLayer && (this.backwardLayer.trainable = e);\n  }\n\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n\n  setWeights(e) {\n    var t = Math.floor(e.length / 2);\n    this.forwardLayer.setWeights(e.slice(0, t)), this.backwardLayer.setWeights(e.slice(t));\n  }\n\n  computeOutputShape(e) {\n    var t,\n        n,\n        r,\n        a = this.forwardLayer.computeOutputShape(e);\n    return Array.isArray(a) && Array.isArray(a[0]) || (a = [a]), a = a, this.returnState ? (r = a.slice(1), t = a[0]) : t = a[0], t = t, \"concat\" === this.mergeMode ? (t[t.length - 1] *= 2, n = [t]) : n = null == this.mergeMode ? [t, t.slice()] : [t], this.returnState ? null == this.mergeMode ? n.concat(r).concat(r.slice()) : [t].concat(r).concat(r.slice()) : singletonOrArray$1(n);\n  }\n\n  apply(e, t) {\n    var n = null == t ? null : t.initialState,\n        r = null == t ? null : t.constants;\n    null == t && (t = {});\n    var a = standardizeArgs$1(e, n, r, this.numConstants);\n    if (e = a.inputs, n = a.initialState, r = a.constants, Array.isArray(e) && (n = e.slice(1), e = e[0]), (null == n || 0 === n.length) && null == r) return super.apply(e, t);\n    var s = [],\n        o = [];\n\n    if (null != n) {\n      var _e230 = n.length;\n      if (_e230 % 2 > 0) throw new ValueError$1(\"When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.\");\n      t.initialState = n, s.push(...n);\n\n      var _r62 = n.map(e => new InputSpec$1({\n        shape: e.shape\n      }));\n\n      this.forwardLayer.stateSpec = _r62.slice(0, _e230 / 2), this.backwardLayer.stateSpec = _r62.slice(_e230 / 2), o.push(..._r62);\n    }\n\n    if (null != r) throw new NotImplementedError$1(\"Support for constants in Bidirectional layers is not implemented yet.\");\n    var i = s[0] instanceof SymbolicTensor$1;\n\n    for (var _e231 of s) {\n      if (_e231 instanceof SymbolicTensor$1 !== i) throw new ValueError$1(\"The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors\");\n    }\n\n    if (i) {\n      var _n81 = [e].concat(s),\n          _r63 = this.inputSpec.concat(o),\n          _a42 = this.inputSpec;\n\n      this.inputSpec = _r63;\n\n      var _i6 = super.apply(_n81, t);\n\n      return this.inputSpec = _a42, _i6;\n    }\n\n    return super.apply(e, t);\n  }\n\n  call(e, t) {\n    return tidy$1(() => {\n      var n = t.initialState;\n      var r, a, s, o;\n      if (null == n) r = this.forwardLayer.call(e, t), a = this.backwardLayer.call(e, t);else {\n        var _s27 = n.slice(0, n.length / 2),\n            _o11 = n.slice(n.length / 2);\n\n        r = this.forwardLayer.call(e, Object.assign(t, {\n          initialState: _s27\n        })), a = this.backwardLayer.call(e, Object.assign(t, {\n          initialState: _o11\n        }));\n      }\n      return this.returnState && (Array.isArray(r) && (s = r.slice(1).concat(a.slice(1))), r = r[0], a = a[0]), this.returnSequences && (a = reverse$5(a, 1)), \"concat\" === this.mergeMode ? o = concatenate$2([r, a]) : \"sum\" === this.mergeMode ? o = add$5(r, a) : \"ave\" === this.mergeMode ? o = mul$1(.5, add$5(r, a)) : \"mul\" === this.mergeMode ? o = mul$1(r, a) : null == this.mergeMode && (o = [r, a]), this.returnState ? null == this.mergeMode ? o.concat(s) : [o].concat(s) : o;\n    });\n  }\n\n  resetStates(e) {\n    this.forwardLayer.resetStates(), this.backwardLayer.resetStates();\n  }\n\n  build(e) {\n    nameScope$1(this.forwardLayer.name, () => {\n      this.forwardLayer.build(e);\n    }), nameScope$1(this.backwardLayer.name, () => {\n      this.backwardLayer.build(e);\n    }), this.built = !0;\n  }\n\n  computeMask(e, t) {\n    var n;\n\n    if (Array.isArray(t) && (t = t[0]), n = this.returnSequences ? null == this.mergeMode ? [t, t] : t : null == this.mergeMode ? [null, null] : null, this.returnState) {\n      var _e232 = this.forwardLayer.states.map(e => null);\n\n      return Array.isArray(n) ? n.concat(_e232).concat(_e232) : [n].concat(_e232).concat(_e232);\n    }\n\n    return n;\n  }\n\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    super.setFastWeightInitDuringBuild(e), null != this.forwardLayer && this.forwardLayer.setFastWeightInitDuringBuild(e), null != this.backwardLayer && this.backwardLayer.setFastWeightInitDuringBuild(e);\n  }\n\n  getConfig() {\n    var e = {\n      mergeMode: this.mergeMode\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  static fromConfig(e, t) {\n    var n = deserialize$1(t.layer);\n    if (delete t.layer, null != t.numConstants) throw new NotImplementedError$1(\"Deserialization of a Bidirectional layer with numConstants present is not supported yet.\");\n    var r = t;\n    return r.layer = n, new e(r);\n  }\n\n}\n\nfunction conv1d$2(e) {\n  return new Conv1D$1(e);\n}\n\nfunction conv2d$5(e) {\n  return new Conv2D$2(e);\n}\n\nfunction dense$1(e) {\n  return new Dense$1(e);\n}\n\nfunction dropout$3(e) {\n  return new Dropout$1(e);\n}\n\nfunction flatten$4(e) {\n  return new Flatten$1(e);\n}\n\nfunction maxPooling1d$1(e) {\n  return new MaxPooling1D$1(e);\n}\n\nfunction maxPooling2d$1(e) {\n  return new MaxPooling2D$1(e);\n}\n\nvar DataType$1, SaverDef$1;\nBidirectional$1.className = \"Bidirectional\", registerClass$1(Bidirectional$1), function (e) {\n  e[e.DT_INVALID = 0] = \"DT_INVALID\", e[e.DT_FLOAT = 1] = \"DT_FLOAT\", e[e.DT_DOUBLE = 2] = \"DT_DOUBLE\", e[e.DT_INT32 = 3] = \"DT_INT32\", e[e.DT_UINT8 = 4] = \"DT_UINT8\", e[e.DT_INT16 = 5] = \"DT_INT16\", e[e.DT_INT8 = 6] = \"DT_INT8\", e[e.DT_STRING = 7] = \"DT_STRING\", e[e.DT_COMPLEX64 = 8] = \"DT_COMPLEX64\", e[e.DT_INT64 = 9] = \"DT_INT64\", e[e.DT_BOOL = 10] = \"DT_BOOL\", e[e.DT_QINT8 = 11] = \"DT_QINT8\", e[e.DT_QUINT8 = 12] = \"DT_QUINT8\", e[e.DT_QINT32 = 13] = \"DT_QINT32\", e[e.DT_BFLOAT16 = 14] = \"DT_BFLOAT16\", e[e.DT_FLOAT_REF = 101] = \"DT_FLOAT_REF\", e[e.DT_DOUBLE_REF = 102] = \"DT_DOUBLE_REF\", e[e.DT_INT32_REF = 103] = \"DT_INT32_REF\", e[e.DT_UINT8_REF = 104] = \"DT_UINT8_REF\", e[e.DT_INT16_REF = 105] = \"DT_INT16_REF\", e[e.DT_INT8_REF = 106] = \"DT_INT8_REF\", e[e.DT_STRING_REF = 107] = \"DT_STRING_REF\", e[e.DT_COMPLEX64_REF = 108] = \"DT_COMPLEX64_REF\", e[e.DT_INT64_REF = 109] = \"DT_INT64_REF\", e[e.DT_BOOL_REF = 110] = \"DT_BOOL_REF\", e[e.DT_QINT8_REF = 111] = \"DT_QINT8_REF\", e[e.DT_QUINT8_REF = 112] = \"DT_QUINT8_REF\", e[e.DT_QINT32_REF = 113] = \"DT_QINT32_REF\", e[e.DT_BFLOAT16_REF = 114] = \"DT_BFLOAT16_REF\";\n}(DataType$1 || (DataType$1 = {})), function (e) {\n  var t;\n  (t = e.CheckpointFormatVersion || (e.CheckpointFormatVersion = {}))[t.LEGACY = 0] = \"LEGACY\", t[t.V1 = 1] = \"V1\", t[t.V2 = 2] = \"V2\";\n}(SaverDef$1 || (SaverDef$1 = {}));\nvar version$c = \"3.8.0\";\nvar ZipMismatchMode$1;\n!function (e) {\n  e[e.FAIL = 0] = \"FAIL\", e[e.SHORTEST = 1] = \"SHORTEST\", e[e.LONGEST = 2] = \"LONGEST\";\n}(ZipMismatchMode$1 || (ZipMismatchMode$1 = {}));\nvar version$b = \"3.8.0\";\n\nfunction assertNotComplex$3(e, t) {\n  Array.isArray(e) || (e = [e]), e.forEach(e => {\n    null != e && assert$6(\"complex64\" !== e.dtype, () => \"\".concat(t, \" does not support complex64 tensors in the CPU backend.\"));\n  });\n}\n\nvar whereImpl$4 = whereImpl$5;\n\nclass MathBackendCPU$1 extends KernelBackend$1 {\n  constructor() {\n    super(), this.blockSize = 48, this.firstUse = !0, this.data = new DataStorage$1(this, engine$1());\n  }\n\n  nextDataId() {\n    return MathBackendCPU$1.nextDataId++;\n  }\n\n  write(e, t, n) {\n    this.firstUse && (this.firstUse = !1, env$1().get(\"IS_NODE\") && warn$1(\"\\n============================\\nHi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.\\n============================\"));\n    var r = {\n      id: this.nextDataId()\n    };\n    return this.data.set(r, {\n      values: e,\n      dtype: n,\n      refCount: 1\n    }), r;\n  }\n\n  makeTensorInfo(e, t, n) {\n    var r;\n\n    if (\"string\" === t && null != n && n.length > 0 && isString$1(n[0])) {\n      var a = n.map(e => encodeString$1(e));\n      r = this.write(a, e, t);\n    } else r = this.write(n, e, t);\n\n    return {\n      dataId: r,\n      shape: e,\n      dtype: t\n    };\n  }\n\n  refCount(e) {\n    return this.data.has(e) ? this.data.get(e).refCount : 0;\n  }\n\n  incRef(e) {\n    this.data.get(e).refCount++;\n  }\n\n  decRef(e) {\n    this.data.has(e) && this.data.get(e).refCount--;\n  }\n\n  move(e, t, n, r, a) {\n    this.data.set(e, {\n      values: t,\n      dtype: r,\n      refCount: a\n    });\n  }\n\n  numDataIds() {\n    return this.data.numDataIds();\n  }\n\n  read(e) {\n    var _this69 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this69.readSync(e);\n    })();\n  }\n\n  readSync(e) {\n    var {\n      dtype: t,\n      complexTensorInfos: n\n    } = this.data.get(e);\n    return \"complex64\" === t ? mergeRealAndImagArrays$1(this.readSync(n.real.dataId), this.readSync(n.imag.dataId)) : this.data.get(e).values;\n  }\n\n  bufferSync(e) {\n    var t = this.readSync(e.dataId);\n    var n = t;\n    if (\"string\" === e.dtype) try {\n      n = t.map(e => decodeString$1(e));\n    } catch (e) {\n      throw new Error(\"Failed to decode encoded string bytes into utf-8\");\n    }\n    return buffer$1(e.shape, e.dtype, n);\n  }\n\n  makeOutput(e, t, n) {\n    var r = this.write(e, t, n);\n    return engine$1().makeTensorFromDataId(r, t, n, this);\n  }\n\n  disposeData(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n\n    if (this.data.has(e)) {\n      if (this.data.get(e).refCount--, !t && this.data.get(e).refCount > 0) return !1;\n      var {\n        complexTensorInfos: n\n      } = this.data.get(e);\n      null != n && (this.disposeData(n.real.dataId, !0), this.disposeData(n.imag.dataId, !0)), this.data.delete(e);\n    }\n\n    return !0;\n  }\n\n  disposeIntermediateTensorInfo(e) {\n    this.disposeData(e.dataId);\n  }\n\n  time(e) {\n    return _asyncToGenerator(function* () {\n      var t = now$1();\n      return e(), {\n        kernelMs: now$1() - t\n      };\n    })();\n  }\n\n  memory() {\n    return {\n      unreliable: !0,\n      reasons: [\"The reported memory is an upper bound. Due to automatic garbage collection, the true allocated memory may be less.\"]\n    };\n  }\n\n  where(e) {\n    assertNotComplex$3([e], \"where\");\n    var t = this.readSync(e.dataId);\n    return whereImpl$4(e.shape, t);\n  }\n\n  dispose() {}\n\n  floatPrecision() {\n    return 32;\n  }\n\n  epsilon() {\n    return super.epsilon();\n  }\n\n}\n\nfunction simpleAbsImpl$1(e) {\n  var t = new Float32Array(e.length);\n\n  for (var n = 0; n < e.length; ++n) {\n    t[n] = Math.abs(e[n]);\n  }\n\n  return t;\n}\n\nMathBackendCPU$1.nextDataId = 0;\n\nvar abs$4 = e => {\n  var {\n    x: t\n  } = e.inputs,\n      n = e.backend;\n  assertNotComplex$3(t, \"abs\");\n  var r = new Float32Array(sizeFromShape$1(t.shape));\n  return r = simpleAbsImpl$1(n.data.get(t.dataId).values), n.makeOutput(r, t.shape, \"float32\");\n},\n    absConfig$3 = {\n  kernelName: Abs$1,\n  backendName: \"cpu\",\n  kernelFunc: abs$4\n};\n\nfunction createSimpleBinaryKernelImpl$1(e) {\n  return (t, n, r, a, s) => {\n    var o = assertAndGetBroadcastShape$1(t, n),\n        i = o.length,\n        l = computeStrides$1(o),\n        u = getTypedArrayFromDType$1(s, sizeFromShape$1(o)),\n        c = t.length,\n        p = n.length,\n        d = computeStrides$1(t),\n        h = computeStrides$1(n),\n        m = getBroadcastDims$3(t, o),\n        f = getBroadcastDims$3(n, o);\n    if (m.length + f.length === 0) for (var _t173 = 0; _t173 < u.length; ++_t173) {\n      u[_t173] = e(r[_t173 % r.length], a[_t173 % a.length]);\n    } else {\n      var _loop21 = function _loop21(_t174) {\n        var n = indexToLoc$1(_t174, i, l),\n            s = n.slice(-c);\n        m.forEach(e => s[e] = 0);\n        var o = locToIndex$1(s, c, d),\n            g = n.slice(-p);\n        f.forEach(e => g[e] = 0);\n        var $ = locToIndex$1(g, p, h);\n        u[_t174] = e(r[o], a[$]);\n      };\n\n      for (var _t174 = 0; _t174 < u.length; ++_t174) {\n        _loop21(_t174);\n      }\n    }\n    return [u, o];\n  };\n}\n\nfunction complex$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    real: r,\n    imag: a\n  } = t,\n      s = n.data.get(r.dataId).values,\n      o = n.data.get(a.dataId).values,\n      i = n.makeTensorInfo(r.shape, \"complex64\");\n  return n.data.get(i.dataId).complexTensorInfos = {\n    real: n.makeTensorInfo(r.shape, \"float32\", s),\n    imag: n.makeTensorInfo(a.shape, \"float32\", o)\n  }, i;\n}\n\nvar complexConfig$3 = {\n  kernelName: Complex$1,\n  backendName: \"cpu\",\n  kernelFunc: complex$4\n};\n\nfunction zeros$3(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"float32\";\n  if (\"complex64\" === n) return complex$4({\n    inputs: {\n      real: zeros$3(e, t, \"float32\"),\n      imag: zeros$3(e, t, \"float32\")\n    },\n    backend: e\n  });\n  var r = makeZerosTypedArray$1(sizeFromShape$1(t), n);\n  return e.makeTensorInfo(t, n, r);\n}\n\nfunction identity$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  return n.incRef(r.dataId), {\n    dataId: r.dataId,\n    shape: r.shape,\n    dtype: r.dtype\n  };\n}\n\nvar identityConfig$3 = {\n  kernelName: Identity$3,\n  backendName: \"cpu\",\n  kernelFunc: identity$4\n};\n\nfunction real$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = n.data.get(r.dataId).complexTensorInfos.real,\n      s = n.data.get(a.dataId).values;\n  return n.makeTensorInfo(a.shape, a.dtype, s);\n}\n\nvar realConfig$3 = {\n  kernelName: Real$1,\n  backendName: \"cpu\",\n  kernelFunc: real$4\n};\n\nfunction cast$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dtype: s\n  } = r;\n\n  if (\"complex64\" === s) {\n    if (\"complex64\" === a.dtype) return identity$4({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    var _e233 = zeros$3(n, a.shape, a.dtype),\n        _t175 = cast$5({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        dtype: \"float32\"\n      }\n    }),\n        _r64 = complex$4({\n      inputs: {\n        real: _t175,\n        imag: _e233\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e233), n.disposeIntermediateTensorInfo(_t175), _r64;\n  }\n\n  if (\"complex64\" === a.dtype) {\n    var _e234 = real$4({\n      inputs: {\n        input: a\n      },\n      backend: n\n    }),\n        _t176 = cast$5({\n      inputs: {\n        x: _e234\n      },\n      backend: n,\n      attrs: {\n        dtype: s\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(_e234), _t176;\n  }\n\n  if (!hasEncodingLoss$1(a.dtype, s)) {\n    var _e235 = identity$4({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    return {\n      dataId: _e235.dataId,\n      shape: _e235.shape,\n      dtype: s\n    };\n  }\n\n  if (\"int32\" === s) {\n    var _e236 = n.data.get(a.dataId).values,\n        _t177 = Int32Array.from(_e236);\n\n    return n.makeTensorInfo(a.shape, \"int32\", _t177);\n  }\n\n  if (\"bool\" === s) {\n    var _e237 = n.data.get(a.dataId).values,\n        _t178 = toTypedArray$1([0], a.dtype),\n        [_r65, _s28] = createSimpleBinaryKernelImpl$1((e, t) => e !== t ? 1 : 0)(a.shape, [], _e237, _t178, \"bool\");\n\n    return n.makeTensorInfo(_s28, \"bool\", _r65);\n  }\n\n  throw new Error(\"Error in Cast: failed to cast \".concat(a.dtype, \" to \").concat(s));\n}\n\nvar castConfig$3 = {\n  kernelName: Cast$1,\n  backendName: \"cpu\",\n  kernelFunc: cast$5\n};\n\nfunction binaryKernelFunc$3(e, t, n, r) {\n  return null == n ? _ref3 => {\n    var {\n      inputs: n,\n      backend: a\n    } = _ref3;\n    var {\n      a: s,\n      b: o\n    } = n,\n        i = a;\n    assertNotComplex$3([s, o], e);\n    var l = i.data.get(s.dataId).values,\n        u = i.data.get(o.dataId).values,\n        c = \"string\" === s.dtype ? fromUint8ToStringArray$1(l) : l,\n        p = \"string\" === s.dtype ? fromUint8ToStringArray$1(u) : u,\n        d = r || s.dtype,\n        [h, m] = t(s.shape, o.shape, c, p, d);\n    return i.makeTensorInfo(m, d, h);\n  } : _ref4 => {\n    var {\n      inputs: e,\n      backend: a\n    } = _ref4;\n    var {\n      a: s,\n      b: o\n    } = e,\n        i = a;\n\n    if (\"complex64\" === s.dtype || \"complex64\" === o.dtype) {\n      var _e238 = cast$5({\n        inputs: {\n          x: s\n        },\n        backend: i,\n        attrs: {\n          dtype: \"complex64\"\n        }\n      }),\n          _t179 = i.data.get(_e238.dataId),\n          _r66 = _t179.complexTensorInfos.imag,\n          _a43 = i.data.get(_t179.complexTensorInfos.real.dataId).values,\n          l = i.data.get(_r66.dataId).values,\n          u = cast$5({\n        inputs: {\n          x: o\n        },\n        backend: i,\n        attrs: {\n          dtype: \"complex64\"\n        }\n      }),\n          c = i.data.get(u.dataId),\n          _p6 = c.complexTensorInfos.imag,\n          d = i.data.get(c.complexTensorInfos.real.dataId).values,\n          h = i.data.get(_p6.dataId).values,\n          [m, f, g] = n(s.shape, o.shape, _a43, l, d, h),\n          $ = i.makeTensorInfo(g, \"float32\", m),\n          y = i.makeTensorInfo(g, \"float32\", f),\n          b = complex$4({\n        inputs: {\n          real: $,\n          imag: y\n        },\n        backend: i\n      });\n\n      return i.disposeIntermediateTensorInfo(_e238), i.disposeIntermediateTensorInfo(u), i.disposeIntermediateTensorInfo($), i.disposeIntermediateTensorInfo(y), b;\n    }\n\n    {\n      var _e239 = i.data.get(s.dataId).values,\n          _n82 = i.data.get(o.dataId).values,\n          _a44 = r || s.dtype,\n          [_l6, _u4] = t(s.shape, o.shape, _e239, _n82, _a44);\n\n      return i.makeTensorInfo(_u4, _a44, _l6);\n    }\n  };\n}\n\nfunction createComplexBinaryKernelImpl$1(e) {\n  return (t, n, r, a, s, o) => {\n    var i = assertAndGetBroadcastShape$1(t, n),\n        l = sizeFromShape$1(i),\n        u = i.length,\n        c = computeStrides$1(i),\n        p = getTypedArrayFromDType$1(\"float32\", l),\n        d = getTypedArrayFromDType$1(\"float32\", l),\n        h = getBroadcastDims$3(t, i),\n        m = getBroadcastDims$3(n, i),\n        f = mergeRealAndImagArrays$1(r, a),\n        g = mergeRealAndImagArrays$1(s, o),\n        $ = t.length,\n        y = computeStrides$1(t),\n        b = n.length,\n        x = computeStrides$1(n);\n    if (h.length + m.length === 0) for (var _t180 = 0; _t180 < p.length; _t180++) {\n      var _n83 = _t180 % f.length,\n          _r67 = _t180 % g.length,\n          _a45 = e(f[2 * _n83], f[2 * _n83 + 1], g[2 * _r67], g[2 * _r67 + 1]);\n\n      p[_t180] = _a45.real, d[_t180] = _a45.imag;\n    } else {\n      var _loop22 = function _loop22(_t181) {\n        var n = indexToLoc$1(_t181, u, c),\n            r = n.slice(-$);\n        h.forEach(e => r[e] = 0);\n        var a = locToIndex$1(r, $, y),\n            s = n.slice(-b);\n        m.forEach(e => s[e] = 0);\n        var o = locToIndex$1(s, b, x),\n            i = e(f[2 * a], f[2 * a + 1], g[2 * o], g[2 * o + 1]);\n        p[_t181] = i.real, d[_t181] = i.imag;\n      };\n\n      for (var _t181 = 0; _t181 < p.length; _t181++) {\n        _loop22(_t181);\n      }\n    }\n    return [p, d, i];\n  };\n}\n\nvar addImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e + t),\n    addComplexImpl$1 = createComplexBinaryKernelImpl$1((e, t, n, r) => ({\n  real: e + n,\n  imag: t + r\n})),\n    add$4 = binaryKernelFunc$3(Add$3, addImpl$1, addComplexImpl$1),\n    addConfig$3 = {\n  kernelName: Add$3,\n  backendName: \"cpu\",\n  kernelFunc: add$4\n};\n\nfunction bincountImpl$1(e, t, n, r, a) {\n  var s = sizeFromShape$1(r),\n      o = makeZerosTypedArray$1(a, n);\n\n  for (var _n84 = 0; _n84 < e.length; _n84++) {\n    var _r68 = e[_n84];\n    if (_r68 < 0) throw new Error(\"Input x must be non-negative!\");\n    _r68 >= a || (o[_r68] += s > 0 ? t[_n84] : 1);\n  }\n\n  return o;\n}\n\nfunction bincountReduceImpl$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = e.shape[0],\n      s = e.shape[1],\n      o = buffer$1([a, n], t.dtype);\n\n  for (var i = 0; i < a; i++) {\n    for (var _a46 = 0; _a46 < s; _a46++) {\n      var _s29 = e.get(i, _a46);\n\n      if (_s29 < 0) throw new Error(\"Input x must be non-negative!\");\n      _s29 >= n || o.set(r ? 1 : t.size > 0 ? o.get(i, _s29) + t.get(i, _a46) : o.get(i, _s29) + 1, i, _s29);\n    }\n  }\n\n  return o;\n}\n\nfunction createSimpleUnaryImpl$1(e) {\n  return (t, n, r) => {\n    var a = getTypedArrayFromDType$1(n, t.length);\n\n    for (var _n85 = 0; _n85 < t.length; ++_n85) {\n      a[_n85] = e(t[_n85], r);\n    }\n\n    return a;\n  };\n}\n\nfunction unaryKernelFunc$3(e, t, n) {\n  return _ref5 => {\n    var {\n      inputs: r,\n      attrs: a,\n      backend: s\n    } = _ref5;\n    var {\n      x: o\n    } = r;\n    if (assertNotComplex$3(o, e), \"string\" === o.dtype || \"string\" === n) throw new Error(\"unaryKernelFunc does not support string input/output\");\n    var i = s,\n        l = i.data.get(o.dataId).values,\n        u = sizeFromShape$1(o.shape),\n        c = n || o.dtype,\n        p = getArrayFromDType$1(c, u);\n\n    for (var _e240 = 0; _e240 < u; ++_e240) {\n      p[_e240] = t(l[_e240], a);\n    }\n\n    return i.makeTensorInfo(o.shape, c, p);\n  };\n}\n\nfunction unaryKernelFuncFromImpl$1(e, t, n) {\n  return _ref6 => {\n    var {\n      inputs: r,\n      attrs: a,\n      backend: s\n    } = _ref6;\n    var {\n      x: o\n    } = r;\n    if (assertNotComplex$3(o, e), \"string\" === o.dtype || \"string\" === n) throw new Error(\"unaryKernelFunc does not support string input/output\");\n    var i = s,\n        l = i.data.get(o.dataId).values,\n        u = n || o.dtype,\n        c = t(l, u, a);\n    return i.makeTensorInfo(o.shape, u, c);\n  };\n}\n\nvar ceilImpl$1 = createSimpleUnaryImpl$1(e => Math.ceil(e)),\n    ceil$4 = unaryKernelFuncFromImpl$1(Ceil$1, ceilImpl$1),\n    ceilConfig$3 = {\n  kernelName: Ceil$1,\n  backendName: \"cpu\",\n  kernelFunc: ceil$4\n};\n\nfunction concatImpl$3(e, t, n, r) {\n  var a = getArrayFromDType$1(n, sizeFromShape$1(t));\n\n  if (r && \"string\" !== n) {\n    var _t182 = 0;\n    e.forEach(e => {\n      var n = sizeFromShape$1(e.shape);\n      a.set(e.vals, _t182), _t182 += n;\n    });\n  } else {\n    var _r69 = 0;\n    e.forEach(e => {\n      var s = \"string\" === n ? fromUint8ToStringArray$1(e.vals) : e.vals;\n      var o = 0;\n\n      for (var _n86 = 0; _n86 < e.shape[0]; ++_n86) {\n        var i = _n86 * t[1] + _r69;\n\n        for (var _t183 = 0; _t183 < e.shape[1]; ++_t183) {\n          a[i + _t183] = s[o++];\n        }\n      }\n\n      _r69 += e.shape[1];\n    });\n  }\n\n  return a;\n}\n\nvar equalImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e === t ? 1 : 0),\n    equal$4 = binaryKernelFunc$3(Equal$1, equalImpl$1, null, \"bool\"),\n    equalConfig$3 = {\n  kernelName: Equal$1,\n  backendName: \"cpu\",\n  kernelFunc: equal$4\n},\n    expImpl$1 = createSimpleUnaryImpl$1(e => Math.exp(e)),\n    exp$4 = unaryKernelFuncFromImpl$1(Exp$1, expImpl$1),\n    expConfig$3 = {\n  kernelName: Exp$1,\n  backendName: \"cpu\",\n  kernelFunc: exp$4\n},\n    expm1Impl$1 = createSimpleUnaryImpl$1(e => Math.expm1(e)),\n    expm1$4 = unaryKernelFuncFromImpl$1(Expm1$1, expm1Impl$1),\n    expm1Config$3 = {\n  kernelName: Expm1$1,\n  backendName: \"cpu\",\n  kernelFunc: expm1$4\n},\n    floorImpl$1 = createSimpleUnaryImpl$1(e => Math.floor(e)),\n    floor$4 = unaryKernelFuncFromImpl$1(Floor$1, floorImpl$1),\n    floorConfig$3 = {\n  kernelName: Floor$1,\n  backendName: \"cpu\",\n  kernelFunc: floor$4\n};\n\nfunction gatherNdImpl$1(e, t, n, r, a, s, o, i, l) {\n  var u = buffer$1([r, s], n);\n\n  for (var _n87 = 0; _n87 < r; _n87++) {\n    var _r70 = [];\n    var c = 0;\n\n    for (var _t184 = 0; _t184 < a; _t184++) {\n      var _s30 = e[_n87 * a + _t184];\n      c += _s30 * o[_t184], _r70.push(_s30);\n    }\n\n    if (c < 0 || c >= l / s) throw new Error(\"Invalid indices: \".concat(_r70, \" does not index into \").concat(i));\n\n    for (var _e241 = 0; _e241 < s; _e241++) {\n      u.values[_n87 * s + _e241] = t.get(...t.indexToLoc(c * s + _e241));\n    }\n  }\n\n  return u;\n}\n\nfunction gatherV2Impl$1(e, t, n) {\n  var r = buffer$1(n, e.dtype);\n\n  for (var _n88 = 0; _n88 < r.size; ++_n88) {\n    var a = r.indexToLoc(_n88).slice(),\n        s = t.locToIndex([a[0], a[2]]);\n    a[2] = t.values[s];\n    var o = e.locToIndex(a);\n    r.values[_n88] = e.values[o];\n  }\n\n  return r;\n}\n\nvar greaterImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e > t ? 1 : 0),\n    greater$5 = binaryKernelFunc$3(Greater$1, greaterImpl$1, null, \"bool\"),\n    greaterConfig$3 = {\n  kernelName: Greater$1,\n  backendName: \"cpu\",\n  kernelFunc: greater$5\n},\n    greaterEqualImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e >= t ? 1 : 0),\n    greaterEqual$4 = binaryKernelFunc$3(GreaterEqual$1, greaterEqualImpl$1, null, \"bool\"),\n    greaterEqualConfig$3 = {\n  kernelName: GreaterEqual$1,\n  backendName: \"cpu\",\n  kernelFunc: greaterEqual$4\n},\n    lessImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e < t ? 1 : 0),\n    less$5 = binaryKernelFunc$3(Less$1, lessImpl$1, null, \"bool\"),\n    lessConfig$3 = {\n  kernelName: Less$1,\n  backendName: \"cpu\",\n  kernelFunc: less$5\n},\n    lessEqualImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e <= t ? 1 : 0),\n    lessEqual$4 = binaryKernelFunc$3(LessEqual$1, lessEqualImpl$1, null, \"bool\"),\n    lessEqualConfig$3 = {\n  kernelName: LessEqual$1,\n  backendName: \"cpu\",\n  kernelFunc: lessEqual$4\n};\n\nfunction linSpaceImpl$1(e, t, n) {\n  var r = (t - e) / (n - 1),\n      a = makeZerosTypedArray$1(n, \"float32\");\n  a[0] = e;\n\n  for (var _e242 = 1; _e242 < a.length; _e242++) {\n    a[_e242] = a[_e242 - 1] + r;\n  }\n\n  return a;\n}\n\nvar logImpl$1 = createSimpleUnaryImpl$1(e => Math.log(e)),\n    log$5 = unaryKernelFuncFromImpl$1(Log$1, logImpl$1),\n    logConfig$3 = {\n  kernelName: Log$1,\n  backendName: \"cpu\",\n  kernelFunc: log$5\n};\n\nfunction maxImpl$3(e, t, n, r) {\n  var a = getTypedArrayFromDType$1(r, sizeFromShape$1(n));\n\n  for (var _n89 = 0; _n89 < a.length; ++_n89) {\n    var _r71 = _n89 * t;\n\n    var s = e[_r71];\n\n    for (var _n90 = 0; _n90 < t; ++_n90) {\n      var _t185 = e[_r71 + _n90];\n      (Number.isNaN(_t185) || _t185 > s) && (s = _t185);\n    }\n\n    a[_n89] = s;\n  }\n\n  return a;\n}\n\nvar maximumImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => Math.max(e, t)),\n    maximum$5 = binaryKernelFunc$3(Maximum$3, maximumImpl$1),\n    maximumConfig$3 = {\n  kernelName: Maximum$3,\n  backendName: \"cpu\",\n  kernelFunc: maximum$5\n},\n    minimumImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => Math.min(e, t)),\n    minimum$5 = binaryKernelFunc$3(Minimum$3, minimumImpl$1),\n    minimumConfig$3 = {\n  kernelName: Minimum$3,\n  backendName: \"cpu\",\n  kernelFunc: minimum$5\n},\n    multiplyImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e * t),\n    multiplyComplexImpl$1 = createComplexBinaryKernelImpl$1((e, t, n, r) => ({\n  real: e * n - t * r,\n  imag: e * r + t * n\n})),\n    multiply$4 = binaryKernelFunc$3(Multiply$3, multiplyImpl$1, multiplyComplexImpl$1),\n    multiplyConfig$3 = {\n  kernelName: Multiply$3,\n  backendName: \"cpu\",\n  kernelFunc: multiply$4\n};\n\nfunction negImpl$1(e, t, n) {\n  var r = createScalarValue$1(-1, n);\n  return multiplyImpl$1([], t, r, e, n);\n}\n\nfunction neg$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  assertNotComplex$3(r, \"neg\");\n  var a = n.data.get(r.dataId).values,\n      [s, o] = negImpl$1(a, r.shape, r.dtype);\n  return n.makeTensorInfo(o, r.dtype, s);\n}\n\nvar negConfig$3 = {\n  kernelName: Neg$1,\n  backendName: \"cpu\",\n  kernelFunc: neg$4\n},\n    notEqualImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e !== t ? 1 : 0),\n    notEqual$4 = binaryKernelFunc$3(NotEqual$1, notEqualImpl$1, null, \"bool\"),\n    notEqualConfig$3 = {\n  kernelName: NotEqual$1,\n  backendName: \"cpu\",\n  kernelFunc: notEqual$4\n};\n\nfunction transposeImpl$3(e, t, n, r, a) {\n  var s = t.length,\n      o = sizeFromShape$1(t),\n      i = computeStrides$1(t),\n      l = computeStrides$1(a),\n      u = getTypedArrayFromDType$1(n, sizeFromShape$1(a));\n\n  for (var _t186 = 0; _t186 < o; ++_t186) {\n    var _n91 = indexToLoc$1(_t186, s, i),\n        _a47 = new Array(_n91.length);\n\n    for (var _e243 = 0; _e243 < _a47.length; _e243++) {\n      _a47[_e243] = _n91[r[_e243]];\n    }\n\n    u[locToIndex$1(_a47, s, l)] = e[_t186];\n  }\n\n  return u;\n}\n\nfunction transpose$4(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    perm: s\n  } = n;\n  assertNotComplex$3(a, \"transpose\");\n  var o = new Array(a.shape.length);\n\n  for (var _e244 = 0; _e244 < o.length; _e244++) {\n    o[_e244] = a.shape[s[_e244]];\n  }\n\n  var i = transposeImpl$3(r.data.get(a.dataId).values, a.shape, a.dtype, s, o);\n  return {\n    dataId: r.write(i, o, a.dtype),\n    shape: o,\n    dtype: a.dtype\n  };\n}\n\nvar transposeConfig$3 = {\n  kernelName: Transpose$1,\n  backendName: \"cpu\",\n  kernelFunc: transpose$4\n};\n\nfunction prodImpl$1(e, t, n, r) {\n  var [a, s] = computeOutAndReduceShapes$1(e, r),\n      o = upcastType$1(t, \"int32\"),\n      i = makeZerosTypedArray$1(sizeFromShape$1(a), o),\n      l = sizeFromShape$1(s);\n\n  for (var _e245 = 0; _e245 < i.length; ++_e245) {\n    var _t187 = _e245 * l;\n\n    var _r72 = 1;\n\n    for (var _e246 = 0; _e246 < l; ++_e246) {\n      _r72 *= n[_t187 + _e246];\n    }\n\n    i[_e245] = _r72;\n  }\n\n  return {\n    outVals: i,\n    outShape: a,\n    outDtype: o\n  };\n}\n\nfunction prod$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$3(a, \"prod\");\n  var i = a.shape.length,\n      l = parseAxisParam$1(s, a.shape),\n      u = getAxesPermutation$1(l, i);\n  var c = l,\n      p = a;\n  var d = [];\n  null != u && (p = transpose$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), d.push(p), c = getInnerMostAxes$1(c.length, i));\n  var h = n.data.get(p.dataId).values,\n      {\n    outVals: m,\n    outShape: f,\n    outDtype: g\n  } = prodImpl$1(p.shape, p.dtype, h, c);\n  var $ = f;\n  return o && ($ = expandShapeToKeepDim$1(f, l)), d.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo($, g, m);\n}\n\nvar prodConfig$3 = {\n  kernelName: Prod$1,\n  backendName: \"cpu\",\n  kernelFunc: prod$4\n};\n\nfunction rangeImpl$1(e, t, n, r) {\n  if (e === t || e < t && n < 0 || t < e && n > 1) return makeZerosTypedArray$1(0, r);\n  var a = makeZerosTypedArray$1(Math.abs(Math.ceil((t - e) / n)), r);\n  t < e && 1 === n && (n = -1), a[0] = e;\n\n  for (var _e247 = 1; _e247 < a.length; _e247++) {\n    a[_e247] = a[_e247 - 1] + n;\n  }\n\n  return a;\n}\n\nvar rsqrtImpl$1 = createSimpleUnaryImpl$1(e => 1 / Math.sqrt(e)),\n    rsqrt$4 = unaryKernelFuncFromImpl$1(Rsqrt$1, rsqrtImpl$1),\n    rsqrtConfig$3 = {\n  kernelName: Rsqrt$1,\n  backendName: \"cpu\",\n  kernelFunc: rsqrt$4\n};\n\nfunction sliceImpl$1(e, t, n, r, a) {\n  var s = isSliceContinous$1(r, t, n),\n      o = sizeFromShape$1(n),\n      i = computeStrides$1(r);\n\n  if (s) {\n    var _n92 = computeFlatOffset$1(t, i);\n\n    return \"string\" === a ? e.slice(_n92, _n92 + o) : e.subarray(_n92, _n92 + o);\n  }\n\n  var l = buffer$1(r, a, \"string\" === a ? fromUint8ToStringArray$1(e) : e),\n      u = buffer$1(n, a);\n\n  for (var _e248 = 0; _e248 < u.size; ++_e248) {\n    var _n93 = u.indexToLoc(_e248),\n        _r73 = _n93.map((e, n) => e + t[n]);\n\n    u.set(l.get(..._r73), ..._n93);\n  }\n\n  return \"string\" === a ? fromStringArrayToUint8$1(u.values) : u.values;\n}\n\nfunction slice$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    size: o\n  } = r;\n  assertNotComplex$3(a, \"slice\");\n  var [i, l] = parseSliceParams$1(a, s, o);\n  assertParamsValid$1(a, i, l);\n  var u = sliceImpl$1(n.data.get(a.dataId).values, i, l, a.shape, a.dtype);\n  return n.makeTensorInfo(l, a.dtype, u);\n}\n\nvar sliceConfig$3 = {\n  kernelName: Slice$1,\n  backendName: \"cpu\",\n  kernelFunc: slice$4\n};\n\nfunction sparseFillEmptyRowsImpl$1(e, t, n, r, a, s, o) {\n  var i = t[0],\n      l = s[0],\n      u = new Array(l),\n      c = new Array(i),\n      p = t[1];\n\n  if (0 === l) {\n    if (0 !== i) throw new Error(\"Received SparseTensor with denseShape[0] = 0 but\\n         indices.shape[0] = \".concat(i));\n    return [getArrayFromDType$1(n, 0), [0, p], getArrayFromDType$1(a, 0), u, c];\n  }\n\n  var d = !0,\n      h = 0;\n  var m = new Array(l).fill(0);\n\n  for (var _t188 = 0; _t188 < i; ++_t188) {\n    var _n94 = e[_t188 * p];\n    if (_n94 < 0) throw new Error(\"indices(\".concat(_t188, \", 0) is invalid: \").concat(_n94, \" < 0\"));\n    if (_n94 >= l) throw new Error(\"indices(\".concat(_t188, \", 0) is invalid: \").concat(_n94, \" >= \").concat(l));\n    ++m[_n94], d = d && _n94 >= h, h = _n94;\n  }\n\n  var f = !0;\n\n  for (var _e249 = 0; _e249 < l; ++_e249) {\n    var _t189 = 0 === m[_e249];\n\n    u[_e249] = _t189, f = f && !_t189, m[_e249] = Math.max(m[_e249], 1), _e249 > 0 && (m[_e249] += m[_e249 - 1]);\n  }\n\n  if (f && d) {\n    var _t190 = e,\n        _n95 = r;\n\n    for (var _e250 = 0; _e250 < i; ++_e250) {\n      c[_e250] = _e250;\n    }\n\n    return [_t190, [i, p], _n95, u, c];\n  }\n\n  {\n    var _t191 = m[l - 1],\n        _s31 = getArrayFromDType$1(n, _t191 * p),\n        _d2 = getArrayFromDType$1(a, _t191),\n        _h2 = new Array(l).fill(0);\n\n    for (var _t192 = 0; _t192 < i; ++_t192) {\n      var _n96 = e[_t192 * p],\n          _a48 = (0 === _n96 ? 0 : m[_n96 - 1]) + _h2[_n96];\n\n      _h2[_n96]++;\n\n      for (var _n97 = 0; _n97 < p; ++_n97) {\n        _s31[_a48 * p + _n97] = e[_t192 * p + _n97];\n      }\n\n      _d2[_a48] = r[_t192], c[_t192] = _a48;\n    }\n\n    for (var _e251 = 0; _e251 < l; ++_e251) {\n      if (0 === _h2[_e251]) {\n        var _t193 = 0 === _e251 ? 0 : m[_e251 - 1];\n\n        _s31[_t193 * p + 0] = _e251;\n\n        for (var _e252 = 1; _e252 < p; ++_e252) {\n          _s31[_t193 * p + _e252] = 0;\n        }\n\n        _d2[_t193] = o;\n      }\n    }\n\n    return [_s31, [_t191, p], _d2, u, c];\n  }\n}\n\nfunction sparseReshapeImpl$1(e, t, n, r, a) {\n  var s = sizeFromShape$1(r),\n      o = t[0],\n      i = a.length,\n      l = [];\n  var u = 1,\n      c = -1;\n\n  for (var _e253 = 0; _e253 < i; ++_e253) {\n    var _t194 = a[_e253];\n\n    if (-1 === _t194) {\n      if (-1 !== c) throw new Error(\"only one output dimension may be -1, not both \".concat(c, \" and \").concat(_e253));\n      c = _e253, l.push(1);\n    } else {\n      if (_t194 < 0) throw new Error(\"size \".concat(_e253, \" must be non-negative, not \").concat(_t194));\n      u *= _t194, l.push(_t194);\n    }\n  }\n\n  if (-1 !== c) {\n    if (u <= 0) throw new Error(\"reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\");\n\n    var _e254 = Math.trunc(s / u);\n\n    if (u * _e254 !== s) throw new Error(\"Input to reshape is a SparseTensor with \".concat(s, \"\\n          dense values, but the requested shape requires a multiple of \").concat(u, \". inputShape=\").concat(r, \" outputShape= \").concat(l));\n    l[c] = _e254;\n  }\n\n  var p = sizeFromShape$1(l);\n  if (p !== s) throw new Error(\"Input to reshape is a tensor with \".concat(s, \" dense values, but the requested shape has \").concat(p, \". inputShape=\").concat(r, \" outputShape=\").concat(l));\n  var d = r.length,\n      h = [];\n\n  if (d > 0) {\n    h[d - 1] = 1;\n\n    for (var _e255 = d - 2; _e255 >= 0; --_e255) {\n      h[_e255] = h[_e255 + 1] * r[_e255 + 1];\n    }\n  }\n\n  var m = [];\n\n  if (i > 0) {\n    m[i - 1] = 1;\n\n    for (var _e256 = i - 2; _e256 >= 0; --_e256) {\n      m[_e256] = m[_e256 + 1] * l[_e256 + 1];\n    }\n  }\n\n  var f = getArrayFromDType$1(n, o * i);\n\n  for (var _t195 = 0; _t195 < o; ++_t195) {\n    var _n98 = 0;\n\n    for (var _r74 = 0; _r74 < d; ++_r74) {\n      _n98 += e[_t195 * d + _r74] * h[_r74];\n    }\n\n    for (var _e257 = 0; _e257 < i; ++_e257) {\n      f[_t195 * i + _e257] = Math.trunc(_n98 / m[_e257]), _n98 %= m[_e257];\n    }\n  }\n\n  return [f, [o, i], l];\n}\n\nfunction sparseSegmentReductionImpl$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : 0;\n  var i = r.length;\n  if (i !== a.length) throw new Error(\"segmentIds and indices should have same size.\");\n  var l = [t[0], e.length / t[0]],\n      u = l[1],\n      c = i > 0 ? a[i - 1] + 1 : 0;\n  if (c < 0) throw new Error(\"segment ids must be >= 0\");\n  var p = t.slice();\n  p[0] = c;\n  var d = getArrayFromDType$1(n, p.reduce((e, t) => e * t, 1));\n  if (0 === i) return c > 0 && d.fill(o), [d, p];\n  if (c <= 0) throw new Error(\"segment ids must be >= 0\");\n  var h = 0,\n      m = 1,\n      f = 0,\n      g = a[h];\n\n  for (;;) {\n    var _t196 = 0;\n\n    if (m < i) {\n      if (_t196 = a[m], g === _t196) {\n        ++m;\n        continue;\n      }\n\n      if (g >= _t196) throw new Error(\"segment ids are not increasing\");\n    }\n\n    if (g < 0 || g >= c) throw new Error(\"Segment id \".concat(g, \" out of range [0, \").concat(c, \"), possibly because segmentIds input is not sorted.\"));\n    g > f && d.fill(o, f * u, g * u);\n\n    for (var _t197 = h; _t197 < m; ++_t197) {\n      var _n99 = r[_t197];\n      if (_n99 < 0 || _n99 >= l[0]) throw new Error(\"Bad: indices[\".concat(_t197, \"] == \").concat(r[_t197], \" out of range [0, \").concat(l[0], \")\"));\n\n      for (var _t198 = 0; _t198 < u; _t198++) {\n        d[g * u + _t198] += e[_n99 * u + _t198];\n      }\n    }\n\n    if (s) for (var _e258 = 0; _e258 < u; _e258++) {\n      d[g * u + _e258] /= m - h;\n    }\n    if (h = m, ++m, f = g + 1, g = _t196, m > i) break;\n  }\n\n  return f < c && d.fill(o, f * u, c * u), [d, p];\n}\n\nvar squaredDifferenceImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => {\n  var n = e - t;\n  return n * n;\n}),\n    squaredDifference$4 = binaryKernelFunc$3(SquaredDifference$1, squaredDifferenceImpl$1),\n    squaredDifferenceConfig$3 = {\n  kernelName: SquaredDifference$1,\n  backendName: \"cpu\",\n  kernelFunc: squaredDifference$4\n};\n\nfunction stridedSliceImpl$1(e, t, n, r) {\n  var a = buffer$1(e, t.dtype);\n\n  for (var _e259 = 0; _e259 < a.size; _e259++) {\n    var s = a.indexToLoc(_e259),\n        o = new Array(s.length);\n\n    for (var _e260 = 0; _e260 < o.length; _e260++) {\n      o[_e260] = s[_e260] * n[_e260] + r[_e260];\n    }\n\n    a.set(t.get(...o), ...s);\n  }\n\n  return a;\n}\n\nclass StringNGramsOp$1 {\n  constructor(e, t, n, r, a, s) {\n    this.separator = encodeString$1(e), this.nGramWidths = t, this.leftPad = encodeString$1(n), this.rightPad = encodeString$1(r), this.padWidth = a, this.preserveShort = s;\n  }\n\n  getPadWidth(e) {\n    return Math.min(this.padWidth < 0 ? e - 1 : this.padWidth, e - 1);\n  }\n\n  getNumNGrams(e, t) {\n    var n = this.getPadWidth(t);\n    return Math.max(0, e + 2 * n - t + 1);\n  }\n\n  createNGrams(e, t, n, r, a, s) {\n    var _this70 = this;\n\n    var _loop23 = function _loop23(o) {\n      var i = _this70.getPadWidth(s),\n          l = Math.max(0, i - o),\n          u = Math.max(0, i - (a - (o + 1))),\n          c = s - (l + u),\n          p = t + (l > 0 ? 0 : o - i);\n\n      var d = 0;\n      d += l * _this70.leftPad.length;\n\n      for (var _t199 = 0; _t199 < c; ++_t199) {\n        d += e[p + _t199].length;\n      }\n\n      d += u * _this70.rightPad.length, d += (l + u + c - 1) * _this70.separator.length, n[r + o] = new Uint8Array(d);\n      var h = n[r + o];\n      var m = 0;\n\n      var f = e => e.forEach(e => h[m++] = e);\n\n      for (var _e261 = 0; _e261 < l; ++_e261) {\n        f(_this70.leftPad), f(_this70.separator);\n      }\n\n      for (var _t200 = 0; _t200 < c - 1; ++_t200) {\n        f(e[p + _t200]), f(_this70.separator);\n      }\n\n      if (c > 0) {\n        f(e[p + c - 1]);\n\n        for (var _e262 = 0; _e262 < u; ++_e262) {\n          f(_this70.separator), f(_this70.rightPad);\n        }\n      } else {\n        for (var _e263 = 0; _e263 < u - 1; ++_e263) {\n          f(_this70.rightPad), f(_this70.separator);\n        }\n\n        f(_this70.rightPad);\n      }\n    };\n\n    for (var o = 0; o < a; ++o) {\n      _loop23(o);\n    }\n  }\n\n  compute(e, t) {\n    var _this71 = this;\n\n    var n = e.length,\n        r = t.length;\n\n    if (r > 0) {\n      var _e264 = t[0];\n      if (0 !== _e264) throw new Error(\"First split value must be 0, got \".concat(_e264));\n\n      for (var _a49 = 1; _a49 < r; ++_a49) {\n        var _r75 = t[_a49] >= _e264;\n\n        if (_r75 = _r75 && t[_a49] <= n, !_r75) throw new Error(\"Invalid split value \".concat(t[_a49], \", must be in [\").concat(_e264, \", \").concat(n, \"]\"));\n        _e264 = t[_a49];\n      }\n\n      if (_e264 !== n) throw new Error(\"Last split value must be data size. Expected \".concat(n, \", got \").concat(_e264));\n    }\n\n    var a = r - 1,\n        s = getArrayFromDType$1(\"int32\", r);\n\n    if (0 === n || 0 === r) {\n      var _e265 = new Array(n);\n\n      for (var _e266 = 0; _e266 <= a; ++_e266) {\n        s[_e266] = 0;\n      }\n\n      return [_e265, s];\n    }\n\n    s[0] = 0;\n\n    var _loop24 = function _loop24(_e267) {\n      var n = t[_e267] - t[_e267 - 1];\n      var r = 0;\n      _this71.nGramWidths.forEach(e => {\n        r += _this71.getNumNGrams(n, e);\n      }), _this71.preserveShort && n > 0 && 0 === r && (r = 1), s[_e267] = s[_e267 - 1] + r;\n    };\n\n    for (var _e267 = 1; _e267 <= a; ++_e267) {\n      _loop24(_e267);\n    }\n\n    var o = new Array(s[a]);\n\n    var _loop25 = function _loop25(_n100) {\n      var r = t[_n100];\n      var a = s[_n100];\n\n      if (_this71.nGramWidths.forEach(s => {\n        var i = _this71.getNumNGrams(t[_n100 + 1] - t[_n100], s);\n\n        _this71.createNGrams(e, r, o, a, i, s), a += i;\n      }), _this71.preserveShort && a === s[_n100]) {\n        var _s32 = t[_n100 + 1] - t[_n100];\n\n        if (0 === _s32) return \"continue\";\n\n        _this71.createNGrams(e, r, o, a, 1, _s32 + 2 * _this71.padWidth);\n      }\n    };\n\n    for (var _n100 = 0; _n100 < a; ++_n100) {\n      var _ret = _loop25(_n100);\n\n      if (_ret === \"continue\") continue;\n    }\n\n    return [o, s];\n  }\n\n}\n\nfunction stringNGramsImpl$1(e, t, n, r, a, s, o, i) {\n  return new StringNGramsOp$1(n, r, a, s, o, i).compute(e, t);\n}\n\nfunction split$3(e, t, n) {\n  if (!e.length) return [];\n\n  if (0 === t.length) {\n    var _t201 = new Array(e.length);\n\n    for (var _n101 = 0; _n101 < e.length; ++_n101) {\n      _t201[_n101] = e.subarray(_n101, _n101 + 1);\n    }\n\n    return _t201;\n  }\n\n  if (1 === t.length) {\n    var _r76 = t[0],\n        _a50 = [];\n    var s = e.indexOf(_r76);\n\n    for (; -1 !== s;) {\n      var _t202 = e.subarray(0, s);\n\n      n && 0 === _t202.length || _a50.push(_t202), s = (e = e.subarray(s + 1)).indexOf(_r76);\n    }\n\n    return n && 0 === e.length || _a50.push(e), _a50;\n  }\n\n  var r = [];\n  var a = 0;\n\n  for (var _s33 = 0; _s33 < e.length + 1; _s33++) {\n    if (_s33 === e.length || -1 !== t.indexOf(e[_s33])) {\n      var _t203 = e.subarray(a, _s33);\n\n      n && 0 === _t203.length || r.push(_t203), a = _s33 + 1;\n    }\n  }\n\n  return r;\n}\n\nfunction stringSplitImpl$1(e, t, n) {\n  var r = e.length,\n      a = [];\n  var s = 0,\n      o = 0;\n  var i = new Array(r);\n\n  for (var _l7 = 0; _l7 < r; ++_l7) {\n    var _r77 = split$3(e[_l7], t, n),\n        _u5 = _r77.length;\n\n    i[_l7] = _u5, s += _u5, o = Math.max(o, _u5), a.push(..._r77);\n  }\n\n  var l = getArrayFromDType$1(\"int32\", 2 * s),\n      u = new Array(s),\n      c = [r, o];\n  var p = 0;\n\n  for (var _e268 = 0; _e268 < r; ++_e268) {\n    for (var _t204 = 0; _t204 < i[_e268]; ++_t204) {\n      l[2 * p] = _e268, l[2 * p + 1] = _t204, u[p] = a[p], ++p;\n    }\n  }\n\n  return [l, u, c];\n}\n\nfunction stringToHashBucketFastImpl$1(e, t) {\n  var n = getArrayFromDType$1(\"int32\", e.length);\n\n  for (var r = 0; r < e.length; ++r) {\n    n[r] = fingerPrint64$1(e[r]).modulo(t).getLowBitsUnsigned();\n  }\n\n  return n;\n}\n\nvar subImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e - t),\n    subComplexImpl$1 = createComplexBinaryKernelImpl$1((e, t, n, r) => ({\n  real: e - n,\n  imag: t - r\n})),\n    sub$4 = binaryKernelFunc$3(Sub$1, subImpl$1, subComplexImpl$1),\n    subConfig$3 = {\n  kernelName: Sub$1,\n  backendName: \"cpu\",\n  kernelFunc: sub$4\n};\n\nfunction tileImpl$1(e, t) {\n  var n = new Array(e.rank);\n\n  for (var _r78 = 0; _r78 < n.length; _r78++) {\n    n[_r78] = e.shape[_r78] * t[_r78];\n  }\n\n  var r = buffer$1(n, e.dtype);\n\n  for (var _t205 = 0; _t205 < r.values.length; ++_t205) {\n    var _n102 = r.indexToLoc(_t205),\n        a = new Array(e.rank);\n\n    for (var _t206 = 0; _t206 < a.length; _t206++) {\n      a[_t206] = _n102[_t206] % e.shape[_t206];\n    }\n\n    var s = e.locToIndex(a);\n    r.values[_t205] = e.values[s];\n  }\n\n  return r;\n}\n\nvar comparePair$1 = (e, t) => {\n  var n = t.value - e.value;\n  return 0 === n ? e.index - t.index : n;\n};\n\nfunction select$5(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : e.length - 1;\n\n  for (; r > n;) {\n    if (r - n > 600) {\n      var _a51 = r - n + 1,\n          _s34 = t - n + 1,\n          _o12 = Math.log(_a51),\n          i = .5 * Math.exp(2 * _o12 / 3),\n          l = .5 * Math.sqrt(_o12 * i * (_a51 - i) / _a51) * Math.sign(_s34 - _a51 / 2);\n\n      select$5(e, t, Math.max(n, Math.floor(t - _s34 * i / _a51 + l)), Math.min(r, Math.floor(t + (_a51 - _s34) * i / _a51 + l)));\n    }\n\n    var a = e[t];\n    var s = n,\n        o = r;\n\n    for (swap$1(e, n, t), comparePair$1(e[r], a) > 0 && swap$1(e, n, r); s < o;) {\n      for (swap$1(e, s, o), s++, o--; comparePair$1(e[s], a) < 0;) {\n        s += 1;\n      }\n\n      for (; comparePair$1(e[o], a) > 0;) {\n        o -= 1;\n      }\n    }\n\n    0 === comparePair$1(e[n], a) ? swap$1(e, n, o) : (o += 1, swap$1(e, o, r)), o <= t && (n = o + 1), t <= o && (r = o - 1);\n  }\n}\n\nfunction topKImpl$1(e, t, n, r, a) {\n  var s = t[t.length - 1],\n      [o, i] = [e.length / s, s],\n      l = getTypedArrayFromDType$1(n, o * r),\n      u = getTypedArrayFromDType$1(\"int32\", o * r);\n\n  var _loop26 = function _loop26(_t207) {\n    var n = _t207 * i,\n        s = e.subarray(n, n + i);\n    var o = new Array(s.length);\n    s.forEach((e, t) => o[t] = {\n      value: e,\n      index: t\n    }), r < o.length && (select$5(o, r), o = o.slice(0, r)), a && o.sort(comparePair$1);\n    var c = _t207 * r,\n        p = l.subarray(c, c + r),\n        d = u.subarray(c, c + r);\n\n    for (var _e269 = 0; _e269 < r; _e269++) {\n      p[_e269] = o[_e269].value, d[_e269] = o[_e269].index;\n    }\n  };\n\n  for (var _t207 = 0; _t207 < o; _t207++) {\n    _loop26(_t207);\n  }\n\n  var c = t.slice();\n  return c[c.length - 1] = r, [buffer$1(c, n, l), buffer$1(c, \"int32\", u)];\n}\n\nfunction uniqueImpl$1(e, t, n, r) {\n  var a = parseAxisParam$1(t, n)[0],\n      s = [1, n[0], 1];\n\n  for (var _e270 = 0; _e270 < a; _e270++) {\n    s[0] *= n[_e270];\n  }\n\n  s[1] = n[a];\n\n  for (var _e271 = a + 1; _e271 < n.length; _e271++) {\n    s[2] *= n[_e271];\n  }\n\n  var o = {},\n      i = new Int32Array(n[a]),\n      l = new TensorBuffer$1(s, r, e),\n      u = [],\n      c = 1 === s[0] && 1 === s[2];\n\n  for (var _t208 = 0; _t208 < n[a]; _t208++) {\n    var _n103 = void 0;\n\n    if (c) _n103 = e[_t208].toString();else {\n      var _e272 = [];\n\n      for (var _n104 = 0; _n104 < s[0]; _n104++) {\n        for (var _r79 = 0; _r79 < s[2]; _r79++) {\n          _e272.push(l.get(_n104, _t208, _r79));\n        }\n      }\n\n      _n103 = _e272.join(\",\");\n    }\n    if (void 0 !== o[_n103]) i[_t208] = o[_n103];else {\n      var _e273 = Object.keys(o).length;\n      o[_n103] = _e273, i[_t208] = _e273, u.push(_t208);\n    }\n  }\n\n  var p = s.slice();\n  p[1] = Object.keys(o).length;\n  var d = new TensorBuffer$1(p, r);\n  u.forEach((e, t) => {\n    for (var _n105 = 0; _n105 < s[0]; _n105++) {\n      for (var _r80 = 0; _r80 < s[2]; _r80++) {\n        d.set(l.get(_n105, e, _r80), _n105, t, _r80);\n      }\n    }\n  });\n  var h = n.slice();\n  return h[a] = p[1], {\n    outputValues: d.values,\n    outputShape: h,\n    indices: i\n  };\n}\n\nvar shared$1 = {\n  __proto__: null,\n  simpleAbsImpl: simpleAbsImpl$1,\n  addImpl: addImpl$1,\n  bincountImpl: bincountImpl$1,\n  bincountReduceImpl: bincountReduceImpl$1,\n  ceilImpl: ceilImpl$1,\n  concatImpl: concatImpl$3,\n  equalImpl: equalImpl$1,\n  expImpl: expImpl$1,\n  expm1Impl: expm1Impl$1,\n  floorImpl: floorImpl$1,\n  gatherNdImpl: gatherNdImpl$1,\n  gatherV2Impl: gatherV2Impl$1,\n  greaterImpl: greaterImpl$1,\n  greaterEqualImpl: greaterEqualImpl$1,\n  lessImpl: lessImpl$1,\n  lessEqualImpl: lessEqualImpl$1,\n  linSpaceImpl: linSpaceImpl$1,\n  logImpl: logImpl$1,\n  maxImpl: maxImpl$3,\n  maximumImpl: maximumImpl$1,\n  minimumImpl: minimumImpl$1,\n  multiplyImpl: multiplyImpl$1,\n  negImpl: negImpl$1,\n  notEqualImpl: notEqualImpl$1,\n  prodImpl: prodImpl$1,\n  rangeImpl: rangeImpl$1,\n  rsqrtImpl: rsqrtImpl$1,\n  sliceImpl: sliceImpl$1,\n  sparseFillEmptyRowsImpl: sparseFillEmptyRowsImpl$1,\n  sparseReshapeImpl: sparseReshapeImpl$1,\n  sparseSegmentReductionImpl: sparseSegmentReductionImpl$1,\n  squaredDifferenceImpl: squaredDifferenceImpl$1,\n  stridedSliceImpl: stridedSliceImpl$1,\n  stringNGramsImpl: stringNGramsImpl$1,\n  stringSplitImpl: stringSplitImpl$1,\n  stringToHashBucketFastImpl: stringToHashBucketFastImpl$1,\n  subImpl: subImpl$1,\n  tileImpl: tileImpl$1,\n  topKImpl: topKImpl$1,\n  transposeImpl: transposeImpl$3,\n  uniqueImpl: uniqueImpl$1\n};\nvar version$a = \"3.8.0\";\nregisterBackend$1(\"cpu\", () => new MathBackendCPU$1(), 1);\nvar elu$6 = unaryKernelFunc$3(Elu$3, e => e >= 0 ? e : Math.exp(e) - 1),\n    eluConfig$3 = {\n  kernelName: Elu$3,\n  backendName: \"cpu\",\n  kernelFunc: elu$6\n};\n\nfunction leakyRelu$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    alpha: s\n  } = r;\n  assertNotComplex$3([a], \"leakyRelu\");\n  var o = sizeFromShape$1(a.shape),\n      i = n.data.get(a.dataId).values,\n      l = getTypedArrayFromDType$1(\"float32\", o);\n\n  for (var _e274 = 0; _e274 < i.length; _e274++) {\n    l[_e274] = i[_e274] < 0 ? s * i[_e274] : i[_e274];\n  }\n\n  return n.makeTensorInfo(a.shape, \"float32\", l);\n}\n\nvar leakyReluConfig$3 = {\n  kernelName: LeakyRelu$1,\n  backendName: \"cpu\",\n  kernelFunc: leakyRelu$4\n},\n    preluImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e < 0 ? t * e : e);\n\nfunction prelu$5(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r,\n    alpha: a\n  } = t;\n  assertNotComplex$3([r, a], \"prelu\");\n  var s = n.data.get(r.dataId).values,\n      o = n.data.get(a.dataId).values,\n      [i, l] = preluImpl$1(r.shape, a.shape, s, o, r.dtype);\n  return n.makeTensorInfo(l, r.dtype, i);\n}\n\nvar preluConfig$3 = {\n  kernelName: Prelu$1,\n  backendName: \"cpu\",\n  kernelFunc: prelu$5\n},\n    relu$5 = unaryKernelFunc$3(Relu$3, e => Math.max(0, e)),\n    reluConfig$3 = {\n  kernelName: Relu$3,\n  backendName: \"cpu\",\n  kernelFunc: relu$5\n},\n    relu6$4 = unaryKernelFunc$3(Relu6$3, e => Math.min(Math.max(0, e), 6)),\n    relu6Config$3 = {\n  kernelName: Relu6$3,\n  backendName: \"cpu\",\n  kernelFunc: relu6$4\n},\n    sigmoid$4 = unaryKernelFunc$3(Sigmoid$3, e => 1 / (1 + Math.exp(-e))),\n    sigmoidConfig$3 = {\n  kernelName: Sigmoid$3,\n  backendName: \"cpu\",\n  kernelFunc: sigmoid$4\n};\n\nfunction applyActivation$2(e, t, n, r, a) {\n  if (\"linear\" === n) return identity$4({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"relu\" === n) return relu$5({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"elu\" === n) return elu$6({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"relu6\" === n) return relu6$4({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"prelu\" === n) return prelu$5({\n    inputs: {\n      x: t,\n      alpha: r\n    },\n    backend: e\n  });\n  if (\"leakyrelu\" === n) return leakyRelu$4({\n    inputs: {\n      x: t\n    },\n    backend: e,\n    attrs: {\n      alpha: a\n    }\n  });\n  if (\"sigmoid\" === n) return sigmoid$4({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  throw new Error(\"Activation \".concat(n, \" has not been implemented for the CPU backend.\"));\n}\n\nfunction reshape$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    shape: s\n  } = r,\n      o = sizeFromShape$1(a.shape),\n      i = inferFromImplicitShape$1(s, o),\n      l = sizeFromShape$1(i);\n  assert$6(o === l, () => \"The new shape (\".concat(i, \") has \").concat(l, \" elements and the old shape (\").concat(a.shape, \") has \").concat(o, \" elements. The new shape and old shape must have the same number of elements.\")), n.incRef(a.dataId);\n  var u = n.data.get(a.dataId);\n\n  if (null != u.complexTensorInfos) {\n    var _e275 = u.complexTensorInfos.imag;\n    u.complexTensorInfos.real.shape = i, _e275.shape = i;\n  }\n\n  return {\n    dataId: a.dataId,\n    shape: i,\n    dtype: a.dtype\n  };\n}\n\nvar reshapeConfig$3 = {\n  kernelName: Reshape$3,\n  backendName: \"cpu\",\n  kernelFunc: reshape$5\n};\n\nfunction batchMatMul$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s\n  } = t,\n      {\n    transposeA: o,\n    transposeB: i\n  } = r;\n  assertNotComplex$3([a, s], \"matMul\");\n  var l = a.shape.length,\n      u = s.shape.length,\n      c = o ? a.shape[l - 2] : a.shape[l - 1],\n      p = i ? s.shape[u - 1] : s.shape[u - 2],\n      d = o ? a.shape[l - 1] : a.shape[l - 2],\n      h = i ? s.shape[u - 2] : s.shape[u - 1],\n      m = a.shape.slice(0, -2),\n      f = s.shape.slice(0, -2),\n      g = sizeFromShape$1(m),\n      $ = sizeFromShape$1(f);\n  assert$6(l >= 2 && u >= 2 && (g === $ || 1 === g || 1 === $), () => \"Error in matMul: the input batch dimensions must either be the same or at least one input batch dimension must be 1. Got input batch dimensions of (\".concat(m, \") and (\").concat(f, \").\"));\n  var y = (g > $ ? a.shape.slice(0, -2) : s.shape.slice(0, -2)).concat([d, h]);\n  assert$6(c === p, () => \"Error in matMul: inner shapes (\".concat(c, \") and (\").concat(p, \") of Tensors with shapes \").concat(a.shape, \" and \").concat(s.shape, \" and transposeA=\").concat(o, \" and transposeB=\").concat(i, \" must match.\"));\n  var b = i ? [$, h, p] : [$, p, h],\n      x = reshape$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: o ? [g, c, d] : [g, d, c]\n    }\n  }),\n      v = reshape$5({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: b\n    }\n  }),\n      I = o ? x.shape[1] : x.shape[2],\n      C = o ? x.shape[2] : x.shape[1],\n      S = i ? v.shape[1] : v.shape[2],\n      k = Math.max(g, $),\n      T = n.data.get(x.dataId).values,\n      N = n.data.get(v.dataId).values,\n      w = computeStrides$1(x.shape),\n      E = computeStrides$1(v.shape),\n      [A, D, R] = o ? [w[0], 1, w[1]] : [w[0], w[1], 1],\n      [_, F, P] = i ? [1, E[1], E[0]] : [E[1], 1, E[0]],\n      O = C * S,\n      M = buffer$1([k, C, S], x.dtype),\n      L = M.values,\n      z = n.blockSize;\n\n  for (var _e276 = 0; _e276 < k; _e276++) {\n    for (var _t209 = 0; _t209 < C; _t209 += z) {\n      for (var _n106 = 0; _n106 < S; _n106 += z) {\n        for (var _r81 = 0; _r81 < I; _r81 += z) {\n          var _a52 = Math.min(_t209 + z, C),\n              _s35 = Math.min(_n106 + z, S),\n              _o13 = Math.min(_r81 + z, I);\n\n          for (var _i7 = _t209; _i7 < _a52; _i7++) {\n            for (var _t210 = _n106; _t210 < _s35; _t210++) {\n              var _n107 = 0;\n\n              for (var _a53 = _r81; _a53 < _o13; _a53++) {\n                var _r82 = Math.min(_e276, g - 1) * A,\n                    _s36 = Math.min(_e276, $ - 1) * P;\n\n                _n107 += T[_r82 + _i7 * D + _a53 * R] * N[_a53 * _ + _t210 * F + _s36];\n              }\n\n              L[_e276 * O + (_i7 * S + _t210)] += _n107;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.disposeIntermediateTensorInfo(x), n.disposeIntermediateTensorInfo(v), n.makeTensorInfo(y, M.dtype, M.values);\n}\n\nvar batchMatMulConfig$3 = {\n  kernelName: BatchMatMul$1,\n  backendName: \"cpu\",\n  kernelFunc: batchMatMul$3\n};\n\nfunction _fusedMatMul$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    transposeA: l,\n    transposeB: u,\n    activation: c,\n    leakyreluAlpha: p\n  } = r;\n  var d, h, m;\n  var f = [];\n  d = batchMatMul$3({\n    inputs: {\n      a,\n      b: s\n    },\n    attrs: {\n      transposeA: l,\n      transposeB: u\n    },\n    backend: n\n  }), o && (h = add$4({\n    inputs: {\n      a: d,\n      b: o\n    },\n    backend: n\n  }), f.push(d), d = h), c && (m = applyActivation$2(n, d, c, i, p), f.push(d), d = m);\n\n  for (var _e277 of f) {\n    n.disposeIntermediateTensorInfo(_e277);\n  }\n\n  return d;\n}\n\nvar _fusedMatMulConfig$3 = {\n  kernelName: _FusedMatMul$1,\n  backendName: \"cpu\",\n  kernelFunc: _fusedMatMul$3\n},\n    acos$4 = unaryKernelFunc$3(Acos$1, e => Math.acos(e)),\n    acosConfig$3 = {\n  kernelName: Acos$1,\n  backendName: \"cpu\",\n  kernelFunc: acos$4\n},\n    acosh$4 = unaryKernelFunc$3(Acosh$1, e => Math.acosh(e)),\n    acoshConfig$3 = {\n  kernelName: Acosh$1,\n  backendName: \"cpu\",\n  kernelFunc: acosh$4\n};\n\nfunction addN$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      r = t;\n  assertNotComplex$3(t, \"addN\");\n  var a = r.map(e => n.data.get(e.dataId).values),\n      s = buffer$1(r[0].shape, r[0].dtype),\n      o = s.values;\n\n  for (var _e278 = 0; _e278 < r.length; _e278++) {\n    var _t211 = a[_e278];\n\n    for (var _e279 = 0; _e279 < o.length; _e279++) {\n      o[_e279] += _t211[_e279];\n    }\n  }\n\n  return n.makeTensorInfo(s.shape, s.dtype, s.values);\n}\n\nvar addNConfig$3 = {\n  kernelName: AddN$1,\n  backendName: \"cpu\",\n  kernelFunc: addN$4\n};\n\nfunction all$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$3(a, \"all\");\n  var i = parseAxisParam$1(s, a.shape);\n  var l = i;\n  var u = getAxesPermutation$1(l, a.shape.length);\n  var c = a;\n  null != u && (c = transpose$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), l = getInnerMostAxes$1(l.length, a.shape.length)), assertAxesAreInnerMostDims$1(\"all\", l, c.shape.length);\n  var [p, d] = computeOutAndReduceShapes$1(c.shape, l),\n      h = sizeFromShape$1(d),\n      m = makeZerosTypedArray$1(sizeFromShape$1(p), c.dtype),\n      f = n.data.get(c.dataId).values;\n\n  for (var _e280 = 0; _e280 < m.length; ++_e280) {\n    var _t212 = _e280 * h;\n\n    var _n108 = f[_t212];\n\n    for (var _e281 = 0; _e281 < h; ++_e281) {\n      var _r83 = f[_t212 + _e281];\n      _n108 = _n108 && _r83;\n    }\n\n    m[_e280] = _n108;\n  }\n\n  null != u && n.disposeIntermediateTensorInfo(c);\n  var g = n.makeTensorInfo(p, c.dtype, m);\n\n  if (o) {\n    var _e282 = reshape$5({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim$1(p, i)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), _e282;\n  }\n\n  return g;\n}\n\nvar allConfig$3 = {\n  kernelName: All$1,\n  backendName: \"cpu\",\n  kernelFunc: all$4\n};\n\nfunction any$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$3(a, \"any\");\n  var i = parseAxisParam$1(s, a.shape);\n  var l = i;\n  var u = getAxesPermutation$1(l, a.shape.length);\n  var c = a;\n  null != u && (c = transpose$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), l = getInnerMostAxes$1(l.length, a.shape.length)), assertAxesAreInnerMostDims$1(\"any\", l, c.shape.length);\n  var [p, d] = computeOutAndReduceShapes$1(c.shape, l),\n      h = sizeFromShape$1(d),\n      m = makeZerosTypedArray$1(sizeFromShape$1(p), c.dtype),\n      f = n.data.get(c.dataId).values;\n\n  for (var _e283 = 0; _e283 < m.length; ++_e283) {\n    var _t213 = _e283 * h;\n\n    var _n109 = f[_t213];\n\n    for (var _e284 = 0; _e284 < h; ++_e284) {\n      var _r84 = f[_t213 + _e284];\n      _n109 = _n109 || _r84;\n    }\n\n    m[_e283] = _n109;\n  }\n\n  null != u && n.disposeIntermediateTensorInfo(c);\n  var g = n.makeTensorInfo(p, c.dtype, m);\n\n  if (o) {\n    var _e285 = reshape$5({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim$1(p, i)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), _e285;\n  }\n\n  return g;\n}\n\nvar anyConfig$3 = {\n  kernelName: Any$1,\n  backendName: \"cpu\",\n  kernelFunc: any$4\n};\n\nfunction argMax$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  assertNotComplex$3(a, \"argMax\");\n  var o = parseAxisParam$1(s, a.shape);\n  var i = getAxesPermutation$1(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes$1(o.length, l.shape.length)), o = [o[0]], assertAxesAreInnerMostDims$1(\"argMax\", o, l.shape.length);\n  var [c, p] = computeOutAndReduceShapes$1(l.shape, o),\n      d = makeZerosTypedArray$1(sizeFromShape$1(c), \"int32\"),\n      h = sizeFromShape$1(p),\n      m = n.data.get(l.dataId).values;\n\n  for (var _e286 = 0; _e286 < d.length; ++_e286) {\n    var _t214 = _e286 * h;\n\n    var _n110 = m[_t214],\n        _r85 = 0;\n\n    for (var _e287 = 0; _e287 < h; ++_e287) {\n      var _a54 = m[_t214 + _e287];\n      _a54 > _n110 && (_n110 = _a54, _r85 = _e287);\n    }\n\n    d[_e286] = _r85;\n  }\n\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo(c, \"int32\", d);\n}\n\nvar argMaxConfig$3 = {\n  kernelName: ArgMax$1,\n  backendName: \"cpu\",\n  kernelFunc: argMax$4\n};\n\nfunction argMin$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  assertNotComplex$3(a, \"argMin\");\n  var o = parseAxisParam$1(s, a.shape);\n  var i = getAxesPermutation$1(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes$1(o.length, l.shape.length)), o = [o[0]], assertAxesAreInnerMostDims$1(\"argMin\", o, l.shape.length);\n  var [c, p] = computeOutAndReduceShapes$1(l.shape, o),\n      d = makeZerosTypedArray$1(sizeFromShape$1(c), \"int32\"),\n      h = sizeFromShape$1(p),\n      m = n.data.get(l.dataId).values;\n\n  for (var _e288 = 0; _e288 < d.length; ++_e288) {\n    var _t215 = _e288 * h;\n\n    var _n111 = m[_t215],\n        _r86 = 0;\n\n    for (var _e289 = 0; _e289 < h; ++_e289) {\n      var _a55 = m[_t215 + _e289];\n      _a55 < _n111 && (_n111 = _a55, _r86 = _e289);\n    }\n\n    d[_e288] = _r86;\n  }\n\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo(c, \"int32\", d);\n}\n\nvar argMinConfig$3 = {\n  kernelName: ArgMin$1,\n  backendName: \"cpu\",\n  kernelFunc: argMin$4\n},\n    asin$4 = unaryKernelFunc$3(Asin$1, e => Math.asin(e)),\n    asinConfig$3 = {\n  kernelName: Asin$1,\n  backendName: \"cpu\",\n  kernelFunc: asin$4\n},\n    asinh$4 = unaryKernelFunc$3(Asinh$1, e => Math.asinh(e)),\n    asinhConfig$3 = {\n  kernelName: Asinh$1,\n  backendName: \"cpu\",\n  kernelFunc: asinh$4\n},\n    atan$4 = unaryKernelFunc$3(Atan$1, e => Math.atan(e)),\n    atanConfig$3 = {\n  kernelName: Atan$1,\n  backendName: \"cpu\",\n  kernelFunc: atan$4\n},\n    atan2Impl$1 = createSimpleBinaryKernelImpl$1((e, t) => Math.atan2(e, t)),\n    atan2$4 = binaryKernelFunc$3(Atan2$1, atan2Impl$1),\n    atan2Config$3 = {\n  kernelName: Atan2$1,\n  backendName: \"cpu\",\n  kernelFunc: atan2$4\n},\n    atanh$4 = unaryKernelFunc$3(Atanh$1, e => Math.atanh(e)),\n    atanhConfig$3 = {\n  kernelName: Atanh$1,\n  backendName: \"cpu\",\n  kernelFunc: atanh$4\n};\n\nfunction pool$2(e, t, n, r, a, s) {\n  var o = a.strideHeight,\n      i = a.strideWidth,\n      l = a.dilationHeight,\n      u = a.dilationWidth,\n      c = a.effectiveFilterHeight,\n      p = a.effectiveFilterWidth,\n      d = a.padInfo.top,\n      h = a.padInfo.left,\n      m = \"max\" === s ? Number.NEGATIVE_INFINITY : Number.POSITIVE_INFINITY,\n      f = buffer$1(a.outShape, n),\n      g = f.values,\n      $ = a.outShape[1] * a.outShape[2] * a.outShape[3],\n      y = a.outShape[2] * a.outShape[3],\n      b = a.outShape[3];\n\n  for (var _t216 = 0; _t216 < a.batchSize; ++_t216) {\n    var _n112 = _t216 * $,\n        _f = _t216 * r[0];\n\n    for (var _t217 = 0; _t217 < a.inChannels; ++_t217) {\n      for (var _$ = 0; _$ < a.outHeight; ++_$) {\n        var x = _$ * o - d,\n            v = Math.max(0, x),\n            I = Math.min(a.inHeight, c + x),\n            C = _n112 + _$ * y;\n\n        for (var _n113 = 0; _n113 < a.outWidth; ++_n113) {\n          var _o14 = _n113 * i - h,\n              _c4 = Math.max(0, _o14),\n              _d3 = Math.min(a.inWidth, p + _o14);\n\n          var _$2 = m,\n              _y = 0,\n              _x50 = 0;\n\n          for (var _n114 = v; _n114 < I; _n114 += l) {\n            var _a56 = _f + _n114 * r[1];\n\n            for (var _n115 = _c4; _n115 < _d3; _n115 += u) {\n              var _o15 = e[_a56 + _n115 * r[2] + _t217];\n              \"max\" === s && _o15 > _$2 ? _$2 = _o15 : \"avg\" === s && (_y += _o15, _x50++);\n            }\n\n            if (isNaN(_$2)) break;\n          }\n\n          g[C + _n113 * b + _t217] = \"avg\" === s ? _y / _x50 : _$2;\n        }\n      }\n    }\n  }\n\n  return f;\n}\n\nfunction maxPoolPositions$1(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = buffer$1(r.outShape, \"int32\"),\n      i = r.strideHeight,\n      l = r.strideWidth,\n      u = r.dilationHeight,\n      c = r.dilationWidth,\n      p = r.effectiveFilterHeight,\n      d = r.effectiveFilterWidth,\n      h = r.padInfo.top,\n      m = r.padInfo.left,\n      f = buffer$1(t, n, e);\n\n  for (var _e290 = 0; _e290 < r.batchSize; ++_e290) {\n    for (var _t218 = 0; _t218 < r.inChannels; ++_t218) {\n      for (var _n116 = 0; _n116 < r.outHeight; ++_n116) {\n        var g = _n116 * i - h;\n        var $ = g;\n\n        for (; $ < 0;) {\n          $ += u;\n        }\n\n        var y = Math.min(r.inHeight, p + g);\n\n        for (var _i8 = 0; _i8 < r.outWidth; ++_i8) {\n          var _p7 = _i8 * l - m;\n\n          var _h3 = _p7;\n\n          for (; _h3 < 0;) {\n            _h3 += c;\n          }\n\n          var b = Math.min(r.inWidth, d + _p7);\n          var x = Number.NEGATIVE_INFINITY,\n              v = -1;\n\n          for (var _n117 = $; _n117 < y; _n117 += u) {\n            var _o16 = _n117 - g;\n\n            for (var _i9 = _h3; _i9 < b; _i9 += c) {\n              var _l8 = _i9 - _p7,\n                  _u6 = f.get(_e290, _n117, _i9, _t218);\n\n              _u6 > x && (x = _u6, v = a ? s ? ((_e290 * r.inHeight + _n117) * r.inWidth + _i9) * r.inChannels + _t218 : (_n117 * r.inWidth + _i9) * r.inChannels + _t218 : _o16 * d + _l8);\n            }\n          }\n\n          o.set(v, _e290, _n116, _i8, _t218);\n        }\n      }\n    }\n  }\n\n  return o;\n}\n\nfunction pool3d$2(e, t, n, r, a, s) {\n  var o = a.strideDepth,\n      i = a.strideHeight,\n      l = a.strideWidth,\n      u = a.dilationDepth,\n      c = a.dilationHeight,\n      p = a.dilationWidth,\n      d = a.effectiveFilterDepth,\n      h = a.effectiveFilterHeight,\n      m = a.effectiveFilterWidth,\n      f = a.padInfo.front,\n      g = a.padInfo.top,\n      $ = a.padInfo.left,\n      y = \"max\" === s ? Number.NEGATIVE_INFINITY : Number.POSITIVE_INFINITY,\n      b = buffer$1(a.outShape, n),\n      x = b.values,\n      v = a.outShape[1] * a.outShape[2] * a.outShape[3] * a.outShape[4],\n      I = a.outShape[2] * a.outShape[3] * a.outShape[4],\n      C = a.outShape[3] * a.outShape[4],\n      S = a.outShape[4];\n\n  for (var _t219 = 0; _t219 < a.batchSize; ++_t219) {\n    var _n118 = _t219 * v,\n        _b = _t219 * r[0];\n\n    for (var _t220 = 0; _t220 < a.inChannels; ++_t220) {\n      for (var _v = 0; _v < a.outDepth; ++_v) {\n        var k = _v * o - f;\n        var T = k;\n\n        for (; T < 0;) {\n          T += u;\n        }\n\n        var N = Math.min(a.inDepth, d + k),\n            w = _n118 + _v * I;\n\n        for (var _n119 = 0; _n119 < a.outHeight; ++_n119) {\n          var _o17 = _n119 * i - g;\n\n          var _d4 = _o17;\n\n          for (; _d4 < 0;) {\n            _d4 += c;\n          }\n\n          var _f2 = Math.min(a.inHeight, h + _o17),\n              _v2 = w + _n119 * C;\n\n          for (var _n120 = 0; _n120 < a.outWidth; ++_n120) {\n            var _o18 = _n120 * l - $;\n\n            var _i10 = _o18;\n\n            for (; _i10 < 0;) {\n              _i10 += p;\n            }\n\n            var _h4 = Math.min(a.inWidth, m + _o18),\n                _g = _v2 + _n120 * S;\n\n            var _I = y,\n                _C = 0,\n                _k = 0;\n\n            for (var _n121 = T; _n121 < N; _n121 += u) {\n              var _a57 = _b + _n121 * r[1];\n\n              for (var _n122 = _d4; _n122 < _f2; _n122 += c) {\n                var _o19 = _a57 + _n122 * r[2];\n\n                for (var _n123 = _i10; _n123 < _h4; _n123 += p) {\n                  var _a58 = e[_o19 + _n123 * r[3] + _t220];\n                  if (\"max\" === s && _a58 > _I ? _I = _a58 : \"avg\" === s && (_C += _a58, _k++), isNaN(_I)) break;\n                }\n\n                if (isNaN(_I)) break;\n              }\n\n              if (isNaN(_I)) break;\n            }\n\n            x[_g + _t220] = \"avg\" === s ? _C / _k : _I;\n          }\n        }\n      }\n    }\n  }\n\n  return b;\n}\n\nfunction maxPool3dPositions$1(e, t) {\n  var n = buffer$1(t.outShape, \"int32\"),\n      r = t.strideDepth,\n      a = t.strideHeight,\n      s = t.strideWidth,\n      o = t.dilationDepth,\n      i = t.dilationHeight,\n      l = t.dilationWidth,\n      u = t.effectiveFilterDepth,\n      c = t.effectiveFilterHeight,\n      p = t.effectiveFilterWidth,\n      d = t.padInfo.front,\n      h = t.padInfo.top,\n      m = t.padInfo.left;\n\n  for (var f = 0; f < t.batchSize; ++f) {\n    for (var g = 0; g < t.inChannels; ++g) {\n      for (var $ = 0; $ < t.outDepth; ++$) {\n        var y = $ * r - d;\n        var b = y;\n\n        for (; b < 0;) {\n          b += o;\n        }\n\n        var x = Math.min(t.inDepth, u + y);\n\n        for (var _r87 = 0; _r87 < t.outHeight; ++_r87) {\n          var _u7 = _r87 * a - h;\n\n          var _d5 = _u7;\n\n          for (; _d5 < 0;) {\n            _d5 += i;\n          }\n\n          var v = Math.min(t.inHeight, c + _u7);\n\n          for (var _a59 = 0; _a59 < t.outWidth; ++_a59) {\n            var _h5 = _a59 * s - m;\n\n            var I = _h5;\n\n            for (; I < 0;) {\n              I += l;\n            }\n\n            var C = Math.min(t.inWidth, p + _h5);\n            var S = Number.NEGATIVE_INFINITY,\n                k = -1;\n\n            for (var _t221 = b; _t221 < x; _t221 += o) {\n              var _n124 = _t221 - y;\n\n              for (var _r88 = _d5; _r88 < v; _r88 += i) {\n                var _a60 = _r88 - _u7;\n\n                for (var _s37 = I; _s37 < C; _s37 += l) {\n                  var _o20 = _s37 - _h5,\n                      _i11 = e.get(f, _t221, _r88, _s37, g);\n\n                  _i11 >= S && (S = _i11, k = _n124 * c * p + _a60 * c + _o20);\n                }\n              }\n            }\n\n            n.set(k, f, $, _r87, _a59, g);\n          }\n        }\n      }\n    }\n  }\n\n  return n;\n}\n\nfunction avgPool$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex$3(a, \"avgPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$6(eitherStridesOrDilationsAreOne$1(o, 1), () => \"Error in avgPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo$1(a.shape, s, o, 1, i, l);\n  var c;\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual$1(u.inShape, u.outShape)) c = identity$4({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });else {\n    var _e291 = n.data.get(a.dataId).values,\n        _t222 = computeStrides$1(a.shape),\n        _r89 = pool$2(_e291, a.shape, a.dtype, _t222, u, \"avg\");\n\n    c = n.makeTensorInfo(u.outShape, a.dtype, _r89.values);\n  }\n  return c;\n}\n\nvar avgPoolConfig$3 = {\n  kernelName: AvgPool$1,\n  backendName: \"cpu\",\n  kernelFunc: avgPool$4\n};\n\nfunction avgPool3D$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l,\n    dataFormat: u\n  } = r;\n  assertNotComplex$3(a, \"avgPool3d\");\n  var c = computePool3DInfo$1(a.shape, s, o, 1, i, l, u),\n      p = pool3d$2(n.data.get(a.dataId).values, a.shape, a.dtype, computeStrides$1(a.shape), c, \"avg\");\n  return n.makeTensorInfo(p.shape, \"float32\", p.values);\n}\n\nvar avgPool3DConfig$3 = {\n  kernelName: AvgPool3D$1,\n  backendName: \"cpu\",\n  kernelFunc: avgPool3D$3\n};\n\nfunction avgPool3DGrad$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      {\n    filterSize: o,\n    strides: i,\n    pad: l,\n    dimRoundingMode: u\n  } = r;\n  assertNotComplex$3([a, s], \"avgPool3DGrad\");\n  var c = computePool3DInfo$1(s.shape, o, i, 1, l, u),\n      p = c.strideDepth,\n      d = c.strideHeight,\n      h = c.strideWidth,\n      m = c.filterDepth,\n      f = c.filterHeight,\n      g = c.filterWidth,\n      $ = c.dilationDepth,\n      y = c.dilationHeight,\n      b = c.dilationWidth,\n      x = c.effectiveFilterDepth,\n      v = c.effectiveFilterHeight,\n      I = c.effectiveFilterWidth,\n      C = x - 1 - c.padInfo.front,\n      S = I - 1 - c.padInfo.left,\n      k = v - 1 - c.padInfo.top,\n      T = buffer$1(s.shape, \"float32\"),\n      N = 1 / (m * f * g),\n      w = n.bufferSync(a);\n\n  for (var _e292 = 0; _e292 < c.batchSize; ++_e292) {\n    for (var _t223 = 0; _t223 < c.inChannels; ++_t223) {\n      for (var _n125 = 0; _n125 < c.inDepth; ++_n125) {\n        for (var _r90 = 0; _r90 < c.inHeight; ++_r90) {\n          for (var _a61 = 0; _a61 < c.inWidth; ++_a61) {\n            var _s38 = _n125 - C,\n                _o21 = _r90 - k,\n                _i12 = _a61 - S;\n\n            var _l9 = 0;\n\n            for (var _n126 = 0; _n126 < x; _n126 += $) {\n              var _r91 = (_s38 + _n126) / p;\n\n              if (!(_r91 < 0 || _r91 >= c.outDepth || Math.floor(_r91) !== _r91)) for (var _n127 = 0; _n127 < v; _n127 += y) {\n                var _a62 = (_o21 + _n127) / d;\n\n                if (!(_a62 < 0 || _a62 >= c.outHeight || Math.floor(_a62) !== _a62)) for (var _n128 = 0; _n128 < I; _n128 += b) {\n                  var _s39 = (_i12 + _n128) / h;\n\n                  _s39 < 0 || _s39 >= c.outWidth || Math.floor(_s39) !== _s39 || (_l9 += w.get(_e292, _r91, _a62, _s39, _t223));\n                }\n              }\n            }\n\n            T.set(_l9 * N, _e292, _n125, _r90, _a61, _t223);\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(T.shape, T.dtype, T.values);\n}\n\nvar avgPool3DGradConfig$2 = {\n  kernelName: AvgPool3DGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: avgPool3DGrad$3\n};\n\nfunction avgPoolGrad$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s;\n  assertNotComplex$3([a, s], \"avgPoolGrad\");\n  var {\n    filterSize: i,\n    strides: l,\n    pad: u\n  } = r,\n      c = computePool2DInfo$1(o.shape, i, l, 1, u),\n      p = c.strideHeight,\n      d = c.strideWidth,\n      h = c.filterHeight,\n      m = c.filterWidth,\n      f = c.dilationHeight,\n      g = c.dilationWidth,\n      $ = c.effectiveFilterHeight,\n      y = c.effectiveFilterWidth,\n      b = y - 1 - c.padInfo.left,\n      x = $ - 1 - c.padInfo.top,\n      v = buffer$1(o.shape, \"float32\"),\n      I = 1 / (h * m),\n      C = n.data.get(a.dataId).values,\n      S = buffer$1(a.shape, \"float32\", C);\n\n  for (var _e293 = 0; _e293 < c.batchSize; ++_e293) {\n    for (var _t224 = 0; _t224 < c.inChannels; ++_t224) {\n      for (var _n129 = 0; _n129 < c.inHeight; ++_n129) {\n        for (var _r92 = 0; _r92 < c.inWidth; ++_r92) {\n          var _a63 = _n129 - x,\n              _s40 = _r92 - b;\n\n          var _o22 = 0;\n\n          for (var _n130 = 0; _n130 < $; _n130 += f) {\n            var _r93 = (_a63 + _n130) / p;\n\n            if (!(_r93 < 0 || _r93 >= c.outHeight || Math.floor(_r93) !== _r93)) for (var _n131 = 0; _n131 < y; _n131 += g) {\n              var _a64 = (_s40 + _n131) / d;\n\n              _a64 < 0 || _a64 >= c.outWidth || Math.floor(_a64) !== _a64 || (_o22 += S.get(_e293, _r93, _a64, _t224));\n            }\n          }\n\n          v.set(_o22 * I, _e293, _n129, _r92, _t224);\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(v.shape, v.dtype, v.values);\n}\n\nvar avgPoolGradConfig$4 = {\n  kernelName: AvgPoolGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: avgPoolGrad$4\n};\n\nfunction batchNorm$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    scale: s,\n    offset: o,\n    mean: i,\n    variance: l\n  } = t;\n  assert$6(i.shape.length === l.shape.length, () => \"Batch normalization gradient requires mean and variance to have equal ranks.\"), assert$6(null == o || i.shape.length === o.shape.length, () => \"Batch normalization gradient requires mean and offset to have equal ranks.\"), assert$6(null == s || i.shape.length === s.shape.length, () => \"Batch normalization gradient requires mean and scale to have equal ranks.\"), assertNotComplex$3([a, i, l, s, o], \"batchNorm\");\n  var {\n    varianceEpsilon: u\n  } = r;\n  null == u && (u = .001);\n  var c = n.data.get(a.dataId).values,\n      p = n.data.get(i.dataId).values,\n      d = n.data.get(l.dataId).values,\n      h = s ? n.data.get(s.dataId).values : new Float32Array([1]),\n      m = o ? n.data.get(o.dataId).values : new Float32Array([0]),\n      f = new Float32Array(c.length),\n      g = m.length,\n      $ = h.length,\n      y = d.length,\n      b = p.length;\n  var x = 0,\n      v = 0,\n      I = 0,\n      C = 0;\n\n  for (var _e294 = 0; _e294 < c.length; ++_e294) {\n    f[_e294] = m[x++] + (c[_e294] - p[v++]) * h[I++] / Math.sqrt(d[C++] + u), x >= g && (x = 0), v >= b && (v = 0), I >= $ && (I = 0), C >= y && (C = 0);\n  }\n\n  return n.makeTensorInfo(a.shape, a.dtype, f);\n}\n\nvar batchNormConfig$3 = {\n  kernelName: FusedBatchNorm$1,\n  backendName: \"cpu\",\n  kernelFunc: batchNorm$4\n};\n\nfunction batchToSpaceND$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    crops: o\n  } = r;\n  assertNotComplex$3([a], \"batchToSpaceND\");\n  var i = s.reduce((e, t) => e * t),\n      l = getReshaped$1(a.shape, s, i),\n      u = getPermuted$1(l.length, s.length),\n      c = getReshapedPermuted$1(a.shape, s, i),\n      p = getSliceBeginCoords$1(o, s.length),\n      d = getSliceSize$1(c, o, s.length),\n      h = reshape$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      m = transpose$4({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }),\n      f = reshape$5({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      g = slice$4({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      begin: p,\n      size: d\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), g;\n}\n\nvar batchToSpaceNDConfig$3 = {\n  kernelName: BatchToSpaceND$1,\n  backendName: \"cpu\",\n  kernelFunc: batchToSpaceND$4\n};\n\nfunction bincount$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o\n  } = r,\n      i = bincountImpl$1(n.data.get(a.dataId).values, n.data.get(s.dataId).values, s.dtype, s.shape, o);\n  return n.makeTensorInfo([o], s.dtype, i);\n}\n\nvar bincountConfig$3 = {\n  kernelName: Bincount$1,\n  backendName: \"cpu\",\n  kernelFunc: bincount$4\n},\n    clip$1 = unaryKernelFunc$3(ClipByValue$1, (e, t) => e > t.clipValueMax ? t.clipValueMax : e < t.clipValueMin ? t.clipValueMin : e),\n    clipConfig$1 = {\n  kernelName: ClipByValue$1,\n  backendName: \"cpu\",\n  kernelFunc: clip$1\n},\n    complexAbs$3 = e => {\n  var {\n    x: t\n  } = e.inputs,\n      n = e.backend,\n      r = new Float32Array(sizeFromShape$1(t.shape)),\n      a = n.data.get(t.dataId),\n      s = a.complexTensorInfos.imag,\n      o = n.data.get(a.complexTensorInfos.real.dataId).values,\n      i = n.data.get(s.dataId).values;\n\n  for (var _e295 = 0; _e295 < o.length; _e295++) {\n    r[_e295] = Math.hypot(o[_e295], i[_e295]);\n  }\n\n  return n.makeOutput(r, t.shape, \"float32\");\n},\n    complexAbsConfig$3 = {\n  kernelName: ComplexAbs$1,\n  backendName: \"cpu\",\n  kernelFunc: complexAbs$3\n};\n\nfunction imag$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = n.data.get(r.dataId).complexTensorInfos.imag,\n      s = n.data.get(a.dataId).values;\n  return n.makeTensorInfo(a.shape, a.dtype, s);\n}\n\nvar imagConfig$3 = {\n  kernelName: Imag$1,\n  backendName: \"cpu\",\n  kernelFunc: imag$4\n};\n\nfunction concat$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r,\n      s = parseAxisParam$1(a, t[0].shape)[0];\n  var o = computeOutShape$4(t.map(e => e.shape), s);\n  if (0 === sizeFromShape$1(o)) return n.makeTensorInfo(o, t[0].dtype, []);\n  var i = t.filter(e => sizeFromShape$1(e.shape) > 0);\n  if (1 === i.length) return identity$4({\n    inputs: {\n      x: i[0]\n    },\n    backend: n\n  });\n\n  if (assertParamsConsistent$1(i.map(e => e.shape), s), \"complex64\" === i[0].dtype) {\n    var _e296 = i.map(e => real$4({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _t225 = i.map(e => imag$4({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _r94 = concat$4({\n      inputs: _e296,\n      backend: n,\n      attrs: {\n        axis: s\n      }\n    }),\n        _a65 = concat$4({\n      inputs: _t225,\n      backend: n,\n      attrs: {\n        axis: s\n      }\n    }),\n        _o23 = complex$4({\n      inputs: {\n        real: _r94,\n        imag: _a65\n      },\n      backend: n\n    });\n\n    return _e296.forEach(e => n.disposeIntermediateTensorInfo(e)), _t225.forEach(e => n.disposeIntermediateTensorInfo(e)), n.disposeIntermediateTensorInfo(_r94), n.disposeIntermediateTensorInfo(_a65), _o23;\n  }\n\n  var l = i.map(e => {\n    var t = sizeFromShape$1(e.shape.slice(s));\n    return reshape$5({\n      inputs: {\n        x: e\n      },\n      backend: n,\n      attrs: {\n        shape: [-1, t]\n      }\n    });\n  }),\n      u = l.map(e => ({\n    vals: n.data.get(e.dataId).values,\n    shape: e.shape\n  }));\n  o = computeOutShape$4(l.map(e => e.shape), 1);\n  var c = concatImpl$3(u, o, t[0].dtype, 1 === l[0].shape[0]),\n      p = computeOutShape$4(i.map(e => e.shape), s),\n      d = n.makeTensorInfo(p, t[0].dtype, c);\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), d;\n}\n\nvar concatConfig$3 = {\n  kernelName: Concat$1,\n  backendName: \"cpu\",\n  kernelFunc: concat$4\n};\n\nfunction conv2D$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dilations: u,\n    dimRoundingMode: c\n  } = r;\n  assertNotComplex$3([a, s], \"conv2d\");\n  var p = convertConv2DDataFormat$1(l),\n      d = computeConv2DInfo$1(a.shape, s.shape, o, u, i, c, !1, p),\n      h = d.filterHeight,\n      m = d.filterWidth,\n      f = d.dilationHeight,\n      g = d.dilationWidth,\n      $ = d.padInfo.left,\n      y = d.padInfo.top,\n      b = \"channelsLast\" === d.dataFormat,\n      x = new TensorBuffer$1(d.outShape, a.dtype),\n      v = computeStrides$1(a.shape),\n      I = computeStrides$1(s.shape),\n      C = v[0],\n      S = b ? v[1] : v[2],\n      k = b ? v[2] : 1,\n      T = b ? 1 : v[1],\n      N = x.strides[0],\n      w = b ? x.strides[1] : x.strides[2],\n      E = b ? x.strides[2] : 1,\n      A = b ? 1 : x.strides[1],\n      D = n.data.get(a.dataId).values,\n      R = n.data.get(s.dataId).values,\n      _ = x.values;\n\n  for (var _e297 = 0; _e297 < d.batchSize; ++_e297) {\n    var _t226 = _e297 * C,\n        _n132 = _e297 * N;\n\n    for (var _e298 = 0; _e298 < d.outHeight; ++_e298) {\n      var _r95 = _n132 + _e298 * w,\n          _a66 = _e298 * d.strideHeight - y;\n\n      for (var _e299 = 0; _e299 < h; ++_e299) {\n        var _n133 = _a66 + _e299 * f;\n\n        if (_n133 < 0 || _n133 >= d.inHeight) continue;\n\n        var _s41 = _e299 * I[0],\n            _o24 = _t226 + _n133 * S;\n\n        for (var _e300 = 0; _e300 < d.outWidth; ++_e300) {\n          var _t227 = _r95 + _e300 * E,\n              _n134 = _e300 * d.strideWidth - $;\n\n          for (var _e301 = 0; _e301 < m; ++_e301) {\n            var _r96 = _n134 + _e301 * g;\n\n            if (_r96 < 0 || _r96 >= d.inWidth) continue;\n\n            var _a67 = _o24 + _r96 * k;\n\n            var _i13 = _s41 + _e301 * I[1];\n\n            for (var _e302 = 0; _e302 < d.inChannels; ++_e302) {\n              var _n135 = D[_a67 + _e302 * T];\n\n              for (var _e303 = 0; _e303 < d.outChannels; ++_e303) {\n                _[_t227 + _e303 * A] += _n135 * R[_i13 + _e303];\n              }\n\n              _i13 += d.outChannels;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(x.shape, x.dtype, _);\n}\n\nvar conv2DConfig$3 = {\n  kernelName: Conv2D$3,\n  backendName: \"cpu\",\n  kernelFunc: conv2D$1\n};\n\nfunction conv2DBackpropFilter$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r;\n  assertNotComplex$3([a, s], \"conv2dBackpropFilter\");\n  var p = convertConv2DDataFormat$1(l),\n      d = computeConv2DInfo$1(a.shape, c, o, 1, i, u, !1, p),\n      {\n    strideHeight: h,\n    strideWidth: m,\n    filterHeight: f,\n    filterWidth: g\n  } = d,\n      $ = \"channelsLast\" === d.dataFormat,\n      y = new TensorBuffer$1(d.filterShape, \"float32\"),\n      b = d.padInfo.left,\n      x = d.padInfo.top,\n      v = n.data.get(a.dataId).values,\n      I = n.data.get(s.dataId).values,\n      C = new TensorBuffer$1(a.shape, a.dtype, v),\n      S = new TensorBuffer$1(s.shape, s.dtype, I);\n\n  for (var _e304 = 0; _e304 < f; ++_e304) {\n    var _t228 = Math.max(0, Math.ceil((x - _e304) / h)),\n        _n136 = Math.min(d.outHeight, (d.inHeight + x - _e304) / h);\n\n    for (var _r97 = 0; _r97 < g; ++_r97) {\n      var _a68 = Math.max(0, Math.ceil((b - _r97) / m)),\n          _s42 = Math.min(d.outWidth, (d.inWidth + b - _r97) / m);\n\n      for (var _o25 = 0; _o25 < d.inChannels; ++_o25) {\n        for (var _i14 = 0; _i14 < d.outChannels; ++_i14) {\n          var _l10 = 0;\n\n          for (var _u8 = 0; _u8 < d.batchSize; ++_u8) {\n            for (var _c5 = _t228; _c5 < _n136; ++_c5) {\n              var _t229 = _e304 + _c5 * h - x;\n\n              for (var _e305 = _a68; _e305 < _s42; ++_e305) {\n                var _n137 = _r97 + _e305 * m - b;\n\n                _l10 += $ ? C.get(_u8, _t229, _n137, _o25) * S.get(_u8, _c5, _e305, _i14) : C.get(_u8, _o25, _t229, _n137) * S.get(_u8, _i14, _c5, _e305);\n              }\n            }\n          }\n\n          y.set(_l10, _e304, _r97, _o25, _i14);\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(y.shape, y.dtype, y.values);\n}\n\nvar conv2DBackpropFilterConfig$3 = {\n  kernelName: Conv2DBackpropFilter$1,\n  backendName: \"cpu\",\n  kernelFunc: conv2DBackpropFilter$4\n};\n\nfunction conv2DBackpropInput$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    inputShape: o,\n    strides: i,\n    pad: l,\n    dataFormat: u,\n    dimRoundingMode: c\n  } = r;\n  assertNotComplex$3([a, s], \"conv2dBackpropInput\");\n  var p = computeStrides$1(s.shape),\n      d = computeStrides$1(a.shape);\n  var h = convertConv2DDataFormat$1(u);\n  var m = computeConv2DInfo$1(o, s.shape, i, 1, l, c, !1, h),\n      f = new TensorBuffer$1(m.inShape, \"float32\"),\n      g = f.values,\n      $ = n.data.get(a.dataId).values,\n      y = n.data.get(s.dataId).values,\n      [b, x, v] = p,\n      {\n    batchSize: I,\n    filterHeight: C,\n    filterWidth: S,\n    inChannels: k,\n    inHeight: T,\n    inWidth: N,\n    outChannels: w,\n    outHeight: E,\n    outWidth: A,\n    strideHeight: D,\n    strideWidth: R\n  } = m;\n  h = m.dataFormat;\n\n  var _ = C - 1 - m.padInfo.top,\n      F = S - 1 - m.padInfo.left,\n      P = \"channelsLast\" === h,\n      O = f.strides[0],\n      M = P ? f.strides[1] : f.strides[2],\n      L = P ? f.strides[2] : 1,\n      z = P ? 1 : f.strides[1],\n      B = d[0],\n      V = P ? d[1] : d[2],\n      G = P ? d[2] : 1,\n      U = P ? 1 : d[1];\n\n  for (var _e306 = 0; _e306 < I; ++_e306) {\n    for (var _t230 = 0; _t230 < k; ++_t230) {\n      for (var _n138 = 0; _n138 < T; ++_n138) {\n        var _r98 = _n138 - _,\n            _a69 = Math.max(0, Math.ceil(_r98 / D)),\n            _s43 = Math.min(E, (C + _r98) / D);\n\n        for (var _o26 = 0; _o26 < N; ++_o26) {\n          var _i15 = _o26 - F,\n              _l11 = Math.max(0, Math.ceil(_i15 / R)),\n              _u9 = Math.min(A, (S + _i15) / R);\n\n          var _c6 = 0;\n\n          for (var _n139 = _a69; _n139 < _s43; ++_n139) {\n            var _a70 = _n139 * D - _r98;\n\n            for (var _r99 = _l11; _r99 < _u9; ++_r99) {\n              var _s44 = B * _e306 + V * _n139 + G * _r99,\n                  _o27 = b * (C - 1 - _a70) + x * (S - 1 - (_r99 * R - _i15)) + v * _t230;\n\n              for (var _e307 = 0; _e307 < w; ++_e307) {\n                _c6 += $[_s44 + U * _e307] * y[_o27 + _e307];\n              }\n            }\n          }\n\n          g[O * _e306 + M * _n138 + L * _o26 + z * _t230] = _c6;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(f.shape, f.dtype, f.values);\n}\n\nvar conv2DBackpropInputConfig$3 = {\n  kernelName: Conv2DBackpropInput$1,\n  backendName: \"cpu\",\n  kernelFunc: conv2DBackpropInput$4\n};\n\nfunction conv3D$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l\n  } = r;\n  assertNotComplex$3([a, s], \"conv3d\");\n  var u = computeConv3DInfo$1(a.shape, s.shape, o, l, i),\n      {\n    filterDepth: c,\n    filterHeight: p,\n    filterWidth: d,\n    dilationDepth: h,\n    dilationHeight: m,\n    dilationWidth: f,\n    padInfo: g\n  } = u,\n      $ = g.front,\n      y = g.left,\n      b = g.top,\n      x = new TensorBuffer$1(u.outShape, a.dtype),\n      v = n.data.get(a.dataId).values,\n      I = n.data.get(s.dataId).values,\n      C = x.values,\n      S = computeStrides$1(a.shape),\n      k = computeStrides$1(s.shape);\n\n  for (var _e308 = 0; _e308 < u.batchSize; ++_e308) {\n    var _t231 = _e308 * S[0],\n        _n140 = _e308 * x.strides[0];\n\n    for (var _e309 = 0; _e309 < u.outDepth; ++_e309) {\n      var _r100 = _n140 + _e309 * x.strides[1],\n          _a71 = _e309 * u.strideDepth - $;\n\n      for (var _e310 = 0; _e310 < c; ++_e310) {\n        var _n141 = _a71 + _e310 * h;\n\n        if (_n141 < 0 || _n141 >= u.inDepth) continue;\n\n        var _s45 = _e310 * k[0],\n            _o28 = _t231 + _n141 * S[1];\n\n        for (var _e311 = 0; _e311 < u.outHeight; ++_e311) {\n          var _t232 = _r100 + _e311 * x.strides[2],\n              _n142 = _e311 * u.strideHeight - b;\n\n          for (var _e312 = 0; _e312 < p; ++_e312) {\n            var _r101 = _n142 + _e312 * m;\n\n            if (_r101 < 0 || _r101 >= u.inHeight) continue;\n\n            var _a72 = _s45 + _e312 * k[1],\n                _i16 = _o28 + _r101 * S[2];\n\n            for (var _e313 = 0; _e313 < u.outWidth; ++_e313) {\n              var _n143 = _t232 + _e313 * u.outChannels,\n                  _r102 = _e313 * u.strideWidth - y;\n\n              for (var _e314 = 0; _e314 < d; ++_e314) {\n                var _t233 = _r102 + _e314 * f;\n\n                if (_t233 < 0 || _t233 >= u.inWidth) continue;\n\n                var _s46 = _i16 + _t233 * u.inChannels;\n\n                var _o29 = _a72 + _e314 * k[2];\n\n                for (var _e315 = 0; _e315 < u.inChannels; ++_e315) {\n                  var _t234 = v[_s46 + _e315];\n\n                  for (var _e316 = 0; _e316 < u.outChannels; ++_e316) {\n                    C[_n143 + _e316] += _t234 * I[_o29 + _e316];\n                  }\n\n                  _o29 += u.outChannels;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(x.shape, x.dtype, x.values);\n}\n\nvar conv3DConfig$3 = {\n  kernelName: Conv3D$3,\n  backendName: \"cpu\",\n  kernelFunc: conv3D$3\n};\n\nfunction conv3DBackpropFilterV2$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    filterShape: l\n  } = r;\n  assertNotComplex$3([a, s], \"conv3dBackpropFilterV2\");\n  var u = computeStrides$1(a.shape),\n      c = computeStrides$1(s.shape),\n      p = computeConv3DInfo$1(a.shape, l, o, 1, i),\n      d = p.strideDepth,\n      h = p.strideHeight,\n      m = p.strideWidth,\n      f = p.filterDepth,\n      g = p.filterHeight,\n      $ = p.filterWidth,\n      y = new TensorBuffer$1(p.filterShape, \"float32\"),\n      b = y.values,\n      [x, v, I, C] = y.strides,\n      S = n.data.get(s.dataId).values,\n      [k, T, N, w] = c,\n      E = n.data.get(a.dataId).values,\n      [A, D, R, _] = u,\n      F = p.padInfo.front,\n      P = p.padInfo.left,\n      O = p.padInfo.top;\n\n  for (var _e317 = 0; _e317 < f; ++_e317) {\n    var _t235 = Math.max(0, Math.ceil((F - _e317) / d)),\n        _n144 = Math.min(p.outDepth, (p.inDepth + F - _e317) / d),\n        _r103 = _e317 * x;\n\n    for (var _a73 = 0; _a73 < g; ++_a73) {\n      var _s47 = Math.max(0, Math.ceil((O - _a73) / h)),\n          _o30 = Math.min(p.outHeight, (p.inHeight + O - _a73) / h),\n          _i17 = _a73 * v + _r103;\n\n      for (var _r104 = 0; _r104 < $; ++_r104) {\n        var _l12 = Math.max(0, Math.ceil((P - _r104) / m)),\n            _u10 = Math.min(p.outWidth, (p.inWidth + P - _r104) / m),\n            _c7 = _r104 * I + _i17;\n\n        for (var _i18 = 0; _i18 < p.inChannels; ++_i18) {\n          var _f3 = _i18 * C + _c7;\n\n          for (var _c8 = 0; _c8 < p.outChannels; ++_c8) {\n            var _g2 = 0;\n\n            for (var _f4 = 0; _f4 < p.batchSize; ++_f4) {\n              var _p8 = _f4 * A,\n                  _$3 = _f4 * k;\n\n              for (var _f5 = _t235; _f5 < _n144; ++_f5) {\n                var _t236 = (_e317 + _f5 * d - F) * D + _p8,\n                    _n145 = _f5 * T + _$3;\n\n                for (var _e318 = _s47; _e318 < _o30; ++_e318) {\n                  var _s48 = (_a73 + _e318 * h - O) * R + _t236,\n                      _o31 = _e318 * N + _n145;\n\n                  for (var _e319 = _l12; _e319 < _u10; ++_e319) {\n                    _g2 += E[(_r104 + _e319 * m - P) * _ + _s48 + _i18] * S[_e319 * w + _o31 + _c8];\n                  }\n                }\n              }\n            }\n\n            b[_f3 + _c8] = _g2;\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(y.shape, y.dtype, y.values);\n}\n\nvar conv3DBackpropFilterV2Config$3 = {\n  kernelName: Conv3DBackpropFilterV2$1,\n  backendName: \"cpu\",\n  kernelFunc: conv3DBackpropFilterV2$3\n};\n\nfunction conv3DBackpropInputV2$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    pad: o,\n    strides: i,\n    inputShape: l\n  } = r;\n  assertNotComplex$3([a], \"conv3dBackpropInputV2\");\n  var u = computeStrides$1(a.shape),\n      c = computeStrides$1(s.shape),\n      p = computeConv3DInfo$1(l, s.shape, i, 1, o),\n      d = new TensorBuffer$1(p.inShape, \"float32\"),\n      h = d.values,\n      [m, f, g, $] = d.strides,\n      y = n.data.get(a.dataId).values,\n      [b, x, v, I] = u,\n      C = n.data.get(s.dataId).values,\n      [S, k, T, N] = c,\n      {\n    batchSize: w,\n    filterDepth: E,\n    filterHeight: A,\n    filterWidth: D,\n    inChannels: R,\n    inDepth: _,\n    inHeight: F,\n    inWidth: P,\n    outChannels: O,\n    outDepth: M,\n    outHeight: L,\n    outWidth: z,\n    strideDepth: B,\n    strideHeight: V,\n    strideWidth: G\n  } = p,\n      U = E - 1 - p.padInfo.front,\n      W = A - 1 - p.padInfo.top,\n      q = D - 1 - p.padInfo.left;\n\n  for (var _e320 = 0; _e320 < w; ++_e320) {\n    for (var _t237 = 0; _t237 < R; ++_t237) {\n      for (var _n146 = 0; _n146 < _; ++_n146) {\n        var _r105 = _n146 - U,\n            _a74 = Math.max(0, Math.ceil(_r105 / B)),\n            _s49 = Math.min(M, (E + _r105) / B);\n\n        for (var _o32 = 0; _o32 < F; ++_o32) {\n          var _i19 = _o32 - W,\n              _l13 = Math.max(0, Math.ceil(_i19 / V)),\n              _u11 = Math.min(L, (A + _i19) / V);\n\n          for (var _c9 = 0; _c9 < P; ++_c9) {\n            var _p9 = _c9 - q,\n                _d6 = Math.max(0, Math.ceil(_p9 / G)),\n                _w = Math.min(z, (D + _p9) / G);\n\n            var _R = 0;\n\n            for (var _n147 = _a74; _n147 < _s49; ++_n147) {\n              var _a75 = _n147 * B - _r105;\n\n              for (var _r106 = _l13; _r106 < _u11; ++_r106) {\n                var _s50 = _r106 * V - _i19;\n\n                for (var _o33 = _d6; _o33 < _w; ++_o33) {\n                  var _i20 = b * _e320 + x * _n147 + v * _r106 + I * _o33,\n                      _l14 = S * (E - 1 - _a75) + k * (A - 1 - _s50) + T * (D - 1 - (_o33 * G - _p9)) + N * _t237;\n\n                  for (var _e321 = 0; _e321 < O; ++_e321) {\n                    _R += y[_i20 + _e321] * C[_l14 + _e321];\n                  }\n                }\n              }\n            }\n\n            h[m * _e320 + f * _n146 + g * _o32 + $ * _c9 + _t237] = _R;\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(d.shape, d.dtype, d.values);\n}\n\nvar conv3DBackpropInputV2Config$1 = {\n  kernelName: Conv3DBackpropInputV2$1,\n  backendName: \"cpu\",\n  kernelFunc: conv3DBackpropInputV2$1\n},\n    cos$4 = unaryKernelFunc$3(Cos$1, e => Math.cos(e)),\n    cosConfig$3 = {\n  kernelName: Cos$1,\n  backendName: \"cpu\",\n  kernelFunc: cos$4\n},\n    cosh$4 = unaryKernelFunc$3(Cosh$1, e => Math.cosh(e)),\n    coshConfig$3 = {\n  kernelName: Cosh$1,\n  backendName: \"cpu\",\n  kernelFunc: cosh$4\n};\n\nfunction cropAndResize$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    image: a,\n    boxes: s,\n    boxInd: o\n  } = t,\n      {\n    cropSize: i,\n    method: l,\n    extrapolationValue: u\n  } = r,\n      [c, p, d, h] = a.shape,\n      m = s.shape[0],\n      [f, g] = i,\n      $ = buffer$1([m, f, g, h], \"float32\"),\n      y = n.data.get(s.dataId).values,\n      b = n.data.get(o.dataId).values,\n      x = n.data.get(a.dataId).values,\n      v = computeStrides$1(a.shape),\n      I = computeStrides$1($.shape);\n\n  for (var _e322 = 0; _e322 < m; _e322++) {\n    var _t238 = 4 * _e322,\n        _n148 = y[_t238],\n        _r107 = y[_t238 + 1],\n        _a76 = y[_t238 + 2],\n        _s51 = y[_t238 + 3],\n        _o34 = b[_e322];\n\n    if (_o34 >= c) continue;\n\n    var _i21 = f > 1 ? (_a76 - _n148) * (p - 1) / (f - 1) : 0,\n        _m = g > 1 ? (_s51 - _r107) * (d - 1) / (g - 1) : 0;\n\n    for (var _t239 = 0; _t239 < f; _t239++) {\n      var _c10 = f > 1 ? _n148 * (p - 1) + _t239 * _i21 : .5 * (_n148 + _a76) * (p - 1);\n\n      if (_c10 < 0 || _c10 > p - 1) for (var _n149 = 0; _n149 < g; _n149++) {\n        for (var _r108 = 0; _r108 < h; _r108++) {\n          $.values[_r108 + _n149 * I[2] + _t239 * I[1] + _e322 * I[0]] = u;\n        }\n      } else if (\"bilinear\" === l) {\n        var _n150 = Math.floor(_c10),\n            _a77 = Math.ceil(_c10),\n            _i22 = _c10 - _n150;\n\n        for (var _l15 = 0; _l15 < g; _l15++) {\n          var _c11 = g > 1 ? _r107 * (d - 1) + _l15 * _m : .5 * (_r107 + _s51) * (d - 1);\n\n          if (_c11 < 0 || _c11 > d - 1) {\n            for (var _n151 = 0; _n151 < h; _n151++) {\n              $.values[_n151 + _l15 * I[2] + _t239 * I[1] + _e322 * I[0]] = u;\n            }\n\n            continue;\n          }\n\n          var _p10 = Math.floor(_c11),\n              _f6 = Math.ceil(_c11),\n              _y2 = _c11 - _p10;\n\n          for (var _r109 = 0; _r109 < h; _r109++) {\n            var _s52 = _r109 + _p10 * v[2] + _n150 * v[1] + _o34 * v[0];\n\n            var _u12 = x[_s52];\n            _s52 = _r109 + _f6 * v[2] + _n150 * v[1] + _o34 * v[0];\n            var _c12 = x[_s52];\n            _s52 = _r109 + _p10 * v[2] + _a77 * v[1] + _o34 * v[0];\n            var _d7 = x[_s52];\n            _s52 = _r109 + _f6 * v[2] + _a77 * v[1] + _o34 * v[0];\n\n            var _h6 = x[_s52],\n                _m2 = _u12 + (_c12 - _u12) * _y2;\n\n            _s52 = _r109 + _l15 * I[2] + _t239 * I[1] + _e322 * I[0], $.values[_s52] = _m2 + (_d7 + (_h6 - _d7) * _y2 - _m2) * _i22;\n          }\n        }\n      } else for (var _n152 = 0; _n152 < g; ++_n152) {\n        var _a78 = g > 1 ? _r107 * (d - 1) + _n152 * _m : .5 * (_r107 + _s51) * (d - 1);\n\n        if (_a78 < 0 || _a78 > d - 1) {\n          for (var _r110 = 0; _r110 < h; _r110++) {\n            $.values[_r110 + _n152 * I[2] + _t239 * I[1] + _e322 * I[0]] = u;\n          }\n\n          continue;\n        }\n\n        var _i23 = Math.round(_a78),\n            _l16 = Math.round(_c10);\n\n        for (var _r111 = 0; _r111 < h; _r111++) {\n          $.values[_r111 + _n152 * I[2] + _t239 * I[1] + _e322 * I[0]] = x[_r111 + _i23 * v[2] + _l16 * v[1] + _o34 * v[0]];\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo($.shape, $.dtype, $.values);\n}\n\nvar cropAndResizeConfig$3 = {\n  kernelName: CropAndResize$1,\n  backendName: \"cpu\",\n  kernelFunc: cropAndResize$4\n};\n\nfunction cumsum$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    exclusive: o,\n    reverse: i\n  } = r;\n  assertNotComplex$3(a, \"cumsum\");\n  var l = getAxesPermutation$1([s], a.shape.length);\n  var u = a;\n  null != l && (u = transpose$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: l\n    }\n  }));\n  var c = getInnerMostAxes$1(1, a.shape.length)[0];\n  if (c !== u.shape.length - 1) throw new Error(\"backend.cumsum in CPU expects an inner-most axis=\".concat(u.shape.length - 1, \" but got axis=\").concat(c));\n  var p = upcastType$1(u.dtype, \"int32\"),\n      d = makeZerosTypedArray$1(sizeFromShape$1(u.shape), p),\n      h = n.data.get(u.dataId).values,\n      m = u.shape[u.shape.length - 1],\n      f = i ? (e, t) => e + m - t - 1 : (e, t) => e + t;\n\n  for (var _e323 = 0; _e323 < h.length; _e323 += m) {\n    for (var _t240 = 0; _t240 < m; _t240++) {\n      var _n153 = f(_e323, _t240);\n\n      if (0 === _t240) d[_n153] = o ? 0 : h[_n153];else {\n        var _r112 = f(_e323, _t240 - 1);\n\n        d[_n153] = o ? h[_r112] + d[_r112] : h[_n153] + d[_r112];\n      }\n    }\n  }\n\n  var g = n.makeTensorInfo(u.shape, p, d);\n\n  if (null != l) {\n    var _e324 = transpose$4({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        perm: getUndoAxesPermutation$1(l)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), n.disposeIntermediateTensorInfo(u), _e324;\n  }\n\n  return g;\n}\n\nvar cumsumConfig$3 = {\n  kernelName: Cumsum$1,\n  backendName: \"cpu\",\n  kernelFunc: cumsum$4\n};\n\nfunction denseBincount$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o,\n    binaryOutput: i\n  } = r;\n\n  if (1 === a.shape.length) {\n    var _e325 = bincountImpl$1(n.data.get(a.dataId).values, n.data.get(s.dataId).values, s.dtype, s.shape, o);\n\n    return n.makeTensorInfo([o], s.dtype, _e325);\n  }\n\n  if (2 === a.shape.length) {\n    var _e326 = bincountReduceImpl$1(n.bufferSync(a), n.bufferSync(s), o, i);\n\n    return n.makeTensorInfo(_e326.shape, s.dtype, _e326.values);\n  }\n\n  throw new Error(\"Error in denseBincount: input must be at most rank 2, but got rank\".concat(a.shape.length, \".\"));\n}\n\nvar denseBincountConfig$3 = {\n  kernelName: DenseBincount$1,\n  backendName: \"cpu\",\n  kernelFunc: denseBincount$4\n};\n\nfunction depthToSpace$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockSize: s,\n    dataFormat: o\n  } = r;\n  assert$6(\"NHWC\" === o, () => \"Only NHWC dataFormat supported on CPU for depthToSpace. Got \".concat(o)), assert$6(s > 1, () => \"blockSize should be > 1 for depthToSpace, but was: \".concat(s));\n  var i = a.shape[0],\n      l = a.shape[1],\n      u = a.shape[2],\n      c = a.shape[3],\n      p = l * s,\n      d = u * s,\n      h = c / (s * s),\n      m = n.data.get(a.dataId).values,\n      f = new Float32Array(i * p * d * h);\n  var g = 0;\n\n  for (var _e327 = 0; _e327 < i; ++_e327) {\n    for (var _t241 = 0; _t241 < p; ++_t241) {\n      var _n154 = Math.floor(_t241 / s),\n          _r113 = _t241 % s;\n\n      for (var _t242 = 0; _t242 < d; ++_t242) {\n        var _a79 = Math.floor(_t242 / s),\n            _o35 = (_r113 * s + _t242 % s) * h;\n\n        for (var _t243 = 0; _t243 < h; ++_t243) {\n          f[g++] = m[_t243 + _o35 + c * (_a79 + u * (_n154 + l * _e327))];\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([i, p, d, h], a.dtype, f);\n}\n\nvar depthToSpaceConfig$3 = {\n  kernelName: DepthToSpace$1,\n  backendName: \"cpu\",\n  kernelFunc: depthToSpace$4\n};\n\nfunction depthwiseConv2dNative$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l,\n    dimRoundingMode: u\n  } = r;\n  assertNotComplex$3([a, s], \"depthwiseConv2DNative\");\n  var c = computeStrides$1(a.shape),\n      p = computeStrides$1(s.shape);\n  var d = l;\n  null == d && (d = [1, 1]), assert$6(eitherStridesOrDilationsAreOne$1(o, d), () => \"Error in depthwiseConv2d: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '\").concat(d, \"'\"));\n  var h = computeConv2DInfo$1(a.shape, s.shape, o, d, i, u, !0),\n      {\n    filterHeight: m,\n    filterWidth: f,\n    dilationHeight: g,\n    dilationWidth: $,\n    padInfo: y\n  } = h,\n      b = y.left,\n      x = y.top,\n      v = h.outChannels / h.inChannels,\n      I = new TensorBuffer$1(h.outShape, a.dtype),\n      C = n.data.get(a.dataId).values,\n      S = n.data.get(s.dataId).values,\n      k = I.values;\n\n  for (var _e328 = 0; _e328 < h.batchSize; ++_e328) {\n    var _t244 = _e328 * c[0],\n        _n155 = _e328 * I.strides[0];\n\n    for (var _e329 = 0; _e329 < h.outHeight; ++_e329) {\n      var _r114 = _n155 + _e329 * I.strides[1],\n          _a80 = _e329 * h.strideHeight - x;\n\n      for (var _e330 = 0; _e330 < m; ++_e330) {\n        var _n156 = _a80 + _e330 * g;\n\n        if (_n156 < 0 || _n156 >= h.inHeight) continue;\n\n        var _s53 = _e330 * p[0],\n            _o36 = _t244 + _n156 * c[1];\n\n        for (var _e331 = 0; _e331 < h.outWidth; ++_e331) {\n          var _t245 = _r114 + _e331 * I.strides[2],\n              _n157 = _e331 * h.strideWidth - b;\n\n          for (var _e332 = 0; _e332 < f; ++_e332) {\n            var _r115 = _n157 + _e332 * $;\n\n            if (_r115 < 0 || _r115 >= h.inWidth) continue;\n\n            var _a81 = _o36 + _r115 * h.inChannels;\n\n            var _i24 = _t245,\n                _l17 = _s53 + _e332 * p[1];\n\n            for (var _e333 = 0; _e333 < h.inChannels; ++_e333) {\n              var _t246 = C[_a81 + _e333];\n\n              for (var _e334 = 0; _e334 < v; ++_e334) {\n                k[_i24 + _e334] += _t246 * S[_l17 + _e334];\n              }\n\n              _i24 += v, _l17 += v;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(I.shape, I.dtype, I.values);\n}\n\nvar depthwiseConv2dNativeConfig$3 = {\n  kernelName: DepthwiseConv2dNative$1,\n  backendName: \"cpu\",\n  kernelFunc: depthwiseConv2dNative$3\n};\n\nfunction depthwiseConv2dNativeBackpropFilter$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r;\n  assertNotComplex$3([a, s], \"depthwiseConv2dNativeBackpropFilter\");\n  var p = computeConv2DInfo$1(a.shape, c, o, i, l, u, !0),\n      {\n    strideHeight: d,\n    strideWidth: h,\n    filterHeight: m,\n    filterWidth: f\n  } = p,\n      g = new TensorBuffer$1(p.filterShape, \"float32\"),\n      $ = p.padInfo.left,\n      y = p.padInfo.top,\n      b = p.outChannels / p.inChannels,\n      x = n.data.get(a.dataId).values,\n      v = new TensorBuffer$1(a.shape, a.dtype, x),\n      I = n.data.get(s.dataId).values,\n      C = new TensorBuffer$1(s.shape, s.dtype, I);\n\n  for (var _e335 = 0; _e335 < m; ++_e335) {\n    var _t247 = Math.max(0, Math.ceil((y - _e335) / d)),\n        _n158 = Math.min(p.outHeight, (p.inHeight + y - _e335) / d);\n\n    for (var _r116 = 0; _r116 < f; ++_r116) {\n      var _a82 = Math.max(0, Math.ceil(($ - _r116) / h)),\n          _s54 = Math.min(p.outWidth, (p.inWidth + $ - _r116) / h);\n\n      for (var _o37 = 0; _o37 < p.outChannels; ++_o37) {\n        var _i25 = Math.trunc(_o37 / b),\n            _l18 = _o37 % b;\n\n        var _u13 = 0;\n\n        for (var _l19 = 0; _l19 < p.batchSize; ++_l19) {\n          for (var _c13 = _t247; _c13 < _n158; ++_c13) {\n            var _t248 = _e335 + _c13 * d - y;\n\n            for (var _e336 = _a82; _e336 < _s54; ++_e336) {\n              _u13 += v.get(_l19, _t248, _r116 + _e336 * h - $, _i25) * C.get(_l19, _c13, _e336, _o37);\n            }\n          }\n        }\n\n        g.set(_u13, _e335, _r116, _i25, _l18);\n      }\n    }\n  }\n\n  return n.makeTensorInfo(g.shape, g.dtype, g.values);\n}\n\nvar depthwiseConv2dNativeBackpropFilterConfig$3 = {\n  kernelName: DepthwiseConv2dNativeBackpropFilter$1,\n  backendName: \"cpu\",\n  kernelFunc: depthwiseConv2dNativeBackpropFilter$4\n};\n\nfunction depthwiseConv2dNativeBackpropInput$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    inputShape: c\n  } = r;\n  assertNotComplex$3([a, s], \"depthwiseConv2DNativeBackpropInput\");\n  var p = computeStrides$1(a.shape),\n      d = computeStrides$1(s.shape),\n      h = computeConv2DInfo$1(c, s.shape, o, i, l, u, !0),\n      m = new TensorBuffer$1(h.inShape, \"float32\"),\n      f = m.values,\n      [g, $, y] = m.strides,\n      b = n.data.get(a.dataId).values,\n      [x, v, I] = p,\n      C = n.data.get(s.dataId).values,\n      [S, k, T] = d,\n      {\n    batchSize: N,\n    filterHeight: w,\n    filterWidth: E,\n    inChannels: A,\n    inHeight: D,\n    inWidth: R,\n    outChannels: _,\n    outHeight: F,\n    outWidth: P,\n    strideHeight: O,\n    strideWidth: M\n  } = h,\n      L = w - 1 - h.padInfo.top,\n      z = E - 1 - h.padInfo.left,\n      B = _ / A;\n\n  for (var _e337 = 0; _e337 < N; ++_e337) {\n    for (var _t249 = 0; _t249 < A; ++_t249) {\n      for (var _n159 = 0; _n159 < D; ++_n159) {\n        var _r117 = _n159 - L,\n            _a83 = Math.max(0, Math.ceil(_r117 / O)),\n            _s55 = Math.min(F, (w + _r117) / O);\n\n        for (var _o38 = 0; _o38 < R; ++_o38) {\n          var _i26 = _o38 - z,\n              _l20 = Math.max(0, Math.ceil(_i26 / M)),\n              _u14 = Math.min(P, (E + _i26) / M);\n\n          var _c14 = 0;\n\n          for (var _n160 = _a83; _n160 < _s55; ++_n160) {\n            var _a84 = _n160 * O - _r117;\n\n            for (var _r118 = _l20; _r118 < _u14; ++_r118) {\n              var _s56 = x * _e337 + v * _n160 + I * _r118,\n                  _o39 = S * (w - 1 - _a84) + k * (E - 1 - (_r118 * M - _i26)) + T * _t249;\n\n              for (var _e338 = 0; _e338 < B; ++_e338) {\n                _c14 += b[_s56 + (_t249 * B + _e338)] * C[_o39 + _e338];\n              }\n            }\n          }\n\n          f[g * _e337 + $ * _n159 + y * _o38 + _t249] = _c14;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(m.shape, m.dtype, m.values);\n}\n\nvar depthwiseConv2dNativeBackpropInputConfig$3 = {\n  kernelName: DepthwiseConv2dNativeBackpropInput$1,\n  backendName: \"cpu\",\n  kernelFunc: depthwiseConv2dNativeBackpropInput$4\n};\n\nfunction diag$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t,\n      a = sizeFromShape$1(r.shape),\n      s = n.data.get(r.dataId).values,\n      o = buffer$1([a, a], r.dtype),\n      i = o.values;\n\n  for (var _e339 = 0; _e339 < s.length; _e339++) {\n    i[_e339 * a + _e339] = s[_e339];\n  }\n\n  var l = [...r.shape, ...r.shape];\n  return n.makeTensorInfo(l, o.dtype, o.values);\n}\n\nvar diagConfig$3 = {\n  kernelName: Diag$1,\n  backendName: \"cpu\",\n  kernelFunc: diag$4\n},\n    dilation2dConfig$1 = {\n  kernelName: Dilation2D$1,\n  backendName: \"cpu\",\n  kernelFunc: _ref7 => {\n    var {\n      inputs: e,\n      backend: t,\n      attrs: n\n    } = _ref7;\n    var {\n      x: r,\n      filter: a\n    } = e,\n        {\n      strides: s,\n      pad: o,\n      dilations: i\n    } = n,\n        l = t,\n        u = l.data.get(r.dataId).values,\n        c = r.shape.length,\n        p = l.data.get(a.dataId).values,\n        d = a.shape.length,\n        {\n      batchSize: h,\n      inHeight: m,\n      inWidth: f,\n      inChannels: g,\n      outHeight: $,\n      outWidth: y,\n      padInfo: b,\n      strideHeight: x,\n      strideWidth: v,\n      filterHeight: I,\n      filterWidth: C,\n      dilationHeight: S,\n      dilationWidth: k,\n      outShape: T\n    } = computeDilation2DInfo$1(r.shape, a.shape, s, o, \"NHWC\", i),\n        N = sizeFromShape$1(T),\n        w = T.length,\n        E = getArrayFromDType$1(r.dtype, N);\n\n    for (var _e340 = 0; _e340 < h; ++_e340) {\n      for (var _t250 = 0; _t250 < $; ++_t250) {\n        var _n161 = _t250 * x - b.top;\n\n        for (var _s57 = 0; _s57 < y; ++_s57) {\n          var _o40 = _s57 * v - b.left;\n\n          for (var _i27 = 0; _i27 < g; ++_i27) {\n            var _l21 = Number.MIN_SAFE_INTEGER;\n\n            for (var _t251 = 0; _t251 < I; ++_t251) {\n              var _s58 = _n161 + _t251 * S;\n\n              if (_s58 >= 0 && _s58 < m) for (var _n162 = 0; _n162 < C; ++_n162) {\n                var _h7 = _o40 + _n162 * k;\n\n                if (_h7 >= 0 && _h7 < f) {\n                  var _o41 = locToIndex$1([_e340, _s58, _h7, _i27], c, computeStrides$1(r.shape)),\n                      _m3 = locToIndex$1([_t251, _n162, _i27], d, computeStrides$1(a.shape)),\n                      _f7 = u[_o41] + p[_m3];\n\n                  _f7 > _l21 && (_l21 = _f7);\n                }\n              }\n            }\n\n            E[locToIndex$1([_e340, _t250, _s57, _i27], w, computeStrides$1(T))] = _l21;\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: l.write(toTypedArray$1(E, r.dtype), T, r.dtype),\n      shape: T,\n      dtype: r.dtype\n    };\n  }\n},\n    dilation2dBackpropFilterConfig$1 = {\n  kernelName: Dilation2DBackpropFilter$1,\n  backendName: \"cpu\",\n  kernelFunc: _ref8 => {\n    var {\n      inputs: e,\n      backend: t,\n      attrs: n\n    } = _ref8;\n    var {\n      x: r,\n      filter: a,\n      dy: s\n    } = e,\n        {\n      strides: o,\n      pad: i,\n      dilations: l\n    } = n,\n        u = t,\n        c = toNestedArray$1(r.shape, u.data.get(r.dataId).values),\n        p = toNestedArray$1(a.shape, u.data.get(a.dataId).values),\n        {\n      batchSize: d,\n      inHeight: h,\n      inWidth: m,\n      inChannels: f,\n      outHeight: g,\n      outWidth: $,\n      padInfo: y,\n      strideHeight: b,\n      strideWidth: x,\n      filterHeight: v,\n      filterWidth: I,\n      dilationHeight: C,\n      dilationWidth: S,\n      outShape: k\n    } = computeDilation2DInfo$1(r.shape, a.shape, o, i, \"NHWC\", l);\n    assert$6(s.rank === k.length, () => \"Error in \".concat(Dilation2DBackpropFilter$1, \", dy must have the same rank as output \").concat(k.length, \", but got \").concat(s.rank));\n    var T = toNestedArray$1(k, u.data.get(s.dataId).values),\n        N = makeZerosNestedTypedArray$1(a.shape, a.dtype);\n\n    for (var _e341 = 0; _e341 < d; ++_e341) {\n      for (var _t252 = 0; _t252 < g; ++_t252) {\n        var _n163 = _t252 * b - y.top;\n\n        for (var _r119 = 0; _r119 < $; ++_r119) {\n          var _a85 = _r119 * x - y.left;\n\n          for (var _s59 = 0; _s59 < f; ++_s59) {\n            var _o42 = Number.MIN_SAFE_INTEGER,\n                _i28 = 0,\n                _l22 = 0;\n\n            for (var _t253 = 0; _t253 < v; ++_t253) {\n              var _r120 = _n163 + _t253 * C;\n\n              if (_r120 >= 0 && _r120 < h) for (var _n164 = 0; _n164 < I; ++_n164) {\n                var _u15 = _a85 + _n164 * S;\n\n                if (_u15 >= 0 && _u15 < m) {\n                  var _a86 = c[_e341][_r120][_u15][_s59] + p[_t253][_n164][_s59];\n\n                  _a86 > _o42 && (_o42 = _a86, _i28 = _t253, _l22 = _n164);\n                }\n              }\n            }\n\n            N[_i28][_l22][_s59] += T[_e341][_t252][_r119][_s59];\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: u.write(toTypedArray$1(N, r.dtype), a.shape, a.dtype),\n      shape: a.shape,\n      dtype: a.dtype\n    };\n  }\n},\n    dilation2dBackpropInputConfig$1 = {\n  kernelName: Dilation2DBackpropInput$1,\n  backendName: \"cpu\",\n  kernelFunc: _ref9 => {\n    var {\n      inputs: e,\n      backend: t,\n      attrs: n\n    } = _ref9;\n    var {\n      x: r,\n      filter: a,\n      dy: s\n    } = e,\n        {\n      strides: o,\n      pad: i,\n      dilations: l\n    } = n,\n        u = t,\n        c = toNestedArray$1(r.shape, u.data.get(r.dataId).values),\n        p = toNestedArray$1(a.shape, u.data.get(a.dataId).values),\n        {\n      batchSize: d,\n      inHeight: h,\n      inWidth: m,\n      inChannels: f,\n      outHeight: g,\n      outWidth: $,\n      padInfo: y,\n      strideHeight: b,\n      strideWidth: x,\n      filterHeight: v,\n      filterWidth: I,\n      dilationHeight: C,\n      dilationWidth: S,\n      outShape: k\n    } = computeDilation2DInfo$1(r.shape, a.shape, o, i, \"NHWC\", l);\n    assert$6(s.rank === k.length, () => \"Error in \".concat(Dilation2DBackpropInput$1, \", dy must have the same rank as output \").concat(k.length, \", but got \").concat(s.rank));\n    var T = toNestedArray$1(k, u.data.get(s.dataId).values),\n        N = makeZerosNestedTypedArray$1(r.shape, r.dtype);\n\n    for (var _e342 = 0; _e342 < d; ++_e342) {\n      for (var _t254 = 0; _t254 < g; ++_t254) {\n        var _n165 = _t254 * b - y.top;\n\n        for (var _r121 = 0; _r121 < $; ++_r121) {\n          var _a87 = _r121 * x - y.left;\n\n          for (var _s60 = 0; _s60 < f; ++_s60) {\n            var _o43 = Number.MIN_SAFE_INTEGER,\n                _i29 = _n165 < 0 ? 0 : _n165,\n                _l23 = _a87 < 0 ? 0 : _a87;\n\n            for (var _t255 = 0; _t255 < v; ++_t255) {\n              var _r122 = _n165 + _t255 * C;\n\n              if (_r122 >= 0 && _r122 < h) for (var _n166 = 0; _n166 < I; ++_n166) {\n                var _u16 = _a87 + _n166 * S;\n\n                if (_u16 >= 0 && _u16 < m) {\n                  var _a88 = c[_e342][_r122][_u16][_s60] + p[_t255][_n166][_s60];\n\n                  _a88 > _o43 && (_o43 = _a88, _i29 = _r122, _l23 = _u16);\n                }\n              }\n            }\n\n            N[_e342][_i29][_l23][_s60] += T[_e342][_t254][_r121][_s60];\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: u.write(toTypedArray$1(N, r.dtype), r.shape, r.dtype),\n      shape: r.shape,\n      dtype: r.dtype\n    };\n  }\n};\n\nfunction sum$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  var i;\n  assertNotComplex$3(a, \"sum\"), i = \"bool\" === a.dtype ? cast$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      dtype: \"int32\"\n    }\n  }) : identity$4({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var l = i.shape.length,\n      u = parseAxisParam$1(s, i.shape),\n      c = getAxesPermutation$1(u, l);\n  var p = u,\n      d = i;\n  null != c && (d = transpose$4({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), p = getInnerMostAxes$1(p.length, l)), assertAxesAreInnerMostDims$1(\"sum\", p, d.shape.length);\n  var [h, m] = computeOutAndReduceShapes$1(d.shape, p);\n  var f = zeros$3(n, h, upcastType$1(d.dtype, \"int32\"));\n  var g = sizeFromShape$1(m),\n      $ = n.data.get(f.dataId).values,\n      y = n.data.get(d.dataId).values;\n\n  for (var _e343 = 0; _e343 < $.length; ++_e343) {\n    var _t256 = _e343 * g;\n\n    var _n167 = 0;\n\n    for (var _e344 = 0; _e344 < g; ++_e344) {\n      _n167 += y[_t256 + _e344];\n    }\n\n    $[_e343] = _n167;\n  }\n\n  if (o) {\n    var _e345 = f;\n    f = reshape$5({\n      inputs: {\n        x: f\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim$1(f.shape, u)\n      }\n    }), n.disposeIntermediateTensorInfo(_e345);\n  }\n\n  return n.disposeIntermediateTensorInfo(i), null != c && n.disposeIntermediateTensorInfo(d), f;\n}\n\nvar sumConfig$3 = {\n  kernelName: Sum$1,\n  backendName: \"cpu\",\n  kernelFunc: sum$5\n};\n\nfunction einsum$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    equation: a\n  } = r,\n      s = t,\n      {\n    allDims: o,\n    summedDims: i,\n    idDims: l\n  } = decodeEinsumEquation$1(a, s.length);\n  checkEinsumDimSizes$1(o.length, l, s);\n  var {\n    path: u,\n    steps: c\n  } = getEinsumComputePath$1(i, l),\n      p = c.length;\n  var d = null,\n      h = o.length;\n  var m = [];\n\n  for (var _e346 = 0; _e346 < p; ++_e346) {\n    for (var _t257 of c[_e346]) {\n      var {\n        permutationIndices: _e347,\n        expandDims: _r123\n      } = getEinsumPermutation$1(h, l[_t257]);\n\n      var _a89 = void 0;\n\n      isIdentityPermutation$1(_e347) ? _a89 = s[_t257] : (_a89 = transpose$4({\n        inputs: {\n          x: s[_t257]\n        },\n        backend: n,\n        attrs: {\n          perm: _e347\n        }\n      }), m.push(_a89));\n\n      var _o44 = _a89.shape.slice();\n\n      for (var _e348 = 0; _e348 < _r123.length; ++_e348) {\n        _o44.splice(_r123[_e348], 0, 1);\n      }\n\n      arraysEqual$1(_a89.shape, _o44) || (_a89 = reshape$5({\n        inputs: {\n          x: _a89\n        },\n        backend: n,\n        attrs: {\n          shape: _o44\n        }\n      }), m.push(_a89)), null === d ? d = _a89 : (d = multiply$4({\n        inputs: {\n          a: _a89,\n          b: d\n        },\n        backend: n\n      }), m.push(d));\n    }\n\n    _e346 < p - 1 && (u[_e346] >= 0 && (d = sum$5({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        axis: u[_e346] - (o.length - h),\n        keepDims: !1\n      }\n    }), m.push(d)), h--);\n  }\n\n  for (var _e349 of m) {\n    _e349 !== d && n.disposeIntermediateTensorInfo(_e349);\n  }\n\n  return d;\n}\n\nvar einsumConfig$3 = {\n  kernelName: Einsum$1,\n  backendName: \"cpu\",\n  kernelFunc: einsum$4\n};\n\nfunction eluGrad$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    dy: r,\n    y: a\n  } = t;\n  assertNotComplex$3([r, a], \"eluGrad\");\n  var s = new Float32Array(sizeFromShape$1(a.shape)),\n      o = n.data.get(a.dataId).values,\n      i = n.data.get(r.dataId).values;\n\n  for (var _e350 = 0; _e350 < o.length; ++_e350) {\n    var _t258 = o[_e350];\n    s[_e350] = _t258 >= 1 ? i[_e350] : i[_e350] * (_t258 + 1);\n  }\n\n  return n.makeTensorInfo(a.shape, \"float32\", s);\n}\n\nvar eluGradConfig$4 = {\n  kernelName: EluGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: eluGrad$3\n},\n    p$1 = ERF_P$1,\n    a1$1 = ERF_A1$1,\n    a2$1 = ERF_A2$1,\n    a3$1 = ERF_A3$1,\n    a4$1 = ERF_A4$1,\n    a5$1 = ERF_A5$1,\n    erf$4 = unaryKernelFunc$3(Erf$1, e => {\n  var t = Math.sign(e),\n      n = Math.abs(e),\n      r = 1 / (1 + p$1 * n);\n  return t * (1 - ((((a5$1 * r + a4$1) * r + a3$1) * r + a2$1) * r + a1$1) * r * Math.exp(-n * n));\n}),\n    erfConfig$3 = {\n  kernelName: Erf$1,\n  backendName: \"cpu\",\n  kernelFunc: erf$4\n};\n\nfunction expandDims$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    input: a\n  } = t,\n      {\n    dim: s\n  } = r,\n      o = a.shape.length,\n      i = a.shape.slice();\n  var l = s;\n  return s < 0 && (assert$6(-(o + 1) <= s, () => \"Axis must be in the interval [\".concat(-(o + 1), \", \").concat(o, \"]\")), l = o + s + 1), i.splice(l, 0, 1), reshape$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: i\n    }\n  });\n}\n\nvar expandDimsConfig$3 = {\n  kernelName: ExpandDims$1,\n  backendName: \"cpu\",\n  kernelFunc: expandDims$5\n},\n    realDivImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e / t),\n    div$2 = binaryKernelFunc$3(RealDiv$1, realDivImpl$1),\n    realDivConfig$3 = {\n  kernelName: RealDiv$1,\n  backendName: \"cpu\",\n  kernelFunc: div$2\n};\n\nfunction fftBatch$1(e, t, n) {\n  var r = e.shape,\n      a = r[0],\n      s = r[1],\n      o = n.data.get(e.dataId),\n      i = o.complexTensorInfos.real,\n      l = o.complexTensorInfos.imag,\n      u = [a, s],\n      c = sizeFromShape$1(u),\n      p = getTypedArrayFromDType$1(\"float32\", c),\n      d = getTypedArrayFromDType$1(\"float32\", c);\n\n  for (var _e351 = 0; _e351 < a; _e351++) {\n    var _r124 = slice$4({\n      inputs: {\n        x: i\n      },\n      backend: n,\n      attrs: {\n        begin: [_e351, 0],\n        size: [1, s]\n      }\n    }),\n        _a90 = slice$4({\n      inputs: {\n        x: l\n      },\n      backend: n,\n      attrs: {\n        begin: [_e351, 0],\n        size: [1, s]\n      }\n    }),\n        _o45 = complex$4({\n      inputs: {\n        real: _r124,\n        imag: _a90\n      },\n      backend: n\n    }),\n        {\n      real: _u17,\n      imag: _c15\n    } = fftImpl$3(_o45, t, n),\n        _h8 = mergeRealAndImagArrays$1(_u17, _c15);\n\n    for (var _t259 = 0; _t259 < s; _t259++) {\n      var _n168 = getComplexWithIndex$1(_h8, _t259);\n\n      p[_e351 * s + _t259] = _n168.real, d[_e351 * s + _t259] = _n168.imag;\n    }\n\n    n.disposeIntermediateTensorInfo(_r124), n.disposeIntermediateTensorInfo(_a90), n.disposeIntermediateTensorInfo(_o45);\n  }\n\n  var h = n.makeTensorInfo(u, \"float32\", p),\n      m = n.makeTensorInfo(u, \"float32\", d),\n      f = complex$4({\n    inputs: {\n      real: h,\n      imag: m\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), f;\n}\n\nfunction fftImpl$3(e, t, n) {\n  var r = sizeFromShape$1(e.shape),\n      a = n.data.get(e.dataId),\n      s = n.data.get(a.complexTensorInfos.real.dataId).values,\n      o = n.data.get(a.complexTensorInfos.imag.dataId).values;\n\n  if (isExponentOf2$1(r)) {\n    var _a91 = fftRadix2$1(s, o, r, t, n),\n        i = [e.shape[0], e.shape[1]];\n\n    if (t) {\n      var _e352 = n.makeTensorInfo(i, \"float32\", _a91.real),\n          _t260 = n.makeTensorInfo(i, \"float32\", _a91.imag),\n          _s61 = n.makeTensorInfo([], \"float32\", createScalarValue$1(r, \"float32\")),\n          _o46 = identity$4({\n        inputs: {\n          x: _s61\n        },\n        backend: n\n      }),\n          l = realDivConfig$3.kernelFunc({\n        inputs: {\n          a: _e352,\n          b: _s61\n        },\n        backend: n\n      }),\n          u = realDivConfig$3.kernelFunc({\n        inputs: {\n          a: _t260,\n          b: _o46\n        },\n        backend: n\n      }),\n          c = n.data.get(l.dataId).values,\n          _p11 = n.data.get(u.dataId).values;\n\n      return n.disposeIntermediateTensorInfo(_e352), n.disposeIntermediateTensorInfo(_t260), n.disposeIntermediateTensorInfo(_s61), n.disposeIntermediateTensorInfo(_o46), n.disposeIntermediateTensorInfo(l), n.disposeIntermediateTensorInfo(u), {\n        real: c,\n        imag: _p11\n      };\n    }\n\n    return _a91;\n  }\n\n  return splitRealAndImagArrays$1(fourierTransformByMatmul$1(mergeRealAndImagArrays$1(s, o), r, t));\n}\n\nfunction isExponentOf2$1(e) {\n  return 0 == (e & e - 1);\n}\n\nfunction fftRadix2$1(e, t, n, r, a) {\n  if (1 === n) return {\n    real: e,\n    imag: t\n  };\n\n  var s = mergeRealAndImagArrays$1(e, t),\n      o = n / 2,\n      i = complexWithEvenIndex$1(s),\n      l = i.real,\n      u = i.imag,\n      c = [l.length],\n      p = a.makeTensorInfo(c, \"float32\", l),\n      d = a.makeTensorInfo(c, \"float32\", u),\n      h = complex$4({\n    inputs: {\n      real: p,\n      imag: d\n    },\n    backend: a\n  }),\n      m = complexWithOddIndex$1(s),\n      f = m.real,\n      g = m.imag,\n      $ = [f.length],\n      y = a.makeTensorInfo($, \"float32\", f),\n      b = a.makeTensorInfo($, \"float32\", g),\n      x = complex$4({\n    inputs: {\n      real: y,\n      imag: b\n    },\n    backend: a\n  }),\n      v = fftRadix2$1(l, u, o, r, a),\n      I = v.real,\n      C = v.imag,\n      S = [I.length],\n      k = a.makeTensorInfo(S, \"float32\", I),\n      T = a.makeTensorInfo(S, \"float32\", C),\n      N = complex$4({\n    inputs: {\n      real: k,\n      imag: T\n    },\n    backend: a\n  }),\n      w = fftRadix2$1(f, g, o, r, a),\n      E = w.real,\n      A = w.imag,\n      D = [E.length],\n      R = a.makeTensorInfo(D, \"float32\", E),\n      _ = a.makeTensorInfo(D, \"float32\", A),\n      F = complex$4({\n    inputs: {\n      real: R,\n      imag: _\n    },\n    backend: a\n  }),\n      P = exponents$1(n, r),\n      O = [P.real.length],\n      M = a.makeTensorInfo(O, \"float32\", P.real),\n      L = a.makeTensorInfo(O, \"float32\", P.imag),\n      z = complex$4({\n    inputs: {\n      real: M,\n      imag: L\n    },\n    backend: a\n  }),\n      B = multiply$4({\n    inputs: {\n      a: z,\n      b: F\n    },\n    backend: a\n  }),\n      V = add$4({\n    inputs: {\n      a: N,\n      b: B\n    },\n    backend: a\n  }),\n      G = sub$4({\n    inputs: {\n      a: N,\n      b: B\n    },\n    backend: a\n  }),\n      U = real$4({\n    inputs: {\n      input: V\n    },\n    backend: a\n  }),\n      W = real$4({\n    inputs: {\n      input: G\n    },\n    backend: a\n  }),\n      q = imag$4({\n    inputs: {\n      input: V\n    },\n    backend: a\n  }),\n      H = imag$4({\n    inputs: {\n      input: G\n    },\n    backend: a\n  }),\n      K = concat$4({\n    inputs: [U, W],\n    backend: a,\n    attrs: {\n      axis: 0\n    }\n  }),\n      j = concat$4({\n    inputs: [q, H],\n    backend: a,\n    attrs: {\n      axis: 0\n    }\n  }),\n      X = a.data.get(K.dataId).values,\n      Y = a.data.get(j.dataId).values;\n\n  return a.disposeIntermediateTensorInfo(p), a.disposeIntermediateTensorInfo(d), a.disposeIntermediateTensorInfo(h), a.disposeIntermediateTensorInfo(y), a.disposeIntermediateTensorInfo(b), a.disposeIntermediateTensorInfo(x), a.disposeIntermediateTensorInfo(k), a.disposeIntermediateTensorInfo(T), a.disposeIntermediateTensorInfo(N), a.disposeIntermediateTensorInfo(R), a.disposeIntermediateTensorInfo(_), a.disposeIntermediateTensorInfo(F), a.disposeIntermediateTensorInfo(M), a.disposeIntermediateTensorInfo(L), a.disposeIntermediateTensorInfo(z), a.disposeIntermediateTensorInfo(B), a.disposeIntermediateTensorInfo(V), a.disposeIntermediateTensorInfo(G), a.disposeIntermediateTensorInfo(U), a.disposeIntermediateTensorInfo(q), a.disposeIntermediateTensorInfo(W), a.disposeIntermediateTensorInfo(H), a.disposeIntermediateTensorInfo(K), a.disposeIntermediateTensorInfo(j), {\n    real: X,\n    imag: Y\n  };\n}\n\nfunction fourierTransformByMatmul$1(e, t, n) {\n  var r = new Float32Array(2 * t);\n\n  for (var a = 0; a < t; a++) {\n    var s = 0,\n        o = 0;\n\n    for (var _r125 = 0; _r125 < t; _r125++) {\n      var i = exponent$1(a * _r125, t, n),\n          l = getComplexWithIndex$1(e, _r125);\n      s += l.real * i.real - l.imag * i.imag, o += l.real * i.imag + l.imag * i.real;\n    }\n\n    n && (s /= t, o /= t), assignToTypedArray$1(r, s, o, a);\n  }\n\n  return r;\n}\n\nfunction fft$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = sizeFromShape$1(r.shape),\n      s = r.shape[r.shape.length - 1],\n      o = reshape$5({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [a / s, s]\n    }\n  }),\n      i = fftBatch$1(o, !1, n),\n      l = reshape$5({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      shape: r.shape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(i), l;\n}\n\nvar fftConfig$3 = {\n  kernelName: FFT$1,\n  backendName: \"cpu\",\n  kernelFunc: fft$4\n};\n\nfunction fill$4(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    shape: r,\n    value: a,\n    dtype: s\n  } = n,\n      o = s || inferDtype$1(a),\n      i = getArrayFromDType$1(o, sizeFromShape$1(r));\n  return fillValues$1(i, a, o), t.makeTensorInfo(r, o, i);\n}\n\nvar fillConfig$3 = {\n  kernelName: Fill$1,\n  backendName: \"cpu\",\n  kernelFunc: fill$4\n};\n\nfunction fillValues$1(e, t, n) {\n  e.fill(t);\n}\n\nvar flipLeftRightConfig$3 = {\n  kernelName: FlipLeftRight$1,\n  backendName: \"cpu\",\n  kernelFunc: _ref10 => {\n    var {\n      inputs: e,\n      backend: t\n    } = _ref10;\n    var {\n      image: n\n    } = e,\n        r = t,\n        a = getTypedArrayFromDType$1(n.dtype, sizeFromShape$1(n.shape)),\n        [s, o, i, l] = n.shape,\n        u = r.data.get(n.dataId).values;\n\n    for (var _e353 = 0; _e353 < s; _e353++) {\n      var _t261 = _e353 * i * o * l;\n\n      for (var _e354 = 0; _e354 < o; _e354++) {\n        var _n169 = _e354 * (i * l);\n\n        for (var _e355 = 0; _e355 < i; _e355++) {\n          var _r126 = _e355 * l;\n\n          for (var _s62 = 0; _s62 < l; _s62++) {\n            var _o47 = Math.round(i - _e355 - 1),\n                c = _t261 + _n169 + _r126 + _s62;\n\n            var _p12 = u[c];\n            _o47 >= 0 && _o47 < i && (_p12 = u[_t261 + _n169 + _o47 * l + _s62]), a[c] = _p12;\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: r.write(a, n.shape, n.dtype),\n      shape: n.shape,\n      dtype: n.dtype\n    };\n  }\n},\n    floorDivImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => Math.floor(e / t)),\n    floorDiv$4 = binaryKernelFunc$3(FloorDiv$1, floorDivImpl$1, null, \"int32\"),\n    floorDivConfig$3 = {\n  kernelName: FloorDiv$1,\n  backendName: \"cpu\",\n  kernelFunc: floorDiv$4\n};\n\nfunction fusedConv2D$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dataFormat: c,\n    dilations: p,\n    dimRoundingMode: d,\n    activation: h,\n    leakyreluAlpha: m\n  } = r;\n  var f = conv2D$1({\n    inputs: {\n      x: a,\n      filter: s\n    },\n    backend: n,\n    attrs: {\n      strides: l,\n      pad: u,\n      dataFormat: c,\n      dilations: p,\n      dimRoundingMode: d\n    }\n  });\n\n  if (o) {\n    var _e356 = f;\n    f = add$4({\n      inputs: {\n        a: f,\n        b: o\n      },\n      backend: n\n    }), n.disposeIntermediateTensorInfo(_e356);\n  }\n\n  if (h) {\n    var _e357 = f;\n    f = applyActivation$2(n, f, h, i, m), n.disposeIntermediateTensorInfo(_e357);\n  }\n\n  return f;\n}\n\nvar fusedConv2DConfig$3 = {\n  kernelName: FusedConv2D$1,\n  backendName: \"cpu\",\n  kernelFunc: fusedConv2D$1\n};\n\nfunction fusedDepthwiseConv2D$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dataFormat: c,\n    dilations: p,\n    dimRoundingMode: d,\n    activation: h,\n    leakyreluAlpha: m\n  } = r;\n  var f = depthwiseConv2dNative$3({\n    inputs: {\n      x: a,\n      filter: s\n    },\n    backend: n,\n    attrs: {\n      strides: l,\n      pad: u,\n      dataFormat: c,\n      dilations: p,\n      dimRoundingMode: d\n    }\n  });\n\n  if (o) {\n    var _e358 = f;\n    f = add$4({\n      inputs: {\n        a: f,\n        b: o\n      },\n      backend: n\n    }), n.disposeIntermediateTensorInfo(_e358);\n  }\n\n  if (h) {\n    var _e359 = f;\n    f = applyActivation$2(n, f, h, i, m), n.disposeIntermediateTensorInfo(_e359);\n  }\n\n  return f;\n}\n\nvar fusedDepthwiseConv2DConfig$3 = {\n  kernelName: FusedDepthwiseConv2D$1,\n  backendName: \"cpu\",\n  kernelFunc: fusedDepthwiseConv2D$3\n};\n\nfunction gatherNd$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    params: r,\n    indices: a\n  } = t,\n      s = sizeFromShape$1(r.shape),\n      o = a.shape,\n      i = o[o.length - 1],\n      [l, u, c, p] = prepareAndValidate$1(r, a);\n  if (0 === u) return n.makeTensorInfo(l, r.dtype, []);\n  var d = gatherNdImpl$1(n.data.get(a.dataId).values, n.bufferSync(r), r.dtype, u, i, c, p, r.shape, s);\n  return n.makeTensorInfo(l, r.dtype, d.values);\n}\n\nvar gatherNdConfig$3 = {\n  kernelName: GatherNd$1,\n  backendName: \"cpu\",\n  kernelFunc: gatherNd$3\n};\n\nfunction gatherV2$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    indices: s\n  } = t,\n      {\n    axis: o,\n    batchDims: i\n  } = r;\n  assertNotComplex$3([a, s], \"gatherV2\");\n  var l = i;\n  null == i && (l = 0);\n  var u = sizeFromShape$1(s.shape),\n      c = collectGatherOpShapeInfo$1(a, s, parseAxisParam$1(o, a.shape)[0], l),\n      p = reshape$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [c.batchSize, c.outerSize, c.dimSize, c.sliceSize]\n    }\n  }),\n      d = reshape$5({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: [c.batchSize, u / c.batchSize]\n    }\n  }),\n      h = [c.batchSize, c.outerSize, u / c.batchSize, c.sliceSize],\n      m = n.bufferSync(d),\n      f = gatherV2Impl$1(n.bufferSync(p), m, h);\n  return n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d), n.makeTensorInfo(c.outputShape, f.dtype, f.values);\n}\n\nvar gatherV2Config$3 = {\n  kernelName: GatherV2$1,\n  backendName: \"cpu\",\n  kernelFunc: gatherV2$3\n};\n\nfunction ifft$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = sizeFromShape$1(r.shape),\n      s = r.shape[r.shape.length - 1],\n      o = reshape$5({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [a / s, s]\n    }\n  }),\n      i = fftBatch$1(o, !0, n),\n      l = reshape$5({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      shape: r.shape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(i), l;\n}\n\nvar ifftConfig$3 = {\n  kernelName: IFFT$1,\n  backendName: \"cpu\",\n  kernelFunc: ifft$4\n},\n    isFinite$5 = unaryKernelFunc$3(IsFinite$1, e => Number.isFinite(e) ? 1 : 0, \"bool\"),\n    isFiniteConfig$3 = {\n  kernelName: IsFinite$1,\n  backendName: \"cpu\",\n  kernelFunc: isFinite$5\n},\n    isInf$4 = unaryKernelFunc$3(IsInf$1, e => Infinity === Math.abs(e) ? 1 : 0, \"bool\"),\n    isInfConfig$3 = {\n  kernelName: IsInf$1,\n  backendName: \"cpu\",\n  kernelFunc: isInf$4\n},\n    isNaN$5 = unaryKernelFunc$3(IsNan$1, e => Number.isNaN(e) ? 1 : 0, \"bool\"),\n    isNaNConfig$3 = {\n  kernelName: IsNan$1,\n  backendName: \"cpu\",\n  kernelFunc: isNaN$5\n};\n\nfunction linSpace$3(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    num: s\n  } = n,\n      o = linSpaceImpl$1(r, a, s);\n  return t.makeTensorInfo([o.length], \"float32\", o);\n}\n\nvar linSpaceConfig$3 = {\n  kernelName: LinSpace$1,\n  backendName: \"cpu\",\n  kernelFunc: linSpace$3\n},\n    log1p$4 = unaryKernelFunc$3(Log1p$1, e => Math.log1p(e)),\n    log1pConfig$3 = {\n  kernelName: Log1p$1,\n  backendName: \"cpu\",\n  kernelFunc: log1p$4\n},\n    logicalAndImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e && t),\n    logicalAnd$4 = binaryKernelFunc$3(LogicalAnd$1, logicalAndImpl$1, null, \"bool\"),\n    logicalAndConfig$3 = {\n  kernelName: LogicalAnd$1,\n  backendName: \"cpu\",\n  kernelFunc: logicalAnd$4\n},\n    logicalNot$4 = unaryKernelFunc$3(LogicalNot$1, e => e ? 0 : 1, \"bool\"),\n    logicalNotConfig$3 = {\n  kernelName: LogicalNot$1,\n  backendName: \"cpu\",\n  kernelFunc: logicalNot$4\n},\n    logicalOrImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => e || t),\n    logicalOr$4 = binaryKernelFunc$3(LogicalOr$1, logicalOrImpl$1, null, \"bool\"),\n    logicalOrConfig$3 = {\n  kernelName: LogicalOr$1,\n  backendName: \"cpu\",\n  kernelFunc: logicalOr$4\n};\n\nfunction lRN$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    depthRadius: s,\n    bias: o,\n    alpha: i,\n    beta: l\n  } = r;\n  assertNotComplex$3(a, \"LRN\");\n  var u = a.shape[3],\n      c = u - 1,\n      p = n.data.get(a.dataId).values,\n      d = sizeFromShape$1(a.shape),\n      h = new Float32Array(d);\n\n  function m(e) {\n    var t = e % u;\n    var n = e - t + Math.max(0, t - s);\n    var r = e - t + Math.min(t + s, c);\n    var a = 0;\n\n    for (; n <= r; n++) {\n      var _e360 = p[n];\n      a += _e360 * _e360;\n    }\n\n    return a;\n  }\n\n  for (var _e361 = 0; _e361 < d; _e361++) {\n    var _t262 = m(_e361),\n        _n170 = p[_e361] * Math.pow(o + i * _t262, -l);\n\n    h[_e361] = _n170;\n  }\n\n  return n.makeTensorInfo(a.shape, a.dtype, h);\n}\n\nvar lRNConfig$1 = {\n  kernelName: LRN$1,\n  backendName: \"cpu\",\n  kernelFunc: lRN$1\n};\n\nfunction lRNGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    y: s,\n    dy: o\n  } = t,\n      {\n    depthRadius: i,\n    bias: l,\n    alpha: u,\n    beta: c\n  } = r;\n  assertNotComplex$3(o, \"LRNGrad\");\n  var p = sizeFromShape$1(o.shape),\n      d = o.shape[3],\n      h = n.data.get(o.dataId).values,\n      m = n.data.get(a.dataId).values,\n      f = n.data.get(s.dataId).values,\n      g = new Float32Array(p),\n      $ = p;\n\n  for (var _e362 = 0; _e362 < $; _e362++) {\n    var _t263 = _e362 % d,\n        _n171 = _e362 - _t263 + Math.max(0, _t263 - i),\n        _r127 = _e362 - _t263 + Math.min(d, _t263 + i + 1);\n\n    var _a92 = 0;\n\n    for (var _e363 = _n171; _e363 < _r127; _e363++) {\n      _a92 += Math.pow(m[_e363], 2);\n    }\n\n    _a92 = u * _a92 + l;\n\n    for (var _t264 = _n171; _t264 < _r127; _t264++) {\n      var _n172 = -2 * u * c * m[_t264] * f[_e362] / _a92;\n\n      _e362 === _t264 && (_n172 += Math.pow(_a92, -c)), _n172 *= h[_e362], g[_t264] += _n172;\n    }\n  }\n\n  return n.makeTensorInfo(o.shape, a.dtype, g);\n}\n\nvar lRNGradConfig$1 = {\n  kernelName: LRNGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: lRNGrad$1\n};\n\nfunction max$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reductionIndices: s,\n    keepDims: o\n  } = r,\n      i = n;\n  var l = a.shape;\n  var u = l.length,\n      c = parseAxisParam$1(s, l);\n  var p = c;\n  var d = getAxesPermutation$1(p, u);\n  var h = i.data.get(a.dataId).values;\n\n  if (null != d) {\n    var _e364 = new Array(u);\n\n    for (var _t265 = 0; _t265 < _e364.length; _t265++) {\n      _e364[_t265] = l[d[_t265]];\n    }\n\n    h = transposeImpl$3(h, l, a.dtype, d, _e364), p = getInnerMostAxes$1(p.length, u), l = _e364;\n  }\n\n  assertNotComplex$3(a, \"max\"), assertAxesAreInnerMostDims$1(\"max\", p, u);\n  var [m, f] = computeOutAndReduceShapes$1(l, p),\n      g = maxImpl$3(h, sizeFromShape$1(f), m, a.dtype),\n      $ = i.write(g, m, a.dtype);\n  var y = m;\n  return o && (y = expandShapeToKeepDim$1(m, c)), {\n    dataId: $,\n    shape: y,\n    dtype: a.dtype\n  };\n}\n\nvar maxConfig$3 = {\n  kernelName: Max$1,\n  backendName: \"cpu\",\n  kernelFunc: max$5\n};\n\nfunction maxPool$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex$3(a, \"maxPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$6(eitherStridesOrDilationsAreOne$1(o, 1), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo$1(a.shape, s, o, 1, i, l);\n  var c;\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual$1(u.inShape, u.outShape)) c = identity$4({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });else {\n    var _e365 = n.data.get(a.dataId).values,\n        _t266 = computeStrides$1(a.shape),\n        _r128 = pool$2(_e365, a.shape, a.dtype, _t266, u, \"max\");\n\n    c = n.makeTensorInfo(u.outShape, a.dtype, _r128.values);\n  }\n  return c;\n}\n\nvar maxPoolConfig$3 = {\n  kernelName: MaxPool$1,\n  backendName: \"cpu\",\n  kernelFunc: maxPool$4\n};\n\nfunction maxPool3D$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l,\n    dataFormat: u\n  } = r;\n  assertNotComplex$3(a, \"maxPool3d\");\n  var c = computePool3DInfo$1(a.shape, s, o, 1, i, l, u),\n      p = pool3d$2(n.data.get(a.dataId).values, a.shape, a.dtype, computeStrides$1(a.shape), c, \"max\");\n  return n.makeTensorInfo(p.shape, \"float32\", p.values);\n}\n\nvar maxPool3DConfig$3 = {\n  kernelName: MaxPool3D$1,\n  backendName: \"cpu\",\n  kernelFunc: maxPool3D$1\n};\n\nfunction maxPool3DGrad$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      {\n    filterSize: o,\n    strides: i,\n    pad: l,\n    dimRoundingMode: u\n  } = r;\n  assertNotComplex$3([a, s], \"maxPool3DGrad\");\n  var c = computePool3DInfo$1(s.shape, o, i, 1, l, u),\n      p = maxPool3dPositions$1(n.bufferSync(s), c),\n      d = c.strideDepth,\n      h = c.strideHeight,\n      m = c.strideWidth,\n      f = c.dilationDepth,\n      g = c.dilationHeight,\n      $ = c.dilationWidth,\n      y = c.effectiveFilterDepth,\n      b = c.effectiveFilterHeight,\n      x = c.effectiveFilterWidth,\n      v = y - 1 - c.padInfo.front,\n      I = x - 1 - c.padInfo.left,\n      C = b - 1 - c.padInfo.top,\n      S = buffer$1(s.shape, \"float32\"),\n      k = n.bufferSync(a);\n\n  for (var _e366 = 0; _e366 < c.batchSize; ++_e366) {\n    for (var _t267 = 0; _t267 < c.inChannels; ++_t267) {\n      for (var _n173 = 0; _n173 < c.inDepth; ++_n173) {\n        for (var _r129 = 0; _r129 < c.inHeight; ++_r129) {\n          for (var _a93 = 0; _a93 < c.inWidth; ++_a93) {\n            var _s63 = _n173 - v,\n                _o48 = _r129 - C,\n                _i30 = _a93 - I;\n\n            var _l24 = 0;\n\n            for (var _n174 = 0; _n174 < y; _n174 += f) {\n              var _r130 = (_s63 + _n174) / d;\n\n              if (!(_r130 < 0 || _r130 >= c.outDepth || Math.floor(_r130) !== _r130)) for (var _a94 = 0; _a94 < b; _a94 += g) {\n                var _s64 = (_o48 + _a94) / h;\n\n                if (!(_s64 < 0 || _s64 >= c.outHeight || Math.floor(_s64) !== _s64)) for (var _o49 = 0; _o49 < x; _o49 += $) {\n                  var _u18 = (_i30 + _o49) / m;\n\n                  if (_u18 < 0 || _u18 >= c.outWidth || Math.floor(_u18) !== _u18) continue;\n\n                  var _d8 = y * b * x - 1 - p.get(_e366, _r130, _s64, _u18, _t267) === _n174 * b * x + _a94 * x + _o49 ? 1 : 0;\n\n                  0 !== _d8 && (_l24 += k.get(_e366, _r130, _s64, _u18, _t267) * _d8);\n                }\n              }\n            }\n\n            S.set(_l24, _e366, _n173, _r129, _a93, _t267);\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(S.shape, S.dtype, S.values);\n}\n\nvar maxPool3DGradConfig$2 = {\n  kernelName: MaxPool3DGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: maxPool3DGrad$3\n};\n\nfunction maxPoolGrad$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s,\n    output: o\n  } = t,\n      i = s;\n  assertNotComplex$3([s, o], \"maxPoolGrad\");\n  var {\n    filterSize: l,\n    strides: u,\n    pad: c,\n    dimRoundingMode: p\n  } = r,\n      d = computePool2DInfo$1(i.shape, l, u, 1, c, p),\n      h = n.data.get(i.dataId).values,\n      m = buffer$1(d.outShape, i.dtype, maxPoolPositions$1(h, i.shape, i.dtype, d).values),\n      f = d.strideHeight,\n      g = d.strideWidth,\n      $ = d.dilationHeight,\n      y = d.dilationWidth,\n      b = d.effectiveFilterHeight,\n      x = d.effectiveFilterWidth,\n      v = x - 1 - d.padInfo.left,\n      I = b - 1 - d.padInfo.top,\n      C = buffer$1(i.shape, \"float32\"),\n      S = n.data.get(a.dataId).values,\n      k = buffer$1(a.shape, \"float32\", S);\n\n  for (var _e367 = 0; _e367 < d.batchSize; ++_e367) {\n    for (var _t268 = 0; _t268 < d.inChannels; ++_t268) {\n      for (var _n175 = 0; _n175 < d.inHeight; ++_n175) {\n        for (var _r131 = 0; _r131 < d.inWidth; ++_r131) {\n          var _a95 = _n175 - I,\n              _s65 = _r131 - v;\n\n          var _o50 = 0;\n\n          for (var _n176 = 0; _n176 < b; _n176 += $) {\n            var _r132 = (_a95 + _n176) / f;\n\n            if (!(_r132 < 0 || _r132 >= d.outHeight || Math.floor(_r132) !== _r132)) for (var _a96 = 0; _a96 < x; _a96 += y) {\n              var _i31 = (_s65 + _a96) / g;\n\n              if (_i31 < 0 || _i31 >= d.outWidth || Math.floor(_i31) !== _i31) continue;\n\n              var _l25 = b * x - 1 - m.get(_e367, _r132, _i31, _t268) === _n176 * x + _a96 ? 1 : 0;\n\n              0 !== _l25 && (_o50 += k.get(_e367, _r132, _i31, _t268) * _l25);\n            }\n          }\n\n          C.set(_o50, _e367, _n175, _r131, _t268);\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(C.shape, C.dtype, C.values);\n}\n\nvar maxPoolGradConfig$4 = {\n  kernelName: MaxPoolGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: maxPoolGrad$4\n};\n\nfunction maxPoolWithArgmaxImpl$3(e, t, n, r, a) {\n  var s = pool$2(e, t, n, computeStrides$1(t), a, \"max\"),\n      o = maxPoolPositions$1(e, t, n, a, !0, r);\n  return [s.values, o.values];\n}\n\nvar maxPoolWithArgmaxConfig$3 = {\n  kernelName: MaxPoolWithArgmax$1,\n  backendName: \"cpu\",\n  kernelFunc: _ref11 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref11;\n    var {\n      x: r\n    } = e,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o,\n      includeBatchInIndex: i\n    } = t,\n        l = n;\n    assertNotComplex$3(r, \"MaxPoolWithArgmax\");\n    var u = l.data.get(r.dataId).values,\n        c = computePool2DInfo$1(r.shape, a, s, [1, 1], o),\n        [p, d] = maxPoolWithArgmaxImpl$3(u, r.shape, r.dtype, i, c),\n        h = l.write(p, c.outShape, r.dtype),\n        m = l.write(d, c.outShape, r.dtype);\n    return [{\n      dataId: h,\n      shape: c.outShape,\n      dtype: r.dtype\n    }, {\n      dataId: m,\n      shape: c.outShape,\n      dtype: \"int32\"\n    }];\n  }\n};\n\nfunction mean$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = parseAxisParam$1(s, a.shape),\n      l = sizeFromShape$1(computeOutAndReduceShapes$1(a.shape, i)[1]),\n      u = [],\n      c = n.makeTensorInfo([], \"float32\", new Float32Array([l]));\n  u.push(c);\n  var p = cast$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      dtype: \"float32\"\n    }\n  });\n  u.push(p);\n  var d = div$2({\n    inputs: {\n      a: p,\n      b: c\n    },\n    backend: n\n  });\n  u.push(d);\n  var h = sum$5({\n    inputs: {\n      x: d\n    },\n    backend: n,\n    attrs: {\n      axis: s,\n      keepDims: o\n    }\n  });\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), h;\n}\n\nvar meanConfig$3 = {\n  kernelName: Mean$1,\n  backendName: \"cpu\",\n  kernelFunc: mean$2\n};\n\nfunction min$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$3(a, \"min\");\n  var i = parseAxisParam$1(s, a.shape);\n  var l = i;\n  var u = getAxesPermutation$1(l, a.shape.length);\n  var c = a;\n  null != u && (c = transpose$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), l = getInnerMostAxes$1(l.length, a.shape.length)), assertAxesAreInnerMostDims$1(\"min\", l, c.shape.length);\n  var [p, d] = computeOutAndReduceShapes$1(c.shape, l),\n      h = sizeFromShape$1(d),\n      m = makeZerosTypedArray$1(sizeFromShape$1(p), c.dtype),\n      f = n.data.get(c.dataId).values;\n\n  for (var _e368 = 0; _e368 < m.length; ++_e368) {\n    var _t269 = _e368 * h;\n\n    var _n177 = f[_t269];\n\n    for (var _e369 = 0; _e369 < h; ++_e369) {\n      var _r133 = f[_t269 + _e369];\n      (Number.isNaN(_r133) || _r133 < _n177) && (_n177 = _r133);\n    }\n\n    m[_e368] = _n177;\n  }\n\n  null != u && n.disposeIntermediateTensorInfo(c);\n  var g = n.makeTensorInfo(p, c.dtype, m);\n\n  if (o) {\n    var _e370 = reshape$5({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim$1(p, i)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), _e370;\n  }\n\n  return g;\n}\n\nvar minConfig$3 = {\n  kernelName: Min$1,\n  backendName: \"cpu\",\n  kernelFunc: min$5\n};\n\nfunction mirrorPad$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    paddings: s,\n    mode: o\n  } = r;\n  assertNotComplex$3(a, \"mirrorPad\");\n  var i = s.map((e, t) => e[0] + a.shape[t] + e[1]),\n      l = s.map(e => e[0]),\n      u = s.map((e, t) => e[0] + a.shape[t]),\n      c = \"reflect\" === o ? 0 : 1,\n      p = n.data.get(a.dataId).values,\n      d = a.shape.length,\n      h = computeStrides$1(a.shape),\n      m = sizeFromShape$1(i),\n      f = i.length,\n      g = computeStrides$1(i),\n      $ = getTypedArrayFromDType$1(a.dtype, m);\n\n  for (var _e371 = 0; _e371 < m; _e371++) {\n    var _t270 = indexToLoc$1(_e371, f, g);\n\n    for (var _e372 = 0; _e372 < f; _e372++) {\n      _t270[_e372] < l[_e372] ? _t270[_e372] = 2 * l[_e372] - _t270[_e372] - c : _t270[_e372] >= u[_e372] && (_t270[_e372] = 2 * (u[_e372] - 1) - _t270[_e372] + c);\n    }\n\n    _t270 = _t270.map((e, t) => e - l[t]);\n\n    var _n178 = locToIndex$1(_t270, d, h);\n\n    $[_e371] = p[_n178];\n  }\n\n  return {\n    dataId: n.write($, i, a.dtype),\n    shape: i,\n    dtype: a.dtype\n  };\n}\n\nvar mirrorPadConfig$3 = {\n  kernelName: MirrorPad$1,\n  backendName: \"cpu\",\n  kernelFunc: mirrorPad$2\n},\n    modImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => {\n  var n = e % t;\n  return e < 0 && t < 0 || e >= 0 && t >= 0 ? n : (n + t) % t;\n}),\n    mod$4 = binaryKernelFunc$3(Mod$1, modImpl$1),\n    modConfig$3 = {\n  kernelName: Mod$1,\n  backendName: \"cpu\",\n  kernelFunc: mod$4\n};\n\nfunction softmax$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    dim: s\n  } = r,\n      o = a.shape.length;\n  var i = s;\n  if (-1 === i && (i = o - 1), i !== o - 1) throw Error(\"Softmax along a non-last dimension is not yet supported. Logits was rank \".concat(o, \" and dim was \").concat(i));\n  var l = parseAxisParam$1([i], a.shape),\n      u = max$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      reductionIndices: l,\n      keepDims: !1\n    }\n  }),\n      c = expandShapeToKeepDim$1(u.shape, l),\n      p = reshape$5({\n    inputs: {\n      x: u\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      d = sub$4({\n    inputs: {\n      a,\n      b: p\n    },\n    backend: n\n  }),\n      h = exp$4({\n    inputs: {\n      x: d\n    },\n    backend: n\n  }),\n      m = sum$5({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      axis: l,\n      keepDims: !1\n    }\n  }),\n      f = reshape$5({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      g = div$2({\n    inputs: {\n      a: h,\n      b: f\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(u), n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d), n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), g;\n}\n\nvar softmaxConfig$3 = {\n  kernelName: Softmax$5,\n  backendName: \"cpu\",\n  kernelFunc: softmax$5\n};\n\nfunction multinomial$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    numSamples: s,\n    seed: o,\n    normalized: i\n  } = r;\n  assertNotComplex$3(a, \"multinomial\");\n  var l = i ? a : softmax$5({\n    inputs: {\n      logits: a\n    },\n    backend: n,\n    attrs: {\n      dim: -1\n    }\n  }),\n      u = l.shape[0],\n      c = l.shape[1],\n      p = n.data.get(l.dataId).values,\n      d = [u, s],\n      h = makeZerosTypedArray$1(sizeFromShape$1(d), \"int32\");\n\n  for (var _e373 = 0; _e373 < u; ++_e373) {\n    var _t271 = _e373 * c,\n        _n179 = new Float32Array(c - 1);\n\n    _n179[0] = p[_t271];\n\n    for (var _e374 = 1; _e374 < _n179.length; ++_e374) {\n      _n179[_e374] = _n179[_e374 - 1] + p[_t271 + _e374];\n    }\n\n    var _r134 = seedrandom$2.alea(o.toString()),\n        _a97 = _e373 * s;\n\n    for (var _e375 = 0; _e375 < s; ++_e375) {\n      var _t272 = _r134();\n\n      h[_a97 + _e375] = _n179.length;\n\n      for (var _r135 = 0; _r135 < _n179.length; _r135++) {\n        if (_t272 < _n179[_r135]) {\n          h[_a97 + _e375] = _r135;\n          break;\n        }\n      }\n    }\n  }\n\n  return i || n.disposeIntermediateTensorInfo(l), n.makeTensorInfo(d, \"int32\", h);\n}\n\nvar multinomialConfig$3 = {\n  kernelName: Multinomial$1,\n  backendName: \"cpu\",\n  kernelFunc: multinomial$4\n},\n    nonMaxSuppressionV3Impl$4 = nonMaxSuppressionV3Impl$5;\n\nfunction nonMaxSuppressionV3$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l\n  } = r;\n  assertNotComplex$3(a, \"NonMaxSuppression\");\n  var u = n.data.get(a.dataId).values,\n      c = n.data.get(s.dataId).values,\n      {\n    selectedIndices: p\n  } = nonMaxSuppressionV3Impl$4(u, c, o, i, l);\n  return n.makeTensorInfo([p.length], \"int32\", new Int32Array(p));\n}\n\nvar nonMaxSuppressionV3Config$3 = {\n  kernelName: NonMaxSuppressionV3$1,\n  backendName: \"cpu\",\n  kernelFunc: nonMaxSuppressionV3$3\n},\n    nonMaxSuppressionV4Impl$4 = nonMaxSuppressionV4Impl$5;\n\nfunction nonMaxSuppressionV4$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    padToMaxOutputSize: u\n  } = r;\n  assertNotComplex$3(a, \"NonMaxSuppressionPadded\");\n  var c = n.data.get(a.dataId).values,\n      p = n.data.get(s.dataId).values,\n      {\n    selectedIndices: d,\n    validOutputs: h\n  } = nonMaxSuppressionV4Impl$4(c, p, o, i, l, u);\n  return [n.makeTensorInfo([d.length], \"int32\", new Int32Array(d)), n.makeTensorInfo([], \"int32\", new Int32Array([h]))];\n}\n\nvar nonMaxSuppressionV4Config$3 = {\n  kernelName: NonMaxSuppressionV4$1,\n  backendName: \"cpu\",\n  kernelFunc: nonMaxSuppressionV4$3\n},\n    nonMaxSuppressionV5Impl$4 = nonMaxSuppressionV5Impl$5;\n\nfunction nonMaxSuppressionV5$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    softNmsSigma: u\n  } = r;\n  assertNotComplex$3(a, \"NonMaxSuppressionWithScore\");\n  var c = n.data.get(a.dataId).values,\n      p = n.data.get(s.dataId).values,\n      d = o,\n      h = i,\n      m = l,\n      f = u,\n      {\n    selectedIndices: g,\n    selectedScores: $\n  } = nonMaxSuppressionV5Impl$4(c, p, d, h, m, f);\n  return [n.makeTensorInfo([g.length], \"int32\", new Int32Array(g)), n.makeTensorInfo([$.length], \"float32\", new Float32Array($))];\n}\n\nvar nonMaxSuppressionV5Config$3 = {\n  kernelName: NonMaxSuppressionV5$1,\n  backendName: \"cpu\",\n  kernelFunc: nonMaxSuppressionV5$3\n};\n\nfunction oneHot$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a\n  } = t,\n      {\n    depth: s,\n    onValue: o,\n    offValue: i\n  } = r;\n  assertNotComplex$3(a, \"oneHot\");\n  var l = sizeFromShape$1(a.shape),\n      u = new Float32Array(l * s);\n  u.fill(i);\n  var c = n.data.get(a.dataId).values;\n\n  for (var _e376 = 0; _e376 < l; ++_e376) {\n    c[_e376] >= 0 && c[_e376] < s && (u[_e376 * s + c[_e376]] = o);\n  }\n\n  return n.makeTensorInfo([...a.shape, s], \"int32\", u);\n}\n\nvar oneHotConfig$3 = {\n  kernelName: OneHot$1,\n  backendName: \"cpu\",\n  kernelFunc: oneHot$4\n};\n\nfunction zerosLike$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  if (\"string\" === r.dtype) throw new Error(\"zerosLike is not supported for string tensors\");\n\n  if (\"complex64\" === r.dtype) {\n    var _e377 = real$4({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t273 = zerosLike$4({\n      inputs: {\n        x: _e377\n      },\n      backend: n\n    }),\n        a = imag$4({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike$4({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex$4({\n      inputs: {\n        real: _t273,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e377), n.disposeIntermediateTensorInfo(_t273), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill$4({\n    backend: n,\n    attrs: {\n      shape: r.shape,\n      value: 0,\n      dtype: r.dtype\n    }\n  });\n}\n\nvar zerosLikeConfig$3 = {\n  kernelName: ZerosLike$1,\n  backendName: \"cpu\",\n  kernelFunc: zerosLike$4\n};\n\nfunction onesLike$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  if (\"string\" === r.dtype) throw new Error(\"onesLike is not supported for string tensors\");\n\n  if (\"complex64\" === r.dtype) {\n    var _e378 = real$4({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t274 = onesLike$4({\n      inputs: {\n        x: _e378\n      },\n      backend: n\n    }),\n        a = imag$4({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike$4({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex$4({\n      inputs: {\n        real: _t274,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e378), n.disposeIntermediateTensorInfo(_t274), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill$4({\n    backend: n,\n    attrs: {\n      shape: r.shape,\n      value: 1,\n      dtype: r.dtype\n    }\n  });\n}\n\nvar onesLikeConfig$3 = {\n  kernelName: OnesLike$1,\n  backendName: \"cpu\",\n  kernelFunc: onesLike$4\n};\n\nfunction pack$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r;\n  if (1 === t.length) return expandDims$5({\n    inputs: {\n      input: t[0]\n    },\n    backend: n,\n    attrs: {\n      dim: a\n    }\n  });\n  var s = t[0].shape,\n      o = t[0].dtype;\n  t.forEach(e => {\n    assertShapesMatch$1(s, e.shape, \"All tensors passed to stack must have matching shapes\"), assert$6(o === e.dtype, () => \"All tensors passed to stack must have matching dtypes\");\n  });\n  var i = [],\n      l = concat$4({\n    inputs: t.map(e => {\n      var t = expandDims$5({\n        inputs: {\n          input: e\n        },\n        backend: n,\n        attrs: {\n          dim: a\n        }\n      });\n      return i.push(t), t;\n    }),\n    backend: n,\n    attrs: {\n      axis: a\n    }\n  });\n  return i.forEach(e => n.disposeIntermediateTensorInfo(e)), l;\n}\n\nvar packConfig$3 = {\n  kernelName: Pack$1,\n  backendName: \"cpu\",\n  kernelFunc: pack$3\n};\n\nfunction padV2$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    paddings: s,\n    constantValue: o\n  } = r;\n  assertNotComplex$3(a, \"pad\");\n  var i = s.map((e, t) => e[0] + a.shape[t] + e[1]),\n      l = s.map(e => e[0]),\n      u = n.data.get(a.dataId).values,\n      c = sizeFromShape$1(a.shape),\n      p = a.shape.length,\n      d = computeStrides$1(a.shape),\n      h = sizeFromShape$1(i),\n      m = i.length,\n      f = computeStrides$1(i),\n      g = getTypedArrayFromDType$1(a.dtype, h);\n  0 !== o && g.fill(o);\n\n  for (var _e379 = 0; _e379 < c; _e379++) {\n    g[locToIndex$1(indexToLoc$1(_e379, p, d).map((e, t) => e + l[t]), m, f)] = u[_e379];\n  }\n\n  return {\n    dataId: n.write(g, i, a.dtype),\n    shape: i,\n    dtype: a.dtype\n  };\n}\n\nvar padV2Config$3 = {\n  kernelName: PadV2$1,\n  backendName: \"cpu\",\n  kernelFunc: padV2$3\n},\n    powImpl$1 = createSimpleBinaryKernelImpl$1((e, t) => Math.pow(e, t)),\n    pow$4 = binaryKernelFunc$3(Pow$1, powImpl$1),\n    powConfig$3 = {\n  kernelName: Pow$1,\n  backendName: \"cpu\",\n  kernelFunc: pow$4\n};\n\nfunction range$6(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    dtype: s,\n    step: o\n  } = n,\n      i = rangeImpl$1(r, a, o, s);\n  return t.makeTensorInfo([i.length], s, i);\n}\n\nvar rangeConfig$3 = {\n  kernelName: Range$1,\n  backendName: \"cpu\",\n  kernelFunc: range$6\n},\n    reciprocal$4 = unaryKernelFunc$3(Reciprocal$1, e => 1 / e),\n    reciprocalConfig$3 = {\n  kernelName: Reciprocal$1,\n  backendName: \"cpu\",\n  kernelFunc: reciprocal$4\n};\n\nfunction resizeBilinear$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r;\n  assertNotComplex$3(a, \"resizeBilinear\");\n  var l = computeStrides$1(a.shape),\n      [u, c] = i,\n      [p, d, h, m] = a.shape,\n      f = n.data.get(a.dataId).values,\n      g = new Float32Array(sizeFromShape$1([p, u, c, m])),\n      $ = [s && u > 1 ? d - 1 : d, s && c > 1 ? h - 1 : h],\n      y = [s && u > 1 ? u - 1 : u, s && c > 1 ? c - 1 : c];\n  var b = 0;\n  var x = $[0] / y[0],\n      v = $[1] / y[1];\n\n  for (var _e380 = 0; _e380 < p; _e380++) {\n    for (var _t275 = 0; _t275 < u; _t275++) {\n      var _n180 = void 0;\n\n      _n180 = o ? x * (_t275 + .5) - .5 : x * _t275;\n\n      var _r136 = Math.max(0, Math.floor(_n180)),\n          _a98 = _n180 - _r136,\n          _s66 = Math.min(d - 1, Math.ceil(_n180)),\n          _i32 = _e380 * l[0] + _r136 * l[1],\n          _u19 = _e380 * l[0] + _s66 * l[1];\n\n      for (var _e381 = 0; _e381 < c; _e381++) {\n        var _t276 = void 0;\n\n        _t276 = o ? v * (_e381 + .5) - .5 : v * _e381;\n\n        var _n181 = Math.max(0, Math.floor(_t276)),\n            _r137 = _t276 - _n181,\n            _s67 = Math.min(h - 1, Math.ceil(_t276)),\n            _c16 = _i32 + _n181 * l[2],\n            _p13 = _u19 + _n181 * l[2],\n            _d9 = _i32 + _s67 * l[2],\n            _$4 = _u19 + _s67 * l[2];\n\n        for (var _e382 = 0; _e382 < m; _e382++) {\n          var _t277 = f[_c16 + _e382],\n              _n182 = f[_p13 + _e382],\n              _s68 = _t277 + (f[_d9 + _e382] - _t277) * _r137;\n\n          g[b++] = _s68 + (_n182 + (f[_$4 + _e382] - _n182) * _r137 - _s68) * _a98;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([p, u, c, m], \"float32\", g);\n}\n\nvar resizeBilinearConfig$3 = {\n  kernelName: ResizeBilinear$1,\n  backendName: \"cpu\",\n  kernelFunc: resizeBilinear$4\n};\n\nfunction resizeBilinearGrad$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r;\n  assertNotComplex$3([s, a], \"resizeBilinearGrad\");\n  var i = computeStrides$1(a.shape),\n      [l, u, c, p] = a.shape,\n      [, d, h] = s.shape,\n      m = new Float32Array(l * u * c * p),\n      f = [o && d > 1 ? u - 1 : u, o && h > 1 ? c - 1 : c],\n      g = [o && d > 1 ? d - 1 : d, o && h > 1 ? h - 1 : h],\n      $ = f[0] / g[0],\n      y = f[1] / g[1],\n      b = n.data.get(s.dataId).values;\n  var x = 0;\n\n  for (var _e383 = 0; _e383 < l; _e383++) {\n    var _t278 = _e383 * i[0];\n\n    for (var _e384 = 0; _e384 < d; _e384++) {\n      var _n183 = _e384 * $,\n          _r138 = Math.floor(_n183),\n          _a99 = Math.min(Math.ceil(_n183), u - 1),\n          _s69 = _t278 + _r138 * i[1],\n          _o51 = _t278 + _a99 * i[1],\n          _l26 = _n183 - _r138,\n          _d10 = 1 - _l26;\n\n      for (var _e385 = 0; _e385 < h; _e385++) {\n        var _t279 = _e385 * y,\n            _n184 = Math.floor(_t279),\n            _r139 = Math.min(Math.ceil(_t279), c - 1),\n            _a100 = _t279 - _n184,\n            _u20 = 1 - _a100,\n            _h9 = _s69 + _n184 * i[2],\n            _f8 = _s69 + _r139 * i[2],\n            _g3 = _o51 + _n184 * i[2],\n            _$5 = _o51 + _r139 * i[2],\n            v = _d10 * _u20,\n            I = _d10 * _a100,\n            C = _l26 * _u20,\n            S = _l26 * _a100;\n\n        for (var _e386 = 0; _e386 < p; _e386++) {\n          var _t280 = b[x++];\n          m[_h9 + _e386] += _t280 * v, m[_f8 + _e386] += _t280 * I, m[_g3 + _e386] += _t280 * C, m[_$5 + _e386] += _t280 * S;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([l, c, u, p], \"float32\", m);\n}\n\nvar resizeBilinearGradConfig$4 = {\n  kernelName: ResizeBilinearGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: resizeBilinearGrad$3\n};\n\nfunction resizeNearestNeighbor$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r;\n  assertNotComplex$3(a, \"resizeNearestNeighbor\");\n  var l = computeStrides$1(a.shape),\n      [u, c] = i,\n      [p, d, h, m] = a.shape,\n      f = n.data.get(a.dataId).values,\n      g = new Float32Array(p * u * c * m),\n      $ = [s && u > 1 ? d - 1 : d, s && c > 1 ? h - 1 : h],\n      y = [s && u > 1 ? u - 1 : u, s && c > 1 ? c - 1 : c],\n      b = $[0] / y[0],\n      x = $[1] / y[1];\n  var v = 0;\n\n  for (var _e387 = 0; _e387 < p; _e387++) {\n    var _t281 = _e387 * l[0];\n\n    for (var _e388 = 0; _e388 < u; _e388++) {\n      var _n185 = o ? b * (_e388 + .5) : b * _e388;\n\n      var _r140 = Math.min(d - 1, s ? Math.round(_n185) : Math.floor(_n185));\n\n      o && (_r140 = Math.max(0, _r140));\n\n      var _a101 = _t281 + _r140 * l[1];\n\n      for (var _e389 = 0; _e389 < c; _e389++) {\n        var _t282 = o ? x * (_e389 + .5) : x * _e389;\n\n        var _n186 = Math.min(h - 1, s ? Math.round(_t282) : Math.floor(_t282));\n\n        o && (_n186 = Math.max(0, _n186));\n\n        var _r141 = _a101 + _n186 * l[2];\n\n        for (var _e390 = 0; _e390 < m; _e390++) {\n          g[v++] = f[_r141 + _e390];\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([p, u, c, m], a.dtype, g);\n}\n\nvar resizeNearestNeighborConfig$3 = {\n  kernelName: ResizeNearestNeighbor$1,\n  backendName: \"cpu\",\n  kernelFunc: resizeNearestNeighbor$4\n};\n\nfunction resizeNearestNeighborGrad$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r;\n  assertNotComplex$3([s, a], \"resizeNearestNeighborGrad\");\n  var i = computeStrides$1(a.shape),\n      l = computeStrides$1(s.shape),\n      [u, c, p, d] = a.shape,\n      [, h, m] = s.shape,\n      f = new Float32Array(u * c * p * d),\n      g = n.data.get(s.dataId).values,\n      $ = [o && h > 1 ? c - 1 : c, o && m > 1 ? p - 1 : p],\n      y = [o && h > 1 ? h - 1 : h, o && m > 1 ? m - 1 : m],\n      b = $[0] / y[0],\n      x = $[1] / y[1],\n      v = 1 / b,\n      I = 1 / x,\n      C = 2 * Math.ceil(v) + 2,\n      S = 2 * Math.ceil(I) + 2;\n\n  for (var _e391 = 0; _e391 < u; _e391++) {\n    var _t283 = _e391 * i[0];\n\n    for (var _e392 = 0; _e392 < c; _e392++) {\n      var _n187 = _t283 + _e392 * i[1],\n          _r142 = Math.floor(_e392 * v),\n          _a102 = Math.floor(_r142 - C / 2);\n\n      for (var _r143 = 0; _r143 < p; _r143++) {\n        var _s70 = _n187 + _r143 * i[2],\n            _u21 = Math.floor(_r143 * I),\n            _$6 = Math.floor(_u21 - S / 2);\n\n        for (var _n188 = 0; _n188 < d; _n188++) {\n          var _i33 = 0;\n\n          for (var _s71 = 0; _s71 < C; _s71++) {\n            var _u22 = _s71 + _a102;\n\n            if (_u22 < 0 || _u22 >= h) continue;\n\n            var _d11 = _t283 + _u22 * l[1],\n                _f9 = _u22 * b;\n\n            if (_e392 === Math.min(c - 1, o ? Math.round(_f9) : Math.floor(_f9))) for (var _e393 = 0; _e393 < S; _e393++) {\n              var _t284 = _e393 + _$6;\n\n              if (_t284 < 0 || _t284 >= m) continue;\n\n              var _a103 = _d11 + _t284 * l[2],\n                  _s72 = _t284 * x;\n\n              _r143 === Math.min(p - 1, o ? Math.round(_s72) : Math.floor(_s72)) && (_i33 += g[_a103 + _n188]);\n            }\n          }\n\n          f[_s70 + _n188] = _i33;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(a.shape, a.dtype, f);\n}\n\nvar resizeNearestNeighborGradConfig$4 = {\n  kernelName: ResizeNearestNeighborGrad$1,\n  backendName: \"cpu\",\n  kernelFunc: resizeNearestNeighborGrad$3\n};\n\nfunction reverse$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dims: s\n  } = r;\n  assertNotComplex$3(a, \"reverse\");\n  var o = a.shape.length,\n      i = parseAxisParam$1(s, a.shape);\n  if (0 === o) return identity$4({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var l = new TensorBuffer$1(a.shape, a.dtype),\n      u = n.bufferSync(a);\n\n  var _loop27 = function _loop27(_e394) {\n    var t = l.indexToLoc(_e394),\n        n = t.slice();\n    i.forEach(e => n[e] = a.shape[e] - 1 - n[e]), l.set(u.get(...n), ...t);\n  };\n\n  for (var _e394 = 0; _e394 < l.size; _e394++) {\n    _loop27(_e394);\n  }\n\n  return n.makeTensorInfo(l.shape, l.dtype, l.values);\n}\n\nvar reverseConfig$3 = {\n  kernelName: Reverse$1,\n  backendName: \"cpu\",\n  kernelFunc: reverse$4\n},\n    rotateWithOffsetConfig$3 = {\n  kernelName: RotateWithOffset$1,\n  backendName: \"cpu\",\n  kernelFunc: _ref12 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref12;\n    var {\n      image: r\n    } = e,\n        {\n      radians: a,\n      fillValue: s,\n      center: o\n    } = t,\n        i = n,\n        l = getTypedArrayFromDType$1(r.dtype, sizeFromShape$1(r.shape)),\n        [u, c, p, d] = r.shape,\n        [h, m] = getImageCenter$1(o, c, p),\n        f = Math.sin(a),\n        g = Math.cos(a),\n        $ = i.data.get(r.dataId).values;\n\n    for (var _e395 = 0; _e395 < u; _e395++) {\n      var _t285 = _e395 * p * c * d;\n\n      for (var _e396 = 0; _e396 < c; _e396++) {\n        var _n189 = _e396 * (p * d);\n\n        for (var _r144 = 0; _r144 < p; _r144++) {\n          var _a104 = _r144 * d;\n\n          for (var _o52 = 0; _o52 < d; _o52++) {\n            var _i34 = [u, _e396, _r144, _o52],\n                y = _i34[2],\n                b = _i34[1];\n            var x = (y - h) * g - (b - m) * f,\n                v = (y - h) * f + (b - m) * g;\n            x = Math.round(x + h), v = Math.round(v + m);\n            var I = s;\n            \"number\" != typeof s && (I = 3 === _o52 ? 255 : s[_o52]), x >= 0 && x < p && v >= 0 && v < c && (I = $[_t285 + v * (p * d) + x * d + _o52]), l[_t285 + _n189 + _a104 + _o52] = I;\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: i.write(l, r.shape, r.dtype),\n      shape: r.shape,\n      dtype: r.dtype\n    };\n  }\n},\n    round$5 = unaryKernelFunc$3(Round$1, e => {\n  var t = Math.floor(e);\n  return e - t < .5 ? Math.floor(e) : e - t > .5 ? Math.ceil(e) : t % 2 == 0 ? t : t + 1;\n}),\n    roundConfig$3 = {\n  kernelName: Round$1,\n  backendName: \"cpu\",\n  kernelFunc: round$5\n};\n\nfunction scatterImpl$1(e, t, n, r, a, s, o, i, l, u) {\n  var c = [r / a, a],\n      p = e.values,\n      d = t.values;\n  if (0 === r) return buffer$1(n, t.dtype);\n  var h = buffer$1(c, t.dtype);\n  h.values.fill(l);\n\n  for (var _e397 = 0; _e397 < s; _e397++) {\n    var _s73 = [];\n    var _l27 = 0;\n\n    for (var _t286 = 0; _t286 < o; _t286++) {\n      var _n190 = p[_e397 * o + _t286];\n      _s73.push(_n190), _l27 += _n190 * i[_t286];\n    }\n\n    if (_l27 < 0 || _l27 >= r / a) throw new Error(\"Invalid indices: \".concat(_s73, \" does not index into \").concat(n));\n\n    for (var _n191 = 0; _n191 < a; _n191++) {\n      u ? h.values[_l27 * a + _n191] += d[_e397 * a + _n191] : h.values[_l27 * a + _n191] = 0 === t.rank ? d[0] : d[_e397 * a + _n191];\n    }\n  }\n\n  return h;\n}\n\nfunction scatterNd$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a,\n    updates: s\n  } = t,\n      {\n    shape: o\n  } = r,\n      {\n    sliceRank: i,\n    numUpdates: l,\n    sliceSize: u,\n    strides: c,\n    outputSize: p\n  } = calculateShapes$1(s, a, o),\n      d = scatterImpl$1(n.bufferSync(a), n.bufferSync(s), o, p, u, l, i, c, 0, !0);\n  return n.makeTensorInfo(o, d.dtype, d.values);\n}\n\nvar scatterNdConfig$3 = {\n  kernelName: ScatterNd$1,\n  backendName: \"cpu\",\n  kernelFunc: scatterNd$3\n};\n\nfunction select$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    condition: r,\n    t: a,\n    e: s\n  } = t;\n  assertNotComplex$3([r, a, s], \"select\");\n  var o = r.shape.length,\n      i = n.data.get(r.dataId).values,\n      l = n.data.get(a.dataId).values,\n      u = n.data.get(s.dataId).values,\n      c = upcastType$1(a.dtype, s.dtype),\n      p = makeZerosTypedArray$1(sizeFromShape$1(a.shape), c);\n  var d = 0;\n  var h = 0 === o || o > 1 || 1 === a.shape.length ? 1 : sizeFromShape$1(a.shape.slice(1));\n\n  for (var _e398 = 0; _e398 < i.length; _e398++) {\n    for (var _t287 = 0; _t287 < h; _t287++) {\n      p[d++] = 1 === i[_e398] ? l[_e398] : u[_e398];\n    }\n  }\n\n  return n.makeTensorInfo(a.shape, c, p);\n}\n\nvar selectConfig$3 = {\n  kernelName: Select$1,\n  backendName: \"cpu\",\n  kernelFunc: select$4\n},\n    scaleAlpha$1 = SELU_SCALEALPHA$1,\n    scale$1 = SELU_SCALE$1,\n    selu$4 = unaryKernelFunc$3(Selu$3, e => e >= 0 ? scale$1 * e : scaleAlpha$1 * (Math.exp(e) - 1)),\n    seluConfig$3 = {\n  kernelName: Selu$3,\n  backendName: \"cpu\",\n  kernelFunc: selu$4\n},\n    sign$4 = unaryKernelFunc$3(Sign$1, e => e < 0 ? -1 : e > 0 ? 1 : 0),\n    signConfig$3 = {\n  kernelName: Sign$1,\n  backendName: \"cpu\",\n  kernelFunc: sign$4\n},\n    sin$4 = unaryKernelFunc$3(Sin$1, e => Math.sin(e)),\n    sinConfig$3 = {\n  kernelName: Sin$1,\n  backendName: \"cpu\",\n  kernelFunc: sin$4\n},\n    sinh$4 = unaryKernelFunc$3(Sinh$1, e => Math.sinh(e)),\n    sinhConfig$3 = {\n  kernelName: Sinh$1,\n  backendName: \"cpu\",\n  kernelFunc: sinh$4\n},\n    epsilon$2 = 1.1920928955078125e-7,\n    threshold$2 = Math.log(epsilon$2) + 2,\n    softplus$4 = unaryKernelFunc$3(Softplus$3, e => {\n  var t = e > -threshold$2,\n      n = e < threshold$2,\n      r = Math.exp(e);\n  var a;\n  return a = n ? r : t ? e : Math.log(1 + r), a;\n}),\n    softplusConfig$3 = {\n  kernelName: Softplus$3,\n  backendName: \"cpu\",\n  kernelFunc: softplus$4\n};\n\nfunction spaceToBatchND$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    paddings: o\n  } = r;\n  assertNotComplex$3([a], \"spaceToBatchND\");\n  var i = sizeFromShape$1(s),\n      l = [[0, 0]];\n  l.push(...o);\n\n  for (var _e399 = 1 + s.length; _e399 < a.shape.length; ++_e399) {\n    l.push([0, 0]);\n  }\n\n  var u = padV2Config$3.kernelFunc({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      paddings: l,\n      constantValue: 0\n    }\n  }),\n      c = getReshaped$1(u.shape, s, i, !1),\n      p = getPermuted$1(c.length, s.length, !1),\n      d = getReshapedPermuted$1(u.shape, s, i, !1),\n      h = reshape$5({\n    inputs: {\n      x: u\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      m = transpose$4({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      perm: p\n    }\n  }),\n      f = reshape$5({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  });\n  return n.disposeIntermediateTensorInfo(u), n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), f;\n}\n\nvar spaceToBatchNDConfig$3 = {\n  kernelName: SpaceToBatchND$1,\n  backendName: \"cpu\",\n  kernelFunc: spaceToBatchND$4\n};\n\nfunction sparseFillEmptyRows$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    indices: r,\n    values: a,\n    denseShape: s,\n    defaultValue: o\n  } = t;\n  if (1 !== s.shape.length) throw new Error(\"Dense shape must be a vector, saw:\\n        \".concat(s.shape));\n  if (2 !== r.shape.length) throw new Error(\"Indices must be a matrix, saw:\\n        \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Values must be a vector, saw:\\n        \".concat(a.shape));\n  if (0 !== o.shape.length) throw new Error(\"Default value must be a scalar, saw:\\n        \".concat(o.shape));\n  var i = n.data.get(r.dataId).values,\n      l = n.data.get(a.dataId).values,\n      u = n.data.get(s.dataId).values,\n      c = n.data.get(o.dataId).values[0],\n      [p, d, h, m, f] = sparseFillEmptyRowsImpl$1(i, r.shape, r.dtype, l, a.dtype, u, c);\n  return [n.makeTensorInfo(d, r.dtype, p), n.makeTensorInfo([d[0]], a.dtype, h), n.makeTensorInfo([m.length], \"bool\", new Uint8Array(m.map(e => Number(e)))), n.makeTensorInfo([f.length], r.dtype, new Int32Array(f))];\n}\n\nvar sparseFillEmptyRowsConfig$3 = {\n  kernelName: SparseFillEmptyRows$1,\n  backendName: \"cpu\",\n  kernelFunc: sparseFillEmptyRows$4\n};\n\nfunction sparseReshape$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    inputIndices: r,\n    inputShape: a,\n    newShape: s\n  } = t;\n  if (2 !== r.shape.length) throw new Error(\"Input indices should be a matrix but received shape\\n        \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Input shape should be a vector but received shape\\n        \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Target shape should be a vector but received shape \".concat(s.shape));\n  var o = Array.from(n.data.get(a.dataId).values),\n      i = n.data.get(r.dataId).values,\n      l = Array.from(n.data.get(s.dataId).values),\n      [u, c, p] = sparseReshapeImpl$1(i, r.shape, r.dtype, o, l);\n  return [n.makeTensorInfo(c, r.dtype, u), n.makeTensorInfo([p.length], s.dtype, new Int32Array(p))];\n}\n\nvar sparseReshapeConfig$3 = {\n  kernelName: SparseReshape$1,\n  backendName: \"cpu\",\n  kernelFunc: sparseReshape$4\n};\n\nfunction sparseSegmentMean$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n          \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n          \".concat(s.shape));\n  var o = n.data.get(r.dataId).values,\n      i = n.data.get(a.dataId).values,\n      l = n.data.get(s.dataId).values,\n      [u, c] = sparseSegmentReductionImpl$1(o, r.shape, r.dtype, i, l, !0);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentMeanConfig$3 = {\n  kernelName: SparseSegmentMean$1,\n  backendName: \"cpu\",\n  kernelFunc: sparseSegmentMean$4\n};\n\nfunction sparseSegmentSum$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n         \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n         \".concat(s.shape));\n  var o = n.data.get(r.dataId).values,\n      i = n.data.get(a.dataId).values,\n      l = n.data.get(s.dataId).values,\n      [u, c] = sparseSegmentReductionImpl$1(o, r.shape, r.dtype, i, l);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentSumConfig$3 = {\n  kernelName: SparseSegmentSum$1,\n  backendName: \"cpu\",\n  kernelFunc: sparseSegmentSum$4\n};\n\nfunction sparseToDense$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    sparseIndices: a,\n    sparseValues: s,\n    defaultValue: o\n  } = t,\n      {\n    outputShape: i\n  } = r,\n      {\n    sliceRank: l,\n    numUpdates: u,\n    sliceSize: c,\n    strides: p,\n    outputSize: d\n  } = calculateShapes$1(s, a, i),\n      h = scatterImpl$1(n.bufferSync(a), n.bufferSync(s), i, d, c, u, l, p, n.data.get(o.dataId).values[0], !1);\n  return n.makeTensorInfo(i, h.dtype, h.values);\n}\n\nvar sparseToDenseConfig$3 = {\n  kernelName: SparseToDense$1,\n  backendName: \"cpu\",\n  kernelFunc: sparseToDense$4\n};\n\nfunction splitV$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    numOrSizeSplits: s,\n    axis: o\n  } = r,\n      i = parseAxisParam$1(o, a.shape)[0],\n      l = prepareSplitSize$1(a, s, i),\n      u = new Array(a.shape.length).fill(0),\n      c = a.shape.slice();\n  return l.map(e => {\n    var t = [...c];\n    t[i] = e;\n    var r = slice$4({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        begin: u,\n        size: t\n      }\n    });\n    return u[i] += e, r;\n  });\n}\n\nvar splitVConfig$3 = {\n  kernelName: SplitV$1,\n  backendName: \"cpu\",\n  kernelFunc: splitV$3\n},\n    sqrt$4 = unaryKernelFunc$3(Sqrt$1, e => Math.sqrt(e)),\n    sqrtConfig$3 = {\n  kernelName: Sqrt$1,\n  backendName: \"cpu\",\n  kernelFunc: sqrt$4\n},\n    squareConfig$3 = {\n  kernelName: Square$1,\n  backendName: \"cpu\",\n  kernelFunc: _ref13 => {\n    var {\n      inputs: e,\n      backend: t\n    } = _ref13;\n    var {\n      x: n\n    } = e,\n        r = t;\n    assertNotComplex$3(n, \"square\");\n    var a = r.data.get(n.dataId).values,\n        s = new Float32Array(a.length);\n\n    for (var _e400 = 0; _e400 < a.length; ++_e400) {\n      var _t288 = a[_e400];\n      s[_e400] = _t288 * _t288;\n    }\n\n    return {\n      dataId: r.write(s, n.shape, n.dtype),\n      shape: n.shape,\n      dtype: n.dtype\n    };\n  }\n},\n    step$4 = unaryKernelFunc$3(Step$1, (e, t) => {\n  var n = t;\n  return isNaN(e) ? NaN : e > 0 ? 1 : n.alpha;\n}),\n    stepConfig$3 = {\n  kernelName: Step$1,\n  backendName: \"cpu\",\n  kernelFunc: step$4\n};\n\nfunction stridedSlice$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    end: o,\n    strides: i,\n    beginMask: l,\n    endMask: u,\n    ellipsisMask: c,\n    newAxisMask: p,\n    shrinkAxisMask: d\n  } = r;\n  assertNotComplex$3(a, \"stridedSlice\");\n  var {\n    nonStrided: h,\n    $begin: m,\n    $strides: f,\n    size: g,\n    newShape: $,\n    outShape: y\n  } = sliceInfo$1(a.shape, s, o, i, l, u, c, p, d),\n      b = reshape$5({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: $\n    }\n  });\n  var x;\n\n  if (h) {\n    var _e401 = slice$4({\n      inputs: {\n        x: b\n      },\n      backend: n,\n      attrs: {\n        begin: m,\n        size: g\n      }\n    });\n\n    x = reshape$5({\n      inputs: {\n        x: _e401\n      },\n      backend: n,\n      attrs: {\n        shape: y\n      }\n    }), n.disposeIntermediateTensorInfo(_e401);\n  } else if (y.some(e => 0 === e)) x = n.makeTensorInfo(y, a.dtype, []);else {\n    var _e402 = stridedSliceImpl$1(y, n.bufferSync(b), f, m);\n\n    x = n.makeTensorInfo(_e402.shape, _e402.dtype, _e402.values);\n  }\n\n  var v = reshape$5({\n    inputs: {\n      x\n    },\n    backend: n,\n    attrs: {\n      shape: y\n    }\n  });\n  return n.disposeIntermediateTensorInfo(b), n.disposeIntermediateTensorInfo(x), v;\n}\n\nvar stridedSliceConfig$3 = {\n  kernelName: StridedSlice$1,\n  backendName: \"cpu\",\n  kernelFunc: stridedSlice$4\n};\n\nfunction stringNGrams$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    separator: a,\n    nGramWidths: s,\n    leftPad: o,\n    rightPad: i,\n    padWidth: l,\n    preserveShortSequences: u\n  } = r,\n      {\n    data: c,\n    dataSplits: p\n  } = t,\n      d = n.data.get(c.dataId).values,\n      h = n.data.get(p.dataId).values,\n      [m, f] = stringNGramsImpl$1(d, h, a, s, o, i, l, u);\n  return [n.makeTensorInfo([m.length], \"string\", m), n.makeTensorInfo(p.shape, \"int32\", f)];\n}\n\nvar stringNGramsConfig$3 = {\n  kernelName: StringNGrams$1,\n  backendName: \"cpu\",\n  kernelFunc: stringNGrams$4\n};\n\nfunction stringSplit$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    skipEmpty: a\n  } = r,\n      {\n    input: s,\n    delimiter: o\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (1 !== s.shape.length) throw new Error(\"Input must be a vector, got shape: \".concat(s.shape));\n  if (0 !== o.shape.length) throw new Error(\"Delimiter must be a scalar, got shape: \".concat(o.shape));\n  var i = n.data.get(s.dataId).values,\n      l = n.data.get(o.dataId).values[0],\n      [u, c, p] = stringSplitImpl$1(i, l, a),\n      d = c.length;\n  return [n.makeTensorInfo([d, 2], \"int32\", u), n.makeTensorInfo([d], \"string\", c), n.makeTensorInfo([2], \"int32\", new Int32Array(p))];\n}\n\nvar stringSplitConfig$3 = {\n  kernelName: StringSplit$1,\n  backendName: \"cpu\",\n  kernelFunc: stringSplit$4\n};\n\nfunction stringToHashBucketFast$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    numBuckets: a\n  } = r,\n      {\n    input: s\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (a <= 0) throw new Error(\"Number of buckets must be at least 1\");\n  var o = stringToHashBucketFastImpl$1(n.data.get(s.dataId).values, a);\n  return n.makeTensorInfo(s.shape, \"int32\", o);\n}\n\nvar stringToHashBucketFastConfig$3 = {\n  kernelName: StringToHashBucketFast$1,\n  backendName: \"cpu\",\n  kernelFunc: stringToHashBucketFast$4\n},\n    tan$4 = unaryKernelFunc$3(Tan$1, e => Math.tan(e)),\n    tanConfig$3 = {\n  kernelName: Tan$1,\n  backendName: \"cpu\",\n  kernelFunc: tan$4\n},\n    tanh$5 = unaryKernelFunc$3(Tanh$3, e => Math.tanh(e)),\n    tanhConfig$3 = {\n  kernelName: Tanh$3,\n  backendName: \"cpu\",\n  kernelFunc: tanh$5\n};\n\nfunction tile$5(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reps: s\n  } = r;\n  assertNotComplex$3(a, \"tile\");\n  var o = tileImpl$1(n.bufferSync(a), s);\n  return n.makeTensorInfo(o.shape, o.dtype, o.values);\n}\n\nvar tileConfig$3 = {\n  kernelName: Tile$1,\n  backendName: \"cpu\",\n  kernelFunc: tile$5\n};\n\nfunction topK$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    k: s,\n    sorted: o\n  } = r;\n  assertNotComplex$3(a, \"topk\");\n  var i = n.data.get(a.dataId).values,\n      [l, u] = topKImpl$1(i, a.shape, a.dtype, s, o);\n  return [n.makeTensorInfo(l.shape, l.dtype, l.values), n.makeTensorInfo(u.shape, u.dtype, u.values)];\n}\n\nvar topKConfig$3 = {\n  kernelName: TopK$1,\n  backendName: \"cpu\",\n  kernelFunc: topK$3\n};\n\nfunction transform$4(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    image: a,\n    transforms: s\n  } = t,\n      {\n    interpolation: o,\n    fillMode: i,\n    fillValue: l,\n    outputShape: u\n  } = n,\n      [c, p, d, h] = a.shape,\n      [m, f] = null != u ? u : [p, d],\n      g = [c, m, f, h],\n      $ = computeStrides$1(a.shape),\n      y = $[0],\n      b = $[1],\n      x = $[2],\n      v = getTypedArrayFromDType$1(a.dtype, sizeFromShape$1(g));\n  v.fill(l);\n  var I = r.data.get(a.dataId).values,\n      C = r.data.get(s.dataId).values;\n\n  for (var _e403 = 0; _e403 < c; ++_e403) {\n    var _t289 = 1 === s.shape[0] ? C : C.subarray(8 * _e403, 8 * _e403 + 8);\n\n    for (var _n192 = 0; _n192 < m; ++_n192) {\n      for (var _r145 = 0; _r145 < f; ++_r145) {\n        for (var _a105 = 0; _a105 < h; ++_a105) {\n          var _s74 = void 0;\n\n          var _u23 = _t289[6] * _r145 + _t289[7] * _n192 + 1;\n\n          if (0 === _u23) continue;\n\n          var _c17 = (_t289[3] * _r145 + _t289[4] * _n192 + _t289[5]) / _u23,\n              _h10 = mapCoord$1((_t289[0] * _r145 + _t289[1] * _n192 + _t289[2]) / _u23, d, i),\n              _m4 = mapCoord$1(_c17, p, i);\n\n          switch (o) {\n            case \"nearest\":\n              _s74 = nearestInterpolation$1(I, p, d, y, b, x, _e403, _m4, _h10, _a105, l);\n              break;\n\n            case \"bilinear\":\n              _s74 = bilinearInterpolation$1(I, p, d, y, b, x, _e403, _m4, _h10, _a105, l);\n              break;\n\n            default:\n              throw new Error(\"Error in Transform: Expect 'nearest' or 'bilinear', but got \".concat(o));\n          }\n\n          v[_e403 * y + _n192 * b + _r145 * x + _a105] = _s74;\n        }\n      }\n    }\n\n    return r.makeTensorInfo(g, a.dtype, v);\n  }\n\n  return {\n    dataId: r.write(v, g, a.dtype),\n    shape: a.shape,\n    dtype: a.dtype\n  };\n}\n\nvar transformConfig$3 = {\n  kernelName: Transform$1,\n  backendName: \"cpu\",\n  kernelFunc: transform$4\n};\n\nfunction mapCoord$1(e, t, n) {\n  switch (n) {\n    case \"reflect\":\n      return mapCoordReflect$1(e, t);\n\n    case \"wrap\":\n      return mapCoordWrap$1(e, t);\n\n    case \"nearest\":\n      return mapCoordNearest$1(e, t);\n\n    case \"constant\":\n    default:\n      return mapCoordConstant$1(e);\n  }\n}\n\nfunction mapCoordReflect$1(e, t) {\n  var n = e;\n  if (n < 0) {\n    if (t <= 1) n = 0;else {\n      var _e404 = 2 * t;\n\n      n < _e404 && (n = _e404 * Math.trunc(-n / _e404) + n), n = n < -t ? n + _e404 : -n - 1;\n    }\n  } else if (n > t - 1) if (t <= 1) n = 0;else {\n    var _e405 = 2 * t;\n\n    n -= _e405 * Math.trunc(n / _e405), n >= t && (n = _e405 - n - 1);\n  }\n  return clamp$1(0, n, t - 1);\n}\n\nfunction mapCoordWrap$1(e, t) {\n  var n = e;\n  return n < 0 ? t <= 1 ? n = 0 : n += t * (Math.trunc(-n / (t - 1)) + 1) : n > t - 1 && (t <= 1 ? n = 0 : n -= t * Math.trunc(n / (t - 1))), clamp$1(0, n, t - 1);\n}\n\nfunction mapCoordConstant$1(e, t) {\n  return e;\n}\n\nfunction mapCoordNearest$1(e, t) {\n  return clamp$1(0, e, t - 1);\n}\n\nfunction readWithFillValue$1(e, t, n, r, a, s, o, i, l, u, c) {\n  return 0 <= i && i < t && 0 <= l && l < n ? e[o * r + i * a + l * s + u] : c;\n}\n\nfunction nearestInterpolation$1(e, t, n, r, a, s, o, i, l, u, c) {\n  return readWithFillValue$1(e, t, n, r, a, s, o, Math.round(i), Math.round(l), u, c);\n}\n\nfunction bilinearInterpolation$1(e, t, n, r, a, s, o, i, l, u, c) {\n  var p = Math.floor(i),\n      d = Math.floor(l),\n      h = p + 1,\n      m = d + 1;\n  return (h - i) * ((m - l) * readWithFillValue$1(e, t, n, r, a, s, o, p, d, u, c) + (l - d) * readWithFillValue$1(e, t, n, r, a, s, o, p, m, u, c)) + (i - p) * ((m - l) * readWithFillValue$1(e, t, n, r, a, s, o, h, d, u, c) + (l - d) * readWithFillValue$1(e, t, n, r, a, s, o, h, m, u, c));\n}\n\nfunction unique$5(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    axis: a\n  } = n,\n      {\n    x: s\n  } = t;\n  assertNotComplex$3(s, \"unique\");\n  var o = r.data.get(s.dataId).values,\n      {\n    outputValues: i,\n    outputShape: l,\n    indices: u\n  } = uniqueImpl$1(o, a, s.shape, s.dtype);\n  return [r.makeTensorInfo(l, s.dtype, i), r.makeTensorInfo([u.length], \"int32\", u)];\n}\n\nvar uniqueConfig$3 = {\n  kernelName: Unique$1,\n  backendName: \"cpu\",\n  kernelFunc: unique$5\n};\n\nfunction unpack$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    value: a\n  } = t;\n  var {\n    axis: s\n  } = r;\n  s < 0 && (s += a.shape.length);\n  var o = a.shape.length,\n      i = a.shape[s],\n      l = new Array(o - 1);\n  var u = 0;\n\n  for (var _e406 = 0; _e406 < o; _e406++) {\n    _e406 !== s && (l[u++] = a.shape[_e406]);\n  }\n\n  var c = new Array(o).fill(0),\n      p = a.shape.slice();\n  p[s] = 1;\n  var d = new Array(i);\n\n  for (var _e407 = 0; _e407 < d.length; _e407++) {\n    c[s] = _e407;\n\n    var _t290 = slice$4({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        begin: c,\n        size: p\n      }\n    });\n\n    d[_e407] = reshape$5({\n      inputs: {\n        x: _t290\n      },\n      backend: n,\n      attrs: {\n        shape: l\n      }\n    }), n.disposeIntermediateTensorInfo(_t290);\n  }\n\n  return d;\n}\n\nvar unpackConfig$3 = {\n  kernelName: Unpack$1,\n  backendName: \"cpu\",\n  kernelFunc: unpack$3\n};\n\nfunction unsortedSegmentSum$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    segmentIds: s\n  } = t,\n      {\n    numSegments: o\n  } = r;\n  assertNotComplex$3(a, \"unsortedSegmentSum\");\n  var i = [],\n      l = [],\n      u = a.shape.length - s.shape.length;\n  var c = s;\n\n  for (var _e408 = 0; _e408 < u; ++_e408) {\n    var _t291 = expandDims$5({\n      inputs: {\n        input: c\n      },\n      backend: n,\n      attrs: {\n        dim: _e408 + 1\n      }\n    });\n\n    c = _t291, l.push(_t291);\n  }\n\n  for (var _e409 = 0; _e409 < o; ++_e409) {\n    var _t292 = createScalarValue$1(_e409, \"int32\"),\n        _r146 = n.makeTensorInfo([], \"int32\", _t292),\n        _s75 = equal$4({\n      inputs: {\n        a: _r146,\n        b: c\n      },\n      backend: n\n    }),\n        _o53 = cast$5({\n      inputs: {\n        x: _s75\n      },\n      backend: n,\n      attrs: {\n        dtype: \"float32\"\n      }\n    }),\n        _u24 = multiply$4({\n      inputs: {\n        a: _o53,\n        b: a\n      },\n      backend: n\n    }),\n        _p14 = sum$5({\n      inputs: {\n        x: _u24\n      },\n      backend: n,\n      attrs: {\n        axis: 0,\n        keepDims: !1\n      }\n    });\n\n    i.push(_p14), l.push(_r146), l.push(_s75), l.push(_o53), l.push(_u24), l.push(_p14);\n  }\n\n  var p = pack$3({\n    inputs: i,\n    backend: n,\n    attrs: {\n      axis: 0\n    }\n  });\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), p;\n}\n\nvar unsortedSegmentSumConfig$3 = {\n  kernelName: UnsortedSegmentSum$1,\n  backendName: \"cpu\",\n  kernelFunc: unsortedSegmentSum$4\n},\n    kernelConfigs$3 = [_fusedMatMulConfig$3, absConfig$3, acosConfig$3, acoshConfig$3, addConfig$3, addNConfig$3, allConfig$3, anyConfig$3, argMaxConfig$3, argMinConfig$3, asinConfig$3, asinhConfig$3, atanConfig$3, atan2Config$3, atanhConfig$3, avgPoolConfig$3, avgPool3DConfig$3, avgPool3DGradConfig$2, avgPoolGradConfig$4, batchMatMulConfig$3, batchNormConfig$3, batchToSpaceNDConfig$3, bincountConfig$3, castConfig$3, ceilConfig$3, clipConfig$1, complexConfig$3, complexAbsConfig$3, concatConfig$3, conv2DBackpropFilterConfig$3, conv2DBackpropInputConfig$3, conv2DConfig$3, conv3DBackpropFilterV2Config$3, conv3DBackpropInputV2Config$1, conv3DConfig$3, cosConfig$3, coshConfig$3, cropAndResizeConfig$3, cumsumConfig$3, denseBincountConfig$3, depthToSpaceConfig$3, depthwiseConv2dNativeConfig$3, depthwiseConv2dNativeBackpropFilterConfig$3, depthwiseConv2dNativeBackpropInputConfig$3, diagConfig$3, dilation2dConfig$1, dilation2dBackpropInputConfig$1, dilation2dBackpropFilterConfig$1, realDivConfig$3, einsumConfig$3, eluConfig$3, eluGradConfig$4, equalConfig$3, erfConfig$3, expConfig$3, expandDimsConfig$3, expm1Config$3, fftConfig$3, fillConfig$3, flipLeftRightConfig$3, floorConfig$3, floorDivConfig$3, fusedConv2DConfig$3, fusedDepthwiseConv2DConfig$3, gatherNdConfig$3, gatherV2Config$3, greaterConfig$3, greaterEqualConfig$3, identityConfig$3, ifftConfig$3, imagConfig$3, isFiniteConfig$3, isInfConfig$3, isNaNConfig$3, leakyReluConfig$3, lessConfig$3, lessEqualConfig$3, linSpaceConfig$3, logConfig$3, log1pConfig$3, logicalAndConfig$3, logicalNotConfig$3, logicalOrConfig$3, lRNConfig$1, lRNGradConfig$1, maximumConfig$3, maxPoolConfig$3, maxPool3DConfig$3, maxPool3DGradConfig$2, maxPoolGradConfig$4, maxPoolWithArgmaxConfig$3, maxConfig$3, meanConfig$3, minConfig$3, minimumConfig$3, mirrorPadConfig$3, modConfig$3, multinomialConfig$3, multiplyConfig$3, negConfig$3, nonMaxSuppressionV3Config$3, nonMaxSuppressionV4Config$3, nonMaxSuppressionV5Config$3, notEqualConfig$3, oneHotConfig$3, onesLikeConfig$3, packConfig$3, padV2Config$3, powConfig$3, preluConfig$3, prodConfig$3, rangeConfig$3, realConfig$3, reciprocalConfig$3, reluConfig$3, relu6Config$3, reshapeConfig$3, resizeBilinearConfig$3, resizeBilinearGradConfig$4, resizeNearestNeighborConfig$3, resizeNearestNeighborGradConfig$4, reverseConfig$3, rotateWithOffsetConfig$3, roundConfig$3, rsqrtConfig$3, scatterNdConfig$3, selectConfig$3, seluConfig$3, sigmoidConfig$3, signConfig$3, sinConfig$3, sinhConfig$3, sliceConfig$3, softmaxConfig$3, softplusConfig$3, spaceToBatchNDConfig$3, sparseFillEmptyRowsConfig$3, sparseReshapeConfig$3, sparseSegmentMeanConfig$3, sparseSegmentSumConfig$3, sparseToDenseConfig$3, splitVConfig$3, sqrtConfig$3, squareConfig$3, squaredDifferenceConfig$3, stepConfig$3, stridedSliceConfig$3, stringNGramsConfig$3, stringSplitConfig$3, stringToHashBucketFastConfig$3, subConfig$3, sumConfig$3, tanConfig$3, tanhConfig$3, tileConfig$3, topKConfig$3, transposeConfig$3, transformConfig$3, uniqueConfig$3, unpackConfig$3, unsortedSegmentSumConfig$3, zerosLikeConfig$3];\n\nfor (var _e410 of kernelConfigs$3) {\n  registerKernel$1(_e410);\n}\n\nvar contexts$1 = {},\n    WEBGL_ATTRIBUTES$1 = {\n  alpha: !1,\n  antialias: !1,\n  premultipliedAlpha: !1,\n  preserveDrawingBuffer: !1,\n  depth: !1,\n  stencil: !1,\n  failIfMajorPerformanceCaveat: !0\n};\n\nfunction setWebGLContext$1(e, t) {\n  contexts$1[e] = t;\n}\n\nfunction getWebGLContext$1(e) {\n  if (!(e in contexts$1)) {\n    var _t293 = getWebGLRenderingContext$1(e);\n\n    if (null === _t293) return console.log(\"Could not get context for WebGL version\", e), null;\n    contexts$1[e] = _t293;\n  }\n\n  var t = contexts$1[e];\n  return t.isContextLost() ? (delete contexts$1[e], getWebGLContext$1(e)) : (t.disable(t.DEPTH_TEST), t.disable(t.STENCIL_TEST), t.disable(t.BLEND), t.disable(t.DITHER), t.disable(t.POLYGON_OFFSET_FILL), t.disable(t.SAMPLE_COVERAGE), t.enable(t.SCISSOR_TEST), t.enable(t.CULL_FACE), t.cullFace(t.BACK), contexts$1[e]);\n}\n\nfunction createCanvas$1(e) {\n  if (\"undefined\" != typeof OffscreenCanvas && 2 === e) return new OffscreenCanvas(300, 150);\n  if (\"undefined\" != typeof document) return document.createElement(\"canvas\");\n  throw new Error(\"Cannot create a canvas in this context\");\n}\n\nfunction getWebGLRenderingContext$1(e) {\n  if (1 !== e && 2 !== e) throw new Error(\"Cannot get WebGL rendering context, WebGL is disabled.\");\n  var t = createCanvas$1(e);\n  return t.addEventListener(\"webglcontextlost\", t => {\n    t.preventDefault(), delete contexts$1[e];\n  }, !1), 1 === e ? t.getContext(\"webgl\", WEBGL_ATTRIBUTES$1) || t.getContext(\"experimental-webgl\", WEBGL_ATTRIBUTES$1) : t.getContext(\"webgl2\", WEBGL_ATTRIBUTES$1);\n}\n\nvar PackingScheme$1, TextureUsage$1, PhysicalTextureType$1;\n\nfunction getUnpackedMatrixTextureShapeWidthHeight$1(e, t) {\n  return [t, e];\n}\n\nfunction getUnpackedArraySizeFromMatrixSize$1(e, t) {\n  return e * t;\n}\n\nfunction getDenseTexShape$1(e) {\n  var t = sizeFromShape$1(e);\n  return sizeToSquarishShape$1(Math.ceil(t / 4));\n}\n\nfunction getPackedMatrixTextureShapeWidthHeight$1(e, t) {\n  return [Math.max(1, Math.ceil(t / 2)), Math.max(1, Math.ceil(e / 2))];\n}\n\nfunction getPackedRGBAArraySizeFromMatrixShape$1(e, t) {\n  var [n, r] = getPackedMatrixTextureShapeWidthHeight$1(e, t);\n  return n * r * 4;\n}\n\nfunction getTextureConfig$1(e, t) {\n  var n = e;\n  var r, a, s, o, i, l, u, c, p, d;\n  return 2 === env$1().getNumber(\"WEBGL_VERSION\") ? (r = n.R32F, a = n.R16F, s = n.RGBA16F, o = n.RGBA32F, i = n.RED, u = 4, c = 1, p = n.HALF_FLOAT, d = n.FLOAT) : (r = e.RGBA, a = e.RGBA, s = e.RGBA, o = n.RGBA, i = e.RGBA, u = 4, c = 4, p = null != t ? t.HALF_FLOAT_OES : null, d = e.FLOAT), l = e.RGBA, {\n    internalFormatFloat: r,\n    internalFormatHalfFloat: a,\n    internalFormatPackedHalfFloat: s,\n    internalFormatPackedFloat: o,\n    textureFormatFloat: i,\n    downloadTextureFormat: l,\n    downloadUnpackNumChannels: u,\n    defaultNumChannels: c,\n    textureTypeHalfFloat: p,\n    textureTypeFloat: d\n  };\n}\n\nfunction callAndCheck$1(e, t) {\n  var n = t();\n  return env$1().getBool(\"DEBUG\") && checkWebGLError$1(e), n;\n}\n\nfunction checkWebGLError$1(e) {\n  var t = e.getError();\n  if (t !== e.NO_ERROR) throw new Error(\"WebGL Error: \" + getWebGLErrorMessage$1(e, t));\n}\n\n!function (e) {\n  e[e.DENSE = 0] = \"DENSE\", e[e.SHARED_BATCH = 1] = \"SHARED_BATCH\";\n}(PackingScheme$1 || (PackingScheme$1 = {})), function (e) {\n  e[e.RENDER = 0] = \"RENDER\", e[e.UPLOAD = 1] = \"UPLOAD\", e[e.PIXELS = 2] = \"PIXELS\", e[e.DOWNLOAD = 3] = \"DOWNLOAD\";\n}(TextureUsage$1 || (TextureUsage$1 = {})), function (e) {\n  e[e.UNPACKED_FLOAT16 = 0] = \"UNPACKED_FLOAT16\", e[e.UNPACKED_FLOAT32 = 1] = \"UNPACKED_FLOAT32\", e[e.PACKED_4X1_UNSIGNED_BYTE = 2] = \"PACKED_4X1_UNSIGNED_BYTE\", e[e.PACKED_2X2_FLOAT32 = 3] = \"PACKED_2X2_FLOAT32\", e[e.PACKED_2X2_FLOAT16 = 4] = \"PACKED_2X2_FLOAT16\";\n}(PhysicalTextureType$1 || (PhysicalTextureType$1 = {}));\nvar MIN_FLOAT16$1 = 5.96e-8,\n    MAX_FLOAT16$1 = 65504;\n\nfunction canBeRepresented$1(e) {\n  return !!(env$1().getBool(\"WEBGL_RENDER_FLOAT32_ENABLED\") || 0 === e || MIN_FLOAT16$1 < Math.abs(e) && Math.abs(e) < MAX_FLOAT16$1);\n}\n\nfunction getWebGLErrorMessage$1(e, t) {\n  switch (t) {\n    case e.NO_ERROR:\n      return \"NO_ERROR\";\n\n    case e.INVALID_ENUM:\n      return \"INVALID_ENUM\";\n\n    case e.INVALID_VALUE:\n      return \"INVALID_VALUE\";\n\n    case e.INVALID_OPERATION:\n      return \"INVALID_OPERATION\";\n\n    case e.INVALID_FRAMEBUFFER_OPERATION:\n      return \"INVALID_FRAMEBUFFER_OPERATION\";\n\n    case e.OUT_OF_MEMORY:\n      return \"OUT_OF_MEMORY\";\n\n    case e.CONTEXT_LOST_WEBGL:\n      return \"CONTEXT_LOST_WEBGL\";\n\n    default:\n      return \"Unknown error code \".concat(t);\n  }\n}\n\nfunction getExtensionOrThrow$1(e, t) {\n  return throwIfNull$1(e, () => e.getExtension(t), 'Extension \"' + t + '\" not supported on this browser.');\n}\n\nfunction createVertexShader$3(e, t) {\n  var n = throwIfNull$1(e, () => e.createShader(e.VERTEX_SHADER), \"Unable to create vertex WebGLShader.\");\n  if (callAndCheck$1(e, () => e.shaderSource(n, t)), callAndCheck$1(e, () => e.compileShader(n)), !1 === e.getShaderParameter(n, e.COMPILE_STATUS)) throw console.log(e.getShaderInfoLog(n)), new Error(\"Failed to compile vertex shader.\");\n  return n;\n}\n\nfunction createFragmentShader$1(e, t) {\n  var n = throwIfNull$1(e, () => e.createShader(e.FRAGMENT_SHADER), \"Unable to create fragment WebGLShader.\");\n  if (callAndCheck$1(e, () => e.shaderSource(n, t)), callAndCheck$1(e, () => e.compileShader(n)), !1 === e.getShaderParameter(n, e.COMPILE_STATUS)) throw logShaderSourceAndInfoLog$1(t, e.getShaderInfoLog(n)), new Error(\"Failed to compile fragment shader.\");\n  return n;\n}\n\nvar lineNumberRegex$1 = /ERROR: [0-9]+:([0-9]+):/g;\n\nfunction logShaderSourceAndInfoLog$1(e, t) {\n  var n = lineNumberRegex$1.exec(t);\n  if (null == n) return console.log(\"Couldn't parse line number in error: \".concat(t)), void console.log(e);\n  var r = +n[1],\n      a = e.split(\"\\n\"),\n      s = a.length.toString().length + 2,\n      o = a.map((e, t) => rightPad$1((t + 1).toString(), s) + e);\n  var i = 0;\n\n  for (var _e411 = 0; _e411 < o.length; _e411++) {\n    i = Math.max(o[_e411].length, i);\n  }\n\n  var l = o.slice(0, r - 1),\n      u = o.slice(r - 1, r),\n      c = o.slice(r);\n  console.log(l.join(\"\\n\")), console.log(t.split(\"\\n\")[0]), console.log(\"%c \".concat(rightPad$1(u[0], i)), \"border:1px solid red; background-color:#e3d2d2; color:#a61717\"), console.log(c.join(\"\\n\"));\n}\n\nfunction createProgram$1(e) {\n  return throwIfNull$1(e, () => e.createProgram(), \"Unable to create WebGLProgram.\");\n}\n\nfunction linkProgram$1(e, t) {\n  if (callAndCheck$1(e, () => e.linkProgram(t)), !1 === e.getProgramParameter(t, e.LINK_STATUS)) throw console.log(e.getProgramInfoLog(t)), new Error(\"Failed to link vertex and fragment shaders.\");\n}\n\nfunction validateProgram$1(e, t) {\n  if (callAndCheck$1(e, () => e.validateProgram(t)), !1 === e.getProgramParameter(t, e.VALIDATE_STATUS)) throw console.log(e.getProgramInfoLog(t)), new Error(\"Shader program validation failed.\");\n}\n\nfunction createStaticVertexBuffer$1(e, t) {\n  var n = throwIfNull$1(e, () => e.createBuffer(), \"Unable to create WebGLBuffer\");\n  return callAndCheck$1(e, () => e.bindBuffer(e.ARRAY_BUFFER, n)), callAndCheck$1(e, () => e.bufferData(e.ARRAY_BUFFER, t, e.STATIC_DRAW)), n;\n}\n\nfunction createStaticIndexBuffer$1(e, t) {\n  var n = throwIfNull$1(e, () => e.createBuffer(), \"Unable to create WebGLBuffer\");\n  return callAndCheck$1(e, () => e.bindBuffer(e.ELEMENT_ARRAY_BUFFER, n)), callAndCheck$1(e, () => e.bufferData(e.ELEMENT_ARRAY_BUFFER, t, e.STATIC_DRAW)), n;\n}\n\nfunction createTexture$1(e) {\n  return throwIfNull$1(e, () => e.createTexture(), \"Unable to create WebGLTexture.\");\n}\n\nfunction validateTextureSize$1(e, t) {\n  var n = env$1().getNumber(\"WEBGL_MAX_TEXTURE_SIZE\");\n  if (e <= 0 || t <= 0) throw new Error(\"Requested texture size [\".concat(e, \"x\").concat(t, \"] is invalid.\"));\n  if (e > n || t > n) throw new Error(\"Requested texture size [\".concat(e, \"x\").concat(t, \"] greater than WebGL maximum on this browser / GPU [\").concat(n, \"x\").concat(n, \"].\"));\n}\n\nfunction createFramebuffer$1(e) {\n  return throwIfNull$1(e, () => e.createFramebuffer(), \"Unable to create WebGLFramebuffer.\");\n}\n\nfunction bindVertexBufferToProgramAttribute$1(e, t, n, r, a, s, o) {\n  var i = e.getAttribLocation(t, n);\n  return -1 !== i && (callAndCheck$1(e, () => e.bindBuffer(e.ARRAY_BUFFER, r)), callAndCheck$1(e, () => e.vertexAttribPointer(i, a, e.FLOAT, !1, s, o)), callAndCheck$1(e, () => e.enableVertexAttribArray(i)), !0);\n}\n\nfunction bindTextureUnit$1(e, t, n) {\n  validateTextureUnit$1(e, n), callAndCheck$1(e, () => e.activeTexture(e.TEXTURE0 + n)), callAndCheck$1(e, () => e.bindTexture(e.TEXTURE_2D, t));\n}\n\nfunction getProgramUniformLocationOrThrow$1(e, t, n) {\n  return throwIfNull$1(e, () => e.getUniformLocation(t, n), 'uniform \"' + n + '\" not present in program.');\n}\n\nfunction getProgramUniformLocation$1(e, t, n) {\n  return e.getUniformLocation(t, n);\n}\n\nfunction bindTextureToProgramUniformSampler$1(e, t, n, r) {\n  callAndCheck$1(e, () => bindTextureUnit$1(e, t, r)), callAndCheck$1(e, () => e.uniform1i(n, r));\n}\n\nfunction bindColorTextureToFramebuffer$1(e, t, n) {\n  callAndCheck$1(e, () => e.bindFramebuffer(e.FRAMEBUFFER, n)), callAndCheck$1(e, () => e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, t, 0));\n}\n\nfunction unbindColorTextureFromFramebuffer$1(e, t) {\n  callAndCheck$1(e, () => e.bindFramebuffer(e.FRAMEBUFFER, t)), callAndCheck$1(e, () => e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, null, 0));\n}\n\nfunction validateFramebuffer$1(e) {\n  var t = e.checkFramebufferStatus(e.FRAMEBUFFER);\n  if (t !== e.FRAMEBUFFER_COMPLETE) throw new Error(\"Error binding framebuffer: \" + getFramebufferErrorMessage$1(e, t));\n}\n\nfunction getFramebufferErrorMessage$1(e, t) {\n  switch (t) {\n    case e.FRAMEBUFFER_INCOMPLETE_ATTACHMENT:\n      return \"FRAMEBUFFER_INCOMPLETE_ATTACHMENT\";\n\n    case e.FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT:\n      return \"FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT\";\n\n    case e.FRAMEBUFFER_INCOMPLETE_DIMENSIONS:\n      return \"FRAMEBUFFER_INCOMPLETE_DIMENSIONS\";\n\n    case e.FRAMEBUFFER_UNSUPPORTED:\n      return \"FRAMEBUFFER_UNSUPPORTED\";\n\n    default:\n      return \"unknown error \".concat(t);\n  }\n}\n\nfunction throwIfNull$1(e, t, n) {\n  var r = callAndCheck$1(e, () => t());\n  if (null == r) throw new Error(n);\n  return r;\n}\n\nfunction validateTextureUnit$1(e, t) {\n  var n = e.MAX_COMBINED_TEXTURE_IMAGE_UNITS - 1,\n      r = t + e.TEXTURE0;\n  if (r < e.TEXTURE0 || r > n) throw new Error(\"textureUnit must be in [gl.TEXTURE0, gl.TEXTURE\".concat(n, \"].\"));\n}\n\nfunction getBatchDim$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 2;\n  return sizeFromShape$1(e.slice(0, e.length - t));\n}\n\nfunction getRowsCols$1(e) {\n  if (0 === e.length) throw Error(\"Cannot get rows and columns of an empty shape array.\");\n  return [e.length > 1 ? e[e.length - 2] : 1, e[e.length - 1]];\n}\n\nfunction getShapeAs3D$1(e) {\n  var t = [1, 1, 1];\n  return 0 === e.length || 1 === e.length && 1 === e[0] || (t = [getBatchDim$1(e), ...getRowsCols$1(e)]), t;\n}\n\nfunction getTextureShapeFromLogicalShape$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  var n = env$1().getNumber(\"WEBGL_MAX_TEXTURE_SIZE\");\n\n  if (t && (n *= 2, 1 === (e = e.map((t, n) => n >= e.length - 2 ? nearestLargerEven$1(e[n]) : e[n])).length && (e = [2, e[0]])), 2 !== e.length) {\n    var _t294 = squeezeShape$1(e);\n\n    e = _t294.newShape;\n  }\n\n  var r = sizeFromShape$1(e);\n  if (e.length <= 1 && r <= n) return [1, r];\n  if (2 === e.length && e[0] <= n && e[1] <= n) return e;\n  if (3 === e.length && e[0] * e[1] <= n && e[2] <= n) return [e[0] * e[1], e[2]];\n  if (3 === e.length && e[0] <= n && e[1] * e[2] <= n) return [e[0], e[1] * e[2]];\n  if (4 === e.length && e[0] * e[1] * e[2] <= n && e[3] <= n) return [e[0] * e[1] * e[2], e[3]];\n  if (4 === e.length && e[0] <= n && e[1] * e[2] * e[3] <= n) return [e[0], e[1] * e[2] * e[3]];\n\n  if (t) {\n    var _t295 = getBatchDim$1(e);\n\n    var _n193 = 2,\n        a = 2;\n    return e.length && ([_n193, a] = getRowsCols$1(e)), r = _t295 * (_n193 / 2) * (a / 2), sizeToSquarishShape$1(r).map(e => 2 * e);\n  }\n\n  return sizeToSquarishShape$1(r);\n}\n\nfunction isEven$1(e) {\n  return e % 2 == 0;\n}\n\nfunction isReshapeFree$1(e, t) {\n  if (arraysEqual$1(e = e.slice(-2), t = t.slice(-2))) return !0;\n  if (!e.length || !t.length) return !0;\n  if (0 === e[0] || 0 === e[1] || 0 === t[0] || 0 === t[1]) return !0;\n\n  if (e.length !== t.length) {\n    var n = e.slice(-1)[0],\n        r = t.slice(-1)[0];\n    if (n === r) return !0;\n    if (isEven$1(n) && isEven$1(r) && (1 === e[0] || 1 === t[0])) return !0;\n  }\n\n  return e[1] === t[1] && isEven$1(e[0]) && isEven$1(t[0]);\n}\n\nvar MAX_TEXTURE_SIZE$1, MAX_TEXTURES_IN_SHADER$1;\n\nfunction getWebGLMaxTextureSize$1(e) {\n  if (null == MAX_TEXTURE_SIZE$1) {\n    var t = getWebGLContext$1(e);\n    MAX_TEXTURE_SIZE$1 = t.getParameter(t.MAX_TEXTURE_SIZE);\n  }\n\n  return MAX_TEXTURE_SIZE$1;\n}\n\nfunction getMaxTexturesInShader$1(e) {\n  if (null == MAX_TEXTURES_IN_SHADER$1) {\n    var t = getWebGLContext$1(e);\n    MAX_TEXTURES_IN_SHADER$1 = t.getParameter(t.MAX_TEXTURE_IMAGE_UNITS);\n  }\n\n  return Math.min(16, MAX_TEXTURES_IN_SHADER$1);\n}\n\nfunction getWebGLDisjointQueryTimerVersion$1(e) {\n  if (0 === e) return 0;\n  var t;\n  var n = getWebGLContext$1(e);\n  return t = hasExtension$1(n, \"EXT_disjoint_timer_query_webgl2\") && 2 === e ? 2 : hasExtension$1(n, \"EXT_disjoint_timer_query\") ? 1 : 0, t;\n}\n\nfunction hasExtension$1(e, t) {\n  return null != e.getExtension(t);\n}\n\nfunction isWebGLVersionEnabled$1(e) {\n  try {\n    if (null != getWebGLContext$1(e)) return !0;\n  } catch (e) {\n    return console.log(\"Error when getting WebGL context: \", e), !1;\n  }\n\n  return !1;\n}\n\nfunction isCapableOfRenderingToFloatTexture$1(e) {\n  if (0 === e) return !1;\n  var t = getWebGLContext$1(e);\n\n  if (1 === e) {\n    if (!hasExtension$1(t, \"OES_texture_float\")) return !1;\n  } else if (!hasExtension$1(t, \"EXT_color_buffer_float\")) return !1;\n\n  return createFloatTextureAndBindToFramebuffer$1(t);\n}\n\nfunction isDownloadFloatTextureEnabled$1(e) {\n  if (0 === e) return !1;\n  var t = getWebGLContext$1(e);\n\n  if (1 !== e) {\n    if (hasExtension$1(t, \"EXT_color_buffer_float\")) return createFloatTextureAndBindToFramebuffer$1(t);\n    var _e412 = \"EXT_color_buffer_half_float\";\n\n    if (hasExtension$1(t, _e412)) {\n      var n = t.getExtension(_e412);\n      return createHalfFloatTextureAndBindToFramebuffer$1(t, n);\n    }\n\n    return !1;\n  }\n\n  return !!hasExtension$1(t, \"OES_texture_float\") && !!hasExtension$1(t, \"WEBGL_color_buffer_float\") && createFloatTextureAndBindToFramebuffer$1(t);\n}\n\nfunction createFloatTextureAndBindToFramebuffer$1(e) {\n  var t = getTextureConfig$1(e),\n      n = e.createTexture();\n  e.bindTexture(e.TEXTURE_2D, n), e.texImage2D(e.TEXTURE_2D, 0, t.internalFormatFloat, 1, 1, 0, t.textureFormatFloat, t.textureTypeFloat, null);\n  var r = e.createFramebuffer();\n  e.bindFramebuffer(e.FRAMEBUFFER, r), e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, n, 0);\n  var a = e.checkFramebufferStatus(e.FRAMEBUFFER) === e.FRAMEBUFFER_COMPLETE;\n  return e.bindTexture(e.TEXTURE_2D, null), e.bindFramebuffer(e.FRAMEBUFFER, null), e.deleteTexture(n), e.deleteFramebuffer(r), a;\n}\n\nfunction createHalfFloatTextureAndBindToFramebuffer$1(e, t) {\n  var n = getTextureConfig$1(e, t),\n      r = e.createTexture();\n  e.bindTexture(e.TEXTURE_2D, r), e.texImage2D(e.TEXTURE_2D, 0, n.internalFormatHalfFloat, 1, 1, 0, n.textureFormatFloat, n.textureTypeHalfFloat, null);\n  var a = e.createFramebuffer();\n  e.bindFramebuffer(e.FRAMEBUFFER, a), e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, r, 0);\n  var s = e.checkFramebufferStatus(e.FRAMEBUFFER) === e.FRAMEBUFFER_COMPLETE;\n  return e.bindTexture(e.TEXTURE_2D, null), e.bindFramebuffer(e.FRAMEBUFFER, null), e.deleteTexture(r), e.deleteFramebuffer(a), s;\n}\n\nfunction isWebGLFenceEnabled$1(e) {\n  return 2 === e && null != getWebGLContext$1(e).fenceSync;\n}\n\nfunction assertNotComplex$2(e, t) {\n  Array.isArray(e) || (e = [e]), e.forEach(e => {\n    null != e && assert$6(\"complex64\" !== e.dtype, () => \"\".concat(t, \" does not support complex64 tensors in the WebGL backend.\"));\n  });\n}\n\nvar ENV$3 = env$1();\n\nfunction getGlslDifferences$1() {\n  var e, t, n, r, a, s, o, i, l, u;\n  return 2 === env$1().getNumber(\"WEBGL_VERSION\") ? (e = \"#version 300 es\", t = \"in\", n = \"out\", r = \"in\", a = \"texture\", s = \"outputColor\", o = \"out vec4 outputColor;\", i = \"\\n      bool isnan_custom(float val) {\\n        return (val > 0.0 || val < 0.0) ? false : val != 0.0;\\n      }\\n\\n      bvec4 isnan_custom(vec4 val) {\\n        return bvec4(isnan_custom(val.x),\\n          isnan_custom(val.y), isnan_custom(val.z), isnan_custom(val.w));\\n      }\\n\\n      #define isnan(value) isnan_custom(value)\\n    \", l = \"\", u = \"\\n      #define round(value) newRound(value)\\n      int newRound(float value) {\\n        return int(floor(value + 0.5));\\n      }\\n\\n      ivec4 newRound(vec4 value) {\\n        return ivec4(floor(value + vec4(0.5)));\\n      }\\n    \") : (e = \"\", t = \"attribute\", n = \"varying\", r = \"varying\", a = \"texture2D\", s = \"gl_FragColor\", o = \"\", i = \"\\n      #define isnan(value) isnan_custom(value)\\n      bool isnan_custom(float val) {\\n        return (val > 0. || val < 1. || val == 0.) ? false : true;\\n      }\\n      bvec4 isnan_custom(vec4 val) {\\n        return bvec4(isnan(val.x), isnan(val.y), isnan(val.z), isnan(val.w));\\n      }\\n    \", l = \"\\n      uniform float INFINITY;\\n\\n      bool isinf(float val) {\\n        return abs(val) == INFINITY;\\n      }\\n      bvec4 isinf(vec4 val) {\\n        return equal(abs(val), vec4(INFINITY));\\n      }\\n    \", u = \"\\n      int round(float value) {\\n        return int(floor(value + 0.5));\\n      }\\n\\n      ivec4 round(vec4 value) {\\n        return ivec4(floor(value + vec4(0.5)));\\n      }\\n    \"), {\n    version: e,\n    attribute: t,\n    varyingVs: n,\n    varyingFs: r,\n    texture2D: a,\n    output: s,\n    defineOutput: o,\n    defineSpecialNaN: i,\n    defineSpecialInf: l,\n    defineRound: u\n  };\n}\n\nfunction getLogicalCoordinatesFromFlatIndex$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"index\";\n  var r = computeStrides$1(t);\n  return r.map((t, a) => \"int \".concat(e[a], \" = \").concat(n, \" / \").concat(t, \"; \").concat(a === r.length - 1 ? \"int \".concat(e[a + 1], \" = \").concat(n, \" - \").concat(e[a], \" * \").concat(t) : \"index -= \".concat(e[a], \" * \").concat(t), \";\")).join(\"\");\n}\n\nfunction getLogicalCoordinatesFromFlatIndexByUniform$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"index\";\n  var r = computeStrides$1(t);\n  return r.map((t, a) => \"int \".concat(e[a], \" = \").concat(n, \" / outShapeStrides[\").concat(a, \"]; \").concat(a === r.length - 1 ? \"int \".concat(e[a + 1], \" = \").concat(n, \" - \").concat(e[a], \" * outShapeStrides[\").concat(a, \"]\") : \"index -= \".concat(e[a], \" * outShapeStrides[\").concat(a, \"]\"), \";\")).join(\"\");\n}\n\nfunction getFlatIndexFrom3D$1(e) {\n  var t = computeStrides$1(e).map(e => e.toString());\n  return \"\\n  int getFlatIndex(ivec3 coords) {\\n    return coords.x * \".concat(t[0], \" + coords.y * \").concat(t[1], \" + coords.z;\\n  }\\n\");\n}\n\nENV$3.registerFlag(\"HAS_WEBGL\", () => ENV$3.getNumber(\"WEBGL_VERSION\") > 0), ENV$3.registerFlag(\"WEBGL_VERSION\", () => isWebGLVersionEnabled$1(2) ? 2 : isWebGLVersionEnabled$1(1) ? 1 : 0), ENV$3.registerFlag(\"WEBGL_CHECK_NUMERICAL_PROBLEMS\", () => !1), ENV$3.registerFlag(\"WEBGL_BUFFER_SUPPORTED\", () => 2 === ENV$3.get(\"WEBGL_VERSION\")), ENV$3.registerFlag(\"WEBGL_CPU_FORWARD\", () => !0), ENV$3.registerFlag(\"WEBGL_FORCE_F16_TEXTURES\", () => !1), ENV$3.registerFlag(\"WEBGL_PACK\", () => ENV$3.getBool(\"HAS_WEBGL\")), ENV$3.registerFlag(\"WEBGL_PACK_NORMALIZATION\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_PACK_CLIP\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_PACK_DEPTHWISECONV\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_PACK_BINARY_OPERATIONS\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_PACK_UNARY_OPERATIONS\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_PACK_ARRAY_OPERATIONS\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_PACK_IMAGE_OPERATIONS\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_PACK_REDUCE\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_LAZILY_UNPACK\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_CONV_IM2COL\", () => ENV$3.getBool(\"WEBGL_PACK\")), ENV$3.registerFlag(\"WEBGL_MAX_TEXTURE_SIZE\", () => getWebGLMaxTextureSize$1(ENV$3.getNumber(\"WEBGL_VERSION\"))), ENV$3.registerFlag(\"WEBGL_MAX_TEXTURES_IN_SHADER\", () => getMaxTexturesInShader$1(ENV$3.getNumber(\"WEBGL_VERSION\"))), ENV$3.registerFlag(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\", () => {\n  var e = ENV$3.getNumber(\"WEBGL_VERSION\");\n  return 0 === e ? 0 : getWebGLDisjointQueryTimerVersion$1(e);\n}), ENV$3.registerFlag(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\", () => ENV$3.getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\") > 0 && !isMobile$1()), ENV$3.registerFlag(\"WEBGL_RENDER_FLOAT32_CAPABLE\", () => isCapableOfRenderingToFloatTexture$1(ENV$3.getNumber(\"WEBGL_VERSION\"))), ENV$3.registerFlag(\"WEBGL_RENDER_FLOAT32_ENABLED\", () => !ENV$3.getBool(\"WEBGL_FORCE_F16_TEXTURES\") && ENV$3.getBool(\"WEBGL_RENDER_FLOAT32_CAPABLE\")), ENV$3.registerFlag(\"WEBGL_DOWNLOAD_FLOAT_ENABLED\", () => isDownloadFloatTextureEnabled$1(ENV$3.getNumber(\"WEBGL_VERSION\"))), ENV$3.registerFlag(\"WEBGL_FENCE_API_ENABLED\", () => isWebGLFenceEnabled$1(ENV$3.getNumber(\"WEBGL_VERSION\"))), ENV$3.registerFlag(\"WEBGL_SIZE_UPLOAD_UNIFORM\", () => ENV$3.getBool(\"WEBGL_RENDER_FLOAT32_ENABLED\") ? 4 : 0), ENV$3.registerFlag(\"WEBGL_DELETE_TEXTURE_THRESHOLD\", () => -1, e => {\n  if (e < 0 && -1 !== e) throw new Error(\"WEBGL_DELETE_TEXTURE_THRESHOLD must be -1 (indicating never delete) or at least 0, but got \".concat(e, \".\"));\n}), ENV$3.registerFlag(\"WEBGL_FLUSH_THRESHOLD\", () => isMobile$1() && ENV$3.getBool(\"IS_CHROME\") ? 1 : -1, e => {\n  if (e < 0 && -1 !== e) throw new Error(\"WEBGL_FLUSH_THRESHOLD must be -1 (indicating never manual flush) or at least 0, but got \".concat(e, \".\"));\n}), ENV$3.registerFlag(\"CPU_HANDOFF_SIZE_THRESHOLD\", () => 128), ENV$3.registerFlag(\"WEBGL_USE_SHAPES_UNIFORMS\", () => !1), ENV$3.registerFlag(\"TOPK_LAST_DIM_CPU_HANDOFF_SIZE_THRESHOLD\", () => 1e5), ENV$3.registerFlag(\"TOPK_K_CPU_HANDOFF_THRESHOLD\", () => 128);\nvar ENCODE_FLOAT_SNIPPET$1 = \"\\n  const float FLOAT_MAX = 1.70141184e38;\\n  const float FLOAT_MIN = 1.17549435e-38;\\n\\n  lowp vec4 encode_float(highp float v) {\\n    if (isnan(v)) {\\n      return vec4(255, 255, 255, 255);\\n    }\\n\\n    highp float av = abs(v);\\n\\n    if(av < FLOAT_MIN) {\\n      return vec4(0.0, 0.0, 0.0, 0.0);\\n    } else if(v > FLOAT_MAX) {\\n      return vec4(0.0, 0.0, 128.0, 127.0) / 255.0;\\n    } else if(v < -FLOAT_MAX) {\\n      return vec4(0.0, 0.0,  128.0, 255.0) / 255.0;\\n    }\\n\\n    highp vec4 c = vec4(0,0,0,0);\\n\\n    highp float e = floor(log2(av));\\n    highp float m = exp2(fract(log2(av))) - 1.0;\\n\\n    c[2] = floor(128.0 * m);\\n    m -= c[2] / 128.0;\\n    c[1] = floor(32768.0 * m);\\n    m -= c[1] / 32768.0;\\n    c[0] = floor(8388608.0 * m);\\n\\n    highp float ebias = e + 127.0;\\n    c[3] = floor(ebias / 2.0);\\n    ebias -= c[3] * 2.0;\\n    c[2] += floor(ebias) * 128.0;\\n\\n    c[3] += 128.0 * step(0.0, -v);\\n\\n    return c / 255.0;\\n  }\\n\";\n\nclass DecodeMatrixProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0, this.outPackingScheme = PackingScheme$1.DENSE;\n    var t = getDenseTexShape$1(e),\n        n = getGlslDifferences$1();\n    this.outputShape = e, this.userCode = \"\\n      ivec3 outCoordsFromFlatIndex(int index) {\\n        \".concat(getLogicalCoordinatesFromFlatIndex$1([\"r\", \"c\", \"d\"], e), \"\\n        return ivec3(r, c, d);\\n      }\\n\\n      void main() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n          vec2(\").concat(t[0], \", \").concat(t[1], \"));\\n        int index = 4 * (resTexRC.x * \").concat(t[1], \" + resTexRC.y);\\n\\n        vec4 result = vec4(0.);\\n\\n        for (int i=0; i<4; i++) {\\n          int flatIndex = index + i;\\n          ivec3 rc = outCoordsFromFlatIndex(flatIndex);\\n          result[i] = getA(rc.x, rc.y, rc.z);\\n        }\\n\\n        \").concat(n.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nclass DecodeMatrixPackedProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outPackingScheme = PackingScheme$1.DENSE;\n    var t = getDenseTexShape$1(e),\n        n = getGlslDifferences$1();\n    this.outputShape = e, this.userCode = \"\\n      ivec3 outCoordsFromFlatIndex(int index) {\\n        \".concat(getLogicalCoordinatesFromFlatIndex$1([\"r\", \"c\", \"d\"], e), \"\\n        return ivec3(r, c, d);\\n      }\\n\\n      void main() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n          vec2(\").concat(t[0], \", \").concat(t[1], \"));\\n        int index = 4 * (resTexRC.x * \").concat(t[1], \" + resTexRC.y);\\n\\n        vec4 result = vec4(0.);\\n\\n        for (int i=0; i<4; i++) {\\n          int flatIndex = index + i;\\n          ivec3 rc = outCoordsFromFlatIndex(flatIndex);\\n          result[i] = getChannel(getA(rc.x, rc.y, rc.z), vec2(rc.y, rc.z));\\n        }\\n\\n        \").concat(n.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeFloatProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.outTexUsage = TextureUsage$1.DOWNLOAD;\n    var t = getGlslDifferences$1();\n    this.outputShape = e, this.userCode = \"\\n      \".concat(ENCODE_FLOAT_SNIPPET$1, \"\\n\\n      void main() {\\n        float x = getAAtOutCoords();\\n        \").concat(t.output, \" = encode_float(x);\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeFloatPackedProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !1, this.outTexUsage = TextureUsage$1.DOWNLOAD;\n    var t = getGlslDifferences$1();\n    this.outputShape = e, this.userCode = \"\\n      \".concat(ENCODE_FLOAT_SNIPPET$1, \"\\n\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n        float x = getChannel(getAAtOutCoords(), vec2(coords.y, coords.z));\\n        \").concat(t.output, \" = encode_float(x);\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeMatrixProgram$1 {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    this.variableNames = [\"A\"];\n    var r = getGlslDifferences$1(),\n        [a, s] = t;\n    this.outputShape = e;\n    var o = \"result\";\n    n && (o = \"floor(result * 255. + 0.5)\"), this.userCode = \"\\n      \".concat(getFlatIndexFrom3D$1(e), \"\\n\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n\\n        int flatIndex = getFlatIndex(coords);\\n        int offset = imod(flatIndex, 4);\\n\\n        flatIndex = idiv(flatIndex, 4, 1.);\\n\\n        int r = flatIndex / \").concat(s, \";\\n        int c = imod(flatIndex, \").concat(s, \");\\n        vec2 uv = (vec2(c, r) + halfCR) / vec2(\").concat(s, \".0, \").concat(a, \".0);\\n        vec4 values = \").concat(r.texture2D, \"(A, uv);\\n\\n        float result;\\n\\n        if(offset == 0) {\\n          result = values[0];\\n        } else if(offset == 1) {\\n          result = values[1];\\n        } else if(offset == 2) {\\n          result = values[2];\\n        } else {\\n          result = values[3];\\n        }\\n\\n        \").concat(r.output, \" = vec4(\").concat(o, \", 0., 0., 0.);\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeMatrixPackedProgram$1 {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0;\n    var r = getGlslDifferences$1(),\n        [a, s] = t;\n    this.outputShape = e;\n    var o = \"\",\n        i = \"result\";\n    n && (i = \"floor(result * 255. + 0.5)\");\n\n    for (var _t296 = 0; _t296 <= 1; _t296++) {\n      for (var _n194 = 0; _n194 <= 1; _n194++) {\n        var _i35 = 2 * _t296 + _n194;\n\n        o += \"\\n          localCoords = coords;\\n          if(localCoords[2] + \".concat(_n194, \" < \").concat(e[2], \") {\\n            localCoords[2] += \").concat(_n194, \";\\n            if(localCoords[1] + \").concat(_t296, \" < \").concat(e[1], \") {\\n              localCoords[1] += \").concat(_t296, \";\\n\\n              flatIndex = getFlatIndex(localCoords);\\n              offset = imod(flatIndex, 4);\\n\\n              flatIndex = idiv(flatIndex, 4, 1.);\\n\\n              r = flatIndex / \").concat(s, \";\\n              c = imod(flatIndex, \").concat(s, \");\\n              uv = (vec2(c, r) + halfCR) / vec2(\").concat(s, \".0, \").concat(a, \".0);\\n              values = \").concat(r.texture2D, \"(A, uv);\\n\\n              if(offset == 0) {\\n                result[\").concat(_i35, \"] = values[0];\\n              } else if(offset == 1) {\\n                result[\").concat(_i35, \"] = values[1];\\n              } else if(offset == 2) {\\n                result[\").concat(_i35, \"] = values[2];\\n              } else {\\n                result[\").concat(_i35, \"] = values[3];\\n              }\\n            }\\n          }\\n        \");\n      }\n    }\n\n    this.userCode = \"\\n      \".concat(getFlatIndexFrom3D$1(e), \"\\n\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n\\n        vec4 result = vec4(0.);\\n        int flatIndex, r, c, offset;\\n        ivec3 localCoords;\\n        vec2 uv;\\n        vec4 values;\\n\\n        \").concat(o, \"\\n\\n        \").concat(r.output, \" = \").concat(i, \";\\n      }\\n    \");\n  }\n\n}\n\nfunction createVertexShader$2(e) {\n  var t = getGlslDifferences$1();\n  return createVertexShader$3(e, \"\".concat(t.version, \"\\n    precision highp float;\\n    \").concat(t.attribute, \" vec3 clipSpacePos;\\n    \").concat(t.attribute, \" vec2 uv;\\n    \").concat(t.varyingVs, \" vec2 resultUV;\\n\\n    void main() {\\n      gl_Position = vec4(clipSpacePos, 1);\\n      resultUV = uv;\\n    }\"));\n}\n\nfunction createVertexBuffer$1(e) {\n  return createStaticVertexBuffer$1(e, new Float32Array([-1, 1, 0, 0, 1, -1, -1, 0, 0, 0, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0]));\n}\n\nfunction createIndexBuffer$1(e) {\n  return createStaticIndexBuffer$1(e, new Uint16Array([0, 1, 2, 2, 1, 3]));\n}\n\nfunction createAndConfigureTexture$1(e, t, n, r, a, s) {\n  validateTextureSize$1(t, n);\n  var o = createTexture$1(e),\n      i = e.TEXTURE_2D;\n  return callAndCheck$1(e, () => e.bindTexture(i, o)), callAndCheck$1(e, () => e.texParameteri(i, e.TEXTURE_WRAP_S, e.CLAMP_TO_EDGE)), callAndCheck$1(e, () => e.texParameteri(i, e.TEXTURE_WRAP_T, e.CLAMP_TO_EDGE)), callAndCheck$1(e, () => e.texParameteri(i, e.TEXTURE_MIN_FILTER, e.NEAREST)), callAndCheck$1(e, () => e.texParameteri(i, e.TEXTURE_MAG_FILTER, e.NEAREST)), callAndCheck$1(e, () => e.texImage2D(i, 0, r, t, n, 0, a, s, null)), callAndCheck$1(e, () => e.bindTexture(e.TEXTURE_2D, null)), o;\n}\n\nfunction getInternalFormatForFloat32MatrixTexture$1(e) {\n  return e.internalFormatFloat;\n}\n\nfunction createFloat32MatrixTexture$1(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight$1(t, n);\n  return createAndConfigureTexture$1(e, a, s, getInternalFormatForFloat32MatrixTexture$1(r), r.textureFormatFloat, e.FLOAT);\n}\n\nfunction getInternalFormatForFloat16MatrixTexture$1(e) {\n  return e.internalFormatHalfFloat;\n}\n\nfunction createFloat16MatrixTexture$1(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight$1(t, n);\n  return createAndConfigureTexture$1(e, a, s, getInternalFormatForFloat16MatrixTexture$1(r), r.textureFormatFloat, r.textureTypeHalfFloat);\n}\n\nfunction getInternalFormatForUnsignedBytesMatrixTexture$1(e) {\n  return e.downloadTextureFormat;\n}\n\nfunction createUnsignedBytesMatrixTexture$1(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight$1(t, n);\n  return createAndConfigureTexture$1(e, a, s, getInternalFormatForUnsignedBytesMatrixTexture$1(r), e.RGBA, e.UNSIGNED_BYTE);\n}\n\nfunction getInternalFormatForPackedMatrixTexture$1(e) {\n  return e.internalFormatPackedFloat;\n}\n\nfunction createPackedMatrixTexture$1(e, t, n, r) {\n  var [a, s] = getPackedMatrixTextureShapeWidthHeight$1(t, n);\n  return createAndConfigureTexture$1(e, a, s, getInternalFormatForPackedMatrixTexture$1(r), e.RGBA, e.FLOAT);\n}\n\nfunction getInternalFormatForFloat16PackedMatrixTexture$1(e) {\n  return e.internalFormatPackedHalfFloat;\n}\n\nfunction createFloat16PackedMatrixTexture$1(e, t, n, r) {\n  var [a, s] = getPackedMatrixTextureShapeWidthHeight$1(t, n);\n  return createAndConfigureTexture$1(e, a, s, getInternalFormatForFloat16PackedMatrixTexture$1(r), e.RGBA, r.textureTypeHalfFloat);\n}\n\nfunction bindVertexProgramAttributeStreams$1(e, t, n) {\n  return callAndCheck$1(e, () => e.bindBuffer(e.ARRAY_BUFFER, n)), bindVertexBufferToProgramAttribute$1(e, t, \"clipSpacePos\", n, 3, 20, 0) && bindVertexBufferToProgramAttribute$1(e, t, \"uv\", n, 2, 20, 12);\n}\n\nfunction uploadDenseMatrixToTexture$1(e, t, n, r, a, s) {\n  var o, i, l;\n  callAndCheck$1(e, () => e.bindTexture(e.TEXTURE_2D, t)), a instanceof Uint8Array ? (o = new Uint8Array(n * r * 4), i = e.UNSIGNED_BYTE, l = e.RGBA) : (o = new Float32Array(n * r * 4), i = e.FLOAT, l = s.internalFormatPackedFloat), o.set(a), callAndCheck$1(e, () => e.texImage2D(e.TEXTURE_2D, 0, l, n, r, 0, e.RGBA, i, o)), callAndCheck$1(e, () => e.bindTexture(e.TEXTURE_2D, null));\n}\n\nfunction uploadPixelDataToTexture$1(e, t, n) {\n  callAndCheck$1(e, () => e.bindTexture(e.TEXTURE_2D, t)), n.data instanceof Uint8Array ? callAndCheck$1(e, () => e.texImage2D(e.TEXTURE_2D, 0, e.RGBA, n.width, n.height, 0, e.RGBA, e.UNSIGNED_BYTE, n.data)) : callAndCheck$1(e, () => e.texImage2D(e.TEXTURE_2D, 0, e.RGBA, e.RGBA, e.UNSIGNED_BYTE, n)), callAndCheck$1(e, () => e.bindTexture(e.TEXTURE_2D, null));\n}\n\nfunction createBufferFromOutputTexture$1(e, t, n, r) {\n  var a = e.createBuffer();\n  callAndCheck$1(e, () => e.bindBuffer(e.PIXEL_PACK_BUFFER, a));\n  var s = 16 * t * n;\n  return callAndCheck$1(e, () => e.bufferData(e.PIXEL_PACK_BUFFER, s, e.STREAM_READ)), callAndCheck$1(e, () => e.readPixels(0, 0, n, t, e.RGBA, e.FLOAT, 0)), callAndCheck$1(e, () => e.bindBuffer(e.PIXEL_PACK_BUFFER, null)), a;\n}\n\nfunction downloadFloat32MatrixFromBuffer$1(e, t, n) {\n  var r = e,\n      a = new Float32Array(n);\n  return r.bindBuffer(r.PIXEL_PACK_BUFFER, t), r.getBufferSubData(r.PIXEL_PACK_BUFFER, 0, a), r.bindBuffer(r.PIXEL_PACK_BUFFER, null), a;\n}\n\nfunction downloadByteEncodedFloatMatrixFromOutputTexture$1(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight$1(t, n),\n      o = new Uint8Array(getUnpackedArraySizeFromMatrixSize$1(t * n, 4));\n  return callAndCheck$1(e, () => e.readPixels(0, 0, a, s, r.downloadTextureFormat, e.UNSIGNED_BYTE, o)), new Float32Array(o.buffer);\n}\n\nfunction downloadPackedMatrixFromBuffer$1(e, t, n, r, a, s, o, i) {\n  var l = e,\n      u = new Float32Array(getPackedRGBAArraySizeFromMatrixShape$1(s, o));\n  return l.bindBuffer(l.PIXEL_PACK_BUFFER, t), l.getBufferSubData(l.PIXEL_PACK_BUFFER, 0, u), l.bindBuffer(l.PIXEL_PACK_BUFFER, null), u;\n}\n\nfunction downloadMatrixFromPackedOutputTexture$1(e, t, n) {\n  var r = new Float32Array(t * n * 4);\n  return callAndCheck$1(e, () => e.readPixels(0, 0, n, t, e.RGBA, e.FLOAT, r)), r;\n}\n\nclass GPGPUContext$1 {\n  constructor(e) {\n    this.outputTexture = null, this.program = null, this.disposed = !1, this.vertexAttrsAreBound = !1, this.itemsToPoll = [];\n    var t = env$1().getNumber(\"WEBGL_VERSION\");\n    null != e ? (this.gl = e, setWebGLContext$1(t, e)) : this.gl = getWebGLContext$1(t);\n    var n = \"WEBGL_color_buffer_float\";\n    var r = \"EXT_color_buffer_half_float\";\n\n    if (1 === env$1().getNumber(\"WEBGL_VERSION\")) {\n      var _e413 = \"OES_texture_half_float\";\n      if (this.textureFloatExtension = getExtensionOrThrow$1(this.gl, \"OES_texture_float\"), hasExtension$1(this.gl, _e413)) this.textureHalfFloatExtension = getExtensionOrThrow$1(this.gl, _e413);else if (env$1().get(\"WEBGL_FORCE_F16_TEXTURES\")) throw new Error(\"GL context does not support half float textures, yet the environment flag WEBGL_FORCE_F16_TEXTURES is set to true.\");\n      if (this.colorBufferFloatExtension = this.gl.getExtension(n), hasExtension$1(this.gl, r)) this.colorBufferHalfFloatExtension = getExtensionOrThrow$1(this.gl, r);else if (env$1().get(\"WEBGL_FORCE_F16_TEXTURES\")) throw new Error(\"GL context does not support color renderable half floats, yet the environment flag WEBGL_FORCE_F16_TEXTURES is set to true.\");\n    } else if (n = \"EXT_color_buffer_float\", hasExtension$1(this.gl, n)) this.colorBufferFloatExtension = this.gl.getExtension(n);else {\n      if (!hasExtension$1(this.gl, r)) throw new Error(\"GL context does not support color renderable floats\");\n      this.colorBufferHalfFloatExtension = this.gl.getExtension(r);\n    }\n\n    this.vertexBuffer = createVertexBuffer$1(this.gl), this.indexBuffer = createIndexBuffer$1(this.gl), this.framebuffer = createFramebuffer$1(this.gl), this.textureConfig = getTextureConfig$1(this.gl, this.textureHalfFloatExtension);\n  }\n\n  get debug() {\n    return env$1().getBool(\"DEBUG\");\n  }\n\n  dispose() {\n    if (this.disposed) return;\n    null != this.program && console.warn(\"Disposing a GPGPUContext that still has a bound WebGLProgram. This is probably a resource leak, delete the program with GPGPUContext.deleteProgram before disposing.\"), null != this.outputTexture && console.warn(\"Disposing a GPGPUContext that still has a bound output matrix texture.  This is probably a resource leak, delete the output matrix texture with GPGPUContext.deleteMatrixTexture before disposing.\");\n    var e = this.gl;\n    callAndCheck$1(e, () => e.finish()), callAndCheck$1(e, () => e.bindFramebuffer(e.FRAMEBUFFER, null)), callAndCheck$1(e, () => e.deleteFramebuffer(this.framebuffer)), callAndCheck$1(e, () => e.bindBuffer(e.ARRAY_BUFFER, null)), callAndCheck$1(e, () => e.bindBuffer(e.ELEMENT_ARRAY_BUFFER, null)), callAndCheck$1(e, () => e.deleteBuffer(this.indexBuffer)), this.disposed = !0;\n  }\n\n  createFloat32MatrixTexture(e, t) {\n    return this.throwIfDisposed(), createFloat32MatrixTexture$1(this.gl, e, t, this.textureConfig);\n  }\n\n  createFloat16MatrixTexture(e, t) {\n    return this.throwIfDisposed(), createFloat16MatrixTexture$1(this.gl, e, t, this.textureConfig);\n  }\n\n  createUnsignedBytesMatrixTexture(e, t) {\n    return this.throwIfDisposed(), createUnsignedBytesMatrixTexture$1(this.gl, e, t, this.textureConfig);\n  }\n\n  uploadPixelDataToTexture(e, t) {\n    this.throwIfDisposed(), uploadPixelDataToTexture$1(this.gl, e, t);\n  }\n\n  uploadDenseMatrixToTexture(e, t, n, r) {\n    this.throwIfDisposed(), uploadDenseMatrixToTexture$1(this.gl, e, t, n, r, this.textureConfig);\n  }\n\n  createFloat16PackedMatrixTexture(e, t) {\n    return this.throwIfDisposed(), createFloat16PackedMatrixTexture$1(this.gl, e, t, this.textureConfig);\n  }\n\n  createPackedMatrixTexture(e, t) {\n    return this.throwIfDisposed(), createPackedMatrixTexture$1(this.gl, e, t, this.textureConfig);\n  }\n\n  deleteMatrixTexture(e) {\n    this.throwIfDisposed(), this.outputTexture === e && (unbindColorTextureFromFramebuffer$1(this.gl, this.framebuffer), this.outputTexture = null), callAndCheck$1(this.gl, () => this.gl.deleteTexture(e));\n  }\n\n  downloadByteEncodedFloatMatrixFromOutputTexture(e, t, n) {\n    return this.downloadMatrixDriver(e, () => downloadByteEncodedFloatMatrixFromOutputTexture$1(this.gl, t, n, this.textureConfig));\n  }\n\n  downloadPackedMatrixFromBuffer(e, t, n, r, a, s) {\n    return downloadPackedMatrixFromBuffer$1(this.gl, e, t, n, r, a, s);\n  }\n\n  downloadFloat32MatrixFromBuffer(e, t) {\n    return downloadFloat32MatrixFromBuffer$1(this.gl, e, t);\n  }\n\n  createBufferFromTexture(e, t, n) {\n    this.bindTextureToFrameBuffer(e);\n    var r = createBufferFromOutputTexture$1(this.gl, t, n);\n    return this.unbindTextureToFrameBuffer(), r;\n  }\n\n  createAndWaitForFence() {\n    var e = this.createFence(this.gl);\n    return this.pollFence(e);\n  }\n\n  createFence(e) {\n    var t, n;\n\n    if (env$1().getBool(\"WEBGL_FENCE_API_ENABLED\")) {\n      var r = e,\n          a = r.fenceSync(r.SYNC_GPU_COMMANDS_COMPLETE, 0);\n      e.flush(), n = () => {\n        var e = r.clientWaitSync(a, 0, 0);\n        return e === r.ALREADY_SIGNALED || e === r.CONDITION_SATISFIED;\n      }, t = a;\n    } else env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\") > 0 ? (t = this.beginQuery(), this.endQuery(), n = () => this.isQueryAvailable(t, env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\"))) : n = () => !0;\n\n    return {\n      query: t,\n      isFencePassed: n\n    };\n  }\n\n  downloadMatrixFromPackedTexture(e, t, n) {\n    return this.downloadMatrixDriver(e, () => downloadMatrixFromPackedOutputTexture$1(this.gl, t, n));\n  }\n\n  createProgram(e) {\n    this.throwIfDisposed();\n    var t = this.gl,\n        n = createFragmentShader$1(t, e);\n    null == this.vertexShader && (this.vertexShader = createVertexShader$2(t));\n    var r = createProgram$1(t);\n    return callAndCheck$1(t, () => t.attachShader(r, this.vertexShader)), callAndCheck$1(t, () => t.attachShader(r, n)), linkProgram$1(t, r), this.debug && validateProgram$1(t, r), this.vertexAttrsAreBound || (this.setProgram(r), this.vertexAttrsAreBound = bindVertexProgramAttributeStreams$1(t, this.program, this.vertexBuffer)), r;\n  }\n\n  deleteProgram(e) {\n    this.throwIfDisposed(), e === this.program && (this.program = null), null != e && callAndCheck$1(this.gl, () => this.gl.deleteProgram(e));\n  }\n\n  setProgram(e) {\n    this.throwIfDisposed(), this.program = e, null != this.program && this.debug && validateProgram$1(this.gl, this.program), callAndCheck$1(this.gl, () => this.gl.useProgram(e));\n  }\n\n  getUniformLocation(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n    return this.throwIfDisposed(), n ? getProgramUniformLocationOrThrow$1(this.gl, e, t) : getProgramUniformLocation$1(this.gl, e, t);\n  }\n\n  getAttributeLocation(e, t) {\n    return this.throwIfDisposed(), callAndCheck$1(this.gl, () => this.gl.getAttribLocation(e, t));\n  }\n\n  getUniformLocationNoThrow(e, t) {\n    return this.throwIfDisposed(), this.gl.getUniformLocation(e, t);\n  }\n\n  setInputMatrixTexture(e, t, n) {\n    this.throwIfDisposed(), this.throwIfNoProgram(), bindTextureToProgramUniformSampler$1(this.gl, e, t, n);\n  }\n\n  setOutputMatrixTexture(e, t, n) {\n    this.setOutputMatrixTextureDriver(e, n, t);\n  }\n\n  setOutputPackedMatrixTexture(e, t, n) {\n    this.throwIfDisposed();\n    var [r, a] = getPackedMatrixTextureShapeWidthHeight$1(t, n);\n    this.setOutputMatrixTextureDriver(e, r, a);\n  }\n\n  setOutputMatrixWriteRegion(e, t, n, r) {\n    this.setOutputMatrixWriteRegionDriver(n, e, r, t);\n  }\n\n  setOutputPackedMatrixWriteRegion(e, t, n, r) {\n    throw new Error(\"setOutputPackedMatrixWriteRegion not implemented.\");\n  }\n\n  debugValidate() {\n    null != this.program && validateProgram$1(this.gl, this.program), validateFramebuffer$1(this.gl);\n  }\n\n  executeProgram() {\n    this.throwIfDisposed(), this.throwIfNoProgram();\n    var e = this.gl;\n    this.debug && this.debugValidate(), callAndCheck$1(e, () => e.drawElements(e.TRIANGLES, 6, e.UNSIGNED_SHORT, 0));\n  }\n\n  blockUntilAllProgramsCompleted() {\n    this.throwIfDisposed(), callAndCheck$1(this.gl, () => this.gl.finish());\n  }\n\n  getQueryTimerExtension() {\n    return null == this.disjointQueryTimerExtension && (this.disjointQueryTimerExtension = getExtensionOrThrow$1(this.gl, 2 === env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\") ? \"EXT_disjoint_timer_query_webgl2\" : \"EXT_disjoint_timer_query\")), this.disjointQueryTimerExtension;\n  }\n\n  getQueryTimerExtensionWebGL2() {\n    return this.getQueryTimerExtension();\n  }\n\n  getQueryTimerExtensionWebGL1() {\n    return this.getQueryTimerExtension();\n  }\n\n  beginQuery() {\n    if (2 === env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\")) {\n      var _e414 = this.gl,\n          _t297 = this.getQueryTimerExtensionWebGL2(),\n          n = _e414.createQuery();\n\n      return _e414.beginQuery(_t297.TIME_ELAPSED_EXT, n), n;\n    }\n\n    var e = this.getQueryTimerExtensionWebGL1(),\n        t = e.createQueryEXT();\n    return e.beginQueryEXT(e.TIME_ELAPSED_EXT, t), t;\n  }\n\n  endQuery() {\n    if (2 === env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\")) {\n      var _e415 = this.gl,\n          t = this.getQueryTimerExtensionWebGL2();\n      return void _e415.endQuery(t.TIME_ELAPSED_EXT);\n    }\n\n    var e = this.getQueryTimerExtensionWebGL1();\n    e.endQueryEXT(e.TIME_ELAPSED_EXT);\n  }\n\n  waitForQueryAndGetTime(e) {\n    var _this72 = this;\n\n    return _asyncToGenerator(function* () {\n      return yield repeatedTry$1(() => _this72.disposed || _this72.isQueryAvailable(e, env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\"))), _this72.getQueryTime(e, env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\"));\n    })();\n  }\n\n  getQueryTime(e, t) {\n    if (0 === t) return null;\n\n    if (2 === t) {\n      var _t298 = this.gl;\n      return _t298.getQueryParameter(e, _t298.QUERY_RESULT) / 1e6;\n    }\n\n    {\n      var _t299 = this.getQueryTimerExtensionWebGL1();\n\n      return _t299.getQueryObjectEXT(e, _t299.QUERY_RESULT_EXT) / 1e6;\n    }\n  }\n\n  isQueryAvailable(e, t) {\n    if (0 === t) return !0;\n\n    if (2 === t) {\n      var _t300 = this.gl,\n          n = this.getQueryTimerExtensionWebGL2(),\n          r = _t300.getQueryParameter(e, _t300.QUERY_RESULT_AVAILABLE);\n\n      return null == this.disjoint && (this.disjoint = this.gl.getParameter(n.GPU_DISJOINT_EXT)), r && !this.disjoint;\n    }\n\n    {\n      var _t301 = this.getQueryTimerExtensionWebGL1(),\n          _n195 = _t301.getQueryObjectEXT(e, _t301.QUERY_RESULT_AVAILABLE_EXT);\n\n      return null == this.disjoint && (this.disjoint = this.gl.getParameter(_t301.GPU_DISJOINT_EXT)), _n195 && !this.disjoint;\n    }\n  }\n\n  pollFence(e) {\n    return new Promise(t => {\n      this.addItemToPoll(() => e.isFencePassed(), () => t());\n    });\n  }\n\n  pollItems() {\n    var e = linearSearchLastTrue$1(this.itemsToPoll.map(e => e.isDoneFn));\n\n    for (var t = 0; t <= e; ++t) {\n      var {\n        resolveFn: _e416\n      } = this.itemsToPoll[t];\n\n      _e416();\n    }\n\n    this.itemsToPoll = this.itemsToPoll.slice(e + 1);\n  }\n\n  addItemToPoll(e, t) {\n    this.itemsToPoll.push({\n      isDoneFn: e,\n      resolveFn: t\n    }), this.itemsToPoll.length > 1 || repeatedTry$1(() => (this.pollItems(), 0 === this.itemsToPoll.length));\n  }\n\n  bindTextureToFrameBuffer(e) {\n    this.throwIfDisposed(), bindColorTextureToFramebuffer$1(this.gl, e, this.framebuffer), this.debug && validateFramebuffer$1(this.gl);\n  }\n\n  unbindTextureToFrameBuffer() {\n    null != this.outputTexture ? (bindColorTextureToFramebuffer$1(this.gl, this.outputTexture, this.framebuffer), this.debug && validateFramebuffer$1(this.gl)) : unbindColorTextureFromFramebuffer$1(this.gl, this.framebuffer);\n  }\n\n  downloadMatrixDriver(e, t) {\n    this.bindTextureToFrameBuffer(e);\n    var n = t();\n    return this.unbindTextureToFrameBuffer(), n;\n  }\n\n  setOutputMatrixTextureDriver(e, t, n) {\n    this.throwIfDisposed();\n    var r = this.gl;\n    bindColorTextureToFramebuffer$1(r, e, this.framebuffer), this.debug && validateFramebuffer$1(r), this.outputTexture = e, callAndCheck$1(r, () => r.viewport(0, 0, t, n)), callAndCheck$1(r, () => r.scissor(0, 0, t, n));\n  }\n\n  setOutputMatrixWriteRegionDriver(e, t, n, r) {\n    this.throwIfDisposed(), callAndCheck$1(this.gl, () => this.gl.scissor(e, t, n, r));\n  }\n\n  throwIfDisposed() {\n    if (this.disposed) throw new Error(\"Attempted to use disposed GPGPUContext.\");\n  }\n\n  throwIfNoProgram() {\n    if (null == this.program) throw new Error(\"No GPU program is currently set.\");\n  }\n\n}\n\nfunction linearSearchLastTrue$1(e) {\n  var t = 0;\n\n  for (; t < e.length && e[t](); ++t) {\n    ;\n  }\n\n  return t - 1;\n}\n\nvar {\n  getBroadcastDims: getBroadcastDims$2\n} = backend_util$1;\n\nfunction makeShader$1(e, t, n) {\n  var r = [];\n\n  if (e.forEach(e => {\n    var t = sizeFromShape$1(e.shapeInfo.logicalShape);\n\n    if (e.shapeInfo.isUniform ? r.push(\"uniform float \".concat(e.name).concat(t > 1 ? \"[\".concat(t, \"]\") : \"\", \";\")) : (r.push(\"uniform sampler2D \".concat(e.name, \";\")), r.push(\"uniform int offset\".concat(e.name, \";\"))), n.enableShapeUniforms) {\n      var {\n        uniformShape: _t302\n      } = getUniformInfoFromShape$1(n.packedInputs, e.shapeInfo.logicalShape, e.shapeInfo.texShape);\n\n      switch (_t302.length) {\n        case 1:\n          r.push(\"uniform int \".concat(e.name, \"Shape;\"));\n          break;\n\n        case 2:\n          r.push(\"uniform ivec2 \".concat(e.name, \"Shape;\"));\n          break;\n\n        case 3:\n          r.push(\"uniform ivec3 \".concat(e.name, \"Shape;\"));\n          break;\n\n        case 4:\n          r.push(\"uniform ivec4 \".concat(e.name, \"Shape;\"));\n      }\n\n      r.push(\"uniform ivec2 \".concat(e.name, \"TexShape;\"));\n    }\n  }), n.enableShapeUniforms) {\n    switch (t.logicalShape.length) {\n      case 1:\n        r.push(\"uniform int outShape;\");\n        break;\n\n      case 2:\n        r.push(\"uniform ivec2 outShape;\"), r.push(\"uniform int outShapeStrides;\");\n        break;\n\n      case 3:\n        r.push(\"uniform ivec3 outShape;\"), r.push(\"uniform ivec2 outShapeStrides;\");\n        break;\n\n      case 4:\n        r.push(\"uniform ivec4 outShape;\"), r.push(\"uniform ivec3 outShapeStrides;\");\n    }\n\n    r.push(\"uniform ivec2 outTexShape;\");\n  }\n\n  n.customUniforms && n.customUniforms.forEach(e => {\n    r.push(\"uniform \".concat(e.type, \" \").concat(e.name).concat(e.arrayIndex ? \"[\".concat(e.arrayIndex, \"]\") : \"\", \";\"));\n  });\n  var a = r.join(\"\\n\"),\n      s = e.map(e => getInputSamplingSnippet$1(e, t, n.packedInputs, n.enableShapeUniforms)).join(\"\\n\"),\n      o = t.texShape,\n      i = getGlslDifferences$1(),\n      l = getFloatTextureSampleSnippet$1(i);\n  var u,\n      c,\n      p = getShaderPrefix$1(i);\n  return t.isPacked ? (u = getPackedOutputSamplingSnippet$1(t.logicalShape, o, n.enableShapeUniforms), c = getFloatTextureSetRGBASnippet$1(i)) : (u = getOutputSamplingSnippet$1(t.logicalShape, o, n.enableShapeUniforms), c = getFloatTextureSetRSnippet$1(i)), n.packedInputs && (p += SHADER_PACKED_PREFIX$1), [p, l, c, a, u, s, n.userCode].join(\"\\n\");\n}\n\nfunction getSamplerFromInInfo$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  var n = e.shapeInfo.logicalShape;\n\n  switch (n.length) {\n    case 0:\n      return getSamplerScalar$1(e, t);\n\n    case 1:\n      return getSampler1D$1(e, t);\n\n    case 2:\n      return getSampler2D$1(e, t);\n\n    case 3:\n      return getSampler3D$1(e, t);\n\n    case 4:\n      return getSampler4D$1(e, t);\n\n    case 5:\n      return getSampler5D$1(e);\n\n    case 6:\n      return getSampler6D$1(e);\n\n    default:\n      throw new Error(\"\".concat(n.length, \"-D input sampling is not yet supported\"));\n  }\n}\n\nfunction getPackedSamplerFromInInfo$1(e, t) {\n  switch (e.shapeInfo.logicalShape.length) {\n    case 0:\n      return getPackedSamplerScalar$1(e);\n\n    case 1:\n      return getPackedSampler1D$1(e, t);\n\n    case 2:\n      return getPackedSampler2D$1(e, t);\n\n    case 3:\n      return getPackedSampler3D$1(e, t);\n\n    default:\n      return getPackedSamplerND$1(e, t);\n  }\n}\n\nfunction getInputSamplingSnippet$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = \"\";\n  return a += n ? getPackedSamplerFromInInfo$1(e, r) : getSamplerFromInInfo$1(e, r), e.shapeInfo.logicalShape.length <= t.logicalShape.length && (a += n ? getPackedSamplerAtOutputCoords$1(e, t) : getSamplerAtOutputCoords$1(e, t)), a;\n}\n\nfunction getPackedOutputSamplingSnippet$1(e, t, n) {\n  switch (e.length) {\n    case 0:\n      return getOutputScalarCoords$1();\n\n    case 1:\n      return getOutputPacked1DCoords$1(e, t, n);\n\n    case 2:\n      return getOutputPacked2DCoords$1(e, t, n);\n\n    case 3:\n      return getOutputPacked3DCoords$1(e, t, n);\n\n    default:\n      return getOutputPackedNDCoords$1(e, t, n);\n  }\n}\n\nfunction getOutputSamplingSnippet$1(e, t, n) {\n  switch (e.length) {\n    case 0:\n      return getOutputScalarCoords$1();\n\n    case 1:\n      return getOutput1DCoords$1(e, t, n);\n\n    case 2:\n      return getOutput2DCoords$1(e, t, n);\n\n    case 3:\n      return getOutput3DCoords$1(e, t, n);\n\n    case 4:\n      return getOutput4DCoords$1(e, t, n);\n\n    case 5:\n      return getOutput5DCoords$1(e, t);\n\n    case 6:\n      return getOutput6DCoords$1(e, t);\n\n    default:\n      throw new Error(\"\".concat(e.length, \"-D output sampling is not yet supported\"));\n  }\n}\n\nfunction getFloatTextureSampleSnippet$1(e) {\n  return \"\\n    float sampleTexture(sampler2D textureSampler, vec2 uv) {\\n      return \".concat(e.texture2D, \"(textureSampler, uv).r;\\n    }\\n  \");\n}\n\nfunction getFloatTextureSetRSnippet$1(e) {\n  return \"\\n    void setOutput(float val) {\\n      \".concat(e.output, \" = vec4(val, 0, 0, 0);\\n    }\\n  \");\n}\n\nfunction getFloatTextureSetRGBASnippet$1(e) {\n  return \"\\n    void setOutput(vec4 val) {\\n      \".concat(e.output, \" = val;\\n    }\\n  \");\n}\n\nfunction getShaderPrefix$1(e) {\n  return \"\".concat(e.version, \"\\n    precision highp float;\\n    precision highp int;\\n    precision highp sampler2D;\\n    \").concat(e.varyingFs, \" vec2 resultUV;\\n    \").concat(e.defineOutput, \"\\n    const vec2 halfCR = vec2(0.5, 0.5);\\n\\n    struct ivec5\\n    {\\n      int x;\\n      int y;\\n      int z;\\n      int w;\\n      int u;\\n    };\\n\\n    struct ivec6\\n    {\\n      int x;\\n      int y;\\n      int z;\\n      int w;\\n      int u;\\n      int v;\\n    };\\n\\n    uniform float NAN;\\n    \").concat(e.defineSpecialNaN, \"\\n    \").concat(e.defineSpecialInf, \"\\n    \").concat(e.defineRound, \"\\n\\n    int imod(int x, int y) {\\n      return x - y * (x / y);\\n    }\\n\\n    int idiv(int a, int b, float sign) {\\n      int res = a / b;\\n      int mod = imod(a, b);\\n      if (sign < 0. && mod != 0) {\\n        res -= 1;\\n      }\\n      return res;\\n    }\\n\\n    //Based on the work of Dave Hoskins\\n    //https://www.shadertoy.com/view/4djSRW\\n    #define HASHSCALE1 443.8975\\n    float random(float seed){\\n      vec2 p = resultUV * seed;\\n      vec3 p3  = fract(vec3(p.xyx) * HASHSCALE1);\\n      p3 += dot(p3, p3.yzx + 19.19);\\n      return fract((p3.x + p3.y) * p3.z);\\n    }\\n\\n    \").concat(SAMPLE_1D_SNIPPET$1, \"\\n    \").concat(SAMPLE_2D_SNIPPET$1, \"\\n    \").concat(SAMPLE_3D_SNIPPET$1, \"\\n  \");\n}\n\nvar SAMPLE_1D_SNIPPET$1 = \"\\nvec2 uvFromFlat(int texNumR, int texNumC, int index) {\\n  int texR = index / texNumC;\\n  int texC = index - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\nvec2 packedUVfrom1D(int texNumR, int texNumC, int index) {\\n  int texelIndex = index / 2;\\n  int texR = texelIndex / texNumC;\\n  int texC = texelIndex - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\n\",\n    SAMPLE_2D_SNIPPET$1 = \"\\nvec2 packedUVfrom2D(int texelsInLogicalRow, int texNumR,\\n  int texNumC, int row, int col) {\\n  int texelIndex = (row / 2) * texelsInLogicalRow + (col / 2);\\n  int texR = texelIndex / texNumC;\\n  int texC = texelIndex - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\n\",\n    SAMPLE_3D_SNIPPET$1 = \"\\nvec2 packedUVfrom3D(int texNumR, int texNumC,\\n    int texelsInBatch, int texelsInLogicalRow, int b,\\n    int row, int col) {\\n  int index = b * texelsInBatch + (row / 2) * texelsInLogicalRow + (col / 2);\\n  int texR = index / texNumC;\\n  int texC = index - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\n\",\n    SHADER_PACKED_PREFIX$1 = \"\\n  float getChannel(vec4 frag, vec2 innerDims) {\\n    vec2 modCoord = mod(innerDims, 2.);\\n    return modCoord.x == 0. ?\\n      (modCoord.y == 0. ? frag.r : frag.g) :\\n      (modCoord.y == 0. ? frag.b : frag.a);\\n  }\\n  float getChannel(vec4 frag, int dim) {\\n    float modCoord = mod(float(dim), 2.);\\n    return modCoord == 0. ? frag.r : frag.g;\\n  }\\n\";\n\nfunction getOutputScalarCoords$1() {\n  return \"\\n    int getOutputCoords() {\\n      return 0;\\n    }\\n  \";\n}\n\nfunction getOutputPacked1DCoords$1(e, t, n) {\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)];\n  return 1 === r[0] ? n ? \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.x * ceil(float(outTexShape[1]) / 2.0));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.x * \".concat(r[1], \".0);\\n      }\\n    \") : 1 === r[1] ? n ? \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.y * ceil(float(outTexShape[0]) / 2.0));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.y * \".concat(r[0], \".0);\\n      }\\n    \") : n ? \"\\n    int getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n      return 2 * (resTexRC.x * packedTexShape[1] + resTexRC.y);\\n    }\\n  \" : \"\\n    int getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n      return 2 * (resTexRC.x * \").concat(r[1], \" + resTexRC.y);\\n    }\\n  \");\n}\n\nfunction getOutput1DCoords$1(e, t, n) {\n  return 1 === t[0] ? n ? \"\\n      int getOutputCoords() {\\n        return int(resultUV.x * float(outTexShape[1]));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return int(resultUV.x * \".concat(t[1], \".0);\\n      }\\n    \") : 1 === t[1] ? n ? \"\\n      int getOutputCoords() {\\n        return int(resultUV.y * float(outTexShape[0]));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return int(resultUV.y * \".concat(t[0], \".0);\\n      }\\n    \") : n ? \"\\n    int getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(outTexShape[0], outTexShape[1]));\\n      return resTexRC.x * outTexShape[1] + resTexRC.y;\\n    }\\n  \" : \"\\n    int getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      return resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n    }\\n  \");\n}\n\nfunction getOutputPacked3DCoords$1(e, t, n) {\n  if (n) return \"\\n    ivec3 getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      int texelsInLogicalRow = int(ceil(float(outShape[2]) / 2.0));\\n      int texelsInBatch = texelsInLogicalRow * int(ceil(float(outShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n      int index = resTexRC.x * packedTexShape[1] + resTexRC.y;\\n\\n      int b = index / texelsInBatch;\\n      index -= b * texelsInBatch;\\n\\n      int r = 2 * (index / texelsInLogicalRow);\\n      int c = imod(index, texelsInLogicalRow) * 2;\\n\\n      return ivec3(b, r, c);\\n    }\\n  \";\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)],\n      a = Math.ceil(e[2] / 2),\n      s = a * Math.ceil(e[1] / 2);\n  return \"\\n    ivec3 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n      int index = resTexRC.x * \").concat(r[1], \" + resTexRC.y;\\n\\n      int b = index / \").concat(s, \";\\n      index -= b * \").concat(s, \";\\n\\n      int r = 2 * (index / \").concat(a, \");\\n      int c = imod(index, \").concat(a, \") * 2;\\n\\n      return ivec3(b, r, c);\\n    }\\n  \");\n}\n\nfunction getOutput3DCoords$1(e, t, n) {\n  if (n) return \"\\n  ivec3 getOutputCoords() {\\n    ivec2 resTexRC = ivec2(resultUV.yx *\\n                           vec2(outTexShape[0], outTexShape[1]));\\n    int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n    \".concat(getLogicalCoordinatesFromFlatIndexByUniform$1([\"r\", \"c\", \"d\"], e), \"\\n    return ivec3(r, c, d);\\n  }\\n\");\n  var r = getLogicalCoordinatesFromFlatIndex$1([\"r\", \"c\", \"d\"], e);\n  return \"\\n    ivec3 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n      \").concat(r, \"\\n      return ivec3(r, c, d);\\n    }\\n  \");\n}\n\nfunction getOutputPackedNDCoords$1(e, t, n) {\n  if (n) return \"\\n    ivec4 getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n      int index = resTexRC.x * packedTexShape[1] + resTexRC.y;\\n\\n      int texelsInLogicalRow = int(ceil(float(outShape[3]) / 2.0));\\n      int texelsInBatch = texelsInLogicalRow * int(ceil(float(outShape[2]) / 2.0));\\n      int texelsInBatchN = texelsInBatch * outShape[1];\\n\\n      int b2 = index / texelsInBatchN;\\n      index -= b2 * texelsInBatchN;\\n\\n      int b = index / texelsInBatch;\\n      index -= b * texelsInBatch;\\n\\n      int r = 2 * (index / texelsInLogicalRow);\\n      int c = imod(index, texelsInLogicalRow) * 2;\\n\\n      return ivec4(b2, b, r, c);\\n    }\\n  \";\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)],\n      a = Math.ceil(e[e.length - 1] / 2),\n      s = a * Math.ceil(e[e.length - 2] / 2);\n  var o = s,\n      i = \"\",\n      l = \"b, r, c\";\n\n  for (var _t303 = 2; _t303 < e.length - 1; _t303++) {\n    o *= e[e.length - _t303 - 1], i = \"\\n      int b\".concat(_t303, \" = index / \").concat(o, \";\\n      index -= b\").concat(_t303, \" * \").concat(o, \";\\n    \") + i, l = \"b\".concat(_t303, \", \") + l;\n  }\n\n  return \"\\n    ivec\".concat(e.length, \" getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\").concat(r[0], \", \").concat(r[1], \"));\\n      int index = resTexRC.x * \").concat(r[1], \" + resTexRC.y;\\n\\n      \").concat(i, \"\\n\\n      int b = index / \").concat(s, \";\\n      index -= b * \").concat(s, \";\\n\\n      int r = 2 * (index / \").concat(a, \");\\n      int c = imod(index, \").concat(a, \") * 2;\\n\\n      return ivec\").concat(e.length, \"(\").concat(l, \");\\n    }\\n  \");\n}\n\nfunction getOutput4DCoords$1(e, t, n) {\n  if (n) return \"\\n    ivec4 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n        vec2(outTexShape[0], outTexShape[1]));\\n      int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n      \".concat(getLogicalCoordinatesFromFlatIndexByUniform$1([\"r\", \"c\", \"d\", \"d2\"], e), \"\\n      return ivec4(r, c, d, d2);\\n    }\\n  \");\n  var r = getLogicalCoordinatesFromFlatIndex$1([\"r\", \"c\", \"d\", \"d2\"], e);\n  return \"\\n    ivec4 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n        vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n      \").concat(r, \"\\n      return ivec4(r, c, d, d2);\\n    }\\n  \");\n}\n\nfunction getOutput5DCoords$1(e, t) {\n  var n = getLogicalCoordinatesFromFlatIndex$1([\"r\", \"c\", \"d\", \"d2\", \"d3\"], e);\n  return \"\\n    ivec5 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx * vec2(\".concat(t[0], \",\\n                             \").concat(t[1], \"));\\n\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n\\n      \").concat(n, \"\\n\\n      ivec5 outShape = ivec5(r, c, d, d2, d3);\\n      return outShape;\\n    }\\n  \");\n}\n\nfunction getOutput6DCoords$1(e, t) {\n  var n = getLogicalCoordinatesFromFlatIndex$1([\"r\", \"c\", \"d\", \"d2\", \"d3\", \"d4\"], e);\n  return \"\\n    ivec6 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n        vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n\\n      \").concat(n, \"\\n\\n      ivec6 result = ivec6(r, c, d, d2, d3, d4);\\n      return result;\\n    }\\n  \");\n}\n\nfunction getOutputPacked2DCoords$1(e, t, n) {\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)];\n  if (arraysEqual$1(e, t)) return n ? \"\\n      ivec2 getOutputCoords() {\\n        ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n        return 2 * ivec2(resultUV.yx * vec2(packedTexShape[0], packedTexShape[1]));\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        return 2 * ivec2(resultUV.yx * vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n      }\\n    \");\n  var a = Math.ceil(e[1] / 2);\n  return n ? \"\\n    ivec2 getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      int texelsInLogicalRow = int(ceil(float(outShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n\\n      int index = resTexRC.x * packedTexShape[1] + resTexRC.y;\\n      int r = 2 * (index / texelsInLogicalRow);\\n      int c = imod(index, texelsInLogicalRow) * 2;\\n\\n      return ivec2(r, c);\\n    }\\n  \" : \"\\n    ivec2 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n\\n      int index = resTexRC.x * \").concat(r[1], \" + resTexRC.y;\\n      int r = 2 * (index / \").concat(a, \");\\n      int c = imod(index, \").concat(a, \") * 2;\\n\\n      return ivec2(r, c);\\n    }\\n  \");\n}\n\nfunction getOutput2DCoords$1(e, t, n) {\n  return arraysEqual$1(e, t) ? n ? \"\\n      ivec2 getOutputCoords() {\\n        return ivec2(resultUV.yx * vec2(outTexShape[0], outTexShape[1]));\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        return ivec2(resultUV.yx * vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      }\\n    \") : 1 === e[1] ? n ? \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(outTexShape[0], outTexShape[1]));\\n        int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n        return ivec2(index, 0);\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n        int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n        return ivec2(index, 0);\\n      }\\n    \") : 1 === e[0] ? n ? \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(outTexShape[0], outTexShape[1]));\\n        int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n        return ivec2(0, index);\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n        int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n        return ivec2(0, index);\\n      }\\n    \") : n ? \"\\n    ivec2 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(outTexShape[0], outTexShape[1]));\\n      int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n      int r = index / outShape[1];\\n      int c = index - r * outShape[1];\\n      return ivec2(r, c);\\n    }\\n  \" : \"\\n    ivec2 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n      int r = index / \").concat(e[1], \";\\n      int c = index - r * \").concat(e[1], \";\\n      return ivec2(r, c);\\n    }\\n  \");\n}\n\nfunction getFlatOffsetUniformName$1(e) {\n  return \"offset\".concat(e);\n}\n\nfunction getPackedSamplerScalar$1(e) {\n  var t = e.name;\n  return \"\\n    vec4 \".concat(\"get\" + t.charAt(0).toUpperCase() + t.slice(1), \"() {\\n      return \").concat(getGlslDifferences$1().texture2D, \"(\").concat(t, \", halfCR);\\n    }\\n  \");\n}\n\nfunction getSamplerScalar$1(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1);\n  if (e.shapeInfo.isUniform) return \"float \".concat(r, \"() {return \").concat(n, \";}\");\n  var [a, s] = e.shapeInfo.texShape;\n  if (1 === a && 1 === s) return \"\\n      float \".concat(r, \"() {\\n        return sampleTexture(\").concat(n, \", halfCR);\\n      }\\n    \");\n  var o = getFlatOffsetUniformName$1(n);\n  if (t) return \"\\n    float \".concat(r, \"() {\\n      vec2 uv = uvFromFlat(\").concat(n, \"TexShape[0], \").concat(n, \"TexShape[1], \").concat(o, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n  var [i, l] = e.shapeInfo.texShape;\n  return \"\\n    float \".concat(r, \"() {\\n      vec2 uv = uvFromFlat(\").concat(i, \", \").concat(l, \", \").concat(o, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getPackedSampler1D$1(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      a = e.shapeInfo.texShape,\n      s = getGlslDifferences$1();\n  if (t) return \"\\n    vec4 \".concat(r, \"(int index) {\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(n, \"TexShape[0]) / 2.0), ceil(float(\").concat(n, \"TexShape[1]) / 2.0));\\n      vec2 uv = packedUVfrom1D(\\n        packedTexShape[0], packedTexShape[1], index);\\n      return \").concat(s.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n  var o = [Math.ceil(a[0] / 2), Math.ceil(a[1] / 2)];\n  return \"\\n    vec4 \".concat(r, \"(int index) {\\n      vec2 uv = packedUVfrom1D(\\n        \").concat(o[0], \", \").concat(o[1], \", index);\\n      return \").concat(s.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler1D$1(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1);\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(r, \"(int index) {\\n        \").concat(getUniformSampler$1(e), \"\\n      }\\n    \");\n  var a = e.shapeInfo.texShape,\n      s = a[0],\n      o = a[1];\n  if (1 === o && 1 === s) return \"\\n      float \".concat(r, \"(int index) {\\n        return sampleTexture(\").concat(n, \", halfCR);\\n      }\\n    \");\n  var i = getFlatOffsetUniformName$1(n);\n  return 1 === o ? t ? \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2(0.5, (float(index + \").concat(i, \") + 0.5) / float(\").concat(n, \"TexShape[0]));\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2(0.5, (float(index + \").concat(i, \") + 0.5) / \").concat(s, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : 1 === s ? t ? \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2((float(index + \").concat(i, \") + 0.5) / float(\").concat(n, \"TexShape[1]), 0.5);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2((float(index + \").concat(i, \") + 0.5) / \").concat(o, \".0, 0.5);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : t ? \"\\n    float \".concat(r, \"(int index) {\\n      vec2 uv = uvFromFlat(\").concat(n, \"TexShape[0], \").concat(n, \"TexShape[1], index + \").concat(i, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \") : \"\\n    float \".concat(r, \"(int index) {\\n      vec2 uv = uvFromFlat(\").concat(s, \", \").concat(o, \", index + \").concat(i, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getPackedSampler2D$1(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = e.shapeInfo.texShape,\n      o = s[0],\n      i = s[1],\n      l = getGlslDifferences$1();\n  if (null != s && arraysEqual$1(n, s)) return t ? \"\\n      vec4 \".concat(a, \"(int row, int col) {\\n        vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n\\n        return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n      vec4 \".concat(a, \"(int row, int col) {\\n        vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(i, \".0, \").concat(o, \".0);\\n\\n        return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n      }\\n    \");\n  if (t) return \"\\n    vec4 \".concat(a, \"(int row, int col) {\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(r, \"TexShape[0]) / 2.0), ceil(float(\").concat(r, \"TexShape[1]) / 2.0));\\n      int valuesPerRow = int(ceil(float(\").concat(r, \"Shape[1]) / 2.0));\\n      vec2 uv = packedUVfrom2D(valuesPerRow, packedTexShape[0], packedTexShape[1], row, col);\\n      return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n  var u = [Math.ceil(s[0] / 2), Math.ceil(s[1] / 2)];\n  return \"\\n    vec4 \".concat(a, \"(int row, int col) {\\n      vec2 uv = packedUVfrom2D(\").concat(Math.ceil(n[1] / 2), \", \").concat(u[0], \", \").concat(u[1], \", row, col);\\n      return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler2D$1(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = e.shapeInfo.texShape;\n  if (null != s && arraysEqual$1(n, s)) return t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col) {\\n      vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(s[1], \".0, \").concat(s[0], \".0);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \");\n  var {\n    newShape: o,\n    keptDims: i\n  } = squeezeShape$1(n);\n\n  if (o.length < n.length) {\n    var _n196 = [\"row\", \"col\"];\n    return \"\\n      \".concat(getSamplerFromInInfo$1(squeezeInputInfo$1(e, o), t), \"\\n      float \").concat(a, \"(int row, int col) {\\n        return \").concat(a, \"(\").concat(getSqueezedParams$1(_n196, i), \");\\n      }\\n    \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(a, \"(int row, int col) {\\n        int index = round(dot(vec2(row, col), vec2(\").concat(n[1], \", 1)));\\n        \").concat(getUniformSampler$1(e), \"\\n      }\\n    \");\n  var l = s[0],\n      u = s[1],\n      c = getFlatOffsetUniformName$1(r);\n  return 1 === u ? t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(r, \"Shape[1], 1, 1));\\n        vec2 uv = vec2(0.5, (index + 0.5) / float(\").concat(r, \"TexShape[0]));\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col) {\\n      float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(n[1], \", 1, 1));\\n      vec2 uv = vec2(0.5, (index + 0.5) / \").concat(l, \".0);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \") : 1 === l ? t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(r, \"Shape[1], 1, 1));\\n        vec2 uv = vec2((index + 0.5) / float(\").concat(r, \"TexShape[1]), 0.5);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col) {\\n      float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(n[1], \", 1, 1));\\n      vec2 uv = vec2((index + 0.5) / \").concat(u, \".0, 0.5);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \") : t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        // Explicitly use integer operations as dot() only works on floats.\\n        int index = row * \").concat(r, \"Shape[1] + col + \").concat(c, \";\\n        vec2 uv = uvFromFlat(\").concat(r, \"TexShape[0], \").concat(r, \"TexShape[1], index);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n  float \".concat(a, \"(int row, int col) {\\n    // Explicitly use integer operations as dot() only works on floats.\\n    int index = row * \").concat(n[1], \" + col + \").concat(c, \";\\n    vec2 uv = uvFromFlat(\").concat(l, \", \").concat(u, \", index);\\n    return sampleTexture(\").concat(r, \", uv);\\n  }\\n\");\n}\n\nfunction getPackedSampler3D$1(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = e.shapeInfo.texShape,\n      o = [Math.ceil(s[0] / 2), Math.ceil(s[1] / 2)];\n\n  if (1 === n[0]) {\n    var _r147 = [1, 2],\n        _s76 = [\"b\", \"row\", \"col\"];\n    return \"\\n        \".concat(getPackedSamplerFromInInfo$1(squeezeInputInfo$1(e, n.slice(1)), t), \"\\n        vec4 \").concat(a, \"(int b, int row, int col) {\\n          return \").concat(a, \"(\").concat(getSqueezedParams$1(_s76, _r147), \");\\n        }\\n      \");\n  }\n\n  var i = getGlslDifferences$1();\n  if (t) return \"\\n    vec4 \".concat(a, \"(int b, int row, int col) {\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(r, \"TexShape[0]) / 2.0), ceil(float(\").concat(r, \"TexShape[1]) / 2.0));\\n      int valuesPerRow = int(ceil(float(\").concat(r, \"Shape[2]) / 2.0));\\n      int texelsInBatch = valuesPerRow * int(ceil(float(\").concat(r, \"Shape[1]) / 2.0));\\n      vec2 uv = packedUVfrom3D(\\n        packedTexShape[0], packedTexShape[1], texelsInBatch, valuesPerRow, b, row, col);\\n      return \").concat(i.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n  var l = o[0],\n      u = o[1],\n      c = Math.ceil(n[2] / 2);\n  return \"\\n    vec4 \".concat(a, \"(int b, int row, int col) {\\n      vec2 uv = packedUVfrom3D(\\n        \").concat(l, \", \").concat(u, \", \").concat(c * Math.ceil(n[1] / 2), \", \").concat(c, \", b, row, col);\\n      return \").concat(i.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler3D$1(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = n[1] * n[2],\n      o = n[2],\n      {\n    newShape: i,\n    keptDims: l\n  } = squeezeShape$1(n);\n\n  if (i.length < n.length) {\n    var _n197 = [\"row\", \"col\", \"depth\"];\n    return \"\\n        \".concat(getSamplerFromInInfo$1(squeezeInputInfo$1(e, i), t), \"\\n        float \").concat(a, \"(int row, int col, int depth) {\\n          return \").concat(a, \"(\").concat(getSqueezedParams$1(_n197, l), \");\\n        }\\n      \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        int index = round(dot(vec3(row, col, depth),\\n                          vec3(\").concat(s, \", \").concat(o, \", 1)));\\n        \").concat(getUniformSampler$1(e), \"\\n      }\\n    \");\n  var u = e.shapeInfo.texShape,\n      c = u[0],\n      p = u[1],\n      d = e.shapeInfo.flatOffset;\n  if (p === s && null == d) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        int stride1 = \").concat(r, \"Shape[2];\\n        float texR = float(row);\\n        float texC = dot(vec2(col, depth), vec2(stride1, 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n        float \".concat(a, \"(int row, int col, int depth) {\\n          float texR = float(row);\\n          float texC = dot(vec2(col, depth), vec2(\").concat(o, \", 1));\\n          vec2 uv = (vec2(texC, texR) + halfCR) /\\n                     vec2(\").concat(p, \".0, \").concat(c, \".0);\\n          return sampleTexture(\").concat(r, \", uv);\\n        }\\n      \");\n  if (p === o && null == d) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        float texR = dot(vec2(row, col), vec2(\").concat(r, \"Shape[1], 1));\\n        float texC = float(depth);\\n        vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col, int depth) {\\n      float texR = dot(vec2(row, col), vec2(\").concat(n[1], \", 1));\\n      float texC = float(depth);\\n      vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\").concat(p, \".0, \").concat(c, \".0);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \");\n  var h = getFlatOffsetUniformName$1(r);\n  return t ? \"\\n    float \".concat(a, \"(int row, int col, int depth) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int stride0 = \").concat(r, \"Shape[1] * \").concat(r, \"Shape[2];\\n      int stride1 = \").concat(r, \"Shape[2];\\n      int index = row * \").concat(s, \" + col * \").concat(o, \" + depth + \").concat(h, \";\\n      vec2 uv = uvFromFlat(\").concat(r, \"TexShape[0], \").concat(r, \"TexShape[1], index);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n    \") : \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        // Explicitly use integer operations as dot() only works on floats.\\n        int index = row * \").concat(s, \" + col * \").concat(o, \" + depth + \").concat(h, \";\\n        vec2 uv = uvFromFlat(\").concat(c, \", \").concat(p, \", index);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n  \");\n}\n\nfunction getPackedSamplerND$1(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      a = getGlslDifferences$1();\n  if (t) return \"\\n    vec4 \".concat(r, \"(int b2, int b, int row, int col) {\\n      int valuesPerRow = int(ceil(float(\").concat(n, \"Shape[3]) / 2.0));\\n      int texelsInBatch = valuesPerRow * int(ceil(float(\").concat(n, \"Shape[2]) / 2.0));\\n      int index = b * texelsInBatch + (row / 2) * valuesPerRow + (col / 2);\\n      texelsInBatch *= \").concat(n, \"Shape[1];\\n      index = b2 * texelsInBatch + index;\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(n, \"TexShape[0]) / 2.0), ceil(float(\").concat(n, \"TexShape[1]) / 2.0));\\n      int texR = index / packedTexShape[1];\\n      int texC = index - texR * packedTexShape[1];\\n      vec2 uv = (vec2(texC, texR) + halfCR) / vec2(packedTexShape[1], packedTexShape[0]); return \").concat(a.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n  var s = e.shapeInfo.logicalShape,\n      o = s.length,\n      i = e.shapeInfo.texShape,\n      l = [Math.ceil(i[0] / 2), Math.ceil(i[1] / 2)],\n      u = l[0],\n      c = l[1],\n      p = Math.ceil(s[o - 1] / 2);\n  var d = p * Math.ceil(s[o - 2] / 2),\n      h = \"int b, int row, int col\",\n      m = \"b * \".concat(d, \" + (row / 2) * \").concat(p, \" + (col / 2)\");\n\n  for (var _e417 = 2; _e417 < o - 1; _e417++) {\n    h = \"int b\".concat(_e417, \", \") + h, d *= s[o - _e417 - 1], m = \"b\".concat(_e417, \" * \").concat(d, \" + \") + m;\n  }\n\n  return \"\\n    vec4 \".concat(r, \"(\").concat(h, \") {\\n      int index = \").concat(m, \";\\n      int texR = index / \").concat(c, \";\\n      int texC = index - texR * \").concat(c, \";\\n      vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\").concat(c, \", \").concat(u, \");\\n      return \").concat(a.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler4D$1(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = n[3],\n      o = n[2] * s,\n      i = n[1] * o,\n      {\n    newShape: l,\n    keptDims: u\n  } = squeezeShape$1(n);\n\n  if (l.length < n.length) {\n    var _n198 = [\"row\", \"col\", \"depth\", \"depth2\"];\n    return \"\\n      \".concat(getSamplerFromInInfo$1(squeezeInputInfo$1(e, l), t), \"\\n      float \").concat(a, \"(int row, int col, int depth, int depth2) {\\n        return \").concat(a, \"(\").concat(getSqueezedParams$1(_n198, u), \");\\n      }\\n    \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        int index = round(dot(vec4(row, col, depth, depth2),\\n                          vec4(\").concat(i, \", \").concat(o, \", \").concat(s, \", 1)));\\n        \").concat(getUniformSampler$1(e), \"\\n      }\\n    \");\n  var c = e.shapeInfo.flatOffset,\n      p = e.shapeInfo.texShape,\n      d = p[0],\n      h = p[1],\n      m = \"int stride2 = \".concat(r, \"Shape[3];\"),\n      f = \"int stride1 = \".concat(r, \"Shape[2] * stride2;\"),\n      g = \"int stride0 = \".concat(r, \"Shape[1] * stride1;\");\n  if (h === i && null == c) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        \").concat(m, \"\\n        \").concat(f, \"\\n        float texR = float(row);\\n        float texC =\\n            dot(vec3(col, depth, depth2),\\n                vec3(stride1, stride2, 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        float texR = float(row);\\n        float texC =\\n            dot(vec3(col, depth, depth2),\\n                vec3(\").concat(o, \", \").concat(s, \", 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \");\n  if (h === s && null == c) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        float texR = dot(vec3(row, col, depth),\\n                         vec3(\").concat(r, \"Shape[1] * \").concat(r, \"Shape[2], \").concat(r, \"Shape[2], 1));\\n        float texC = float(depth2);\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        float texR = dot(vec3(row, col, depth),\\n                         vec3(\").concat(n[1] * n[2], \", \").concat(n[2], \", 1));\\n        float texC = float(depth2);\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \");\n  var $ = getFlatOffsetUniformName$1(r);\n  return t ? \"\\n    float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      \").concat(m, \"\\n      \").concat(f, \"\\n      \").concat(g, \"\\n      int index = row * stride0 + col * stride1 +\\n          depth * stride2 + depth2;\\n      vec2 uv = uvFromFlat(\").concat(r, \"TexShape[0], \").concat(r, \"TexShape[1], index + \").concat($, \");\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \") : \"\\n    float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int index = row * \").concat(i, \" + col * \").concat(o, \" +\\n          depth * \").concat(s, \" + depth2;\\n      vec2 uv = uvFromFlat(\").concat(d, \", \").concat(h, \", index + \").concat($, \");\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler5D$1(e) {\n  var t = e.shapeInfo.logicalShape,\n      n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      a = t[4],\n      s = t[3] * a,\n      o = t[2] * s,\n      i = t[1] * o,\n      {\n    newShape: l,\n    keptDims: u\n  } = squeezeShape$1(t);\n\n  if (l.length < t.length) {\n    var _t304 = [\"row\", \"col\", \"depth\", \"depth2\", \"depth3\"];\n    return \"\\n      \".concat(getSamplerFromInInfo$1(squeezeInputInfo$1(e, l)), \"\\n      float \").concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        return \").concat(r, \"(\").concat(getSqueezedParams$1(_t304, u), \");\\n      }\\n    \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        float index = dot(\\n          vec4(row, col, depth, depth2),\\n          vec4(\").concat(i, \", \").concat(o, \", \").concat(s, \", \").concat(a, \")) +\\n          depth3;\\n        \").concat(getUniformSampler$1(e), \"\\n      }\\n    \");\n  var c = e.shapeInfo.flatOffset,\n      p = e.shapeInfo.texShape,\n      d = p[0],\n      h = p[1];\n  return h === i && null == c ? \"\\n      float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        int texR = row;\\n        float texC = dot(vec4(col, depth, depth2, depth3),\\n                         vec4(\").concat(o, \", \").concat(s, \", \").concat(a, \", 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : h === a && null == c ? \"\\n      float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        float texR = dot(\\n          vec4(row, col, depth, depth2),\\n          vec4(\").concat(t[1] * t[2] * t[3], \",\\n               \").concat(t[2] * t[3], \", \").concat(t[3], \", 1));\\n        int texC = depth3;\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int index = row * \").concat(i, \" + col * \").concat(o, \" + depth * \").concat(s, \" +\\n          depth2 * \").concat(a, \" + depth3 + \").concat(getFlatOffsetUniformName$1(n), \";\\n      vec2 uv = uvFromFlat(\").concat(d, \", \").concat(h, \", index);\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler6D$1(e) {\n  var t = e.shapeInfo.logicalShape,\n      n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      {\n    newShape: a,\n    keptDims: s\n  } = squeezeShape$1(t);\n\n  if (a.length < t.length) {\n    var _t305 = [\"row\", \"col\", \"depth\", \"depth2\", \"depth3\", \"depth4\"];\n    return \"\\n      \".concat(getSamplerFromInInfo$1(squeezeInputInfo$1(e, a)), \"\\n      float \").concat(r, \"(int row, int col, int depth,\\n                    int depth2, int depth3, int depth4) {\\n        return \").concat(r, \"(\").concat(getSqueezedParams$1(_t305, s), \");\\n      }\\n    \");\n  }\n\n  var o = t[5],\n      i = t[4] * o,\n      l = t[3] * i,\n      u = t[2] * l,\n      c = t[1] * u;\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(r, \"(int row, int col, int depth,\\n                  int depth2, int depth3, int depth4) {\\n        int index = round(dot(\\n          vec4(row, col, depth, depth2),\\n          vec4(\").concat(c, \", \").concat(u, \", \").concat(l, \", \").concat(i, \")) +\\n          dot(\\n            vec2(depth3, depth4),\\n            vec2(\").concat(o, \", 1)));\\n        \").concat(getUniformSampler$1(e), \"\\n      }\\n    \");\n  var p = e.shapeInfo.flatOffset,\n      d = e.shapeInfo.texShape,\n      h = d[0],\n      m = d[1];\n  return m === c && null == p ? \"\\n      float \".concat(r, \"(int row, int col, int depth,\\n                    int depth2, int depth3, int depth4) {\\n        int texR = row;\\n        float texC = dot(vec4(col, depth, depth2, depth3),\\n          vec4(\").concat(u, \", \").concat(l, \", \").concat(i, \", \").concat(o, \")) +\\n               float(depth4);\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(m, \".0, \").concat(h, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : m === o && null == p ? \"\\n      float \".concat(r, \"(int row, int col, int depth,\\n                    int depth2, int depth3, int depth4) {\\n        float texR = dot(vec4(row, col, depth, depth2),\\n          vec4(\").concat(t[1] * t[2] * t[3] * t[4], \",\\n               \").concat(t[2] * t[3] * t[4], \",\\n               \").concat(t[3] * t[4], \",\\n               \").concat(t[4], \")) + float(depth3);\\n        int texC = depth4;\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(m, \".0, \").concat(h, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(r, \"(int row, int col, int depth,\\n                  int depth2, int depth3, int depth4) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int index = row * \").concat(c, \" + col * \").concat(u, \" + depth * \").concat(l, \" +\\n          depth2 * \").concat(i, \" + depth3 * \").concat(o, \" + depth4 + \").concat(getFlatOffsetUniformName$1(n), \";\\n      vec2 uv = uvFromFlat(\").concat(h, \", \").concat(m, \", index);\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getUniformSampler$1(e) {\n  var t = e.name,\n      n = sizeFromShape$1(e.shapeInfo.logicalShape);\n  return n < 2 ? \"return \".concat(t, \";\") : \"\\n    for (int i = 0; i < \".concat(n, \"; i++) {\\n      if (i == index) {\\n        return \").concat(t, \"[i];\\n      }\\n    }\\n  \");\n}\n\nfunction getPackedSamplerAtOutputCoords$1(e, t) {\n  var n = e.name,\n      r = n.charAt(0).toUpperCase() + n.slice(1),\n      a = \"get\" + r + \"AtOutCoords\",\n      s = e.shapeInfo.logicalShape.length,\n      o = t.logicalShape.length,\n      i = getBroadcastDims$2(e.shapeInfo.logicalShape, t.logicalShape),\n      l = getCoordsDataType$1(o),\n      u = o - s;\n  var c;\n  var p = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"];\n  c = 0 === s ? \"\" : o < 2 && i.length >= 1 ? \"coords = 0;\" : i.map(e => \"coords.\".concat(p[e + u], \" = 0;\")).join(\"\\n\");\n  var d = \"\";\n  d = o < 2 && s > 0 ? \"coords\" : e.shapeInfo.logicalShape.map((e, t) => \"coords.\".concat(p[t + u])).join(\", \");\n  var h = \"return outputValue;\";\n  var m = 1 === sizeFromShape$1(e.shapeInfo.logicalShape),\n      f = 1 === sizeFromShape$1(t.logicalShape);\n\n  if (1 !== s || m || f) {\n    if (m && !f) h = 1 === o ? \"\\n        return vec4(outputValue.x, outputValue.x, 0., 0.);\\n      \" : \"\\n        return vec4(outputValue.x);\\n      \";else if (i.length) {\n      var _e418 = s - 2,\n          _t306 = s - 1;\n\n      i.indexOf(_e418) > -1 && i.indexOf(_t306) > -1 ? h = \"return vec4(outputValue.x);\" : i.indexOf(_e418) > -1 ? h = \"return vec4(outputValue.x, outputValue.y, outputValue.x, outputValue.y);\" : i.indexOf(_t306) > -1 && (h = \"return vec4(outputValue.xx, outputValue.zz);\");\n    }\n  } else h = \"\\n      return vec4(outputValue.xy, outputValue.xy);\\n    \";\n\n  return \"\\n    vec4 \".concat(a, \"() {\\n      \").concat(l, \" coords = getOutputCoords();\\n      \").concat(c, \"\\n      vec4 outputValue = get\").concat(r, \"(\").concat(d, \");\\n      \").concat(h, \"\\n    }\\n  \");\n}\n\nfunction getSamplerAtOutputCoords$1(e, t) {\n  var n = e.name,\n      r = n.charAt(0).toUpperCase() + n.slice(1),\n      a = \"get\" + r + \"AtOutCoords\",\n      s = e.shapeInfo.logicalShape.length,\n      o = t.logicalShape.length;\n  if (!e.shapeInfo.isUniform && s === o && null == e.shapeInfo.flatOffset && arraysEqual$1(e.shapeInfo.texShape, t.texShape)) return \"\\n      float \".concat(a, \"() {\\n        return sampleTexture(\").concat(n, \", resultUV);\\n      }\\n    \");\n  var i = getCoordsDataType$1(o),\n      l = getBroadcastDims$2(e.shapeInfo.logicalShape, t.logicalShape),\n      u = o - s;\n  var c;\n  var p = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"];\n  c = 0 === s ? \"\" : o < 2 && l.length >= 1 ? \"coords = 0;\" : l.map(e => \"coords.\".concat(p[e + u], \" = 0;\")).join(\"\\n\");\n  var d = \"\";\n  return d = o < 2 && s > 0 ? \"coords\" : e.shapeInfo.logicalShape.map((e, t) => \"coords.\".concat(p[t + u])).join(\", \"), \"\\n    float \".concat(a, \"() {\\n      \").concat(i, \" coords = getOutputCoords();\\n      \").concat(c, \"\\n      return get\").concat(r, \"(\").concat(d, \");\\n    }\\n  \");\n}\n\nfunction getCoordsDataType$1(e) {\n  if (e <= 1) return \"int\";\n  if (2 === e) return \"ivec2\";\n  if (3 === e) return \"ivec3\";\n  if (4 === e) return \"ivec4\";\n  if (5 === e) return \"ivec5\";\n  if (6 === e) return \"ivec6\";\n  throw Error(\"GPU for rank \".concat(e, \" is not yet supported\"));\n}\n\nfunction getUniformInfoFromShape$1(e, t, n) {\n  var {\n    newShape: r\n  } = squeezeShape$1(t),\n      a = t.length,\n      s = e && 3 === a && 1 === t[0],\n      o = s ? t.slice(1) : r,\n      i = !e && a > 1 && !arraysEqual$1(t, n) && r.length < a || s;\n  return {\n    useSqueezeShape: i,\n    uniformShape: i ? o : t\n  };\n}\n\nfunction squeezeInputInfo$1(e, t) {\n  var n = JSON.parse(JSON.stringify(e));\n  return n.shapeInfo.logicalShape = t, n;\n}\n\nfunction getSqueezedParams$1(e, t) {\n  return t.map(t => e[t]).join(\", \");\n}\n\nfunction compileProgram$1(e, t, n, r) {\n  var a = n.map((e, n) => {\n    var r = {\n      logicalShape: e.shape,\n      texShape: e.isUniform ? null : e.texData.texShape,\n      isUniform: e.isUniform,\n      isPacked: !e.isUniform && e.texData.isPacked,\n      flatOffset: null\n    };\n    return null != e.texData && null != e.texData.slice && e.texData.slice.flatOffset > 0 && (r.flatOffset = e.texData.slice.flatOffset), {\n      name: t.variableNames[n],\n      shapeInfo: r\n    };\n  }),\n      s = a.map(e => e.shapeInfo),\n      o = {\n    logicalShape: r.shape,\n    texShape: r.texData.texShape,\n    isUniform: !1,\n    isPacked: r.texData.isPacked,\n    flatOffset: null\n  },\n      i = makeShader$1(a, o, t),\n      l = e.createProgram(i);\n  var u = null;\n  var c = e.getUniformLocation(l, \"NAN\", !1);\n  1 === env$1().getNumber(\"WEBGL_VERSION\") && (u = e.getUniformLocation(l, \"INFINITY\", !1));\n  var p = !1,\n      d = {},\n      h = {},\n      m = {};\n\n  for (var _n199 = 0; _n199 < t.variableNames.length; _n199++) {\n    var _r148 = t.variableNames[_n199];\n    d[_r148] = e.getUniformLocation(l, _r148, p), d[\"offset\".concat(_r148)] = e.getUniformLocation(l, \"offset\".concat(_r148), p), t.enableShapeUniforms && (h[\"\".concat(_r148, \"Shape\")] = e.getUniformLocation(l, \"\".concat(_r148, \"Shape\"), p), m[\"\".concat(_r148, \"TexShape\")] = e.getUniformLocation(l, \"\".concat(_r148, \"TexShape\"), p));\n  }\n\n  var f, g, $;\n  t.enableShapeUniforms && (f = e.getUniformLocation(l, \"outShape\", p), $ = e.getUniformLocation(l, \"outShapeStrides\", p), g = e.getUniformLocation(l, \"outTexShape\", p));\n  var y = [];\n  return t.customUniforms && t.customUniforms.forEach((t, n) => {\n    y[n] = e.getUniformLocation(l, t.name, p);\n  }), {\n    program: t,\n    source: i,\n    webGLProgram: l,\n    uniformLocations: d,\n    customUniformLocations: y,\n    inShapeInfos: s,\n    outShapeInfo: o,\n    infLoc: u,\n    nanLoc: c,\n    inShapesLocations: h,\n    inTexShapesLocations: m,\n    outShapeLocation: f,\n    outShapeStridesLocation: $,\n    outTexShapeLocation: g\n  };\n}\n\nfunction validateBinaryAndProgram$1(e, t) {\n  if (e.length !== t.length) throw Error(\"Binary was compiled with \".concat(e.length, \" inputs, but was executed with \").concat(t.length, \" inputs\"));\n  e.forEach((e, n) => {\n    var r = e.logicalShape,\n        a = t[n],\n        s = a.shape;\n    if (!arraysEqual$1(r, s)) throw Error(\"Binary was compiled with different shapes than the current args. Shapes \".concat(r, \" and \").concat(s, \" must match\"));\n    if (e.isUniform && a.isUniform) return;\n    var o = e.texShape,\n        i = a.isUniform ? null : a.texData.texShape;\n    if (!arraysEqual$1(o, i)) throw Error(\"Binary was compiled with different texture shapes than the current args. Shape \".concat(o, \" and \").concat(i, \" must match\"));\n  });\n}\n\nfunction runProgram$1(e, t, n, r, a) {\n  t.program.enableShapeUniforms || (validateBinaryAndProgram$1(t.inShapeInfos, n), validateBinaryAndProgram$1([t.outShapeInfo], [r]));\n  var s = r.texData.texture,\n      o = r.texData.texShape;\n  r.texData.isPacked ? e.setOutputPackedMatrixTexture(s, o[0], o[1]) : e.setOutputMatrixTexture(s, o[0], o[1]), e.setProgram(t.webGLProgram), 1 === env$1().getNumber(\"WEBGL_VERSION\") && null !== t.infLoc && e.gl.uniform1f(t.infLoc, Infinity), null !== t.nanLoc && e.gl.uniform1f(t.nanLoc, NaN), n.forEach((n, r) => {\n    var a = t.program.variableNames[r],\n        s = t.uniformLocations[a],\n        o = t.uniformLocations[\"offset\".concat(a)],\n        i = t.inShapesLocations[\"\".concat(a, \"Shape\")],\n        l = t.inTexShapesLocations[\"\".concat(a, \"TexShape\")];\n\n    if (i) {\n      var {\n        uniformShape: _r149\n      } = getUniformInfoFromShape$1(t.program.packedInputs, n.shape, n.texData.texShape);\n\n      switch (_r149.length) {\n        case 1:\n          e.gl.uniform1iv(i, new Int32Array(_r149));\n          break;\n\n        case 2:\n          e.gl.uniform2iv(i, new Int32Array(_r149));\n          break;\n\n        case 3:\n          e.gl.uniform3iv(i, new Int32Array(_r149));\n          break;\n\n        case 4:\n          e.gl.uniform4iv(i, new Int32Array(_r149));\n      }\n    }\n\n    if (l && e.gl.uniform2i(l, n.texData.texShape[0], n.texData.texShape[1]), null != s) if (n.isUniform) {\n      if (sizeFromShape$1(n.shape) < 2) e.gl.uniform1f(s, n.uniformValues[0]);else {\n        var _t307 = n.uniformValues;\n        _t307 instanceof Float32Array || (_t307 = new Float32Array(_t307)), e.gl.uniform1fv(s, _t307);\n      }\n    } else null != n.texData.slice && null != o && e.gl.uniform1i(o, n.texData.slice.flatOffset), e.setInputMatrixTexture(n.texData.texture, s, r);\n  });\n  var i = t.outShapeLocation;\n  if (i) switch (r.shape.length) {\n    case 1:\n      e.gl.uniform1iv(i, new Int32Array(r.shape));\n      break;\n\n    case 2:\n      e.gl.uniform2iv(i, new Int32Array(r.shape));\n      break;\n\n    case 3:\n      e.gl.uniform3iv(i, new Int32Array(r.shape));\n      break;\n\n    case 4:\n      e.gl.uniform4iv(i, new Int32Array(r.shape));\n  }\n\n  if (t.outShapeStridesLocation) {\n    var _n200 = computeStrides$1(r.shape);\n\n    switch (r.shape.length) {\n      case 2:\n        e.gl.uniform1iv(t.outShapeStridesLocation, new Int32Array(_n200));\n        break;\n\n      case 3:\n        e.gl.uniform2iv(t.outShapeStridesLocation, new Int32Array(_n200));\n        break;\n\n      case 4:\n        e.gl.uniform3iv(t.outShapeStridesLocation, new Int32Array(_n200));\n    }\n  }\n\n  t.outTexShapeLocation && e.gl.uniform2i(t.outTexShapeLocation, r.texData.texShape[0], r.texData.texShape[1]), t.program.customUniforms && a && t.program.customUniforms.forEach((n, r) => {\n    var s = t.customUniformLocations[r],\n        o = a[r];\n    if (\"float\" === n.type) e.gl.uniform1fv(s, o);else if (\"vec2\" === n.type) e.gl.uniform2fv(s, o);else if (\"vec3\" === n.type) e.gl.uniform3fv(s, o);else if (\"vec4\" === n.type) e.gl.uniform4fv(s, o);else if (\"int\" === n.type) e.gl.uniform1iv(s, o);else if (\"ivec2\" === n.type) e.gl.uniform2iv(s, o);else if (\"ivec3\" === n.type) e.gl.uniform3iv(s, o);else {\n      if (\"ivec4\" !== n.type) throw Error(\"uniform type \".concat(n.type, \" is not supported yet.\"));\n      e.gl.uniform4iv(s, o);\n    }\n  }), e.executeProgram();\n}\n\nfunction makeShaderKey$1(e, t, n) {\n  var r = \"\";\n  t.concat(n).forEach(t => {\n    var a = null != t.texData && null != t.texData.slice && t.texData.slice.flatOffset > 0;\n\n    if (e.enableShapeUniforms && !t.isUniform) {\n      var s = t.texData.texShape,\n          {\n        useSqueezeShape: o,\n        uniformShape: i\n      } = getUniformInfoFromShape$1(e.packedInputs, t.shape, s);\n      var l = \"\",\n          u = \"\",\n          c = \"\";\n\n      if (1 === i.length && e.packedInputs) {\n        var _e419 = [Math.ceil(s[0] / 2), Math.ceil(s[1] / 2)];\n        l = \"\".concat(_e419[0] > 1, \"_\").concat(_e419[1] > 1);\n      } else if (2 !== i.length || e.packedInputs) {\n        if (i.length > 2 && !e.packedInputs) {\n          var _e420 = computeStrides$1(i);\n\n          c = \"\".concat(_e420[0] === s[1], \"_\").concat(_e420[_e420.length - 1] === s[1]);\n        }\n      } else u = \"\".concat(i[0] > 1, \"_\").concat(i[1] > 1);\n\n      var _p15 = t.shape.length,\n          d = 2 === _p15 && arraysEqual$1(t.shape, s),\n          h = 1 === sizeFromShape$1(t.shape),\n          m = getBroadcastDims$3(t.shape, n.shape),\n          f = !e.packedInputs && _p15 === n.shape.length && arraysEqual$1(s, n.texData.texShape);\n      r += \"\".concat(_p15, \"_\").concat(f, \"_\").concat(o, \"_\").concat(i.length, \"_\").concat(h, \"_\").concat(m, \"_\").concat(d, \"_\").concat(l, \"_\").concat(u, \"_\").concat(c, \"_\").concat(e.packedInputs || _p15 > 2 ? \"\" : \"\".concat(s[0] > 1, \"_\").concat(s[1] > 1), \"_\").concat(a);\n    } else r += \"\".concat(t.shape, \"_\").concat(t.isUniform ? \"uniform\" : t.texData.texShape, \"_\").concat(a);\n  });\n  var a = e.constructor.name;\n  return a += \"_\" + r + \"_\" + e.userCode + \"\".concat(env$1().getNumber(\"WEBGL_VERSION\")), a;\n}\n\nfunction useShapeUniforms$1(e) {\n  return env$1().getBool(\"WEBGL_USE_SHAPES_UNIFORMS\") && e <= 4;\n}\n\nvar {\n  addImpl: addImplCPU$1,\n  bincountImpl: bincountImplCPU$1,\n  bincountReduceImpl: bincountReduceImplCPU$1,\n  ceilImpl: ceilImplCPU$1,\n  concatImpl: concatImplCPU$1,\n  equalImpl: equalImplCPU$1,\n  expImpl: expImplCPU$1,\n  expm1Impl: expm1ImplCPU$1,\n  floorImpl: floorImplCPU$1,\n  gatherNdImpl: gatherNdImplCPU$1,\n  gatherV2Impl: gatherV2ImplCPU$1,\n  greaterImpl: greaterImplCPU$1,\n  greaterEqualImpl: greaterEqualImplCPU$1,\n  lessImpl: lessImplCPU$1,\n  lessEqualImpl: lessEqualImplCPU$1,\n  linSpaceImpl: linSpaceImplCPU$1,\n  logImpl: logImplCPU$1,\n  maxImpl: maxImplCPU$1,\n  maximumImpl: maximumImplCPU$1,\n  minimumImpl: minimumImplCPU$1,\n  multiplyImpl: multiplyImplCPU$1,\n  negImpl: negImplCPU$1,\n  notEqualImpl: notEqualImplCPU$1,\n  prodImpl: prodImplCPU$1,\n  rangeImpl: rangeImplCPU$1,\n  rsqrtImpl: rsqrtImplCPU$1,\n  simpleAbsImpl: simpleAbsImplCPU$1,\n  sliceImpl: sliceImplCPU$1,\n  sparseFillEmptyRowsImpl: sparseFillEmptyRowsImplCPU$1,\n  sparseReshapeImpl: sparseReshapeImplCPU$1,\n  sparseSegmentReductionImpl: sparseSegmentReductionImplCPU$1,\n  stridedSliceImpl: stridedSliceImplCPU$1,\n  stringNGramsImpl: stringNGramsImplCPU$1,\n  stringSplitImpl: stringSplitImplCPU$1,\n  stringToHashBucketFastImpl: stringToHashBucketFastImplCPU$1,\n  subImpl: subImplCPU$1,\n  tileImpl: tileImplCPU$1,\n  topKImpl: topKImplCPU$1,\n  transposeImpl: transposeImplCPU$1,\n  uniqueImpl: uniqueImplCPU$1\n} = shared$1;\n\nfunction getVecChannels$1(e, t) {\n  return [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"].slice(0, t).map(t => \"\".concat(e, \".\").concat(t));\n}\n\nfunction getChannels$1(e, t) {\n  return 1 === t ? [e] : getVecChannels$1(e, t);\n}\n\nfunction getSourceCoords$5(e, t) {\n  if (1 === e) return \"rc\";\n  var n = \"\";\n\n  for (var r = 0; r < e; r++) {\n    n += t[r], r < e - 1 && (n += \",\");\n  }\n\n  return n;\n}\n\nclass PackProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0, this.outputShape = e;\n    var t = e.length;\n    if (0 === t) this.userCode = \"\\n        void main() {\\n          setOutput(vec4(getA(), 0., 0., 0.));\\n        }\\n      \";else {\n      var n = getChannels$1(\"rc\", t),\n          r = getCoordsDataType$1(t),\n          a = getOutOfBoundsCondition$1(t, e, n),\n          s = getSetup$1(t, e[e.length - 1], e[e.length - 2], n),\n          o = getOutput$1(e, n);\n      this.userCode = \"\\n        void main() {\\n          \".concat(r, \" rc = getOutputCoords();\\n\\n          if(\").concat(a, \") {\\n            setOutput(vec4(0));\\n          } else {\\n            \").concat(s, \"\\n\\n            setOutput(vec4(\").concat(o, \"));\\n          }\\n        }\\n      \");\n    }\n  }\n\n}\n\nfunction getSourceCoordsArr$1(e, t) {\n  var n = [];\n\n  for (var r = 0; r <= 1; r++) {\n    for (var a = 0; a <= 1; a++) {\n      var s = \"\".concat(0 === r ? \"r\" : \"rp1\", \", \").concat(0 === a ? \"c\" : \"cp1\");\n\n      for (var _n201 = 2; _n201 < e; _n201++) {\n        s = \"\".concat(t[t.length - 1 - _n201], \",\") + s;\n      }\n\n      n.push(s);\n    }\n  }\n\n  return n;\n}\n\nfunction getOutOfBoundsCondition$1(e, t, n) {\n  if (1 === e) return \"rc > \".concat(t[0]);\n  var r = \"\";\n\n  for (var a = e - 2; a < e; a++) {\n    r += \"\".concat(n[a], \" >= \").concat(t[a]), a < e - 1 && (r += \"||\");\n  }\n\n  return r;\n}\n\nfunction getSetup$1(e, t, n, r) {\n  if (1 === e) return \"\";\n  var a = r.slice(-2);\n  return \"\\n    int r = \".concat(a[0], \";\\n    int c = \").concat(a[1], \";\\n    int rp1 = r + 1;\\n    int cp1 = c + 1;\\n\\n    bool cEdge = cp1 >= \").concat(t, \";\\n    bool rEdge = rp1 >= \").concat(n, \";\\n  \");\n}\n\nfunction getOutput$1(e, t) {\n  var n = e.length,\n      r = getSourceCoordsArr$1(n, t);\n  return 1 === n ? \"getA(rc),\\n            rc + 1 >= \".concat(e[0], \" ? 0. : getA(rc + 1),\\n            0, 0\") : \"getA(\".concat(r[0], \"),\\n          cEdge ? 0. : getA(\").concat(r[1], \"),\\n          rEdge ? 0. : getA(\").concat(r[2], \"),\\n          rEdge || cEdge ? 0. : getA(\").concat(r[3], \")\");\n}\n\nclass ReshapePackedProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e;\n    var n = \"\";\n\n    for (var _e421 = 0; _e421 < 4; _e421++) {\n      var _t308 = \"thisRC = rc;\";\n      _e421 % 2 == 1 && (_t308 += \"thisRC.z += 1;\"), _e421 > 1 && (_t308 += \"thisRC.y += 1;\"), n += \"\\n        \".concat(_t308, \"\\n        \").concat(_e421 > 0 ? \"if(thisRC.y < rows && thisRC.z < cols){\" : \"\", \"\\n          int flatIndex = getFlatIndex(thisRC);\\n\\n          ivec3 inputRC = inputCoordsFromReshapedOutCoords(flatIndex);\\n          vec2 inputRCInnerDims = vec2(float(inputRC.y),float(inputRC.z));\\n\\n          result[\").concat(_e421, \"] =\\n            getChannel(getA(inputRC.x, inputRC.y, inputRC.z), inputRCInnerDims);\\n        \").concat(_e421 > 0 ? \"}\" : \"\", \"\\n      \");\n    }\n\n    this.userCode = \"\\n      \".concat(getReshapedInputCoords$1(t), \"\\n      \").concat(getFlatIndexFrom3D$1(e), \"\\n\\n      void main() {\\n        ivec3 rc = getOutputCoords();\\n\\n        vec4 result = vec4(0.);\\n\\n        ivec3 thisRC;\\n        int rows = \").concat(e[1], \";\\n        int cols = \").concat(e[2], \";\\n\\n        \").concat(n, \"\\n\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction getReshapedInputCoords$1(e) {\n  return \"\\n    ivec3 inputCoordsFromReshapedOutCoords(int index) {\\n      \".concat(getLogicalCoordinatesFromFlatIndex$1([\"r\", \"c\", \"d\"], e), \"\\n      return ivec3(r, c, d);\\n    }\\n  \");\n}\n\nclass TextureManager$1 {\n  constructor(e) {\n    this.gpgpu = e, this.numUsedTextures = 0, this.numFreeTextures = 0, this._numBytesAllocated = 0, this._numBytesFree = 0, this.freeTextures = {}, this.logEnabled = !1, this.usedTextures = {};\n  }\n\n  acquireTexture(e, t, n) {\n    var r = getPhysicalFromLogicalTextureType$1(t, n),\n        a = getKeyFromTextureShape$1(e, r, n);\n    a in this.freeTextures || (this.freeTextures[a] = []), a in this.usedTextures || (this.usedTextures[a] = []);\n    var s = computeBytes$1(e, r, this.gpgpu.gl, this.gpgpu.textureConfig, n);\n\n    if (this.freeTextures[a].length > 0) {\n      this.numFreeTextures--, this.numUsedTextures++, this._numBytesFree -= s, this.log();\n\n      var _e422 = this.freeTextures[a].shift();\n\n      return this.usedTextures[a].push(_e422), _e422;\n    }\n\n    var o;\n    return r === PhysicalTextureType$1.PACKED_2X2_FLOAT32 ? o = this.gpgpu.createPackedMatrixTexture(e[0], e[1]) : r === PhysicalTextureType$1.PACKED_2X2_FLOAT16 ? o = this.gpgpu.createFloat16PackedMatrixTexture(e[0], e[1]) : r === PhysicalTextureType$1.UNPACKED_FLOAT32 ? o = this.gpgpu.createFloat32MatrixTexture(e[0], e[1]) : r === PhysicalTextureType$1.UNPACKED_FLOAT16 ? o = this.gpgpu.createFloat16MatrixTexture(e[0], e[1]) : r === PhysicalTextureType$1.PACKED_4X1_UNSIGNED_BYTE && (o = this.gpgpu.createUnsignedBytesMatrixTexture(e[0], e[1])), this.usedTextures[a].push(o), this.numUsedTextures++, this._numBytesAllocated += s, this.log(), o;\n  }\n\n  releaseTexture(e, t, n, r) {\n    if (null == this.freeTextures) return;\n    var a = getPhysicalFromLogicalTextureType$1(n, r),\n        s = getKeyFromTextureShape$1(t, a, r);\n    s in this.freeTextures || (this.freeTextures[s] = []);\n    var o = computeBytes$1(t, a, this.gpgpu.gl, this.gpgpu.textureConfig, r),\n        i = env$1().get(\"WEBGL_DELETE_TEXTURE_THRESHOLD\");\n    -1 !== i && this._numBytesAllocated > i ? (this.gpgpu.deleteMatrixTexture(e), this._numBytesAllocated -= o) : (this.freeTextures[s].push(e), this.numFreeTextures++, this._numBytesFree += o), this.numUsedTextures--;\n    var l = this.usedTextures[s],\n        u = l.indexOf(e);\n    if (u < 0) throw new Error(\"Cannot release a texture that was never provided by this texture manager\");\n    l.splice(u, 1), this.log();\n  }\n\n  log() {\n    if (!this.logEnabled) return;\n    console.log(\"Free/Used\", \"\".concat(this.numFreeTextures, \" / \").concat(this.numUsedTextures), \"(\".concat(this.numFreeTextures + this.numUsedTextures, \")\"));\n    var e = this._numBytesFree / this._numBytesAllocated;\n    console.log(\"Bytes allocated: \".concat(this._numBytesAllocated)), console.log(\"Bytes unused: \".concat(this._numBytesFree, \" (\").concat(Math.round(100 * e), \"%)\"));\n  }\n\n  get numBytesAllocated() {\n    return this._numBytesAllocated;\n  }\n\n  get numBytesFree() {\n    return this._numBytesFree;\n  }\n\n  getNumUsedTextures() {\n    return this.numUsedTextures;\n  }\n\n  getNumFreeTextures() {\n    return this.numFreeTextures;\n  }\n\n  dispose() {\n    if (null != this.freeTextures) {\n      for (var _e423 in this.freeTextures) {\n        this.freeTextures[_e423].forEach(e => {\n          this.gpgpu.deleteMatrixTexture(e);\n        });\n      }\n\n      for (var _e424 in this.usedTextures) {\n        this.usedTextures[_e424].forEach(e => {\n          this.gpgpu.deleteMatrixTexture(e);\n        });\n      }\n\n      this.freeTextures = null, this.usedTextures = null, this.numUsedTextures = 0, this.numFreeTextures = 0, this._numBytesAllocated = 0, this._numBytesFree = 0;\n    }\n  }\n\n}\n\nfunction numBytesForInternalFormat$1(e, t) {\n  if (t === e.R32F) return 4;\n  if (t === e.R16F) return 2;\n  if (t === e.RGBA32F) return 16;\n  if (t === e.RGBA) return 16;\n  if (t === e.RGBA16F) return 8;\n  throw new Error(\"Unknown internal format \".concat(t));\n}\n\nfunction computeBytes$1(e, t, n, r, a) {\n  var s = internalFormatForPhysicalTexType$1(t, r);\n  var o;\n\n  if (a) {\n    var [_t309, _n202] = getPackedMatrixTextureShapeWidthHeight$1(e[0], e[1]);\n    o = _t309 * _n202;\n  } else {\n    var [_t310, _n203] = getUnpackedMatrixTextureShapeWidthHeight$1(e[0], e[1]);\n    o = _t310 * _n203;\n  }\n\n  return o * numBytesForInternalFormat$1(n, s);\n}\n\nfunction internalFormatForPhysicalTexType$1(e, t) {\n  switch (e) {\n    case PhysicalTextureType$1.PACKED_2X2_FLOAT32:\n      return getInternalFormatForPackedMatrixTexture$1(t);\n\n    case PhysicalTextureType$1.PACKED_2X2_FLOAT16:\n      return getInternalFormatForFloat16PackedMatrixTexture$1(t);\n\n    case PhysicalTextureType$1.UNPACKED_FLOAT32:\n      return getInternalFormatForFloat32MatrixTexture$1(t);\n\n    case PhysicalTextureType$1.UNPACKED_FLOAT16:\n      return getInternalFormatForFloat16MatrixTexture$1(t);\n\n    case PhysicalTextureType$1.PACKED_4X1_UNSIGNED_BYTE:\n      return getInternalFormatForUnsignedBytesMatrixTexture$1(t);\n\n    default:\n      throw new Error(\"Unknown physical texture type \".concat(e));\n  }\n}\n\nfunction getPhysicalTextureForRendering$1(e) {\n  return env$1().getBool(\"WEBGL_RENDER_FLOAT32_ENABLED\") ? e ? PhysicalTextureType$1.PACKED_2X2_FLOAT32 : PhysicalTextureType$1.UNPACKED_FLOAT32 : e ? PhysicalTextureType$1.PACKED_2X2_FLOAT16 : PhysicalTextureType$1.UNPACKED_FLOAT16;\n}\n\nfunction getPhysicalFromLogicalTextureType$1(e, t) {\n  if (e === TextureUsage$1.UPLOAD) return PhysicalTextureType$1.PACKED_2X2_FLOAT32;\n  if (e === TextureUsage$1.RENDER || null == e) return getPhysicalTextureForRendering$1(t);\n  if (e === TextureUsage$1.DOWNLOAD || e === TextureUsage$1.PIXELS) return PhysicalTextureType$1.PACKED_4X1_UNSIGNED_BYTE;\n  throw new Error(\"Unknown logical texture type \".concat(e));\n}\n\nfunction getKeyFromTextureShape$1(e, t, n) {\n  return \"\".concat(e[0], \"_\").concat(e[1], \"_\").concat(t, \"_\").concat(n);\n}\n\nclass UnaryOpProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.outputShape = e, this.enableShapeUniforms = useShapeUniforms$1(this.outputShape.length), this.userCode = \"\\n      float unaryOperation(float x) {\\n        \".concat(t, \"\\n      }\\n\\n      void main() {\\n        float x = getAAtOutCoords();\\n        float y = unaryOperation(x);\\n\\n        setOutput(y);\\n      }\\n    \");\n  }\n\n}\n\nvar CHECK_NAN_SNIPPET$5 = \"if (isnan(x)) return x;\",\n    LINEAR$3 = \"return x;\",\n    ABS$3 = \"return abs(x);\",\n    ELU$6 = \"return (x >= 0.0) ? x : (exp(x) - 1.0);\",\n    RELU$5 = CHECK_NAN_SNIPPET$5 + \"\\n  return (x < 0.0) ? 0.0 : x;\\n\",\n    RELU6$5 = CHECK_NAN_SNIPPET$5 + \"\\n  return (x < 0.0) ? 0.0 : min(6.0, x);\\n\",\n    CLONE$1 = \"return x;\",\n    SIGMOID$5 = \"return 1.0 / (1.0 + exp(-1.0 * x));\",\n    LINEAR$2 = \"return x;\",\n    ELU$5 = \"\\n  vec4 result;\\n\\n  result.r = (x.r >= 0.0) ? x.r : (exp(x.r) - 1.0);\\n  result.g = (x.g >= 0.0) ? x.g : (exp(x.g) - 1.0);\\n  result.b = (x.b >= 0.0) ? x.b : (exp(x.b) - 1.0);\\n  result.a = (x.a >= 0.0) ? x.a : (exp(x.a) - 1.0);\\n\\n  return result;\\n\",\n    RELU$4 = \"\\n  vec4 result = x * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    RELU6$4 = \"\\n  vec4 result = min(x, vec4(6.)) * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    SIGMOID$4 = \"return 1.0 / (1.0 + exp(-1.0 * x));\";\n\nclass UnaryOpPackedProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e, this.enableShapeUniforms = useShapeUniforms$1(this.outputShape.length), this.userCode = \"\\n      vec4 unaryOperation(vec4 x) {\\n        \".concat(t, \"\\n      }\\n\\n      void main() {\\n        vec4 x = getAAtOutCoords();\\n        vec4 y = unaryOperation(x);\\n\\n        setOutput(y);\\n      }\\n    \");\n  }\n\n}\n\nclass UnpackProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !1, this.outputShape = e;\n    var t = e.length,\n        n = getChannels$1(\"rc\", t),\n        r = getCoordsDataType$1(t),\n        a = getSourceCoords$5(t, n),\n        s = n.slice(-2),\n        o = t <= 1 ? \"rc\" : \"vec2(\".concat(s.join(\",\"), \")\");\n    this.userCode = \"\\n      void main() {\\n        \".concat(r, \" rc = getOutputCoords();\\n        vec4 packedInput = getA(\").concat(a, \");\\n\\n        setOutput(getChannel(packedInput, \").concat(o, \"));\\n      }\\n    \");\n  }\n\n}\n\nvar whereImpl$3 = whereImpl$5,\n    EPSILON_FLOAT32$2 = 1e-7,\n    EPSILON_FLOAT16$2 = 1e-4,\n    binaryCaches$1 = {};\n\nfunction getBinaryCache$1(e) {\n  return e in binaryCaches$1 || (binaryCaches$1[e] = {}), binaryCaches$1[e];\n}\n\nvar CPU_HANDOFF_SIZE_THRESHOLD$1 = env$1().getNumber(\"CPU_HANDOFF_SIZE_THRESHOLD\"),\n    BEFORE_PAGING_CONSTANT$1 = 600;\n\nfunction numMBBeforeWarning$1() {\n  return null == env$1().global.screen ? 1024 : env$1().global.screen.height * env$1().global.screen.width * window.devicePixelRatio * BEFORE_PAGING_CONSTANT$1 / 1024 / 1024;\n}\n\nclass MathBackendWebGL$1 extends KernelBackend$1 {\n  constructor(e) {\n    if (super(), this.pendingRead = new WeakMap(), this.pendingDisposal = new WeakSet(), this.dataRefCount = new WeakMap(), this.numBytesInGPU = 0, this.uploadWaitMs = 0, this.downloadWaitMs = 0, this.lastGlFlushTime = 0, this.warnedAboutMemory = !1, this.pendingDeletes = 0, this.disposed = !1, !env$1().getBool(\"HAS_WEBGL\")) throw new Error(\"WebGL is not supported on this device\");\n\n    if (null == e) {\n      var _e425 = getWebGLContext$1(env$1().getNumber(\"WEBGL_VERSION\"));\n\n      this.binaryCache = getBinaryCache$1(env$1().getNumber(\"WEBGL_VERSION\")), this.gpgpu = new GPGPUContext$1(_e425), this.canvas = _e425.canvas, this.gpgpuCreatedLocally = !0;\n    } else this.gpgpu = e, this.binaryCache = {}, this.gpgpuCreatedLocally = !1, this.canvas = e.gl.canvas;\n\n    this.textureManager = new TextureManager$1(this.gpgpu), this.numMBBeforeWarning = numMBBeforeWarning$1(), this.texData = new DataStorage$1(this, engine$1());\n  }\n\n  nextDataId() {\n    return MathBackendWebGL$1.nextDataId++;\n  }\n\n  numDataIds() {\n    return this.texData.numDataIds() - this.pendingDeletes;\n  }\n\n  write(e, t, n) {\n    if ((env$1().getBool(\"WEBGL_CHECK_NUMERICAL_PROBLEMS\") || env$1().getBool(\"DEBUG\")) && this.checkNumericalProblems(e), \"complex64\" === n && null != e) throw new Error(\"Cannot write to a complex64 dtype. Please use tf.complex(real, imag).\");\n    var r = {\n      id: this.nextDataId()\n    };\n    return this.texData.set(r, {\n      shape: t,\n      dtype: n,\n      values: e,\n      usage: TextureUsage$1.UPLOAD,\n      refCount: 1\n    }), r;\n  }\n\n  refCount(e) {\n    return this.texData.has(e) ? this.texData.get(e).refCount : 0;\n  }\n\n  incRef(e) {\n    this.texData.get(e).refCount++;\n  }\n\n  decRef(e) {\n    this.texData.has(e) && this.texData.get(e).refCount--;\n  }\n\n  move(e, t, n, r, a) {\n    if (env$1().getBool(\"DEBUG\") && this.checkNumericalProblems(t), \"complex64\" === r) throw new Error(\"Cannot write to a complex64 dtype. Please use tf.complex(real, imag).\");\n    this.texData.set(e, {\n      shape: n,\n      dtype: r,\n      values: t,\n      usage: TextureUsage$1.UPLOAD,\n      refCount: a\n    });\n  }\n\n  disposeIntermediateTensorInfo(e) {\n    this.disposeData(e.dataId);\n  }\n\n  readSync(e) {\n    var t = this.texData.get(e),\n        {\n      values: n,\n      dtype: r,\n      complexTensorInfos: a,\n      slice: s,\n      shape: o,\n      isPacked: i\n    } = t;\n\n    if (null != s) {\n      var _t311;\n\n      _t311 = i ? new UnaryOpPackedProgram$1(o, CLONE$1) : new UnaryOpProgram$1(o, CLONE$1);\n\n      var _n204 = this.runWebGLProgram(_t311, [{\n        dataId: e,\n        shape: o,\n        dtype: r\n      }], r),\n          _a106 = this.readSync(_n204.dataId);\n\n      return this.disposeIntermediateTensorInfo(_n204), _a106;\n    }\n\n    if (null != n) return this.convertAndCacheOnCPU(e);\n    if (\"string\" === r) return n;\n    var l = null != this.activeTimers;\n    var u, c;\n    return l && (u = now$1()), c = \"complex64\" === r ? mergeRealAndImagArrays$1(this.readSync(a.real.dataId), this.readSync(a.imag.dataId)) : this.getValuesFromTexture(e), l && (this.downloadWaitMs += now$1() - u), this.convertAndCacheOnCPU(e, c);\n  }\n\n  read(e) {\n    var _this73 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this73.pendingRead.has(e)) {\n        var _t312 = _this73.pendingRead.get(e);\n\n        return new Promise(e => _t312.push(e));\n      }\n\n      var t = _this73.texData.get(e),\n          {\n        values: n,\n        shape: r,\n        slice: a,\n        dtype: s,\n        complexTensorInfos: o,\n        isPacked: i\n      } = t;\n\n      if (null != a) {\n        var _t313;\n\n        _t313 = i ? new UnaryOpPackedProgram$1(r, CLONE$1) : new UnaryOpProgram$1(r, CLONE$1);\n\n        var _n205 = _this73.runWebGLProgram(_t313, [{\n          dataId: e,\n          shape: r,\n          dtype: s\n        }], s),\n            _a107 = _this73.read(_n205.dataId);\n\n        return _this73.disposeIntermediateTensorInfo(_n205), _a107;\n      }\n\n      if (null != n) return _this73.convertAndCacheOnCPU(e);\n      if (!env$1().getBool(\"WEBGL_DOWNLOAD_FLOAT_ENABLED\") && 2 === env$1().getNumber(\"WEBGL_VERSION\")) throw new Error(\"tensor.data() with WEBGL_DOWNLOAD_FLOAT_ENABLED=false and WEBGL_VERSION=2 not yet supported.\");\n      var l,\n          u,\n          c = null;\n\n      if (\"complex64\" !== s && env$1().get(\"WEBGL_BUFFER_SUPPORTED\")) {\n        l = _this73.decode(e);\n\n        var _t314 = _this73.texData.get(l.dataId);\n\n        c = _this73.gpgpu.createBufferFromTexture(_t314.texture, ...getDenseTexShape$1(r));\n      }\n\n      if (_this73.pendingRead.set(e, []), \"complex64\" !== s && (yield _this73.gpgpu.createAndWaitForFence()), \"complex64\" === s) {\n        var _e426 = yield Promise.all([_this73.read(o.real.dataId), _this73.read(o.imag.dataId)]);\n\n        u = mergeRealAndImagArrays$1(_e426[0], _e426[1]);\n      } else if (null == c) u = _this73.getValuesFromTexture(e);else {\n        var _e427 = sizeFromShape$1(r);\n\n        u = _this73.gpgpu.downloadFloat32MatrixFromBuffer(c, _e427);\n      }\n\n      if (null != l && _this73.disposeIntermediateTensorInfo(l), null != c) {\n        var _e428 = _this73.gpgpu.gl;\n        callAndCheck$1(_e428, () => _e428.deleteBuffer(c));\n      }\n\n      var p = _this73.convertAndCacheOnCPU(e, u),\n          d = _this73.pendingRead.get(e);\n\n      return _this73.pendingRead.delete(e), d.forEach(e => e(p)), _this73.pendingDisposal.has(e) && (_this73.pendingDisposal.delete(e), _this73.disposeData(e) && engine$1().removeDataId(e, _this73), _this73.pendingDeletes--), p;\n    })();\n  }\n\n  bufferSync(e) {\n    var t = this.readSync(e.dataId);\n    var n = t;\n    if (\"string\" === e.dtype) try {\n      n = t.map(e => decodeString$1(e));\n    } catch (e) {\n      throw new Error(\"Failed to decode encoded string bytes into utf-8\");\n    }\n    return buffer$1(e.shape, e.dtype, n);\n  }\n\n  checkNumericalProblems(e) {\n    if (null != e) for (var t = 0; t < e.length; t++) {\n      var n = e[t];\n\n      if (!canBeRepresented$1(n)) {\n        if (env$1().getBool(\"WEBGL_RENDER_FLOAT32_CAPABLE\")) throw Error(\"The value \".concat(n, \" cannot be represented with your current settings. Consider enabling float32 rendering: 'tf.env().set('WEBGL_RENDER_FLOAT32_ENABLED', true);'\"));\n        throw Error(\"The value \".concat(n, \" cannot be represented on this device.\"));\n      }\n    }\n  }\n\n  getValuesFromTexture(e) {\n    var {\n      shape: t,\n      dtype: n,\n      isPacked: r\n    } = this.texData.get(e),\n        a = sizeFromShape$1(t);\n\n    if (env$1().getBool(\"WEBGL_DOWNLOAD_FLOAT_ENABLED\")) {\n      var _n206 = this.decode(e),\n          _r150 = this.texData.get(_n206.dataId),\n          _s77 = this.gpgpu.downloadMatrixFromPackedTexture(_r150.texture, ...getDenseTexShape$1(t)).subarray(0, a);\n\n      return this.disposeIntermediateTensorInfo(_n206), _s77;\n    }\n\n    var s = env$1().getBool(\"WEBGL_PACK\") && !0 === r,\n        o = s ? getShapeAs3D$1(t) : t,\n        i = s ? new EncodeFloatPackedProgram$1(o) : new EncodeFloatProgram$1(o),\n        l = this.runWebGLProgram(i, [{\n      shape: o,\n      dtype: n,\n      dataId: e\n    }], \"float32\"),\n        u = this.texData.get(l.dataId),\n        c = this.gpgpu.downloadByteEncodedFloatMatrixFromOutputTexture(u.texture, u.texShape[0], u.texShape[1]).subarray(0, a);\n    return this.disposeIntermediateTensorInfo(l), c;\n  }\n\n  timerAvailable() {\n    return env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0;\n  }\n\n  time(e) {\n    var _this74 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = _this74.activeTimers,\n          n = [];\n      var r = !1;\n      null == _this74.programTimersStack ? (_this74.programTimersStack = n, r = !0) : _this74.activeTimers.push(n), _this74.activeTimers = n, e();\n      var a = flatten$6(_this74.activeTimers.map(e => e.query)).filter(e => null != e),\n          s = flatten$6(_this74.activeTimers.map(e => e.name)).filter(e => null != e);\n      _this74.activeTimers = t, r && (_this74.programTimersStack = null);\n      var o = {\n        uploadWaitMs: _this74.uploadWaitMs,\n        downloadWaitMs: _this74.downloadWaitMs,\n        kernelMs: null,\n        wallMs: null\n      };\n\n      if (env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0) {\n        var _e429 = yield Promise.all(a);\n\n        o.kernelMs = sum$7(_e429), o.getExtraProfileInfo = () => _e429.map((e, t) => ({\n          name: s[t],\n          ms: e\n        })).map(e => \"\".concat(e.name, \": \").concat(e.ms)).join(\", \");\n      } else o.kernelMs = {\n        error: \"WebGL query timers are not supported in this environment.\"\n      };\n\n      return _this74.uploadWaitMs = 0, _this74.downloadWaitMs = 0, o;\n    })();\n  }\n\n  memory() {\n    return {\n      unreliable: !1,\n      numBytesInGPU: this.numBytesInGPU,\n      numBytesInGPUAllocated: this.textureManager.numBytesAllocated,\n      numBytesInGPUFree: this.textureManager.numBytesFree\n    };\n  }\n\n  startTimer() {\n    return env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0 ? this.gpgpu.beginQuery() : {\n      startMs: now$1(),\n      endMs: null\n    };\n  }\n\n  endTimer(e) {\n    return env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0 ? (this.gpgpu.endQuery(), e) : (e.endMs = now$1(), e);\n  }\n\n  getQueryTime(e) {\n    var _this75 = this;\n\n    return _asyncToGenerator(function* () {\n      return env$1().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0 ? _this75.gpgpu.waitForQueryAndGetTime(e) : e.endMs - e.startMs;\n    })();\n  }\n\n  disposeData(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    if (this.pendingDisposal.has(e)) return !1;\n    if (!this.texData.has(e)) return !0;\n    if (t ? this.texData.get(e).refCount = 0 : this.texData.get(e).refCount--, !t && this.texData.get(e).refCount > 0) return !1;\n    if (this.pendingRead.has(e)) return this.pendingDisposal.add(e), this.pendingDeletes++, !1;\n    this.releaseGPUData(e);\n    var {\n      complexTensorInfos: n\n    } = this.texData.get(e);\n    return null != n && (this.disposeData(n.real.dataId, t), this.disposeData(n.imag.dataId, t)), this.texData.delete(e), !0;\n  }\n\n  releaseGPUData(e) {\n    var {\n      texture: t,\n      dtype: n,\n      texShape: r,\n      usage: a,\n      isPacked: s,\n      slice: o\n    } = this.texData.get(e),\n        i = o && o.origDataId || e,\n        l = this.dataRefCount.get(i);\n    l > 1 ? this.dataRefCount.set(i, l - 1) : (this.dataRefCount.delete(i), null != t && (this.numBytesInGPU -= this.computeBytes(r, n), this.textureManager.releaseTexture(t, r, a, s)));\n    var u = this.texData.get(e);\n    u.texture = null, u.texShape = null, u.isPacked = !1, u.slice = null;\n  }\n\n  getTexture(e) {\n    return this.uploadToGPU(e), this.texData.get(e).texture;\n  }\n\n  getDataInfo(e) {\n    return this.texData.get(e);\n  }\n\n  shouldExecuteOnCPU(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : CPU_HANDOFF_SIZE_THRESHOLD$1;\n    return env$1().getBool(\"WEBGL_CPU_FORWARD\") && e.every(e => null == this.texData.get(e.dataId).texture && sizeFromShape$1(e.shape) < t);\n  }\n\n  getGPGPUContext() {\n    return this.gpgpu;\n  }\n\n  where(e) {\n    warn$1(\"tf.where() in webgl locks the UI thread. Call tf.whereAsync() instead\");\n    var t = e.dataSync();\n    return whereImpl$3(e.shape, t);\n  }\n\n  packedUnaryOp(e, t, n) {\n    var r = new UnaryOpPackedProgram$1(e.shape, t),\n        a = this.compileAndRun(r, [e], n);\n    return engine$1().makeTensorFromDataId(a.dataId, a.shape, a.dtype);\n  }\n\n  abs(e) {\n    if (this.shouldExecuteOnCPU([e]) && \"complex64\" !== e.dtype) {\n      var _t315 = simpleAbsImplCPU$1(this.texData.get(e.dataId).values);\n\n      return this.makeOutput(e.shape, e.dtype, _t315);\n    }\n\n    if (env$1().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\")) return this.packedUnaryOp(e, ABS$3, e.dtype);\n    var t = new UnaryOpProgram$1(e.shape, ABS$3),\n        n = this.compileAndRun(t, [e]);\n    return engine$1().makeTensorFromDataId(n.dataId, n.shape, n.dtype);\n  }\n\n  makeTensorInfo(e, t, n) {\n    var r;\n\n    if (\"string\" === t && null != n && n.length > 0 && isString$1(n[0])) {\n      var a = n.map(e => encodeString$1(e));\n      r = this.write(a, e, t);\n    } else r = this.write(n, e, t);\n\n    return this.texData.get(r).usage = null, {\n      dataId: r,\n      shape: e,\n      dtype: t\n    };\n  }\n\n  makeOutput(e, t, n) {\n    var {\n      dataId: r\n    } = this.makeTensorInfo(e, t, n);\n    return engine$1().makeTensorFromDataId(r, e, t, this);\n  }\n\n  unpackTensor(e) {\n    var t = new UnpackProgram$1(e.shape);\n    return this.runWebGLProgram(t, [e], e.dtype);\n  }\n\n  packTensor(e) {\n    var t = new PackProgram$1(e.shape);\n    return this.runWebGLProgram(t, [e], e.dtype, null, !0);\n  }\n\n  packedReshape(e, t) {\n    var n = [getBatchDim$1(e.shape), ...getRowsCols$1(e.shape)],\n        r = {\n      dtype: e.dtype,\n      shape: n,\n      dataId: e.dataId\n    },\n        a = [getBatchDim$1(t), ...getRowsCols$1(t)],\n        s = new ReshapePackedProgram$1(a, n),\n        o = this.runWebGLProgram(s, [r], e.dtype, null, !0);\n    return {\n      dataId: o.dataId,\n      shape: t,\n      dtype: o.dtype\n    };\n  }\n\n  decode(e) {\n    var t = this.texData.get(e),\n        {\n      isPacked: n,\n      shape: r,\n      dtype: a\n    } = t,\n        s = getShapeAs3D$1(r);\n    var o;\n    return o = n ? new DecodeMatrixPackedProgram$1(s) : new DecodeMatrixProgram$1(s), {\n      dtype: a,\n      shape: r,\n      dataId: this.runWebGLProgram(o, [{\n        shape: s,\n        dtype: a,\n        dataId: e\n      }], a, null, !0).dataId\n    };\n  }\n\n  runWebGLProgram(e, t, n, r) {\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    var s = this.makeTensorInfo(e.outputShape, n),\n        o = this.texData.get(s.dataId);\n\n    if (e.packedOutput && (o.isPacked = !0), e.outPackingScheme === PackingScheme$1.DENSE) {\n      var _t316 = getDenseTexShape$1(e.outputShape);\n\n      o.texShape = _t316.map(e => 2 * e);\n    }\n\n    if (null != e.outTexUsage && (o.usage = e.outTexUsage), 0 === sizeFromShape$1(s.shape)) return o.values = getTypedArrayFromDType$1(s.dtype, 0), s;\n    var i = [],\n        l = t.map(t => {\n      if (\"complex64\" === t.dtype) throw new Error(\"GPGPUProgram does not support complex64 input. For complex64 dtypes, please separate the program into real and imaginary parts.\");\n      var n = this.texData.get(t.dataId);\n\n      if (null == n.texture) {\n        if (!e.packedInputs && sizeFromShape$1(t.shape) <= env$1().getNumber(\"WEBGL_SIZE_UPLOAD_UNIFORM\")) return {\n          shape: t.shape,\n          texData: null,\n          isUniform: !0,\n          uniformValues: n.values\n        };\n        e.packedInputs && (n.isPacked = !0, n.shape = t.shape);\n      } else if (!!n.isPacked != !!e.packedInputs) t = n.isPacked ? this.unpackTensor(t) : this.packTensor(t), i.push(t), n = this.texData.get(t.dataId);else if (n.isPacked && !isReshapeFree$1(n.shape, t.shape)) {\n        var _e430 = t,\n            _r151 = t.shape;\n        t.shape = n.shape, t = this.packedReshape(t, _r151), i.push(t), n = this.texData.get(t.dataId), _e430.shape = _r151;\n      }\n\n      return this.uploadToGPU(t.dataId), {\n        shape: t.shape,\n        texData: n,\n        isUniform: !1\n      };\n    });\n    this.uploadToGPU(s.dataId);\n    var u = {\n      shape: s.shape,\n      texData: o,\n      isUniform: !1\n    },\n        c = makeShaderKey$1(e, l, u),\n        p = this.getAndSaveBinary(c, () => compileProgram$1(this.gpgpu, e, l, u)),\n        d = null != this.activeTimers;\n    var h;\n    d && (h = this.startTimer()), runProgram$1(this.gpgpu, p, l, u, r), i.forEach(e => this.disposeIntermediateTensorInfo(e)), d && (h = this.endTimer(h), this.activeTimers.push({\n      name: e.constructor.name,\n      query: this.getQueryTime(h)\n    }));\n    var m = env$1().get(\"WEBGL_FLUSH_THRESHOLD\");\n\n    if (m > 0) {\n      var _e431 = now$1();\n\n      _e431 - this.lastGlFlushTime > m && (this.gpgpu.gl.flush(), this.lastGlFlushTime = _e431);\n    }\n\n    if (!env$1().getBool(\"WEBGL_LAZILY_UNPACK\") && o.isPacked && !1 === a) {\n      var _e432 = this.unpackTensor(s);\n\n      return this.disposeIntermediateTensorInfo(s), _e432;\n    }\n\n    return s;\n  }\n\n  compileAndRun(e, t, n, r) {\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    return this.runWebGLProgram(e, t, n = n || t[0].dtype, r, a);\n  }\n\n  getAndSaveBinary(e, t) {\n    return e in this.binaryCache || (this.binaryCache[e] = t()), this.binaryCache[e];\n  }\n\n  getTextureManager() {\n    return this.textureManager;\n  }\n\n  dispose() {\n    this.disposed || (env$1().getBool(\"IS_TEST\") || Object.keys(this.binaryCache).forEach(e => {\n      this.gpgpu.deleteProgram(this.binaryCache[e].webGLProgram), delete this.binaryCache[e];\n    }), this.textureManager.dispose(), null != this.canvas && \"undefined\" != typeof HTMLCanvasElement && this.canvas instanceof HTMLCanvasElement ? this.canvas.remove() : this.canvas = null, this.gpgpuCreatedLocally && (this.gpgpu.program = null, this.gpgpu.dispose()), this.disposed = !0);\n  }\n\n  floatPrecision() {\n    return null == this.floatPrecisionValue && (this.floatPrecisionValue = tidy$1(() => {\n      if (!env$1().get(\"WEBGL_RENDER_FLOAT32_ENABLED\")) {\n        var _e433 = env$1().getBool(\"DEBUG\");\n\n        env$1().set(\"DEBUG\", !1);\n        var t = this.abs(scalar$1(1e-8)).dataSync()[0];\n        if (env$1().set(\"DEBUG\", _e433), t > 0) return 32;\n      }\n\n      return 16;\n    })), this.floatPrecisionValue;\n  }\n\n  epsilon() {\n    return 32 === this.floatPrecision() ? EPSILON_FLOAT32$2 : EPSILON_FLOAT16$2;\n  }\n\n  uploadToGPU(e) {\n    var t = this.texData.get(e),\n        {\n      shape: n,\n      dtype: r,\n      values: a,\n      texture: s,\n      usage: o,\n      isPacked: i\n    } = t;\n    if (null != s) return;\n    var l = null != this.activeTimers;\n    var u;\n    l && (u = now$1());\n    var c = t.texShape;\n\n    if (null == c && (c = getTextureShapeFromLogicalShape$1(n, i), t.texShape = c), null != a) {\n      var _e434 = getShapeAs3D$1(n);\n\n      var _s78,\n          _o54 = c[1],\n          _p16 = c[0];\n\n      var d = a instanceof Uint8Array;\n      i ? ([_o54, _p16] = getPackedMatrixTextureShapeWidthHeight$1(c[0], c[1]), _s78 = new EncodeMatrixPackedProgram$1(_e434, [_p16, _o54], d)) : _s78 = new EncodeMatrixProgram$1(_e434, [_p16, _o54], d);\n      var h = this.makeTensorInfo([_p16, _o54], r);\n      this.texData.get(h.dataId).usage = d ? TextureUsage$1.PIXELS : TextureUsage$1.UPLOAD, this.gpgpu.uploadDenseMatrixToTexture(this.getTexture(h.dataId), _o54, _p16, a);\n      var m = this.runWebGLProgram(_s78, [h], r, null, !0),\n          f = this.texData.get(m.dataId);\n      t.texture = f.texture, t.texShape = f.texShape, t.isPacked = f.isPacked, t.usage = f.usage, this.disposeIntermediateTensorInfo(h), this.texData.delete(m.dataId), t.values = null, l && (this.uploadWaitMs += now$1() - u);\n    } else {\n      var _e435 = this.acquireTexture(c, o, r, i);\n\n      t.texture = _e435;\n    }\n  }\n\n  convertAndCacheOnCPU(e, t) {\n    var n = this.texData.get(e),\n        {\n      dtype: r\n    } = n;\n    return this.releaseGPUData(e), null != t && (n.values = float32ToTypedArray$1(t, r)), n.values;\n  }\n\n  acquireTexture(e, t, n, r) {\n    if (this.numBytesInGPU += this.computeBytes(e, n), !this.warnedAboutMemory && this.numBytesInGPU > 1024 * this.numMBBeforeWarning * 1024) {\n      var _e436 = (this.numBytesInGPU / 1024 / 1024).toFixed(2);\n\n      this.warnedAboutMemory = !0, console.warn(\"High memory usage in GPU: \".concat(_e436, \" MB, most likely due to a memory leak\"));\n    }\n\n    return this.textureManager.acquireTexture(e, t, r);\n  }\n\n  computeBytes(e, t) {\n    return e[0] * e[1] * bytesPerElement$1(t);\n  }\n\n}\n\nfunction float32ToTypedArray$1(e, t) {\n  if (\"float32\" === t || \"complex64\" === t) return e;\n\n  if (\"int32\" === t || \"bool\" === t) {\n    var n = \"int32\" === t ? new Int32Array(e.length) : new Uint8Array(e.length);\n\n    for (var _t317 = 0; _t317 < n.length; ++_t317) {\n      n[_t317] = Math.round(e[_t317]);\n    }\n\n    return n;\n  }\n\n  throw new Error(\"Unknown dtype \".concat(t));\n}\n\nMathBackendWebGL$1.nextDataId = 0;\nvar version$9 = \"3.8.0\";\nisBrowser$1() && registerBackend$1(\"webgl\", () => new MathBackendWebGL$1(), 2);\nvar CHECK_NAN_SNIPPET$4 = \"\\n  if (isnan(a)) return a;\\n  if (isnan(b)) return b;\\n\";\n\nclass BinaryOpProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"A\", \"B\"], this.outputShape = assertAndGetBroadcastShape$1(t, n), this.enableShapeUniforms = useShapeUniforms$1(this.outputShape.length), this.userCode = \"\\n      float binaryOperation(float a, float b) {\\n        \".concat(e, \"\\n      }\\n\\n      void main() {\\n        float a = getAAtOutCoords();\\n        float b = getBAtOutCoords();\\n        setOutput(binaryOperation(a, b));\\n      }\\n    \");\n  }\n\n}\n\nvar CHECK_NAN_SNIPPET$3 = \"\\n  result.r = isNaN.r > 0. ? NAN : result.r;\\n  result.g = isNaN.g > 0. ? NAN : result.g;\\n  result.b = isNaN.b > 0. ? NAN : result.b;\\n  result.a = isNaN.a > 0. ? NAN : result.a;\\n\";\n\nclass BinaryOpPackedProgram$1 {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    this.variableNames = [\"A\", \"B\"], this.supportsBroadcasting = !0, this.packedInputs = !0, this.packedOutput = !0, this.outputShape = assertAndGetBroadcastShape$1(t, n);\n    var a = this.outputShape.length;\n    this.enableShapeUniforms = useShapeUniforms$1(a);\n    var s = \"\";\n    if (r) if (0 === a || 1 === sizeFromShape$1(this.outputShape)) s = \"\\n          result.y = 0.;\\n          result.z = 0.;\\n          result.w = 0.;\\n        \";else if (s = \"\\n          \".concat(getCoordsDataType$1(a), \" coords = getOutputCoords();\\n        \"), 1 === a) s += this.enableShapeUniforms ? \"\\n            result.y = (coords + 1) >= outShape ? 0. : result.y;\\n            result.z = 0.;\\n            result.w = 0.;\\n          \" : \"\\n            result.y = (coords + 1) >= \".concat(this.outputShape[0], \" ? 0. : result.y;\\n            result.z = 0.;\\n            result.w = 0.;\\n          \");else {\n      var _e437 = getChannels$1(\"coords\", a);\n\n      s += this.enableShapeUniforms ? \"\\n            bool nextRowOutOfBounds =\\n              (\".concat(_e437[a - 2], \" + 1) >= outShape[\").concat(a, \" - 2];\\n            bool nextColOutOfBounds =\\n              (\").concat(_e437[a - 1], \" + 1) >= outShape[\").concat(a, \" - 1];\\n            result.y = nextColOutOfBounds ? 0. : result.y;\\n            result.z = nextRowOutOfBounds ? 0. : result.z;\\n            result.w = nextColOutOfBounds || nextRowOutOfBounds ? 0. : result.w;\\n          \") : \"\\n            bool nextRowOutOfBounds =\\n              (\".concat(_e437[a - 2], \" + 1) >= \").concat(this.outputShape[a - 2], \";\\n            bool nextColOutOfBounds =\\n              (\").concat(_e437[a - 1], \" + 1) >= \").concat(this.outputShape[a - 1], \";\\n            result.y = nextColOutOfBounds ? 0. : result.y;\\n            result.z = nextRowOutOfBounds ? 0. : result.z;\\n            result.w = nextColOutOfBounds || nextRowOutOfBounds ? 0. : result.w;\\n          \");\n    }\n    this.userCode = \"\\n      vec4 binaryOperation(vec4 a, vec4 b) {\\n        \".concat(e, \"\\n      }\\n\\n      void main() {\\n        vec4 a = getAAtOutCoords();\\n        vec4 b = getBAtOutCoords();\\n\\n        vec4 result = binaryOperation(a, b);\\n        \").concat(s, \"\\n\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction identity$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  return n.incRef(r.dataId), {\n    dataId: r.dataId,\n    shape: r.shape,\n    dtype: r.dtype\n  };\n}\n\nvar identityConfig$2 = {\n  kernelName: Identity$3,\n  backendName: \"webgl\",\n  kernelFunc: identity$3\n};\n\nfunction complex$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    real: r,\n    imag: a\n  } = t,\n      s = n.makeTensorInfo(r.shape, \"complex64\"),\n      o = n.texData.get(s.dataId),\n      i = identity$3({\n    inputs: {\n      x: r\n    },\n    backend: n\n  }),\n      l = identity$3({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  return o.complexTensorInfos = {\n    real: i,\n    imag: l\n  }, s;\n}\n\nvar complexConfig$2 = {\n  kernelName: Complex$1,\n  backendName: \"webgl\",\n  kernelFunc: complex$3\n},\n    LEAKYRELU$1 = \"return (a < 0.) ? b * a : a;\",\n    LEAKYRELU_PACKED$1 = \"\\n  vec4 aLessThanZero = vec4(lessThan(a, vec4(0.)));\\n  return (aLessThanZero * (b * a)) + ((vec4(1.0) - aLessThanZero) * a);\\n\";\n\nfunction leakyRelu$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    alpha: s\n  } = r,\n      o = n.makeTensorInfo([], \"float32\", createScalarValue$1(s, \"float32\")),\n      i = env$1().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram$1(LEAKYRELU_PACKED$1, a.shape, o.shape) : new BinaryOpProgram$1(LEAKYRELU$1, a.shape, o.shape),\n      l = n.runWebGLProgram(i, [a, o], a.dtype);\n  return n.disposeIntermediateTensorInfo(o), l;\n}\n\nvar leakyReluConfig$2 = {\n  kernelName: LeakyRelu$1,\n  backendName: \"webgl\",\n  kernelFunc: leakyRelu$3\n},\n    PRELU$1 = \"return (a < 0.) ? b * a : a;\",\n    PRELU_PACKED$1 = \"\\n  vec4 aLessThanZero = vec4(lessThan(a, vec4(0.)));\\n  return (aLessThanZero * (b * a)) + ((vec4(1.0) - aLessThanZero) * a);\\n\";\n\nfunction prelu$4(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r,\n    alpha: a\n  } = t,\n      s = env$1().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram$1(PRELU_PACKED$1, r.shape, a.shape) : new BinaryOpProgram$1(PRELU$1, r.shape, a.shape);\n  return n.runWebGLProgram(s, [r, a], r.dtype);\n}\n\nvar preluConfig$2 = {\n  kernelName: Prelu$1,\n  backendName: \"webgl\",\n  kernelFunc: prelu$4\n},\n    CHECK_NAN_SNIPPET_UNARY$1 = \"if (isnan(x)) return x;\",\n    CHECK_NAN_SNIPPET_BINARY$1 = \"\\n  if (isnan(a)) return a;\\n  if (isnan(b)) return b;\\n\",\n    CHECK_NAN_SNIPPET_BINARY_PACKED$1 = \"\\n  result.r = isNaN.r > 0. ? NAN : result.r;\\n  result.g = isNaN.g > 0. ? NAN : result.g;\\n  result.b = isNaN.b > 0. ? NAN : result.b;\\n  result.a = isNaN.a > 0. ? NAN : result.a;\\n\";\n\nfunction unaryKernelFunc$2(_ref14) {\n  var {\n    opSnippet: e,\n    packedOpSnippet: t,\n    cpuKernelImpl: n,\n    dtype: r\n  } = _ref14;\n  return _ref15 => {\n    var {\n      inputs: a,\n      backend: s\n    } = _ref15;\n    var {\n      x: o\n    } = a,\n        i = s,\n        l = r || o.dtype;\n\n    if (i.shouldExecuteOnCPU([o]) && null != n) {\n      var _e438 = i.texData.get(o.dataId),\n          _t318 = n(_e438.values, l);\n\n      return i.makeTensorInfo(o.shape, l, _t318);\n    }\n\n    var u;\n    return u = env$1().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\") && null != t ? new UnaryOpPackedProgram$1(o.shape, t) : new UnaryOpProgram$1(o.shape, e), i.runWebGLProgram(u, [o], l);\n  };\n}\n\nfunction binaryKernelFunc$2(_ref16) {\n  var {\n    opSnippet: e,\n    packedOpSnippet: t,\n    checkOutOfBounds: n = !1,\n    supportsComplex: r = !1,\n    cpuKernelImpl: a,\n    dtype: s\n  } = _ref16;\n  return _ref17 => {\n    var {\n      inputs: o,\n      backend: i\n    } = _ref17;\n    var {\n      a: l,\n      b: u\n    } = o,\n        c = i;\n\n    if (r && \"complex64\" === l.dtype) {\n      var _t319 = c.texData.get(l.dataId),\n          _n207 = c.texData.get(u.dataId),\n          [_r152, _a108] = [[_t319.complexTensorInfos.real, _n207.complexTensorInfos.real], [_t319.complexTensorInfos.imag, _n207.complexTensorInfos.imag]].map(t => {\n        var [n, r] = t,\n            a = {\n          dataId: n.dataId,\n          dtype: n.dtype,\n          shape: l.shape\n        },\n            s = {\n          dataId: r.dataId,\n          dtype: r.dtype,\n          shape: u.shape\n        },\n            o = new BinaryOpProgram$1(e, l.shape, u.shape);\n        return c.runWebGLProgram(o, [a, s], upcastType$1(n.dtype, r.dtype));\n      }),\n          _s79 = complex$3({\n        inputs: {\n          real: _r152,\n          imag: _a108\n        },\n        backend: c\n      });\n\n      return c.disposeIntermediateTensorInfo(_r152), c.disposeIntermediateTensorInfo(_a108), _s79;\n    }\n\n    var p = s || upcastType$1(l.dtype, u.dtype);\n\n    if ((\"string\" === l.dtype || \"string\" === u.dtype || c.shouldExecuteOnCPU([l, u])) && null != a) {\n      var _e439 = c.texData.get(l.dataId).values,\n          _t320 = c.texData.get(u.dataId).values,\n          _n208 = \"string\" === l.dtype ? fromUint8ToStringArray$1(_e439) : _e439,\n          _r153 = \"string\" === l.dtype ? fromUint8ToStringArray$1(_t320) : _t320,\n          [_s80, _o55] = a(l.shape, u.shape, _n208, _r153, p),\n          _i36 = c.makeTensorInfo(_o55, p);\n\n      return c.texData.get(_i36.dataId).values = _s80, _i36;\n    }\n\n    var d;\n    return d = env$1().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") && null != t ? new BinaryOpPackedProgram$1(t, l.shape, u.shape, n) : new BinaryOpProgram$1(e, l.shape, u.shape), c.runWebGLProgram(d, [l, u], p);\n  };\n}\n\nfunction mapActivationToShaderProgram$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  if (\"linear\" === e) return t ? LINEAR$2 : LINEAR$3;\n  if (\"relu\" === e) return t ? RELU$4 : RELU$5;\n  if (\"elu\" === e) return t ? ELU$5 : ELU$6;\n  if (\"relu6\" === e) return t ? RELU6$4 : RELU6$5;\n  if (\"prelu\" === e) return t ? PRELU_PACKED$1 : PRELU$1;\n  if (\"leakyrelu\" === e) return t ? LEAKYRELU_PACKED$1 : LEAKYRELU$1;\n  if (\"sigmoid\" === e) return t ? SIGMOID$4 : SIGMOID$5;\n  throw new Error(\"Activation \".concat(e, \" has not been implemented for the WebGL backend.\"));\n}\n\nclass MatMulPackedProgram$1 {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n    var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : null;\n    var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : !1;\n    var l = arguments.length > 8 && arguments[8] !== undefined ? arguments[8] : !1;\n    this.variableNames = [\"matrixA\", \"matrixB\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = n;\n    var u = Math.ceil((r ? e[1] : e[2]) / 2),\n        c = r ? \"i * 2, rc.y\" : \"rc.y, i * 2\",\n        p = a ? \"rc.z, i * 2\" : \"i * 2, rc.z\",\n        d = r ? [\"a.xxyy\", \"a.zzww\"] : [\"a.xxzz\", \"a.yyww\"],\n        h = a ? [\"b.xzxz\", \"b.ywyw\"] : [\"b.xyxy\", \"b.zwzw\"];\n    var m = \"\",\n        f = \"\";\n    o && (m = i ? \"vec4 activation(vec4 a) {\\n          vec4 b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(o, \"\\n        }\") : l ? \"vec4 activation(vec4 a) {\\n          vec4 b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(o, \"\\n        }\") : \"vec4 activation(vec4 x) {\\n          \".concat(o, \"\\n        }\"), f = \"result = activation(result);\");\n    var g = s ? \"result += getBiasAtOutCoords();\" : \"\";\n    s && this.variableNames.push(\"bias\"), i && this.variableNames.push(\"preluActivationWeights\"), l && this.variableNames.push(\"leakyreluAlpha\");\n    var $ = \"rc.x\",\n        y = \"rc.x\";\n    e[0] < t[0] ? $ = \"int(min(float(rc.x), \".concat(e[0] - 1, \".))\") : t[0] < e[0] && (y = \"int(min(float(rc.x), \".concat(t[0] - 1, \".))\")), this.userCode = \"\\n      \".concat(m, \"\\n\\n      const float sharedDimension = \").concat(u, \".0;\\n\\n      vec4 dot2x2ARowBCol(ivec3 rc) {\\n        vec4 result = vec4(0);\\n        for (int i = 0; i < \").concat(u, \"; i++) {\\n          int batchA = \").concat($, \";\\n          int batchB = \").concat(y, \";\\n          vec4 a = getMatrixA(batchA, \").concat(c, \");\\n          vec4 b = getMatrixB(batchB, \").concat(p, \");\\n\\n          // These swizzled products need to be separately added.\\n          // See: https://github.com/tensorflow/tfjs/issues/1735\\n          result += (\").concat(d[0], \" * \").concat(h[0], \");\\n          result += (\").concat(d[1], \" * \").concat(h[1], \");\\n        }\\n        return result;\\n      }\\n\\n      void main() {\\n        ivec3 rc = getOutputCoords();\\n        vec4 result = dot2x2ARowBCol(rc);\\n\\n        \").concat(g, \"\\n\\n        \").concat(f, \"\\n\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar COMPLEX_MULTIPLY$1 = {\n  REAL: \"return areal * breal - aimag * bimag;\",\n  IMAG: \"return areal * bimag + aimag * breal;\"\n};\n\nclass BinaryOpComplexProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"AReal\", \"AImag\", \"BReal\", \"BImag\"], this.outputShape = assertAndGetBroadcastShape$1(t, n), this.userCode = \"\\n      float binaryOpComplex(\\n          float areal, float aimag, float breal, float bimag) {\\n        \".concat(e, \"\\n      }\\n\\n      void main() {\\n        float areal = getARealAtOutCoords();\\n        float aimag = getAImagAtOutCoords();\\n        float breal = getBRealAtOutCoords();\\n        float bimag = getBImagAtOutCoords();\\n        setOutput(binaryOpComplex(areal, aimag, breal, bimag));\\n      }\\n    \");\n  }\n\n}\n\nvar MUL$1 = \"return a * b;\";\n\nfunction multiply$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    a: r,\n    b: a\n  } = t,\n      s = upcastType$1(r.dtype, a.dtype);\n\n  if (\"complex64\" === r.dtype) {\n    var _e440 = n.texData.get(r.dataId),\n        _t321 = n.texData.get(a.dataId),\n        _s81 = new BinaryOpComplexProgram$1(COMPLEX_MULTIPLY$1.REAL, r.shape, a.shape),\n        _o56 = new BinaryOpComplexProgram$1(COMPLEX_MULTIPLY$1.IMAG, r.shape, a.shape),\n        i = [{\n      dataId: _e440.complexTensorInfos.real.dataId,\n      dtype: _e440.complexTensorInfos.real.dtype,\n      shape: r.shape\n    }, {\n      dataId: _e440.complexTensorInfos.imag.dataId,\n      dtype: _e440.complexTensorInfos.imag.dtype,\n      shape: r.shape\n    }, {\n      dataId: _t321.complexTensorInfos.real.dataId,\n      dtype: _t321.complexTensorInfos.real.dtype,\n      shape: a.shape\n    }, {\n      dataId: _t321.complexTensorInfos.imag.dataId,\n      dtype: _t321.complexTensorInfos.imag.dtype,\n      shape: a.shape\n    }],\n        l = n.runWebGLProgram(_s81, i, \"float32\"),\n        u = n.runWebGLProgram(_o56, i, \"float32\"),\n        c = complex$3({\n      inputs: {\n        real: l,\n        imag: u\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(l), n.disposeIntermediateTensorInfo(u), c;\n  }\n\n  if (n.shouldExecuteOnCPU([r, a])) {\n    var _e441 = n.texData.get(r.dataId),\n        _t322 = n.texData.get(a.dataId),\n        [_o57, _i37] = multiplyImplCPU$1(r.shape, a.shape, _e441.values, _t322.values, s),\n        _l28 = n.makeTensorInfo(_i37, s);\n\n    return n.texData.get(_l28.dataId).values = _o57, _l28;\n  }\n\n  var o;\n  return o = env$1().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram$1(MUL$1, r.shape, a.shape) : new BinaryOpProgram$1(MUL$1, r.shape, a.shape), n.runWebGLProgram(o, [r, a], s);\n}\n\nvar multiplyConfig$2 = {\n  kernelName: Multiply$3,\n  backendName: \"webgl\",\n  kernelFunc: multiply$3\n};\n\nfunction packedReshape$1(e, t, n) {\n  var r = [getBatchDim$1(e.shape), ...getRowsCols$1(e.shape)],\n      a = {\n    dtype: e.dtype,\n    shape: r,\n    dataId: e.dataId\n  },\n      s = [getBatchDim$1(t), ...getRowsCols$1(t)],\n      o = new ReshapePackedProgram$1(s, r),\n      i = n.runWebGLProgram(o, [a], e.dtype, null, !0);\n  return {\n    dataId: i.dataId,\n    shape: t,\n    dtype: i.dtype\n  };\n}\n\nfunction reshape$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    shape: s\n  } = r,\n      o = n,\n      i = sizeFromShape$1(a.shape),\n      l = inferFromImplicitShape$1(s, i),\n      u = sizeFromShape$1(l);\n  assert$6(i === u, () => \"The new shape (\".concat(l, \") has \").concat(u, \" elements and the old shape (\").concat(a.shape, \") has \").concat(i, \" elements. The new shape and old shape must have the same number of elements.\"));\n  var c = o.texData.get(a.dataId);\n  return !c.isPacked || isReshapeFree$1(a.shape, l) || null !== c.texture && isReshapeFree$1(c.shape, l) ? (o.incRef(a.dataId), {\n    dataId: a.dataId,\n    shape: l,\n    dtype: a.dtype\n  }) : packedReshape$1(a, l, o);\n}\n\nvar reshapeConfig$2 = {\n  kernelName: Reshape$3,\n  backendName: \"webgl\",\n  kernelFunc: reshape$4\n};\n\nclass MeanProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"x\"];\n    var {\n      windowSize: n,\n      batchSize: r,\n      inSize: a,\n      outSize: s\n    } = e;\n    this.outputShape = [r, s];\n    var o = 4 * Math.floor(n / 4),\n        i = n % 4;\n    var l = \"sumValue += dot(values, ones);\";\n\n    if (null != t) {\n      var _e442 = 1 / t;\n\n      l = \"sumValue += dot(values * \".concat(isInt$1(_e442) ? _e442.toPrecision(2) : _e442, \", ones);\");\n    }\n\n    var u = \"\";\n    a % n > 0 && (u = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return 0.0;\\n        }\\n      \")), this.userCode = \"\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float getValue(int batch, int inIdx) {\\n        \".concat(u, \"\\n        return getX(batch, inIdx);\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = outIdx * \").concat(n, \";\\n\\n        float sumValue = 0.0;\\n\\n        for (int i = 0; i < \").concat(o, \"; i += 4) {\\n          int inIdx = inOffset + i;\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            getValue(batch, inIdx + 3)\\n          );\\n\\n          \").concat(l, \"\\n        }\\n\\n        int inIdx = inOffset + \").concat(o, \";\\n        if (\").concat(1 === i, \") {\\n          vec4 values = vec4(getValue(batch, inIdx), 0.0, 0.0, 0.0);\\n\\n          \").concat(l, \"\\n        } else if (\").concat(2 === i, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1), 0.0, 0.0);\\n\\n          \").concat(l, \"\\n        } else if (\").concat(3 === i, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2), 0.0);\\n\\n          \").concat(l, \"\\n        }\\n        setOutput(sumValue);\\n      }\\n    \");\n  }\n\n}\n\nclass ReduceProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"x\"];\n    var {\n      windowSize: n,\n      batchSize: r,\n      inSize: a,\n      outSize: s\n    } = e;\n    this.outputShape = [r, s];\n    var o = \"0.0\",\n        i = \"\";\n    \"prod\" === t ? o = \"1.0\" : \"min\" === t ? (o = \"1.0 / 1e-20\", i = \"min\") : \"max\" === t && (o = \"-1.0 / 1e-20\", i = \"max\");\n    var l = \"\".concat(t, \"(\").concat(t, \"(\").concat(t, \"(minMaxValue[0], minMaxValue[1]), minMaxValue[2]), minMaxValue[3])\");\n    \"sum\" === t ? l = \"sumValue\" : \"prod\" === t ? l = \"prodValue\" : \"all\" === t ? l = \"allValue\" : \"any\" === t && (l = \"anyValue\");\n    var u = 4 * Math.floor(n / 4),\n        c = n % 4;\n    var p = \"\\n      if (\".concat(\"sum\" === t, \") {\\n        sumValue += dot(values, ones);\\n      } else if (\").concat(\"prod\" === t, \") {\\n        vec2 tmp = vec2(values[0], values[1]) * vec2(values[2], values[3]);\\n        prodValue *= tmp[0] * tmp[1];\\n      } else {\\n        minMaxValue = \").concat(i, \"(values, minMaxValue);\\n        if (\").concat(\"min\" === t, \" || \").concat(\"max\" === t, \") {\\n          minMaxValue = \").concat(i, \"(values, minMaxValue);\\n          bvec4 isNaN = isnan(values);\\n          if (isNaN.r || isNaN.g || isNaN.b || isNaN.a) {\\n            minMaxValue = vec4(NAN);\\n          }\\n        }\\n      }\\n    \"),\n        d = \"vec4\";\n    \"all\" === t ? (o = \"1.0\", p = \"\\n        bool reducedAllValue = all(values);\\n        float floatedReducedAllValue = float(reducedAllValue);\\n        allValue = float(allValue >= 1.0 && floatedReducedAllValue >= 1.0);\\n      \", d = \"bvec4\") : \"any\" === t && (o = \"0.0\", p = \"\\n        bool reducedAnyValue = any(values);\\n        float floatedReducedAnyValue = float(reducedAnyValue);\\n        anyValue = float(anyValue >= 1.0 || floatedReducedAnyValue >= 1.0);\\n      \", d = \"bvec4\");\n    var h = \"\";\n    a % n > 0 && (h = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return initializationValue;\\n        }\\n      \")), this.userCode = \"\\n      const float initializationValue = \".concat(o, \";\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float getValue(int batch, int inIdx) {\\n        \").concat(h, \"\\n        return getX(batch, inIdx);\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = outIdx * \").concat(n, \";\\n\\n        vec4 minMaxValue = vec4(\").concat(o, \");\\n        float prodValue = 1.0;\\n        float sumValue = 0.0;\\n        float allValue = 1.0;\\n        float anyValue = 0.0;\\n\\n        for (int i = 0; i < \").concat(u, \"; i += 4) {\\n          int inIdx = inOffset + i;\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            getValue(batch, inIdx + 3)\\n          );\\n\\n          \").concat(p, \"\\n        }\\n\\n        int inIdx = inOffset + \").concat(u, \";\\n        if (\").concat(1 === c, \") {\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            initializationValue,\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          \").concat(p, \"\\n        } else if (\").concat(2 === c, \") {\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          \").concat(p, \"\\n        } else if (\").concat(3 === c, \") {\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            initializationValue\\n          );\\n\\n          \").concat(p, \"\\n        }\\n        setOutput(\").concat(l, \");\\n      }\\n    \");\n  }\n\n}\n\nfunction getReductionStages$1(e) {\n  var t = [];\n\n  for (; 0 === t.length || 1 !== t[t.length - 1].outSize;) {\n    var n = t.length ? t[t.length - 1].outSize : e[1],\n        r = computeOptimalWindowSize$1(n);\n    t.push({\n      inSize: n,\n      windowSize: r,\n      outSize: Math.ceil(n / r)\n    });\n  }\n\n  return t;\n}\n\nfunction reduce$1(e, t, n, r) {\n  var a = getReductionStages$1(e.shape);\n  var s = e;\n\n  for (var o = 0; o < a.length; o++) {\n    var {\n      inSize: i,\n      windowSize: l,\n      outSize: u\n    } = a[o];\n\n    var c = void 0,\n        _p17 = void 0;\n\n    c = \"mean\" === n ? 0 === o ? new MeanProgram$1({\n      windowSize: l,\n      inSize: i,\n      batchSize: e.shape[0],\n      outSize: u\n    }, i) : new MeanProgram$1({\n      windowSize: l,\n      inSize: i,\n      batchSize: e.shape[0],\n      outSize: u\n    }) : new ReduceProgram$1({\n      windowSize: l,\n      inSize: i,\n      batchSize: e.shape[0],\n      outSize: u\n    }, n), _p17 = s, s = r.runWebGLProgram(c, [s], t), _p17.dataId !== e.dataId && r.disposeIntermediateTensorInfo(_p17);\n  }\n\n  return s;\n}\n\nclass TransposeProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"A\"];\n    var n = new Array(e.length);\n\n    for (var _r154 = 0; _r154 < n.length; _r154++) {\n      n[_r154] = e[t[_r154]];\n    }\n\n    this.outputShape = n, this.rank = n.length;\n    var r = getCoordsDataType$1(this.rank),\n        a = getSwitchedCoords$1(t);\n    this.userCode = \"\\n    void main() {\\n      \".concat(r, \" resRC = getOutputCoords();\\n      setOutput(getA(\").concat(a, \"));\\n    }\\n    \");\n  }\n\n}\n\nfunction getSwitchedCoords$1(e) {\n  var t = e.length;\n  if (t > 6) throw Error(\"Transpose for rank \".concat(t, \" is not yet supported\"));\n  var n = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\", \"resRC.u\", \"resRC.v\"],\n      r = new Array(t);\n\n  for (var _t323 = 0; _t323 < e.length; _t323++) {\n    r[e[_t323]] = n[_t323];\n  }\n\n  return r.join();\n}\n\nclass TransposePackedProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0;\n    var n = new Array(e.length);\n\n    for (var _r155 = 0; _r155 < n.length; _r155++) {\n      n[_r155] = e[t[_r155]];\n    }\n\n    if (this.outputShape = n, this.rank = n.length, this.rank > 6) throw Error(\"Packed transpose for rank \".concat(this.rank, \" is not yet supported.\"));\n    var r = getCoordsDataType$1(this.rank),\n        a = getVecChannels$1(\"rc\", this.rank),\n        s = new Array(this.rank);\n\n    for (var _e443 = 0; _e443 < t.length; _e443++) {\n      s[t[_e443]] = a[_e443];\n    }\n\n    var o = \"vec2(\".concat(s.slice(-2).join(), \")\"),\n        i = \"++\".concat(a[this.rank - 1], \" < \").concat(n[this.rank - 1]),\n        l = \"getChannel(getA(\".concat(s.join(), \"), \").concat(o, \")\");\n    this.userCode = \"\\n    void main() {\\n      \".concat(r, \" rc = getOutputCoords();\\n      vec4 result = vec4(0.);\\n      result[0] = \").concat(l, \";\\n      if(\").concat(i, \") {\\n        result[1] = \").concat(l, \";\\n      }\\n      --\").concat(a[this.rank - 1], \";\\n      if(++\").concat(a[this.rank - 2], \" < \").concat(n[this.rank - 2], \") {\\n        result[2] = \").concat(l, \";\\n        if(\").concat(i, \") {\\n          result[3] = \").concat(l, \";\\n        }\\n      }\\n      setOutput(result);\\n    }\\n    \");\n  }\n\n}\n\nfunction transposeImpl$2(e, t, n) {\n  var r = env$1().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new TransposePackedProgram$1(e.shape, t) : new TransposeProgram$1(e.shape, t);\n  return n.runWebGLProgram(r, [e], e.dtype);\n}\n\nfunction sumImpl$1(e, t, n, r) {\n  var a = e.shape.length,\n      s = parseAxisParam$1(t, e.shape);\n  var o = s;\n  var i = getAxesPermutation$1(o, a),\n      l = null != i;\n  var u = e;\n  l && (u = transposeImpl$2(e, i, r), o = getInnerMostAxes$1(o.length, a)), assertAxesAreInnerMostDims$1(\"sum\", o, a);\n  var [c, p] = computeOutAndReduceShapes$1(u.shape, o);\n  var d = c;\n  n && (d = expandShapeToKeepDim$1(c, s));\n  var h = sizeFromShape$1(p),\n      m = reshape$4({\n    inputs: {\n      x: u\n    },\n    attrs: {\n      shape: [sizeFromShape$1(e.shape) / h, h]\n    },\n    backend: r\n  }),\n      f = reduce$1(m, sumOutType$1(e.dtype), \"sum\", r),\n      g = reshape$4({\n    inputs: {\n      x: f\n    },\n    attrs: {\n      shape: d\n    },\n    backend: r\n  });\n  return r.disposeIntermediateTensorInfo(m), r.disposeIntermediateTensorInfo(f), l && r.disposeIntermediateTensorInfo(u), g;\n}\n\nfunction sum$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  return sumImpl$1(a, s, o, n);\n}\n\nvar sumConfig$2 = {\n  kernelName: Sum$1,\n  backendName: \"webgl\",\n  kernelFunc: sum$4\n};\n\nfunction transpose$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    perm: s\n  } = r,\n      o = n,\n      i = new Array(a.shape.length);\n\n  for (var _e444 = 0; _e444 < i.length; _e444++) {\n    i[_e444] = a.shape[s[_e444]];\n  }\n\n  var l;\n\n  if (o.shouldExecuteOnCPU([a])) {\n    var _e445 = o.texData.get(a.dataId),\n        _t324 = transposeImplCPU$1(_e445.values, a.shape, a.dtype, s, i);\n\n    l = o.makeTensorInfo(i, a.dtype), o.texData.get(l.dataId).values = _t324;\n  } else l = transposeImpl$2(a, s, o);\n\n  return l;\n}\n\nvar transposeConfig$2 = {\n  kernelName: Transpose$1,\n  backendName: \"webgl\",\n  kernelFunc: transpose$3\n},\n    MATMUL_SHARED_DIM_THRESHOLD$1 = 1e3;\n\nfunction batchMatMulImpl$1(_ref18) {\n  var {\n    a: e,\n    b: t,\n    transposeA: n,\n    transposeB: r,\n    backend: a,\n    bias: s = null,\n    preluActivationWeights: o = null,\n    leakyreluAlpha: i = 0,\n    activation: l = null\n  } = _ref18;\n  var u = e.shape.length,\n      c = t.shape.length,\n      p = n ? e.shape[u - 2] : e.shape[u - 1],\n      d = r ? t.shape[c - 1] : t.shape[c - 2],\n      h = n ? e.shape[u - 1] : e.shape[u - 2],\n      m = r ? t.shape[c - 2] : t.shape[c - 1],\n      f = e.shape.slice(0, -2),\n      g = t.shape.slice(0, -2),\n      $ = sizeFromShape$1(f),\n      y = sizeFromShape$1(g);\n  assert$6(u >= 2 && c >= 2 && ($ === y || 1 === $ || 1 === y), () => \"Error in matMul: the input batch dimensions must either be the same or at least one input batch dimension must be 1. Got input batch dimensions of (\".concat(f, \") and (\").concat(g, \").\"));\n  var b = ($ > y ? e.shape.slice(0, -2) : t.shape.slice(0, -2)).concat([h, m]);\n  assert$6(p === d, () => \"Error in matMul: inner shapes (\".concat(p, \") and (\").concat(d, \") of Tensors with shapes \").concat(e.shape, \" and \").concat(t.shape, \" and transposeA=\").concat(n, \" and transposeB=\").concat(r, \" must match.\"));\n  var x = n ? [$, p, h] : [$, h, p],\n      v = r ? [y, m, d] : [y, d, m],\n      I = reshape$4({\n    inputs: {\n      x: e\n    },\n    backend: a,\n    attrs: {\n      shape: x\n    }\n  }),\n      C = reshape$4({\n    inputs: {\n      x: t\n    },\n    backend: a,\n    attrs: {\n      shape: v\n    }\n  }),\n      S = [I, C],\n      k = Math.max($, y),\n      T = n ? I.shape[1] : I.shape[2],\n      N = null != s,\n      w = null != o,\n      E = \"leakyrelu\" === l,\n      A = null != l ? mapActivationToShaderProgram$1(l, !0) : null;\n  var D;\n\n  if ((1 === h || 1 === m) && T > MATMUL_SHARED_DIM_THRESHOLD$1 && !1 === (N || w || E || null != A)) {\n    var _e446 = I,\n        _t325 = C;\n    n && (_e446 = transpose$3({\n      inputs: {\n        x: I\n      },\n      backend: a,\n      attrs: {\n        perm: [0, 2, 1]\n      }\n    }), S.push(_e446)), r && (_t325 = transpose$3({\n      inputs: {\n        x: C\n      },\n      backend: a,\n      attrs: {\n        perm: [0, 2, 1]\n      }\n    }), S.push(_t325));\n\n    var _s82 = 1 === m;\n\n    var _o58 = _e446;\n    1 !== m && (_o58 = reshape$4({\n      inputs: {\n        x: _e446\n      },\n      backend: a,\n      attrs: {\n        shape: [k, T, 1]\n      }\n    }), S.push(_o58));\n\n    var _i38 = 1 === m ? 2 : 1;\n\n    var _l29 = _t325;\n    _s82 && (_l29 = reshape$4({\n      inputs: {\n        x: _t325\n      },\n      backend: a,\n      attrs: {\n        shape: [k, 1, T]\n      }\n    }), S.push(_l29));\n\n    var _u25 = multiply$3({\n      inputs: {\n        a: _o58,\n        b: _l29\n      },\n      backend: a\n    });\n\n    D = sum$4({\n      inputs: {\n        x: _u25\n      },\n      backend: a,\n      attrs: {\n        axis: _i38,\n        keepDims: !0\n      }\n    }), S.push(_u25);\n  } else {\n    var _l30 = upcastType$1(e.dtype, t.dtype),\n        _u26 = new MatMulPackedProgram$1(x, v, [k, h, m], n, r, N, A, w, E),\n        _c18 = [I, C];\n\n    if (null != s && _c18.push(s), w && _c18.push(o), E) {\n      var _e447 = a.makeTensorInfo([], \"float32\", createScalarValue$1(i, \"float32\"));\n\n      _c18.push(_e447), S.push(_e447);\n    }\n\n    D = a.runWebGLProgram(_u26, _c18, _l30);\n  }\n\n  var R = reshape$4({\n    inputs: {\n      x: D\n    },\n    backend: a,\n    attrs: {\n      shape: b\n    }\n  });\n  S.push(D);\n\n  for (var _e448 of S) {\n    a.disposeIntermediateTensorInfo(_e448);\n  }\n\n  return R;\n}\n\nfunction _fusedMatMul$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    transposeA: l,\n    transposeB: u,\n    activation: c,\n    leakyreluAlpha: p\n  } = r;\n  return batchMatMulImpl$1({\n    a,\n    b: s,\n    transposeA: l,\n    transposeB: u,\n    backend: n,\n    bias: o,\n    preluActivationWeights: i,\n    leakyreluAlpha: p,\n    activation: c\n  });\n}\n\nvar _fusedMatMulConfig$2 = {\n  kernelName: _FusedMatMul$1,\n  backendName: \"webgl\",\n  kernelFunc: _fusedMatMul$2\n},\n    ABS$2 = \"return abs(x);\";\n\nfunction abs$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n\n  if (n.shouldExecuteOnCPU([r]) && \"complex64\" !== r.dtype) {\n    var _e449 = n.texData.get(r.dataId),\n        _t326 = simpleAbsImplCPU$1(_e449.values);\n\n    return n.makeTensorInfo(r.shape, r.dtype, _t326);\n  }\n\n  var a;\n  return a = env$1().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\") ? new UnaryOpPackedProgram$1(r.shape, ABS$2) : new UnaryOpProgram$1(r.shape, ABS$2), n.runWebGLProgram(a, [r], r.dtype);\n}\n\nvar absConfig$2 = {\n  kernelName: Abs$1,\n  backendName: \"webgl\",\n  kernelFunc: abs$3\n},\n    ACOS$1 = CHECK_NAN_SNIPPET$5 + \"\\n  if (abs(x) > 1.) {\\n    return NAN;\\n  }\\n  return acos(x);\\n\",\n    acos$3 = unaryKernelFunc$2({\n  opSnippet: ACOS$1\n}),\n    acosConfig$2 = {\n  kernelName: Acos$1,\n  backendName: \"webgl\",\n  kernelFunc: acos$3\n},\n    ACOSH$1 = CHECK_NAN_SNIPPET$5 + \"\\n  if (x < 1.0) return NAN;\\nreturn log(x + sqrt(x * x - 1.0));\",\n    acosh$3 = unaryKernelFunc$2({\n  opSnippet: ACOSH$1\n}),\n    acoshConfig$2 = {\n  kernelName: Acosh$1,\n  backendName: \"webgl\",\n  kernelFunc: acosh$3\n},\n    ADD$1 = \"return a + b;\",\n    addKernelFunc$1 = binaryKernelFunc$2({\n  opSnippet: ADD$1,\n  packedOpSnippet: ADD$1,\n  supportsComplex: !0,\n  cpuKernelImpl: addImplCPU$1\n}),\n    addConfig$2 = {\n  kernelName: Add$3,\n  backendName: \"webgl\",\n  kernelFunc: addKernelFunc$1\n};\n\nclass AddNProgram$1 {\n  constructor(e, t) {\n    this.outputShape = [], this.outputShape = e, this.variableNames = t.map((e, t) => \"T\".concat(t));\n    var n = [];\n    this.variableNames.forEach(e => {\n      n.push(\"float v\".concat(e, \" = get\").concat(e, \"AtOutCoords();\"));\n    });\n    var r = this.variableNames.map(e => \"v\".concat(e)).join(\" + \");\n    this.userCode = \"\\n      void main() {\\n        \".concat(n.join(\"\\n        \"), \"\\n\\n        float result = \").concat(r, \";\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nclass AddNPackedProgram$1 {\n  constructor(e, t) {\n    this.outputShape = [], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e, this.variableNames = t.map((e, t) => \"T\".concat(t));\n    var n = [];\n    this.variableNames.forEach(e => {\n      n.push(\"vec4 v\".concat(e, \" = get\").concat(e, \"AtOutCoords();\"));\n    });\n    var r = this.variableNames.map(e => \"v\".concat(e)).join(\" + \");\n    this.userCode = \"\\n      void main() {\\n        \".concat(n.join(\"\\n        \"), \"\\n\\n        vec4 result = \").concat(r, \";\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction addN$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      r = t;\n  if (1 === r.length) return identity$3({\n    inputs: {\n      x: r[0]\n    },\n    backend: n\n  });\n\n  if (r.length > env$1().get(\"WEBGL_MAX_TEXTURES_IN_SHADER\")) {\n    var _e450 = Math.floor(r.length / 2),\n        _t327 = addN$3({\n      inputs: r.slice(0, _e450),\n      backend: n\n    }),\n        _a109 = addN$3({\n      inputs: r.slice(_e450),\n      backend: n\n    });\n\n    return addN$3({\n      inputs: [_t327, _a109],\n      backend: n\n    });\n  }\n\n  var a = r.map(e => e.dtype).reduce((e, t) => upcastType$1(e, t)),\n      s = r.map(e => e.shape),\n      o = env$1().getBool(\"WEBGL_PACK\") ? new AddNPackedProgram$1(r[0].shape, s) : new AddNProgram$1(r[0].shape, s);\n  return n.runWebGLProgram(o, r, a);\n}\n\nvar addNConfig$2 = {\n  kernelName: AddN$1,\n  backendName: \"webgl\",\n  kernelFunc: addN$3\n};\n\nfunction all$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam$1(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation$1(u, i);\n  var p = a;\n  null != c && (p = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), u = getInnerMostAxes$1(u.length, i)), assertAxesAreInnerMostDims$1(\"all\", u, i);\n  var [d, h] = computeOutAndReduceShapes$1(p.shape, u),\n      m = reshape$4({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, sizeFromShape$1(h)]\n    }\n  }),\n      f = reduce$1(m, m.dtype, \"all\", n);\n  var g;\n  return g = reshape$4(o ? {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: expandShapeToKeepDim$1(d, l)\n    }\n  } : {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  }), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), null != c && n.disposeIntermediateTensorInfo(p), g;\n}\n\nvar allConfig$2 = {\n  kernelName: All$1,\n  backendName: \"webgl\",\n  kernelFunc: all$3\n};\n\nfunction any$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam$1(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation$1(u, i);\n  var p = a;\n  null != c && (p = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), u = getInnerMostAxes$1(u.length, i)), assertAxesAreInnerMostDims$1(\"any\", u, i);\n  var [d, h] = computeOutAndReduceShapes$1(p.shape, u),\n      m = reshape$4({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, sizeFromShape$1(h)]\n    }\n  }),\n      f = reduce$1(m, m.dtype, \"any\", n);\n  var g;\n  return g = reshape$4(o ? {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: expandShapeToKeepDim$1(d, l)\n    }\n  } : {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  }), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), null != c && n.disposeIntermediateTensorInfo(p), g;\n}\n\nvar anyConfig$2 = {\n  kernelName: Any$1,\n  backendName: \"webgl\",\n  kernelFunc: any$3\n};\n\nclass ArgMinMaxProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"A\"];\n    var {\n      windowSize: r,\n      batchSize: a,\n      outSize: s\n    } = e;\n    n || this.variableNames.push(\"bestIndicesA\"), this.outputShape = [a, s], this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = outIdx * \".concat(r, \";\\n\\n        int bestIndex = inOffset;\\n        float bestValue = getA(batch, bestIndex);\\n\\n        for (int i = 0; i < \").concat(r, \"; i++) {\\n          int inIdx = \").concat(n ? \"inOffset + i;\" : \"round(getBestIndicesA(batch, inOffset + i));\", \";\\n          float candidate = getA(batch, inIdx);\\n          if (candidate \").concat(\"max\" === t ? \">\" : \"<\", \" bestValue) {\\n            bestValue = candidate;\\n            bestIndex = inIdx;\\n          }\\n        }\\n        setOutput(float(bestIndex));\\n      }\\n    \");\n  }\n\n}\n\nclass ArgMinMaxPackedProgram$1 {\n  constructor(e, t, n, r) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, assert$6(e.length > 2, () => \"Packed arg\".concat(n.charAt(0).toUpperCase() + n.slice(1), \" supports only inputs with rank above 2.\"));\n    var a = Math.ceil(e[e.length - 1] / t);\n    this.outputShape = e.slice(0, -1), a > 1 && this.outputShape.push(a), r || this.variableNames.push(\"bestIndicesA\");\n    var s = this.outputShape,\n        o = s.length,\n        i = getCoordsDataType$1(o),\n        l = getChannels$1(\"coords\", o);\n    var u, c;\n\n    if (1 === a) {\n      c = o + 1;\n\n      var _e451 = getCoordsDataType$1(c);\n\n      u = \"\\n        \".concat(_e451, \" sourceLocR = \").concat(_e451, \"(\").concat(l.join(), \", 0);\\n        ++\").concat(l[o - 1], \";\\n        \").concat(_e451, \" sourceLocG = \").concat(_e451, \"(\").concat(l.join(), \", 0);\\n        ++\").concat(l[o - 2], \";\\n        \").concat(_e451, \" sourceLocA = \").concat(_e451, \"(\").concat(l.join(), \", 0);\\n        --\").concat(l[o - 1], \";\\n        \").concat(_e451, \" sourceLocB = \").concat(_e451, \"(\").concat(l.join(), \", 0);\\n        --\").concat(l[o - 2], \";\");\n    } else c = o, u = \"\\n        \".concat(i, \" sourceLocR = coords;\\n        ++\").concat(l[o - 1], \";\\n        \").concat(i, \" sourceLocG = coords;\\n        ++\").concat(l[o - 2], \";\\n        \").concat(i, \" sourceLocA = coords;\\n        --\").concat(l[o - 1], \";\\n        \").concat(i, \" sourceLocB = coords;\\n        --\").concat(l[o - 2], \";\");\n\n    var p = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"].slice(0, c),\n        d = \".\" + p[c - 1],\n        h = p.map(e => \"int \" + e),\n        m = getChannels$1(\"sourceLocR\", c - 1).concat(\"inIdx.r\"),\n        f = getChannels$1(\"sourceLocG\", c - 1).concat(\"inIdx.g\"),\n        g = getChannels$1(\"sourceLocB\", c - 1).concat(\"inIdx.b\"),\n        $ = getChannels$1(\"sourceLocA\", c - 1).concat(\"inIdx.a\"),\n        y = \"max\" === n ? \"greaterThan\" : \"lessThan\",\n        b = r ? \"\" : \"\\n          inIdx = round(vec4(getBestIndicesAChannel(\".concat(m.join(), \"),\\n                             getBestIndicesAChannel(\").concat(f.join(), \"),\\n                             getBestIndicesAChannel(\").concat(g.join(), \"),\\n                             getBestIndicesAChannel(\").concat($.join(), \")));\"),\n        x = \"vec4(\\n            getAChannel(\".concat(m.join(), \"),\\n            hasNextCol ? getAChannel(\").concat(f.join(), \") : 0.,\\n            hasNextRow ? getAChannel(\").concat(g.join(), \") : 0.,\\n            hasNextRow && hasNextCol ? getAChannel(\").concat($.join(), \") : 0.)\"),\n        v = r ? \"\" : \"\\n      float getBestIndicesAChannel(\".concat(h.join(), \") {\\n        return getChannel(getBestIndicesA(\").concat(p.join(), \"),\\n                                          vec2(\").concat(p.slice(-2).join(), \"));\\n      }\");\n    this.userCode = \"\\n      float getAChannel(\".concat(h.join(), \") {\\n        return getChannel(getA(\").concat(p.join(), \"),\\n                               vec2(\").concat(p.slice(-2).join(), \"));\\n      }\\n      \").concat(v, \"\\n      void main() {\\n        \").concat(i, \" coords = getOutputCoords();\\n        bool hasNextCol = \").concat(l[o - 1], \" < \").concat(s[o - 1] - 1, \";\\n        bool hasNextRow = \").concat(l[o - 2], \" < \").concat(s[o - 2] - 1, \";\\n        \").concat(u, \"\\n        ivec4 srcIdx = ivec4(sourceLocR\").concat(d, \", sourceLocG\").concat(d, \",\\n          sourceLocB\").concat(d, \", sourceLocA\").concat(d, \") * \").concat(t, \";\\n        ivec4 inIdx = srcIdx;\\n        vec4 bestIndex = vec4(inIdx);\\n        vec4 bestValue = \").concat(x, \";\\n\\n        for (int i = 0; i < \").concat(t, \"; i++) {\\n          inIdx = srcIdx;\\n          \").concat(b, \"\\n          vec4 candidate = \").concat(x, \";\\n          bvec4 nan = isnan(candidate);\\n          bvec4 replace = bvec4(\\n            vec4(\").concat(y, \"(candidate, bestValue)) * (vec4(1.0) - vec4(nan)));\\n\\n          bestValue = vec4(replace.x  ? candidate.x : bestValue.x,\\n                           replace.y  ? candidate.y : bestValue.y,\\n                           replace.z  ? candidate.z : bestValue.z,\\n                           replace.w  ? candidate.w : bestValue.w);\\n          bestIndex = mix(bestIndex, vec4(inIdx), vec4(replace));\\n          srcIdx++;\\n        }\\n        setOutput(bestIndex);\\n      }\\n    \");\n  }\n\n}\n\nfunction argReduce$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n  var a = t.shape[0],\n      s = t.shape[1];\n  null != r && (a = r.shape[0], s = r.shape[1]);\n  var o = computeOptimalWindowSize$1(s),\n      i = {\n    windowSize: o,\n    inSize: s,\n    batchSize: a,\n    outSize: Math.ceil(s / o)\n  },\n      l = new ArgMinMaxProgram$1(i, n, null == r),\n      u = [t];\n  null != r && u.push(r);\n  var c = e.runWebGLProgram(l, u, \"int32\");\n  if (1 === c.shape[1]) return c;\n  var p = argReduce$1(e, t, n, c);\n  return e.disposeIntermediateTensorInfo(c), p;\n}\n\nfunction argReducePacked$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n  var a = null != r ? r.shape : t.shape,\n      s = computeOptimalWindowSize$1(a[a.length - 1]),\n      o = new ArgMinMaxPackedProgram$1(a, s, n, null == r),\n      i = e.runWebGLProgram(o, null == r ? [t] : [t, r], \"int32\");\n\n  if (i.shape.length === t.shape.length) {\n    var _r156 = argReducePacked$1(e, t, n, i);\n\n    return e.disposeIntermediateTensorInfo(i), _r156;\n  }\n\n  return i;\n}\n\nfunction argMinMaxReduce$1(e, t, n, r) {\n  var a = [n];\n\n  if (assertAxesAreInnerMostDims$1(\"arg\" + r.charAt(0).toUpperCase() + r.slice(1), a, t.shape.length), !env$1().getBool(\"WEBGL_PACK_REDUCE\") || t.shape.length <= 2) {\n    var _n209 = [],\n        [s, o] = computeOutAndReduceShapes$1(t.shape, a),\n        i = sizeFromShape$1(o),\n        l = reshape$4({\n      inputs: {\n        x: t\n      },\n      backend: e,\n      attrs: {\n        shape: [-1, i]\n      }\n    });\n\n    _n209.push(l);\n\n    var u = argReduce$1(e, l, r);\n\n    _n209.push(u);\n\n    var c = reshape$4({\n      inputs: {\n        x: u\n      },\n      backend: e,\n      attrs: {\n        shape: s\n      }\n    });\n    return _n209.forEach(t => e.disposeIntermediateTensorInfo(t)), c;\n  }\n\n  return argReducePacked$1(e, t, r);\n}\n\nfunction argMax$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  var o = parseAxisParam$1(s, a.shape);\n  var i = getAxesPermutation$1(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes$1(o.length, l.shape.length)), assertAxesAreInnerMostDims$1(\"argMax\", [o[0]], l.shape.length);\n  var c = argMinMaxReduce$1(n, l, o[0], \"max\");\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), c;\n}\n\nvar argMaxConfig$2 = {\n  kernelName: ArgMax$1,\n  backendName: \"webgl\",\n  kernelFunc: argMax$3\n};\n\nfunction argMin$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  var o = parseAxisParam$1(s, a.shape);\n  var i = getAxesPermutation$1(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes$1(o.length, l.shape.length)), assertAxesAreInnerMostDims$1(\"argMin\", [o[0]], l.shape.length);\n  var c = argMinMaxReduce$1(n, l, o[0], \"min\");\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), c;\n}\n\nvar argMinConfig$2 = {\n  kernelName: ArgMin$1,\n  backendName: \"webgl\",\n  kernelFunc: argMin$3\n},\n    ASIN$1 = CHECK_NAN_SNIPPET$5 + \"\\n  if (abs(x) > 1.) {\\n    return NAN;\\n  }\\n  return asin(x);\\n\",\n    asin$3 = unaryKernelFunc$2({\n  opSnippet: ASIN$1\n}),\n    asinConfig$2 = {\n  kernelName: Asin$1,\n  backendName: \"webgl\",\n  kernelFunc: asin$3\n},\n    ASINH$1 = CHECK_NAN_SNIPPET$5 + \"return log(x + sqrt(x * x + 1.0));\",\n    asinh$3 = unaryKernelFunc$2({\n  opSnippet: ASINH$1\n}),\n    asinhConfig$2 = {\n  kernelName: Asinh$1,\n  backendName: \"webgl\",\n  kernelFunc: asinh$3\n},\n    ATAN$1 = CHECK_NAN_SNIPPET$5 + \"\\n  return atan(x);\\n\",\n    atan$3 = unaryKernelFunc$2({\n  opSnippet: ATAN$1\n}),\n    atanConfig$2 = {\n  kernelName: Atan$1,\n  backendName: \"webgl\",\n  kernelFunc: atan$3\n},\n    ATAN2$1 = CHECK_NAN_SNIPPET_BINARY$1 + \"\\n  return atan(a, b);\\n\",\n    ATAN2_PACKED$1 = \"\\n  vec4 result = atan(a, b);\\n  vec4 isNaN = min(vec4(isnan(a)) + vec4(isnan(b)), vec4(1.0));\\n  \" + CHECK_NAN_SNIPPET_BINARY_PACKED$1 + \"\\n  return result;\\n\",\n    atan2$3 = binaryKernelFunc$2({\n  opSnippet: ATAN2$1,\n  packedOpSnippet: ATAN2_PACKED$1\n}),\n    atan2Config$2 = {\n  kernelName: Atan2$1,\n  backendName: \"webgl\",\n  kernelFunc: atan2$3\n},\n    ATANH$1 = CHECK_NAN_SNIPPET$5 + \"\\n  if ((x < -1.0) || (x > 1.0)) return NAN;\\nreturn (log(1.0 + x) - log(1.0 - x)) / 2.0;\",\n    atanh$3 = unaryKernelFunc$2({\n  opSnippet: ATANH$1\n}),\n    atanhConfig$2 = {\n  kernelName: Atanh$1,\n  backendName: \"webgl\",\n  kernelFunc: atanh$3\n};\n\nclass Pool2DProgram$1 {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    if (this.variableNames = [\"x\"], \"avg\" === t && n) throw new Error(\"Cannot compute positions for average pool.\");\n    var s = e.filterWidth,\n        o = e.strideHeight,\n        i = e.strideWidth,\n        l = e.dilationHeight,\n        u = e.dilationWidth,\n        c = e.effectiveFilterHeight,\n        p = e.effectiveFilterWidth,\n        d = e.padInfo.top,\n        h = e.padInfo.left;\n    this.outputShape = e.outShape;\n    var m = \"avg\" === t;\n    var f = \"0.0\";\n    if (m || (f = \"-1.0 / 1e-20\"), n) return void (this.userCode = \"\\n        const ivec2 strides = ivec2(\".concat(o, \", \").concat(i, \");\\n        const ivec2 pads = ivec2(\").concat(d, \", \").concat(h, \");\\n\\n        void main() {\\n          ivec4 coords = getOutputCoords();\\n          int batch = coords[0];\\n          int d = coords[3];\\n\\n          ivec2 xRCCorner = coords.yz * strides - pads;\\n          int xRCorner = xRCCorner.x;\\n          int xCCorner = xRCCorner.y;\\n\\n          // max/min x(?, ?, d) to get y(yR, yC, d).\\n          // ? = to be determined\\n          float minMaxValue = 0.0;\\n          float minMaxValueFound = 0.0;\\n          int minMaxPosition = 0;\\n          float avgValue = 0.0;\\n\\n          for (int wR = 0; wR < \").concat(c, \";\\n              wR += \").concat(l, \") {\\n            int xR = xRCorner + wR;\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int wC = 0; wC < \").concat(p, \";\\n                wC += \").concat(u, \") {\\n              int xC = xCCorner + wC;\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              float value = getX(batch, xR, xC, d);\\n\\n              // If a min / max value has already been found, use it. If not,\\n              // use the current value.\\n              float currMinMaxValue = mix(\\n                  value, minMaxValue, minMaxValueFound);\\n              if (value >= currMinMaxValue) {\\n                minMaxValue = value;\\n                minMaxValueFound = 1.0;\\n                minMaxPosition = \").concat(r ? a ? \"((batch  * \".concat(e.inHeight, \" + xR) * \").concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + d\") : \"(xR * \".concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + d\") : \"wR * \".concat(p, \" + wC\"), \";\\n              }\\n            }\\n          }\\n          setOutput(float(minMaxPosition));\\n        }\\n      \"));\n    var g = \"\".concat(t, \"(\").concat(t, \"(\").concat(t, \"(minMaxValue[0], minMaxValue[1]), minMaxValue[2]), minMaxValue[3])\");\n    \"avg\" === t && (g = \"avgValue / count\");\n    var $ = 4 * Math.floor(s / 4),\n        y = s % 4,\n        b = \"\\n      if (\".concat(m, \") {\\n        avgValue += dot(values, ones);\\n      } else {\\n        minMaxValue = max(values, minMaxValue);\\n      }\\n    \");\n    this.userCode = \"\\n      const ivec2 strides = ivec2(\".concat(o, \", \").concat(i, \");\\n      const ivec2 pads = ivec2(\").concat(d, \", \").concat(h, \");\\n      const float initializationValue = \").concat(f, \";\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float count = 0.0;\\n\\n      float getValue(int batch, int xR, int xC, int d) {\\n        if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n          return initializationValue;\\n        }\\n        count += 1.0;\\n        return getX(batch, xR, xC, d);\\n      }\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d = coords[3];\\n\\n        ivec2 xRCCorner = coords.yz * strides - pads;\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        // max/min x(?, ?, d) to get y(yR, yC, d).\\n        // ? = to be determined\\n        vec4 minMaxValue = vec4(\").concat(f, \");\\n        float avgValue = 0.0;\\n        count = 0.0;\\n\\n        for (int wR = 0; wR < \").concat(c, \";\\n            wR += \").concat(l, \") {\\n          int xR = xRCorner + wR;\\n\\n          if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n            continue;\\n          }\\n\\n          for (int wC = 0; wC < \").concat($, \"; wC += 4) {\\n            int xC = xCCorner + wC * \").concat(u, \";\\n\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              getValue(batch, xR, xC + \").concat(u, \", d),\\n              getValue(batch, xR, xC + 2 * \").concat(u, \", d),\\n              getValue(batch, xR, xC + 3 * \").concat(u, \", d)\\n            );\\n\\n            \").concat(b, \"\\n          }\\n\\n          int xC = xCCorner + \").concat($, \";\\n          if (\").concat(1 === y, \") {\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              initializationValue,\\n              initializationValue,\\n              initializationValue\\n            );\\n\\n            \").concat(b, \"\\n          } else if (\").concat(2 === y, \") {\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              getValue(batch, xR, xC + \").concat(u, \", d),\\n              initializationValue,\\n              initializationValue\\n            );\\n\\n            \").concat(b, \"\\n          } else if (\").concat(3 === y, \") {\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              getValue(batch, xR, xC + \").concat(u, \", d),\\n              getValue(batch, xR, xC + 2 * \").concat(u, \", d),\\n              initializationValue\\n            );\\n\\n            \").concat(b, \"\\n          }\\n        }\\n        setOutput(\").concat(g, \");\\n      }\\n    \");\n  }\n\n}\n\nclass Pool3DProgram$1 {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    if (this.variableNames = [\"x\"], \"avg\" === t && n) throw new Error(\"Cannot compute positions for average pool.\");\n    var s = e.filterWidth,\n        o = e.strideDepth,\n        i = e.strideHeight,\n        l = e.strideWidth,\n        u = e.dilationDepth,\n        c = e.dilationHeight,\n        p = e.dilationWidth,\n        d = e.effectiveFilterDepth,\n        h = e.effectiveFilterHeight,\n        m = e.effectiveFilterWidth,\n        f = e.padInfo.front,\n        g = e.padInfo.top,\n        $ = e.padInfo.left;\n    this.outputShape = e.outShape;\n    var y = \"avg\" === t;\n    var b = \"0.0\";\n    if (y || (b = \"-1.0 / 1e-20\"), n) return void (this.userCode = \"\\n        const ivec3 strides =\\n            ivec3(\".concat(o, \", \").concat(i, \", \").concat(l, \");\\n        const ivec3 pads = ivec3(\").concat(f, \", \").concat(g, \", \").concat($, \");\\n\\n        void main() {\\n          ivec5 coords = getOutputCoords();\\n          int batch = coords.x;\\n          int ch = coords.u;\\n\\n          ivec3 xCorner = ivec3(coords.y, coords.z, coords.w) * strides - pads;\\n          int xDCorner = xCorner.x;\\n          int xRCorner = xCorner.y;\\n          int xCCorner = xCorner.z;\\n\\n          // max/min x(?, ?, ?, ch) to get y(yD, yR, yC, ch).\\n          // ? = to be determined\\n          float minMaxValue = 0.0;\\n          float minMaxValueFound = 0.0;\\n          int minMaxPosition = 0;\\n\\n          for (int wD = 0; wD < \").concat(d, \";\\n              wD += \").concat(u, \") {\\n            int xD = xDCorner + wD;\\n\\n            if (xD < 0 || xD >= \").concat(e.inDepth, \") {\\n              continue;\\n            }\\n\\n            for (int wR = 0; wR < \").concat(h, \";\\n                wR += \").concat(c, \") {\\n              int xR = xRCorner + wR;\\n\\n              if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n                continue;\\n              }\\n\\n              for (int wC = 0; wC < \").concat(m, \";\\n                  wC += \").concat(p, \") {\\n                int xC = xCCorner + wC;\\n\\n                if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                  continue;\\n                }\\n\\n                float value = getX(batch, xD, xR, xC, ch);\\n\\n                // If a min / max value has already been found, use it. If not,\\n                // use the current value.\\n                float currMinMaxValue = mix(\\n                    value, minMaxValue, minMaxValueFound);\\n                if (value >= currMinMaxValue) {\\n                  minMaxValue = value;\\n                  minMaxValueFound = 1.0;\\n                  minMaxPosition = \").concat(r ? a ? \"(((batch * \".concat(e.inDepth, \" + xD) * \").concat(e.inHeight, \" + xR) * \").concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + ch\") : \"((xD * \".concat(e.inHeight, \" + xR) * \").concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + ch\") : \"wD * \".concat(h, \" * \").concat(m, \" +\\n                      wR * \").concat(m, \" + wC\"), \";\\n                }\\n              }\\n            }\\n          }\\n          setOutput(float(minMaxPosition));\\n        }\\n      \"));\n    var x = \"\".concat(t, \"(\").concat(t, \"(\").concat(t, \"(minMaxValue[0], minMaxValue[1]), minMaxValue[2]), minMaxValue[3])\");\n    \"avg\" === t && (x = \"avgValue / count\");\n    var v = 4 * Math.floor(s / 4),\n        I = s % 4,\n        C = \"\\n      if (\".concat(y, \") {\\n        avgValue += dot(values, ones);\\n      } else {\\n        minMaxValue = max(values, minMaxValue);\\n      }\\n    \");\n    this.userCode = \"\\n      const ivec3 strides =\\n        ivec3(\".concat(o, \", \").concat(i, \", \").concat(l, \");\\n      const ivec3 pads = ivec3(\").concat(f, \", \").concat(g, \", \").concat($, \");\\n      const float initializationValue = \").concat(b, \";\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float count = 0.0;\\n\\n      float getValue(int batch, int xD, int xR, int xC, int ch) {\\n        if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n          return initializationValue;\\n        }\\n        count += 1.0;\\n        return getX(batch, xD, xR, xC, ch);\\n      }\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int ch = coords.u;\\n\\n        ivec3 xCorner = ivec3(coords.y, coords.z, coords.w) * strides - pads;\\n        int xDCorner = xCorner.x;\\n        int xRCorner = xCorner.y;\\n        int xCCorner = xCorner.z;\\n\\n        // max/min x(?, ?, ?, d) to get y(yD, yR, yC, ch).\\n        // ? = to be determined\\n        vec4 minMaxValue = vec4(\").concat(b, \");\\n        float avgValue = 0.0;\\n        count = 0.0;\\n\\n        for (int wD = 0; wD < \").concat(d, \";\\n            wD += \").concat(u, \") {\\n          int xD = xDCorner + wD;\\n\\n          if (xD < 0 || xD >= \").concat(e.inDepth, \") {\\n            continue;\\n          }\\n\\n          for (int wR = 0; wR < \").concat(h, \";\\n            wR += \").concat(c, \") {\\n            int xR = xRCorner + wR;\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int wC = 0; wC < \").concat(v, \"; wC += 4) {\\n              int xC = xCCorner + wC * \").concat(p, \";\\n\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                getValue(batch, xD, xR, xC + \").concat(p, \", ch),\\n                getValue(batch, xD, xR, xC + 2 * \").concat(p, \", ch),\\n                getValue(batch, xD, xR, xC + 3 * \").concat(p, \", ch)\\n              );\\n\\n              \").concat(C, \"\\n            }\\n\\n            int xC = xCCorner + \").concat(v, \";\\n            if (\").concat(1 === I, \") {\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                initializationValue,\\n                initializationValue,\\n                initializationValue\\n              );\\n\\n              \").concat(C, \"\\n            } else if (\").concat(2 === I, \") {\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                getValue(batch, xD, xR, xC + \").concat(p, \", ch),\\n                initializationValue,\\n                initializationValue\\n              );\\n\\n              \").concat(C, \"\\n            } else if (\").concat(3 === I, \") {\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                getValue(batch, xD, xR, xC + \").concat(p, \", ch),\\n                getValue(batch, xD, xR, xC + 2 * \").concat(p, \", ch),\\n                initializationValue\\n              );\\n\\n              \").concat(C, \"\\n            }\\n          }\\n          setOutput(\").concat(x, \");\\n        }\\n      }\\n    \");\n  }\n\n}\n\nfunction avgPool$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex$2(a, \"avgPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$6(eitherStridesOrDilationsAreOne$1(o, 1), () => \"Error in avgPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo$1(a.shape, s, o, 1, i, l);\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual$1(u.inShape, u.outShape)) return identity$3({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var c = new Pool2DProgram$1(u, \"avg\", !1);\n  return n.runWebGLProgram(c, [a], \"float32\");\n}\n\nvar avgPoolConfig$2 = {\n  kernelName: AvgPool$1,\n  backendName: \"webgl\",\n  kernelFunc: avgPool$3\n};\n\nfunction avgPool3D$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l,\n    dataFormat: u\n  } = r,\n      c = computePool3DInfo$1(a.shape, s, o, [1, 1, 1], i, l, u),\n      p = new Pool3DProgram$1(c, \"avg\", !1);\n  return n.runWebGLProgram(p, [a], \"float32\");\n}\n\nvar avgPool3DConfig$2 = {\n  kernelName: AvgPool3D$1,\n  backendName: \"webgl\",\n  kernelFunc: avgPool3D$2\n};\n\nclass AvgPool2DBackpropProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"dy\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterHeight,\n        n = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n      const float avgMultiplier = float(\").concat(1 / (e.filterHeight * e.filterWidth), \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n\\n        ivec2 dyRCCorner = coords.yz - pads;\\n        int dyRCorner = dyRCCorner.x;\\n        int dyCCorner = dyRCCorner.y;\\n\\n        // Convolve dy(?, ?, d) with pos mask(:, :, d) to get dx(xR, xC, d).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(t, \";\\n            wR += \").concat(e.dilationHeight, \") {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          for (int wC = 0; wC < \").concat(n, \";\\n            wC+= \").concat(e.dilationWidth, \") {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            float dyValue = getDy(b, idyR, idyC, d);\\n\\n            dotProd += dyValue * avgMultiplier;\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass AvgPool3DBackpropProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"dy\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterDepth,\n        n = e.effectiveFilterHeight,\n        r = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec3 pads = ivec3(\".concat(t - 1 - e.padInfo.front, \", \").concat(n - 1 - e.padInfo.top, \", \").concat(r - 1 - e.padInfo.left, \");\\n      const float avgMultiplier = float(\").concat(1 / (e.filterDepth * e.filterHeight * e.filterWidth), \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int ch = coords.u;\\n\\n        ivec3 dyCorner = ivec3(coords.y, coords.z, coords.w) - pads;\\n        int dyDCorner = dyCorner.x;\\n        int dyRCorner = dyCorner.y;\\n        int dyCCorner = dyCorner.z;\\n\\n        // Convolve dy(?, ?, ?, d) with pos mask(:, :, :, ch) to get\\n        // dx(xD, xR, xC, ch).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n\\n        for (int wD = 0; wD < \").concat(t, \";\\n            wD += \").concat(e.dilationDepth, \") {\\n          float dyD = float(dyDCorner + wD) / \").concat(e.strideDepth, \".0;\\n\\n          if (dyD < 0.0 || dyD >= \").concat(e.outDepth, \".0 || fract(dyD) > 0.0) {\\n            continue;\\n          }\\n          int idyD = int(dyD);\\n\\n          for (int wR = 0; wR < \").concat(n, \";\\n              wR += \").concat(e.dilationHeight, \") {\\n            float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n            if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 ||\\n                fract(dyR) > 0.0) {\\n              continue;\\n            }\\n            int idyR = int(dyR);\\n\\n            for (int wC = 0; wC < \").concat(r, \";\\n                wC += \").concat(e.dilationWidth, \") {\\n              float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n              if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                  fract(dyC) > 0.0) {\\n                continue;\\n              }\\n              int idyC = int(dyC);\\n\\n              float dyValue = getDy(batch, idyD, idyR, idyC, ch);\\n\\n              dotProd += dyValue * avgMultiplier;\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction avgPool3DGrad$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s,\n      {\n    filterSize: i,\n    strides: l,\n    pad: u,\n    dimRoundingMode: c\n  } = r,\n      p = computePool3DInfo$1(o.shape, i, l, [1, 1, 1], u, c),\n      d = new AvgPool3DBackpropProgram$1(p);\n  return n.runWebGLProgram(d, [a], o.dtype);\n}\n\nvar avgPoolGrad3DConfig$1 = {\n  kernelName: AvgPool3DGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: avgPool3DGrad$2\n};\n\nfunction avgPoolGrad$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s;\n  assertNotComplex$2([a, s], \"avgPoolGrad\");\n  var {\n    filterSize: i,\n    strides: l,\n    pad: u\n  } = r,\n      c = computePool2DInfo$1(o.shape, i, l, 1, u),\n      p = new AvgPool2DBackpropProgram$1(c);\n  return n.runWebGLProgram(p, [a], o.dtype);\n}\n\nvar avgPoolGradConfig$3 = {\n  kernelName: AvgPoolGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: avgPoolGrad$3\n};\n\nfunction batchMatMul$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s\n  } = t,\n      {\n    transposeA: o,\n    transposeB: i\n  } = r;\n  return batchMatMulImpl$1({\n    a,\n    b: s,\n    transposeA: o,\n    transposeB: i,\n    backend: n\n  });\n}\n\nvar batchMatMulConfig$2 = {\n  kernelName: BatchMatMul$1,\n  backendName: \"webgl\",\n  kernelFunc: batchMatMul$2\n};\n\nclass BatchNormProgram$1 {\n  constructor(e, t, n, r, a, s) {\n    this.outputShape = [], this.variableNames = [\"x\", \"mean\", \"variance\"], assertAndGetBroadcastShape$1(e, t), assertAndGetBroadcastShape$1(e, n);\n    var o = \"0.0\";\n    null != r && (assertAndGetBroadcastShape$1(e, r), this.variableNames.push(\"offset\"), o = \"getOffsetAtOutCoords()\");\n    var i = \"1.0\";\n    null != a && (assertAndGetBroadcastShape$1(e, a), this.variableNames.push(\"scale\"), i = \"getScaleAtOutCoords()\"), this.outputShape = e, this.userCode = \"\\n      void main() {\\n        float x = getXAtOutCoords();\\n        float mean = getMeanAtOutCoords();\\n        float variance = getVarianceAtOutCoords();\\n        float offset = \".concat(o, \";\\n        float scale = \").concat(i, \";\\n        float inv = scale * inversesqrt(variance + float(\").concat(s, \"));\\n        setOutput(dot(vec3(x, -mean, offset), vec3(inv, inv, 1)));\\n      }\\n    \");\n  }\n\n}\n\nclass BatchNormPackedProgram$1 {\n  constructor(e, t, n, r, a, s) {\n    this.packedInputs = !0, this.packedOutput = !0, this.variableNames = [\"x\", \"mean\", \"variance\"], assertAndGetBroadcastShape$1(e, t), assertAndGetBroadcastShape$1(e, n);\n    var o = \"vec4(0.0)\";\n    null != r && (assertAndGetBroadcastShape$1(e, r), this.variableNames.push(\"offset\"), o = \"getOffsetAtOutCoords()\");\n    var i = \"vec4(1.0)\";\n    null != a && (assertAndGetBroadcastShape$1(e, a), this.variableNames.push(\"scale\"), i = \"getScaleAtOutCoords()\"), this.outputShape = e, this.userCode = \"\\n      void main() {\\n        vec4 offset = \".concat(o, \";\\n        vec4 scale = \").concat(i, \";\\n\\n        vec4 x = getXAtOutCoords();\\n        vec4 mean = getMeanAtOutCoords();\\n        vec4 variance = getVarianceAtOutCoords();\\n\\n        vec4 inv = scale * inversesqrt(variance + vec4(\").concat(s, \"));\\n\\n        setOutput((x - mean) * inv + offset);\\n      }\\n    \");\n  }\n\n}\n\nvar batchNorm$3 = _ref19 => {\n  var {\n    inputs: e,\n    backend: t,\n    attrs: n\n  } = _ref19;\n  var {\n    x: r,\n    mean: a,\n    variance: s,\n    offset: o,\n    scale: i\n  } = e;\n  assert$6(a.shape.length === s.shape.length, () => \"Batch normalization gradient requires mean and variance to have equal ranks.\"), assert$6(null == o || a.shape.length === o.shape.length, () => \"Batch normalization gradient requires mean and offset to have equal ranks.\"), assert$6(null == i || a.shape.length === i.shape.length, () => \"Batch normalization gradient requires mean and scale to have equal ranks.\");\n  var {\n    varianceEpsilon: l\n  } = n;\n  null == l && (l = .001);\n  var u = [r, a, s];\n  var c = null;\n  null != o && (c = o.shape, u.push(o));\n  var p = null;\n  null != i && (p = i.shape, u.push(i));\n  var d = env$1().getBool(\"WEBGL_PACK_NORMALIZATION\") ? new BatchNormPackedProgram$1(r.shape, a.shape, s.shape, c, p, l) : new BatchNormProgram$1(r.shape, a.shape, s.shape, c, p, l);\n  return t.runWebGLProgram(d, u, u[0].dtype);\n},\n    batchNormConfig$2 = {\n  kernelName: FusedBatchNorm$1,\n  backendName: \"webgl\",\n  kernelFunc: batchNorm$3\n};\n\nclass SliceProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"source\"], this.outputShape = e, this.rank = e.length;\n    var t = getCoordsDataType$1(this.rank);\n    this.customUniforms = [{\n      name: \"start\",\n      arrayIndex: this.rank,\n      type: \"int\"\n    }];\n    var n = getCoords$3(this.rank);\n    var r;\n    r = \"\\n        \".concat(t, \" sourceLoc;\\n        \").concat(t, \" coords = getOutputCoords();\\n        \").concat(e.map((e, t) => \"sourceLoc.\".concat(coords$1[t], \" = start[\").concat(t, \"] + coords.\").concat(coords$1[t], \";\")).join(\"\\n\"), \"\\n      \"), this.userCode = \"\\n      void main() {\\n        \".concat(r, \"\\n        setOutput(getSource(\").concat(n, \"));\\n      }\\n    \");\n  }\n\n}\n\nvar coords$1 = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"];\n\nfunction getCoords$3(e) {\n  if (1 === e) return \"sourceLoc\";\n  if (e <= 6) return coords$1.slice(0, e).map(e => \"sourceLoc.\" + e).join(\",\");\n  throw Error(\"Slicing for rank \".concat(e, \" is not yet supported\"));\n}\n\nclass SlicePackedProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"source\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e, this.rank = e.length, this.customUniforms = [{\n      name: \"start\",\n      arrayIndex: this.rank,\n      type: \"int\"\n    }];\n    var t = getCoordsDataType$1(this.rank),\n        n = getChannels$1(\"coords\", this.rank),\n        r = getChannels$1(\"sourceLoc\", this.rank),\n        a = 1 === this.rank ? \"sourceLoc\" : \"vec2(\".concat(r.slice(-2).join(), \")\"),\n        s = \"getChannel(getSource(\".concat(r.join(), \"), \").concat(a, \")\"),\n        o = \"\\n      result.x = \".concat(s, \";\\n      if (++\").concat(n[this.rank - 1], \" < \").concat(e[this.rank - 1], \") {\\n        ++\").concat(r[this.rank - 1], \";\\n        result.y = \").concat(s, \";\\n        --\").concat(r[this.rank - 1], \";\\n      }\\n    \"),\n        i = 1 === this.rank ? \"\" : \"\\n      --\".concat(n[this.rank - 1], \";\\n      if (++\").concat(n[this.rank - 2], \" < \").concat(e[this.rank - 2], \") {\\n        ++\").concat(r[this.rank - 2], \";\\n        result.z = \").concat(s, \";\\n        if (++\").concat(n[this.rank - 1], \" < \").concat(e[this.rank - 1], \") {\\n          ++\").concat(r[this.rank - 1], \";\\n          result.w = \").concat(s, \";\\n        }\\n      }\\n    \"),\n        l = this.rank <= 4 ? \"sourceLoc = coords +\\n            \".concat(t, \"(\").concat(e.map((e, t) => \"start[\".concat(t, \"]\")).join(), \");\") : e.map((e, t) => \"\".concat(r[t], \" = \").concat(n[t], \" + start[\").concat(t, \"];\")).join(\"\\n\");\n    this.userCode = \"\\n      void main() {\\n        \".concat(t, \" coords = getOutputCoords();\\n        \").concat(t, \" sourceLoc;\\n        \").concat(l, \"\\n        vec4 result = vec4(0.);\\n        \").concat(o, \"\\n        \").concat(i, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction shallowSlice$1(e, t, n, r) {\n  var a = r.texData.get(e.dataId),\n      s = r.makeTensorInfo(n, e.dtype),\n      o = r.texData.get(s.dataId);\n  Object.assign(o, a), o.refCount = 1, o.shape = n, o.dtype = e.dtype;\n  var i = computeFlatOffset$1(t, computeStrides$1(e.shape));\n  a.slice && (i += a.slice.flatOffset), o.slice = {\n    flatOffset: i,\n    origDataId: a.slice && a.slice.origDataId || e.dataId\n  };\n  var l = r.dataRefCount.get(o.slice.origDataId) || 1;\n  return r.dataRefCount.set(o.slice.origDataId, l + 1), s;\n}\n\nfunction slice$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    size: o\n  } = r,\n      [i, l] = parseSliceParams$1(a, s, o);\n  if (assertParamsValid$1(a, i, l), 0 === sizeFromShape$1(l)) return n.makeTensorInfo(l, a.dtype, []);\n\n  if (n.shouldExecuteOnCPU([a]) || \"string\" === a.dtype) {\n    var _e452 = n.texData.get(a.dataId),\n        _t328 = sliceImplCPU$1(_e452.values, i, l, a.shape, a.dtype);\n\n    return n.makeTensorInfo(l, a.dtype, _t328);\n  }\n\n  var {\n    isPacked: u\n  } = n.texData.get(a.dataId),\n      c = isSliceContinous$1(a.shape, i, l);\n\n  if (u || !c) {\n    var _e453 = env$1().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new SlicePackedProgram$1(l) : new SliceProgram$1(l);\n\n    return n.runWebGLProgram(_e453, [a], a.dtype, [i]);\n  }\n\n  return n.uploadToGPU(a.dataId), shallowSlice$1(a, i, l, n);\n}\n\nvar sliceConfig$2 = {\n  kernelName: Slice$1,\n  backendName: \"webgl\",\n  kernelFunc: slice$3\n},\n    batchToSpaceND$3 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    crops: o\n  } = r;\n  assert$6(a.shape.length <= 4, () => \"batchToSpaceND for rank > 4 with a WebGL backend not implemented yet\");\n  var i = s.reduce((e, t) => e * t),\n      l = getReshaped$1(a.shape, s, i),\n      u = getPermuted$1(l.length, s.length),\n      c = getReshapedPermuted$1(a.shape, s, i),\n      p = getSliceBeginCoords$1(o, s.length),\n      d = getSliceSize$1(c, o, s.length),\n      h = [],\n      m = reshape$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      f = transpose$3({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }),\n      g = reshape$4({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      $ = slice$3({\n    inputs: {\n      x: g\n    },\n    backend: n,\n    attrs: {\n      begin: p,\n      size: d\n    }\n  });\n  return h.push(m), h.push(f), h.push(g), h.forEach(e => n.disposeIntermediateTensorInfo(e)), $;\n},\n    batchToSpaceNDConfig$2 = {\n  kernelName: BatchToSpaceND$1,\n  backendName: \"webgl\",\n  kernelFunc: batchToSpaceND$3\n};\n\nfunction bincount$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o\n  } = r,\n      i = n.readSync(a.dataId),\n      l = n.readSync(s.dataId),\n      u = bincountImplCPU$1(i, l, s.dtype, s.shape, o);\n  return n.makeTensorInfo([o], s.dtype, u);\n}\n\nvar bincountConfig$2 = {\n  kernelName: Bincount$1,\n  backendName: \"webgl\",\n  kernelFunc: bincount$3\n},\n    NOT_EQUAL$1 = \"return float(a != b);\",\n    notEqual$3 = binaryKernelFunc$2({\n  opSnippet: NOT_EQUAL$1,\n  cpuKernelImpl: notEqualImplCPU$1,\n  dtype: \"bool\"\n}),\n    notEqualConfig$2 = {\n  kernelName: NotEqual$1,\n  backendName: \"webgl\",\n  kernelFunc: notEqual$3\n};\n\nfunction real$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return identity$3({\n    inputs: {\n      x: n.texData.get(r.dataId).complexTensorInfos.real\n    },\n    backend: n\n  });\n}\n\nvar realConfig$2 = {\n  kernelName: Real$1,\n  backendName: \"webgl\",\n  kernelFunc: real$3\n},\n    TO_INT$1 = \"return float(int(x));\";\n\nfunction int$1(e, t) {\n  var n = new UnaryOpProgram$1(e.shape, TO_INT$1),\n      r = t.runWebGLProgram(n, [e], \"int32\");\n  return {\n    dataId: r.dataId,\n    shape: r.shape,\n    dtype: r.dtype\n  };\n}\n\nfunction cast$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dtype: s\n  } = r;\n\n  if (\"complex64\" === s) {\n    if (\"complex64\" === a.dtype) return identity$3({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    var _e454 = zeros$4(a.shape),\n        _t329 = cast$4({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        dtype: \"float32\"\n      }\n    }),\n        _r157 = complex$3({\n      inputs: {\n        real: _t329,\n        imag: _e454\n      },\n      backend: n\n    });\n\n    return _e454.dispose(), n.disposeIntermediateTensorInfo(_t329), _r157;\n  }\n\n  if (\"complex64\" === a.dtype) {\n    var _e455 = real$3({\n      inputs: {\n        input: a\n      },\n      backend: n\n    }),\n        _t330 = cast$4({\n      inputs: {\n        x: _e455\n      },\n      backend: n,\n      attrs: {\n        dtype: s\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(_e455), _t330;\n  }\n\n  if (!hasEncodingLoss$1(a.dtype, s)) {\n    var _e456 = identity$3({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    return {\n      dataId: _e456.dataId,\n      shape: _e456.shape,\n      dtype: s\n    };\n  }\n\n  if (\"int32\" === s) return int$1(a, n);\n\n  if (\"bool\" === s) {\n    var _e457 = n.makeTensorInfo([], \"bool\", getTypedArrayFromDType$1(\"bool\", 1)),\n        _t331 = notEqual$3({\n      inputs: {\n        a,\n        b: _e457\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e457), _t331;\n  }\n\n  throw new Error(\"Error in Cast: failed to cast \".concat(a.dtype, \" to \").concat(s));\n}\n\nvar castConfig$2 = {\n  kernelName: Cast$1,\n  backendName: \"webgl\",\n  kernelFunc: cast$4\n},\n    CEIL$1 = \"return ceil(x);\",\n    ceil$3 = unaryKernelFunc$2({\n  opSnippet: CEIL$1,\n  packedOpSnippet: CEIL$1,\n  cpuKernelImpl: ceilImplCPU$1\n}),\n    ceilConfig$2 = {\n  kernelName: Ceil$1,\n  backendName: \"webgl\",\n  kernelFunc: ceil$3\n};\n\nclass ClipProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.customUniforms = [{\n      name: \"minVal\",\n      type: \"float\"\n    }, {\n      name: \"maxVal\",\n      type: \"float\"\n    }], this.outputShape = e, this.userCode = \"\\n\\n      void main() {\\n        float value = getAAtOutCoords();\\n        if (isnan(value)) {\\n          setOutput(value);\\n          return;\\n        }\\n\\n        setOutput(clamp(value, minVal, maxVal));\\n      }\\n    \";\n  }\n\n}\n\nclass ClipPackedProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.customUniforms = [{\n      name: \"minVal\",\n      type: \"float\"\n    }, {\n      name: \"maxVal\",\n      type: \"float\"\n    }], this.outputShape = e, this.userCode = \"\\n      void main() {\\n        vec4 value = getAAtOutCoords();\\n\\n        if (any(isnan(value))) {\\n          setOutput(value);\\n          return;\\n        }\\n\\n        setOutput(clamp(value, vec4(minVal), vec4(maxVal)));\\n      }\\n    \";\n  }\n\n}\n\nfunction clipByValue$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    clipValueMin: s,\n    clipValueMax: o\n  } = r;\n  var i;\n  return i = env$1().getBool(\"WEBGL_PACK_CLIP\") ? new ClipPackedProgram$1(a.shape) : new ClipProgram$1(a.shape), n.runWebGLProgram(i, [a], a.dtype, [[s], [o]]);\n}\n\nvar clipByValueConfig$1 = {\n  kernelName: ClipByValue$1,\n  backendName: \"webgl\",\n  kernelFunc: clipByValue$2\n};\n\nclass ComplexAbsProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"real\", \"imag\"], this.outputShape = e, this.userCode = \"\\n      void main() {\\n        float re = abs(getRealAtOutCoords());\\n        float im = abs(getImagAtOutCoords());\\n        float mx = max(re, im);\\n\\n        // sadly the length function in glsl is not underflow-safe\\n        // (at least not on Intel GPUs). So the safe solution is\\n        // to ensure underflow-safety in all cases.\\n        setOutput(\\n          mx == 0.0 ? 0.0 : mx * length(vec2(1, min(re, im)/mx))\\n        );\\n      }\\n    \";\n  }\n\n}\n\nfunction makeComplexComponentTensorInfo$1(e, t) {\n  return {\n    dataId: t.dataId,\n    dtype: t.dtype,\n    shape: e.shape\n  };\n}\n\nfunction complexAbs$2(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t,\n      a = n.texData.get(r.dataId),\n      s = new ComplexAbsProgram$1(r.shape),\n      o = [makeComplexComponentTensorInfo$1(r, a.complexTensorInfos.real), makeComplexComponentTensorInfo$1(r, a.complexTensorInfos.imag)];\n  return n.runWebGLProgram(s, o, o[0].dtype);\n}\n\nvar complexAbsConfig$2 = {\n  kernelName: ComplexAbs$1,\n  backendName: \"webgl\",\n  kernelFunc: complexAbs$2\n};\n\nclass ConcatProgram$1 {\n  constructor(e) {\n    this.outputShape = [], this.outputShape = computeOutShape$4(e, 1), this.variableNames = e.map((e, t) => \"T\".concat(t));\n    var t = new Array(e.length - 1);\n    t[0] = e[0][1];\n\n    for (var _n210 = 1; _n210 < t.length; _n210++) {\n      t[_n210] = t[_n210 - 1] + e[_n210][1];\n    }\n\n    var n = [\"if (yC < \".concat(t[0], \") setOutput(getT0(yR, yC));\")];\n\n    for (var _e458 = 1; _e458 < t.length; _e458++) {\n      n.push(\"else if (yC < \".concat(t[_e458], \") setOutput(getT\").concat(_e458, \"(yR, yC-\").concat(t[_e458 - 1], \"));\"));\n    }\n\n    n.push(\"else setOutput(getT\".concat(t.length, \"(yR, yC-\").concat(t[t.length - 1], \"));\")), this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int yR = coords.x;\\n        int yC = coords.y;\\n\\n        \".concat(n.join(\"\\n        \"), \"\\n      }\\n    \");\n  }\n\n}\n\nclass ConcatPackedProgram$1 {\n  constructor(e, t) {\n    this.packedInputs = !0, this.packedOutput = !0, this.outputShape = [], this.outputShape = computeOutShape$4(e, t);\n    var n = this.outputShape,\n        r = n.length,\n        a = getCoordsDataType$1(r),\n        s = getChannels$1(\"coords\", r),\n        o = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"].slice(0, r);\n    this.variableNames = e.map((e, t) => \"T\".concat(t));\n    var i = new Array(e.length - 1);\n    i[0] = e[0][t];\n\n    for (var _n211 = 1; _n211 < i.length; _n211++) {\n      i[_n211] = i[_n211 - 1] + e[_n211][t];\n    }\n\n    var l = o[t],\n        u = o.slice(-2),\n        c = o.join();\n    var p = \"if (\".concat(l, \" < \").concat(i[0], \") {\\n        return getChannel(\\n            getT0(\").concat(c, \"), vec2(\").concat(u.join(), \"));\\n        }\");\n\n    for (var _e459 = 1; _e459 < i.length; _e459++) {\n      var _t332 = i[_e459 - 1];\n      p += \"\\n        if (\".concat(l, \" < \").concat(i[_e459], \"  && \").concat(l, \" >= \").concat(i[_e459 - 1], \") {\\n          return getChannel(\\n            getT\").concat(_e459, \"(\").concat(shiftedChannels$1(o, l, _t332), \"),\\n            vec2(\").concat(shiftedChannels$1(u, l, _t332), \"));\\n        }\");\n    }\n\n    var d = i[i.length - 1];\n    p += \"\\n        return getChannel(\\n          getT\".concat(i.length, \"(\").concat(shiftedChannels$1(o, l, d), \"),\\n          vec2(\").concat(shiftedChannels$1(u, l, d), \"));\"), this.userCode = \"\\n      float getValue(\".concat(o.map(e => \"int \" + e), \") {\\n        \").concat(p, \"\\n      }\\n\\n      void main() {\\n        \").concat(a, \" coords = getOutputCoords();\\n        vec4 result = vec4(getValue(\").concat(s, \"), 0., 0., 0.);\\n\\n        \").concat(s[r - 1], \" = \").concat(s[r - 1], \" + 1;\\n        if (\").concat(s[r - 1], \" < \").concat(n[r - 1], \") {\\n          result.g = getValue(\").concat(s, \");\\n        }\\n\\n        \").concat(s[r - 2], \" = \").concat(s[r - 2], \" + 1;\\n        if (\").concat(s[r - 2], \" < \").concat(n[r - 2], \") {\\n          result.a = getValue(\").concat(s, \");\\n        }\\n\\n        \").concat(s[r - 1], \" = \").concat(s[r - 1], \" - 1;\\n        if (\").concat(s[r - 2], \" < \").concat(n[r - 2], \" &&\\n            \").concat(s[r - 1], \" < \").concat(n[r - 1], \") {\\n          result.b = getValue(\").concat(s, \");\\n        }\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction shiftedChannels$1(e, t, n) {\n  var r = e.indexOf(t);\n  return e.map((e, t) => t === r ? \"\".concat(e, \" - \").concat(n) : e).join();\n}\n\nfunction imag$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return identity$3({\n    inputs: {\n      x: n.texData.get(r.dataId).complexTensorInfos.imag\n    },\n    backend: n\n  });\n}\n\nvar imagConfig$2 = {\n  kernelName: Imag$1,\n  backendName: \"webgl\",\n  kernelFunc: imag$3\n};\n\nfunction concatImpl$2(e, t, n) {\n  var r = e[0].dtype;\n\n  if (\"complex64\" === r) {\n    var _r158 = e.map(e => real$3({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _a110 = e.map(e => imag$3({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _s83 = concatImpl$2(_r158, t, n),\n        _o59 = concatImpl$2(_a110, t, n),\n        _i39 = complex$3({\n      inputs: {\n        real: _s83,\n        imag: _o59\n      },\n      backend: n\n    });\n\n    return _r158.forEach(e => n.disposeIntermediateTensorInfo(e)), _a110.forEach(e => n.disposeIntermediateTensorInfo(e)), n.disposeIntermediateTensorInfo(_s83), n.disposeIntermediateTensorInfo(_o59), _i39;\n  }\n\n  var a = n.shouldExecuteOnCPU(e);\n\n  if (\"string\" === r && (a = !0), a) {\n    var _a111 = e.map(e => {\n      var r = sizeFromShape$1(e.shape.slice(t));\n      return reshape$4({\n        inputs: {\n          x: e\n        },\n        backend: n,\n        attrs: {\n          shape: [-1, r]\n        }\n      });\n    }),\n        _s84 = _a111.map(e => ({\n      vals: n.readSync(e.dataId),\n      shape: e.shape\n    })),\n        _o60 = computeOutShape$4(_a111.map(e => e.shape), 1),\n        _i40 = concatImplCPU$1(_s84, _o60, r, 1 === _a111[0].shape[0]),\n        _l31 = computeOutShape$4(e.map(e => e.shape), t),\n        _u27 = n.makeTensorInfo(_l31, r, _i40);\n\n    return _a111.forEach(e => n.disposeIntermediateTensorInfo(e)), _u27;\n  }\n\n  if (e.length > env$1().getNumber(\"WEBGL_MAX_TEXTURES_IN_SHADER\")) {\n    var _r159 = Math.floor(e.length / 2),\n        _a112 = concatImpl$2(e.slice(0, _r159), t, n),\n        _s85 = concatImpl$2(e.slice(_r159), t, n),\n        _o61 = concatImpl$2([_a112, _s85], t, n);\n\n    return n.disposeIntermediateTensorInfo(_a112), n.disposeIntermediateTensorInfo(_s85), _o61;\n  }\n\n  if (env$1().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") && e[0].shape.length > 1) {\n    var _a113 = new ConcatPackedProgram$1(e.map(e => e.shape), t);\n\n    return n.runWebGLProgram(_a113, e, r);\n  }\n\n  var {\n    tensors2D: s,\n    outShape: o\n  } = computeTensors2D$1(e, t, n),\n      i = new ConcatProgram$1(s.map(e => e.shape)),\n      l = n.runWebGLProgram(i, s, r);\n  s.forEach(e => n.disposeIntermediateTensorInfo(e));\n  var u = reshape$4({\n    inputs: {\n      x: l\n    },\n    attrs: {\n      shape: o\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(l), u;\n}\n\nfunction computeTensors2D$1(e, t, n) {\n  var r = computeOutShape$4(e.map(e => e.shape), t);\n  return {\n    tensors2D: e.map(e => reshape$4({\n      inputs: {\n        x: e\n      },\n      attrs: {\n        shape: [-1, sizeFromShape$1(e.shape.slice(t))]\n      },\n      backend: n\n    })),\n    outShape: r\n  };\n}\n\nfunction concat$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r,\n      s = parseAxisParam$1(a, t[0].shape)[0],\n      o = computeOutShape$4(t.map(e => e.shape), s);\n  if (0 === sizeFromShape$1(o)) return n.makeTensorInfo(o, t[0].dtype, []);\n  var i = t.filter(e => sizeFromShape$1(e.shape) > 0);\n  return 1 === i.length ? identity$3({\n    inputs: {\n      x: i[0]\n    },\n    backend: n\n  }) : (assertParamsConsistent$1(i.map(e => e.shape), s), concatImpl$2(i, s, n));\n}\n\nvar concatConfig$2 = {\n  kernelName: Concat$1,\n  backendName: \"webgl\",\n  kernelFunc: concat$3\n};\n\nclass Conv2DProgram$1 {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var s = e.padInfo.top,\n        o = e.padInfo.left,\n        i = e.strideHeight,\n        l = e.strideWidth,\n        u = e.dilationHeight,\n        c = e.dilationWidth,\n        p = e.filterHeight,\n        d = e.filterWidth,\n        h = 4 * Math.floor(e.inChannels / 4),\n        m = e.inChannels % 4,\n        f = \"channelsLast\" === e.dataFormat,\n        g = f ? 1 : 2,\n        $ = f ? 2 : 3,\n        y = f ? 3 : 1;\n    var b = \"\",\n        x = \"\";\n    n && (b = r ? \"float activation(float a) {\\n          float b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(n, \"\\n        }\") : a ? \"float activation(float a) {\\n          float b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(n, \"\\n        }\") : \"\\n          float activation(float x) {\\n            \".concat(n, \"\\n          }\\n        \"), x = \"result = activation(result);\");\n    var v = t ? \"result += getBiasAtOutCoords();\" : \"\";\n    t && this.variableNames.push(\"bias\"), r && this.variableNames.push(\"preluActivationWeights\"), a && this.variableNames.push(\"leakyreluAlpha\"), this.userCode = \"\\n      \".concat(b, \"\\n\\n      const ivec2 strides = ivec2(\").concat(i, \", \").concat(l, \");\\n      const ivec2 pads = ivec2(\").concat(s, \", \").concat(o, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d2 = coords[\").concat(y, \"];\\n\\n        ivec2 xRCCorner =\\n            ivec2(coords[\").concat(g, \"], coords[\").concat($, \"]) * strides - pads;\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        // Convolve x(?, ?, d1) with w(:, :, d1, d2) to get y(yR, yC, d2).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(p, \"; wR++) {\\n          int xR = xRCorner + wR * \").concat(u, \";\\n\\n          if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n            continue;\\n          }\\n\\n          for (int wC = 0; wC < \").concat(d, \"; wC++) {\\n            int xC = xCCorner + wC * \").concat(c, \";\\n\\n            if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n              continue;\\n            }\\n\\n            for (int d1 = 0; d1 < \").concat(h, \"; d1 += 4) {\\n              vec4 wValues = vec4(\\n                getW(wR, wC, d1, d2),\\n                getW(wR, wC, d1 + 1, d2),\\n                getW(wR, wC, d1 + 2, d2),\\n                getW(wR, wC, d1 + 3, d2)\\n              );\\n\\n              if (\").concat(f, \") {\\n                vec4 xValues = vec4(\\n                  getX(batch, xR, xC, d1),\\n                  getX(batch, xR, xC, d1 + 1),\\n                  getX(batch, xR, xC, d1 + 2),\\n                  getX(batch, xR, xC, d1 + 3)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else {\\n                vec4 xValues = vec4(\\n                  getX(batch, d1, xR, xC),\\n                  getX(batch, d1 + 1, xR, xC),\\n                  getX(batch, d1 + 2, xR, xC),\\n                  getX(batch, d1 + 3, xR, xC)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n            }\\n\\n            if (\").concat(1 === m, \") {\\n\\n              if (\").concat(f, \") {\\n                dotProd +=\\n                    getX(batch, xR, xC, \").concat(h, \") *\\n                    getW(wR, wC, \").concat(h, \", d2);\\n              } else {\\n                dotProd +=\\n                    getX(batch, \").concat(h, \", xR, xC) *\\n                    getW(wR, wC, \").concat(h, \", d2);\\n              }\\n\\n            } else if (\").concat(2 === m, \") {\\n              vec2 wValues = vec2(\\n                getW(wR, wC, \").concat(h, \", d2),\\n                getW(wR, wC, \").concat(h, \" + 1, d2)\\n              );\\n\\n              if (\").concat(f, \") {\\n                vec2 xValues = vec2(\\n                  getX(batch, xR, xC, \").concat(h, \"),\\n                  getX(batch, xR, xC, \").concat(h, \" + 1)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else {\\n                vec2 xValues = vec2(\\n                  getX(batch, \").concat(h, \", xR, xC),\\n                  getX(batch, \").concat(h, \" + 1, xR, xC)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n\\n            } else if (\").concat(3 === m, \") {\\n              vec3 wValues = vec3(\\n                getW(wR, wC, \").concat(h, \", d2),\\n                getW(wR, wC, \").concat(h, \" + 1, d2),\\n                getW(wR, wC, \").concat(h, \" + 2, d2)\\n              );\\n\\n              if (\").concat(f, \") {\\n                vec3 xValues = vec3(\\n                  getX(batch, xR, xC, \").concat(h, \"),\\n                  getX(batch, xR, xC, \").concat(h, \" + 1),\\n                  getX(batch, xR, xC, \").concat(h, \" + 2)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else {\\n                vec3 xValues = vec3(\\n                  getX(batch, \").concat(h, \", xR, xC),\\n                  getX(batch, \").concat(h, \" + 1, xR, xC),\\n                  getX(batch, \").concat(h, \" + 2, xR, xC)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n\\n            }\\n          }\\n        }\\n\\n        float result = dotProd;\\n        \").concat(v, \"\\n        \").concat(x, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv3DProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var t = e.padInfo.front,\n        n = e.padInfo.top,\n        r = e.padInfo.left,\n        a = e.strideDepth,\n        s = e.strideHeight,\n        o = e.strideWidth,\n        i = e.dilationDepth,\n        l = e.dilationHeight,\n        u = e.dilationWidth,\n        c = e.filterDepth,\n        p = e.filterHeight,\n        d = e.filterWidth,\n        h = 4 * Math.floor(e.inChannels / 4),\n        m = e.inChannels % 4;\n    this.userCode = \"\\n      const ivec3 strides = ivec3(\".concat(a, \", \").concat(s, \", \").concat(o, \");\\n      const ivec3 pads = ivec3(\").concat(t, \", \").concat(n, \", \").concat(r, \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int d2 = coords.u;\\n\\n        ivec3 xFRCCorner = ivec3(coords.y, coords.z, coords.w) * strides - pads;\\n        int xFCorner = xFRCCorner.x;\\n        int xRCorner = xFRCCorner.y;\\n        int xCCorner = xFRCCorner.z;\\n\\n        // Convolve x(?, ?, ?, d1) with w(:, :, :, d1, d2) to get\\n        // y(yF, yR, yC, d2). ? = to be determined. : = across all\\n        // values in that axis.\\n        float dotProd = 0.0;\\n        for (int wF = 0; wF < \").concat(c, \"; wF++) {\\n          int xF = xFCorner + wF * \").concat(i, \";\\n\\n          if (xF < 0 || xF >= \").concat(e.inDepth, \") {\\n            continue;\\n          }\\n\\n          for (int wR = 0; wR < \").concat(p, \"; wR++) {\\n            int xR = xRCorner + wR * \").concat(l, \";\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int wC = 0; wC < \").concat(d, \"; wC++) {\\n              int xC = xCCorner + wC * \").concat(u, \";\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              for (int d1 = 0; d1 < \").concat(h, \"; d1 += 4) {\\n                vec4 xValues = vec4(\\n                  getX(batch, xF, xR, xC, d1),\\n                  getX(batch, xF, xR, xC, d1 + 1),\\n                  getX(batch, xF, xR, xC, d1 + 2),\\n                  getX(batch, xF, xR, xC, d1 + 3)\\n                );\\n                vec4 wValues = vec4(\\n                  getW(wF, wR, wC, d1, d2),\\n                  getW(wF, wR, wC, d1 + 1, d2),\\n                  getW(wF, wR, wC, d1 + 2, d2),\\n                  getW(wF, wR, wC, d1 + 3, d2)\\n                );\\n\\n                dotProd += dot(xValues, wValues);\\n              }\\n\\n              if (\").concat(1 === m, \") {\\n                dotProd +=\\n                  getX(batch, xF, xR, xC, \").concat(h, \") *\\n                  getW(wF, wR, wC, \").concat(h, \", d2);\\n              } else if (\").concat(2 === m, \") {\\n                vec2 xValues = vec2(\\n                  getX(batch, xF, xR, xC, \").concat(h, \"),\\n                  getX(batch, xF, xR, xC, \").concat(h, \" + 1)\\n                );\\n                vec2 wValues = vec2(\\n                  getW(wF, wR, wC, \").concat(h, \", d2),\\n                  getW(wF, wR, wC, \").concat(h, \" + 1, d2)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else if (\").concat(3 === m, \") {\\n                vec3 xValues = vec3(\\n                  getX(batch, xF, xR, xC, \").concat(h, \"),\\n                  getX(batch, xF, xR, xC, \").concat(h, \" + 1),\\n                  getX(batch, xF, xR, xC, \").concat(h, \" + 2)\\n                );\\n                vec3 wValues = vec3(\\n                  getW(wF, wR, wC, \").concat(h, \", d2),\\n                  getW(wF, wR, wC, \").concat(h, \" + 1, d2),\\n                  getW(wF, wR, wC, \").concat(h, \" + 2, d2)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Im2ColPackedProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e;\n    var {\n      filterWidth: r,\n      inChannels: a,\n      strideWidth: s,\n      strideHeight: o,\n      padInfo: i,\n      outWidth: l,\n      dilationWidth: u,\n      dilationHeight: c,\n      dataFormat: p\n    } = n,\n        {\n      left: d,\n      top: h\n    } = i,\n        m = a * r,\n        f = getGlslDifferences$1(),\n        g = \"channelsLast\" === p,\n        $ = g ? 0 : 1,\n        y = g ? 1 : 2;\n    var b = \"\";\n\n    for (var _n212 = 0; _n212 <= 1; _n212++) {\n      for (var _r160 = 0; _r160 <= 1; _r160++) {\n        b += \"\\n          blockIndex = rc.y + \".concat(_r160, \";\\n          pos = rc.x + \").concat(_n212, \";\\n\\n          if(blockIndex < \").concat(e[1], \" && pos < \").concat(e[0], \") {\\n            offsetY = int(blockIndex / (\").concat(l, \")) * \").concat(o, \" - \").concat(h, \";\\n            d0 = offsetY + \").concat(c, \" * (pos / \").concat(m, \");\\n\\n            if(d0 < \").concat(t[$], \" && d0 >= 0) {\\n\\n              offsetX = int(mod(float(blockIndex), \").concat(l, \".) * \").concat(s, \". - \").concat(d, \".);\\n              d1 = offsetX + \").concat(u, \" * (int(mod(float(pos), \").concat(m, \".) / \").concat(a, \".));\\n\\n              if(d1 < \").concat(t[y], \" && d1 >= 0) {\\n\\n                ch = int(mod(float(pos), \").concat(a, \".));\\n\\n                if (\").concat(g, \") {\\n                  innerDims = vec2(d1, ch);\\n                  result[\").concat(2 * _n212 + _r160, \"] = getChannel(\\n                    getA(d0, int(innerDims.x),\\n                    int(innerDims.y)), innerDims);\\n                } else {\\n                  innerDims = vec2(d0, d1);\\n                  result[\").concat(2 * _n212 + _r160, \"] = getChannel(\\n                    getA(ch, int(innerDims.x),\\n                    int(innerDims.y)), innerDims);\\n                }\\n              }\\n            }\\n          }\\n        \");\n      }\n    }\n\n    this.userCode = \"\\n      void main() {\\n        ivec2 rc = getOutputCoords();\\n\\n        vec4 result = vec4(0);\\n\\n        int blockIndex, pos, offsetY, d0, offsetX, d1, ch;\\n        vec2 innerDims;\\n\\n        \".concat(b, \"\\n\\n        \").concat(f.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nfunction conv2dByMatMul$1(_ref20) {\n  var {\n    x: e,\n    filter: t,\n    convInfo: n,\n    backend: r,\n    bias: a = null,\n    preluActivationWeights: s = null,\n    leakyreluAlpha: o = 0,\n    activation: i = null\n  } = _ref20;\n  var l = e.shape,\n      u = r.texData.get(e.dataId),\n      c = \"channelsLast\" === n.dataFormat;\n  var p;\n  var d = [],\n      h = l[2] % 2 != 0 && !!u.isPacked;\n\n  if ((1 != l[0] * l[1] * l[2] && 1 !== n.outChannels || !(n.inChannels > MATMUL_SHARED_DIM_THRESHOLD$1)) && env$1().getBool(\"WEBGL_LAZILY_UNPACK\") && env$1().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") && h) {\n    var _h11 = {\n      dataId: e.dataId,\n      shape: [1, c ? l[0] * l[1] * (l[2] + 1) : l[0] * l[2] * (l[3] + 1), n.inChannels],\n      dtype: e.dtype\n    },\n        m = u.shape;\n    u.shape = u.shape.slice(), u.shape[u.shape.length - 2]++, assert$6(isReshapeFree$1(u.shape, _h11.shape), () => \"packed reshape \".concat(u.shape, \" to \").concat(_h11.shape, \" isn't free\"));\n    var f = reshape$4({\n      inputs: {\n        x: t\n      },\n      backend: r,\n      attrs: {\n        shape: [1, n.inChannels, n.outChannels]\n      }\n    });\n    d.push(f);\n    var g = batchMatMulImpl$1({\n      a: _h11,\n      b: f,\n      backend: r,\n      transposeA: !1,\n      transposeB: !1,\n      bias: a,\n      activation: i,\n      preluActivationWeights: s,\n      leakyreluAlpha: o\n    }),\n        $ = r.texData.get(g.dataId);\n    assert$6($.isPacked, () => \"batchMatMul result is expected to be packed\"), u.shape = m, $.shape = n.outShape, p = identity$3({\n      inputs: {\n        x: g\n      },\n      backend: r\n    }), p.shape = n.outShape, d.push(g);\n  } else {\n    var _u28 = reshape$4({\n      inputs: {\n        x: e\n      },\n      backend: r,\n      attrs: {\n        shape: [1, c ? l[0] * l[1] * l[2] : l[0] * l[2] * l[3], n.inChannels]\n      }\n    }),\n        _h12 = reshape$4({\n      inputs: {\n        x: t\n      },\n      backend: r,\n      attrs: {\n        shape: [1, n.inChannels, n.outChannels]\n      }\n    }),\n        _m5 = batchMatMulImpl$1({\n      a: _u28,\n      b: _h12,\n      transposeA: !1,\n      transposeB: !1,\n      backend: r,\n      bias: a,\n      activation: i,\n      preluActivationWeights: s,\n      leakyreluAlpha: o\n    });\n\n    p = reshape$4({\n      inputs: {\n        x: _m5\n      },\n      backend: r,\n      attrs: {\n        shape: n.outShape\n      }\n    }), d.push(_u28), d.push(_h12), d.push(_m5);\n  }\n\n  for (var _e460 of d) {\n    r.disposeIntermediateTensorInfo(_e460);\n  }\n\n  return p;\n}\n\nfunction conv2dWithIm2Row$1(_ref21) {\n  var {\n    x: e,\n    filter: t,\n    convInfo: n,\n    backend: r,\n    bias: a = null,\n    preluActivationWeights: s = null,\n    leakyreluAlpha: o = 0,\n    activation: i = null\n  } = _ref21;\n  var {\n    filterWidth: l,\n    filterHeight: u,\n    inChannels: c,\n    outWidth: p,\n    outHeight: d,\n    dataFormat: h\n  } = n,\n      m = \"channelsLast\" === h,\n      f = l * u * c,\n      g = d * p,\n      $ = [f, g],\n      y = [],\n      b = reshape$4({\n    inputs: {\n      x: e\n    },\n    backend: r,\n    attrs: {\n      shape: e.shape.slice(1)\n    }\n  }),\n      x = reshape$4({\n    inputs: {\n      x: t\n    },\n    backend: r,\n    attrs: {\n      shape: [1, f, sizeFromShape$1(t.shape) / f]\n    }\n  });\n  y.push(b), y.push(x);\n  var v = new Im2ColPackedProgram$1($, b.shape, n),\n      I = r.runWebGLProgram(v, [b], \"float32\"),\n      C = reshape$4({\n    inputs: {\n      x: I\n    },\n    backend: r,\n    attrs: {\n      shape: [1, $[0], $[1]]\n    }\n  });\n  y.push(I), y.push(C);\n  var S = null != a,\n      k = null != s,\n      T = \"leakyrelu\" === i,\n      N = i ? mapActivationToShaderProgram$1(i, !0) : null,\n      w = new MatMulPackedProgram$1(C.shape, x.shape, [1, g, n.outChannels], !0, !1, S, N, k, T),\n      E = [C, x];\n\n  if (a && E.push(a), k && E.push(s), T) {\n    var _e461 = r.makeTensorInfo([], \"float32\", createScalarValue$1(o, \"float32\"));\n\n    E.push(_e461), y.push(_e461);\n  }\n\n  var A = r.runWebGLProgram(w, E, \"float32\"),\n      D = reshape$4({\n    inputs: {\n      x: A\n    },\n    backend: r,\n    attrs: {\n      shape: m ? [1, d, p, n.outChannels] : [1, n.outChannels, d, p]\n    }\n  });\n  y.push(A);\n\n  for (var _e462 of y) {\n    r.disposeIntermediateTensorInfo(_e462);\n  }\n\n  return D;\n}\n\nfunction conv2d$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dilations: u,\n    dimRoundingMode: c\n  } = r,\n      p = convertConv2DDataFormat$1(l),\n      d = computeConv2DInfo$1(a.shape, s.shape, o, u, i, c, !1, p);\n  var h;\n  if (1 !== d.filterHeight || 1 !== d.filterWidth || 1 !== d.dilationHeight || 1 !== d.dilationWidth || 1 !== d.strideHeight || 1 !== d.strideWidth || \"SAME\" !== d.padInfo.type && \"VALID\" !== d.padInfo.type) {\n    if (env$1().getBool(\"WEBGL_CONV_IM2COL\") && 1 === a.shape[0]) h = conv2dWithIm2Row$1({\n      x: a,\n      filter: s,\n      convInfo: d,\n      backend: n\n    });else {\n      var _e463 = new Conv2DProgram$1(d);\n\n      h = n.runWebGLProgram(_e463, [a, s], \"float32\");\n    }\n  } else h = conv2dByMatMul$1({\n    x: a,\n    filter: s,\n    convInfo: d,\n    backend: n\n  });\n  var m = reshape$4({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      shape: d.outShape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), m;\n}\n\nvar conv2DConfig$2 = {\n  kernelName: Conv2D$3,\n  backendName: \"webgl\",\n  kernelFunc: conv2d$4\n};\n\nclass Conv2DDerFilterProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"x\", \"dy\"], this.outputShape = e.filterShape, this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int wR = coords.x;\\n        int wC = coords.y;\\n        int d1 = coords.z;\\n        int d2 = coords.w;\\n\\n        // Convolve x(?, ?, d1) with dy(:, :, d2) to get dw(wR, wC, d1, d2).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n\\n        for (int b = 0; b < \".concat(e.batchSize, \"; b++) {\\n          for (int yR = 0; yR < \").concat(e.outHeight, \"; yR++) {\\n            int xR = wR + yR * \").concat(e.strideHeight, \" - \").concat(e.padInfo.top, \";\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int yC = 0; yC < \").concat(e.outWidth, \"; yC++) {\\n              int xC = wC + yC * \").concat(e.strideWidth, \" - \").concat(e.padInfo.left, \";\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              if (\").concat(\"channelsLast\" === e.dataFormat, \") {\\n                float dyValue = getDy(b, yR, yC, d2);\\n                float xValue = getX(b, xR, xC, d1);\\n                dotProd += (xValue * dyValue);\\n              } else {\\n                float dyValue = getDy(b, d2, yR, yC);\\n                float xValue = getX(b, d1, xR, xC);\\n                dotProd += (xValue * dyValue);\\n              }\\n\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv2DDerInputProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"W\"], this.outputShape = e.inShape;\n    var t = e.filterHeight,\n        n = e.filterWidth,\n        r = \"channelsLast\" === e.dataFormat;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d1 = coords[\").concat(r ? 3 : 1, \"];\\n\\n        ivec2 dyCorner = ivec2(coords[\").concat(r ? 1 : 2, \"], coords[\").concat(r ? 2 : 3, \"]) - pads;\\n        int dyRCorner = dyCorner.x;\\n        int dyCCorner = dyCorner.y;\\n\\n        // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(t, \"; wR++) {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          int wRPerm = \").concat(t, \" - 1 - wR;\\n\\n          for (int wC = 0; wC < \").concat(n, \"; wC++) {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            int wCPerm = \").concat(n, \" - 1 - wC;\\n\\n            for (int d2 = 0; d2 < \").concat(e.outChannels, \"; d2++) {\\n\\n              if (\").concat(r, \") {\\n                float xValue = getDy(batch, idyR, idyC, d2);\\n                float wValue = getW(wRPerm, wCPerm, d1, d2);\\n                dotProd += xValue * wValue;\\n              } else {\\n                float xValue = getDy(batch, d2, idyR, idyC);\\n                float wValue = getW(wRPerm, wCPerm, d1, d2);\\n                dotProd += xValue * wValue;\\n              }\\n\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv3DDerFilterProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"x\", \"dy\"], this.outputShape = e.filterShape, this.userCode = \"\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int wF = coords.x;\\n        int wR = coords.y;\\n        int wC = coords.z;\\n        int d1 = coords.w;\\n        int d2 = coords.u;\\n\\n        float dotProd = 0.0;\\n\\n        for (int b = 0; b < \".concat(e.batchSize, \"; b++) {\\n          for (int yF = 0; yF < \").concat(e.outDepth, \"; yF++) {\\n            int xF = wF + yF * \").concat(e.strideDepth, \" - \").concat(e.padInfo.front, \";\\n\\n            if (xF < 0 || xF >= \").concat(e.inDepth, \") {\\n              continue;\\n            }\\n\\n            for (int yR = 0; yR < \").concat(e.outHeight, \"; yR++) {\\n              int xR = wR + yR * \").concat(e.strideHeight, \" - \").concat(e.padInfo.top, \";\\n\\n              if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n                continue;\\n              }\\n\\n              for (int yC = 0; yC < \").concat(e.outWidth, \"; yC++) {\\n                int xC = wC + yC * \").concat(e.strideWidth, \" - \").concat(e.padInfo.left, \";\\n\\n                if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                  continue;\\n                }\\n\\n                float dyValue = getDy(b, yF, yR, yC, d2);\\n                float xValue = getX(b, xF, xR, xC, d1);\\n                dotProd += (xValue * dyValue);\\n              }\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv3DDerInputProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"W\"], this.outputShape = e.inShape;\n    var t = e.filterDepth,\n        n = e.filterHeight,\n        r = e.filterWidth;\n    this.userCode = \"\\n      const ivec3 pads = ivec3(\".concat(t - 1 - e.padInfo.front, \", \").concat(n - 1 - e.padInfo.top, \", \").concat(r - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int d1 = coords.u;\\n\\n\\n        ivec3 dyCorner = ivec3(coords.y, coords.z, coords.w) - pads;\\n        int dyFCorner = dyCorner.x;\\n        int dyRCorner = dyCorner.y;\\n        int dyCCorner = dyCorner.z;\\n\\n        float dotProd = 0.0;\\n        for (int wF = 0; wF < \").concat(t, \"; wF++) {\\n          float dyF = float(dyFCorner + wF) / \").concat(e.strideDepth, \".0;\\n\\n          if (dyF < 0.0 || dyF >= \").concat(e.outDepth, \".0 || fract(dyF) > 0.0) {\\n            continue;\\n          }\\n          int idyF = int(dyF);\\n\\n          int wFPerm = \").concat(t, \" - 1 - wF;\\n\\n          for (int wR = 0; wR < \").concat(n, \"; wR++) {\\n            float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n            if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 ||\\n              fract(dyR) > 0.0) {\\n              continue;\\n            }\\n            int idyR = int(dyR);\\n\\n            int wRPerm = \").concat(n, \" - 1 - wR;\\n\\n            for (int wC = 0; wC < \").concat(r, \"; wC++) {\\n              float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n              if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                  fract(dyC) > 0.0) {\\n                continue;\\n              }\\n              int idyC = int(dyC);\\n\\n              int wCPerm = \").concat(r, \" - 1 - wC;\\n\\n              for (int d2 = 0; d2 < \").concat(e.outChannels, \"; d2++) {\\n                float xValue = getDy(batch, idyF, idyR, idyC, d2);\\n                float wValue = getW(wFPerm, wRPerm, wCPerm, d1, d2);\\n                dotProd += xValue * wValue;\\n              }\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction conv2DBackpropFilter$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r,\n      p = convertConv2DDataFormat$1(l),\n      d = computeConv2DInfo$1(a.shape, c, o, 1, i, u, !1, p),\n      h = new Conv2DDerFilterProgram$1(d);\n  return n.runWebGLProgram(h, [a, s], \"float32\");\n}\n\nvar conv2DBackpropFilterConfig$2 = {\n  kernelName: Conv2DBackpropFilter$1,\n  backendName: \"webgl\",\n  kernelFunc: conv2DBackpropFilter$3\n};\n\nfunction conv2DBackpropInput$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    inputShape: o,\n    strides: i,\n    pad: l,\n    dataFormat: u,\n    dimRoundingMode: c\n  } = r,\n      p = convertConv2DDataFormat$1(u),\n      d = computeConv2DInfo$1(o, s.shape, i, 1, l, c, !1, p),\n      h = new Conv2DDerInputProgram$1(d);\n  return n.runWebGLProgram(h, [a, s], \"float32\");\n}\n\nvar conv2DBackpropInputConfig$2 = {\n  kernelName: Conv2DBackpropInput$1,\n  backendName: \"webgl\",\n  kernelFunc: conv2DBackpropInput$3\n};\n\nfunction conv3D$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l\n  } = r,\n      u = computeConv3DInfo$1(a.shape, s.shape, o, l, i),\n      c = new Conv3DProgram$1(u);\n  return n.runWebGLProgram(c, [a, s], \"float32\");\n}\n\nvar conv3DConfig$2 = {\n  kernelName: Conv3D$3,\n  backendName: \"webgl\",\n  kernelFunc: conv3D$2\n};\n\nfunction conv3DBackpropFilterV2$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    filterShape: l\n  } = r,\n      u = computeConv3DInfo$1(a.shape, l, o, 1, i),\n      c = new Conv3DDerFilterProgram$1(u);\n  return n.runWebGLProgram(c, [a, s], \"float32\");\n}\n\nvar conv3DBackpropFilterV2Config$2 = {\n  kernelName: Conv3DBackpropFilterV2$1,\n  backendName: \"webgl\",\n  kernelFunc: conv3DBackpropFilterV2$2\n};\n\nfunction conv3DBackpropInput$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    pad: o,\n    strides: i,\n    inputShape: l\n  } = r,\n      u = computeConv3DInfo$1(l, s.shape, i, 1, o),\n      c = new Conv3DDerInputProgram$1(u);\n  return n.runWebGLProgram(c, [a, s], \"float32\");\n}\n\nvar conv3DBackpropInputConfig$1 = {\n  kernelName: Conv3DBackpropInputV2$1,\n  backendName: \"webgl\",\n  kernelFunc: conv3DBackpropInput$2\n},\n    COS$1 = CHECK_NAN_SNIPPET_UNARY$1 + \"\\n  return cos(x);\\n\",\n    cos$3 = unaryKernelFunc$2({\n  opSnippet: COS$1\n}),\n    cosConfig$2 = {\n  kernelName: Cos$1,\n  backendName: \"webgl\",\n  kernelFunc: cos$3\n},\n    COSH$1 = \"\\n  float e2x = exp(-x);\\n  return (e2x + 1.0 / e2x) / 2.0;\\n\",\n    cosh$3 = unaryKernelFunc$2({\n  opSnippet: COSH$1\n}),\n    coshConfig$2 = {\n  kernelName: Cosh$1,\n  backendName: \"webgl\",\n  kernelFunc: cosh$3\n};\n\nclass CropAndResizeProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"Image\", \"Boxes\", \"BoxInd\"], this.outputShape = [];\n    var [s, o, i, l] = e,\n        [u] = t,\n        [c, p] = n;\n    this.outputShape = [u, c, p, l];\n    var d = \"bilinear\" === r ? 1 : 0,\n        [h, m] = [o - 1 + \".0\", i - 1 + \".0\"],\n        [f, g, $] = c > 1 ? [\"\" + (o - 1) / (c - 1), \"(y2-y1) * height_ratio\", \"y1*\".concat(h, \" + float(y)*(height_scale)\")] : [\"0.0\", \"0.0\", \"0.5 * (y1+y2) * \".concat(h)],\n        [y, b, x] = p > 1 ? [\"\" + (i - 1) / (p - 1), \"(x2-x1) * width_ratio\", \"x1*\".concat(m, \" + float(x)*(width_scale)\")] : [\"0.0\", \"0.0\", \"0.5 * (x1+x2) * \".concat(m)];\n    this.userCode = \"\\n      const float height_ratio = float(\".concat(f, \");\\n      const float width_ratio = float(\").concat(y, \");\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int y = coords[1];\\n        int x = coords[2];\\n        int d = coords[3];\\n\\n        // get box vals\\n        float y1 = getBoxes(b,0);\\n        float x1 = getBoxes(b,1);\\n        float y2 = getBoxes(b,2);\\n        float x2 = getBoxes(b,3);\\n\\n        // get image in batch index\\n        int bInd = round(getBoxInd(b));\\n        if(bInd < 0 || bInd >= \").concat(s, \") {\\n          return;\\n        }\\n\\n        float height_scale = \").concat(g, \";\\n        float width_scale = \").concat(b, \";\\n\\n        float in_y = \").concat($, \";\\n        if( in_y < 0.0 || in_y > \").concat(h, \" ) {\\n          setOutput(float(\").concat(a, \"));\\n          return;\\n        }\\n        float in_x = \").concat(x, \";\\n        if( in_x < 0.0 || in_x > \").concat(m, \" ) {\\n          setOutput(float(\").concat(a, \"));\\n          return;\\n        }\\n\\n        vec2 sourceFracIndexCR = vec2(in_x,in_y);\\n        if(\").concat(d, \" == 1) {\\n          // Compute the four integer indices.\\n          ivec2 sourceFloorCR = ivec2(sourceFracIndexCR);\\n          ivec2 sourceCeilCR = ivec2(ceil(sourceFracIndexCR));\\n\\n          float topLeft = getImage(b, sourceFloorCR.y, sourceFloorCR.x, d);\\n          float bottomLeft = getImage(b, sourceCeilCR.y, sourceFloorCR.x, d);\\n          float topRight = getImage(b, sourceFloorCR.y, sourceCeilCR.x, d);\\n          float bottomRight = getImage(b, sourceCeilCR.y, sourceCeilCR.x, d);\\n\\n          vec2 fracCR = sourceFracIndexCR - vec2(sourceFloorCR);\\n\\n          float top = topLeft + (topRight - topLeft) * fracCR.x;\\n          float bottom = bottomLeft + (bottomRight - bottomLeft) * fracCR.x;\\n          float newValue = top + (bottom - top) * fracCR.y;\\n          setOutput(newValue);\\n        } else {\\n          // Compute the coordinators of nearest neighbor point.\\n          ivec2 sourceNearestCR = ivec2(floor(\\n            sourceFracIndexCR + vec2(0.5,0.5)));\\n          float newValue = getImage(b, sourceNearestCR.y, sourceNearestCR.x, d);\\n          setOutput(newValue);\\n        }\\n      }\\n    \");\n  }\n\n}\n\nvar cropAndResize$3 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    image: a,\n    boxes: s,\n    boxInd: o\n  } = t,\n      {\n    cropSize: i,\n    method: l,\n    extrapolationValue: u\n  } = r,\n      c = new CropAndResizeProgram$1(a.shape, s.shape, i, l, u);\n  return n.runWebGLProgram(c, [a, s, o], \"float32\");\n},\n    cropAndResizeConfig$2 = {\n  kernelName: CropAndResize$1,\n  backendName: \"webgl\",\n  kernelFunc: cropAndResize$3\n};\n\nclass CumSumProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.customUniforms = [{\n      name: \"index\",\n      type: \"float\"\n    }], this.outputShape = e;\n    var r = e.length,\n        a = t ? \"0.0\" : \"getX(\".concat(getCoords$2(r, \"coords\"), \")\"),\n        s = e[e.length - 1];\n    var o = \"\",\n        i = \"\";\n    t ? (o = n ? \"end != \" + (s - 1) : \"end != 0\", i = n ? \"end + 1\" : \"end - 1\") : (o = n ? \"end + pow2 < \".concat(s) : \"end >= pow2\", i = n ? \"end + pow2\" : \"end - pow2\"), this.userCode = \"\\n      void main() {\\n        \".concat(getCoordsDataType$1(r), \" coords = getOutputCoords();\\n        int end = \").concat(getFinalCoord$1(r, \"coords\"), \";\\n        float val = \").concat(a, \";\\n        int pow2 = int(pow(2.0, index));\\n        if (\").concat(o, \") {\\n          int idx = \").concat(i, \";\\n          \").concat(getFinalCoord$1(r, \"coords\"), \" = idx;\\n          val += getX(\").concat(getCoords$2(r, \"coords\"), \");\\n        }\\n        setOutput(val);\\n      }\\n    \");\n  }\n\n}\n\nfunction getCoords$2(e, t) {\n  if (1 === e) return \"\".concat(t);\n  if (2 === e) return \"\".concat(t, \".x, \").concat(t, \".y\");\n  if (3 === e) return \"\".concat(t, \".x, \").concat(t, \".y, \").concat(t, \".z\");\n  if (4 === e) return \"\".concat(t, \".x, \").concat(t, \".y, \").concat(t, \".z, \").concat(t, \".w\");\n  throw Error(\"Cumulative sum for rank \".concat(e, \" is not yet supported\"));\n}\n\nfunction getFinalCoord$1(e, t) {\n  if (1 === e) return \"\".concat(t);\n  if (2 === e) return \"\".concat(t, \".y\");\n  if (3 === e) return \"\".concat(t, \".z\");\n  if (4 === e) return \"\".concat(t, \".w\");\n  throw Error(\"Cumulative sum for rank \".concat(e, \" is not yet supported\"));\n}\n\nfunction cumsum$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    exclusive: o,\n    reverse: i\n  } = r,\n      l = a.shape.length,\n      u = getAxesPermutation$1([s], l);\n  var c = a;\n  null != u && (c = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }));\n  var p = getInnerMostAxes$1(1, l)[0];\n  if (p !== l - 1) throw new Error(\"WebGL cumsum shader expects an inner-most axis=\".concat(a.shape.length - 1, \" but got axis=\").concat(s));\n  var d = c.shape[p];\n  var h = identity$3({\n    inputs: {\n      x: c\n    },\n    backend: n\n  });\n\n  for (var _e464 = 0; _e464 <= Math.ceil(Math.log2(d)) - 1; _e464++) {\n    var _t333 = new CumSumProgram$1(c.shape, !1, i),\n        _r161 = h;\n\n    h = n.runWebGLProgram(_t333, [h], h.dtype, [[_e464]]), n.disposeIntermediateTensorInfo(_r161);\n  }\n\n  if (o) {\n    var _e465 = new CumSumProgram$1(c.shape, o, i),\n        _t334 = h;\n\n    h = n.runWebGLProgram(_e465, [h], h.dtype), n.disposeIntermediateTensorInfo(_t334);\n  }\n\n  if (null != u) {\n    var _e466 = transpose$3({\n      inputs: {\n        x: h\n      },\n      backend: n,\n      attrs: {\n        perm: getUndoAxesPermutation$1(u)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(c), _e466;\n  }\n\n  return h;\n}\n\nvar cumsumConfig$2 = {\n  kernelName: Cumsum$1,\n  backendName: \"webgl\",\n  kernelFunc: cumsum$3\n};\n\nfunction denseBincount$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o,\n    binaryOutput: i\n  } = r;\n\n  if (1 === a.shape.length) {\n    var _e467 = n.readSync(a.dataId),\n        _t335 = n.readSync(s.dataId),\n        _r162 = bincountImplCPU$1(_e467, _t335, s.dtype, s.shape, o);\n\n    return n.makeTensorInfo([o], s.dtype, _r162);\n  }\n\n  if (2 === a.shape.length) {\n    var _e468 = n.bufferSync(a),\n        _t336 = n.bufferSync(s),\n        _r163 = bincountReduceImplCPU$1(_e468, _t336, o, i);\n\n    return n.makeTensorInfo(_r163.shape, s.dtype, _r163.values);\n  }\n\n  throw new Error(\"Error in denseBincount: input must be at most rank 2, but got rank\".concat(a.shape.length, \".\"));\n}\n\nvar denseBincountConfig$2 = {\n  kernelName: DenseBincount$1,\n  backendName: \"webgl\",\n  kernelFunc: denseBincount$3\n};\n\nclass DepthToSpaceProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.outputShape = [], this.outputShape = e, this.blockSize = t, this.dataFormat = n, this.userCode = \"\\n    void main() {\\n      ivec4 coords = getOutputCoords();\\n      int b = coords[0];\\n      int h = \".concat(this.getHeightCoordString(), \";\\n      int w = \").concat(this.getWidthCoordString(), \";\\n      int d = \").concat(this.getDepthCoordString(), \";\\n\\n      int in_h = h / \").concat(t, \";\\n      int offset_h = imod(h, \").concat(t, \");\\n      int in_w = w / \").concat(t, \";\\n      int offset_w = imod(w, \").concat(t, \");\\n      int offset_d = (offset_h * \").concat(t, \" + offset_w) *\\n        \").concat(this.getOutputDepthSize(), \";\\n      int in_d = d + offset_d;\\n\\n      float result = \").concat(this.getInputSamplingString(), \";\\n      setOutput(result);\\n    }\\n  \");\n  }\n\n  getHeightCoordString() {\n    return \"NHWC\" === this.dataFormat ? \"coords[1]\" : \"coords[2]\";\n  }\n\n  getWidthCoordString() {\n    return \"NHWC\" === this.dataFormat ? \"coords[2]\" : \"coords[3]\";\n  }\n\n  getDepthCoordString() {\n    return \"NHWC\" === this.dataFormat ? \"coords[3]\" : \"coords[1]\";\n  }\n\n  getOutputDepthSize() {\n    return \"NHWC\" === this.dataFormat ? this.outputShape[3] : this.outputShape[1];\n  }\n\n  getInputSamplingString() {\n    return \"NHWC\" === this.dataFormat ? \"getX(b, in_h, in_w, in_d)\" : \"getX(b, in_d, in_h, in_w)\";\n  }\n\n}\n\nfunction depthToSpace$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockSize: s,\n    dataFormat: o\n  } = r;\n  assert$6(s > 1, () => \"blockSize should be > 1 for depthToSpace, but was: \".concat(s));\n  var i = a.shape[0],\n      l = (\"NHWC\" === o ? a.shape[1] : a.shape[2]) * s,\n      u = (\"NHWC\" === o ? a.shape[2] : a.shape[3]) * s,\n      c = (\"NHWC\" === o ? a.shape[3] : a.shape[1]) / (s * s),\n      p = new DepthToSpaceProgram$1(\"NHWC\" === o ? [i, l, u, c] : [i, c, l, u], s, o);\n  return n.runWebGLProgram(p, [a], a.dtype);\n}\n\nvar depthToSpaceConfig$2 = {\n  kernelName: DepthToSpace$1,\n  backendName: \"webgl\",\n  kernelFunc: depthToSpace$3\n};\n\nclass DepthwiseConv2DProgram$1 {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var s = e.inHeight,\n        o = e.inWidth,\n        i = e.padInfo.top,\n        l = e.padInfo.left,\n        u = e.strideHeight,\n        c = e.strideWidth,\n        p = e.dilationHeight,\n        d = e.dilationWidth,\n        h = e.filterHeight,\n        m = e.filterWidth,\n        f = e.outChannels / e.inChannels;\n    var g = \"\",\n        $ = \"\";\n    n && (g = r ? \"float activation(float a) {\\n          float b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(n, \"\\n        }\") : a ? \"float activation(float a) {\\n          float b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(n, \"\\n        }\") : \"\\n          float activation(float x) {\\n            \".concat(n, \"\\n          }\\n        \"), $ = \"result = activation(result);\");\n    var y = t ? \"result += getBiasAtOutCoords();\" : \"\";\n    t && this.variableNames.push(\"bias\"), r && this.variableNames.push(\"preluActivationWeights\"), a && this.variableNames.push(\"leakyreluAlpha\"), this.userCode = \"\\n      \".concat(g, \"\\n\\n      const ivec2 strides = ivec2(\").concat(u, \", \").concat(c, \");\\n      const ivec2 pads = ivec2(\").concat(i, \", \").concat(l, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords.x;\\n        ivec2 xRCCorner = coords.yz * strides - pads;\\n        int d2 = coords.w;\\n        int d1 = d2 / \").concat(f, \";\\n        int q = d2 - d1 * \").concat(f, \";\\n\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        // Convolve x(?, ?, d1) with w(:, :, d1, q) to get y(yR, yC, d2).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        // TO DO(dsmilkov): Flatten the two for loops and vec4 the operations.\\n        for (int wR = 0; wR < \").concat(h, \"; wR++) {\\n          int xR = xRCorner + wR * \").concat(p, \";\\n\\n          if (xR < 0 || xR >= \").concat(s, \") {\\n            continue;\\n          }\\n\\n          for (int wC = 0; wC < \").concat(m, \"; wC++) {\\n            int xC = xCCorner + wC * \").concat(d, \";\\n\\n            if (xC < 0 || xC >= \").concat(o, \") {\\n              continue;\\n            }\\n\\n            float xVal = getX(batch, xR, xC, d1);\\n            float wVal = getW(wR, wC, d1, q);\\n            dotProd += xVal * wVal;\\n          }\\n        }\\n\\n        float result = dotProd;\\n        \").concat(y, \"\\n        \").concat($, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nclass DepthwiseConvPacked2DProgram$1 {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    this.variableNames = [\"x\", \"W\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e.outShape;\n    var s = e.outChannels / e.inChannels,\n        o = e.inHeight,\n        i = e.inWidth,\n        l = e.padInfo.top,\n        u = e.padInfo.left,\n        c = e.strideHeight,\n        p = e.strideWidth,\n        d = e.dilationHeight,\n        h = e.dilationWidth,\n        m = e.filterHeight,\n        f = e.filterWidth,\n        g = f;\n    var $ = \"\\n      int xR; int xC; int xCOffset;\\n      vec4 wTexel; vec4 previous; vec4 final;\";\n\n    for (var _e469 = 0; _e469 < f; _e469++) {\n      $ += \"\\n          vec4 xTexelC\".concat(2 * _e469, \";\\n          int xTexelC\").concat(2 * _e469, \"Ready;\\n          vec4 xTexelC\").concat(2 * _e469 + 1, \";\\n          int xTexelC\").concat(2 * _e469 + 1, \"Ready;\\n          vec4 xC\").concat(_e469, \";\");\n    }\n\n    for (var _e470 = 0; _e470 < m; _e470++) {\n      for (var _e471 = 0; _e471 < f; _e471++) {\n        $ += \"\\n          xTexelC\".concat(2 * _e471, \" = vec4(0.0);\\n          xTexelC\").concat(2 * _e471, \"Ready = 0;\\n          xTexelC\").concat(2 * _e471 + 1, \" = vec4(0.0);\\n          xTexelC\").concat(2 * _e471 + 1, \"Ready = 0;\\n          xC\").concat(_e471, \" = vec4(0.0);\");\n      }\n\n      $ += \"\\n        xR = xRCorner + \".concat(_e470 * d, \";\\n        if (xR >=0 && xR < \").concat(o, \") {\\n      \");\n\n      for (var _t337 = 0; _t337 < (g + 1) / 2; _t337++) {\n        var _n213 = 2 * _t337,\n            _r164 = _n213 * h;\n\n        if ($ += \"\\n          xC = xCCorner + \".concat(_r164, \";\\n          \"), 1 === p) {\n          if (_n213 < f && (u % 2 == 1 ? ($ += \"\\n                xCOffset = xC + 1;\\n                if (xCOffset >= 0 && xCOffset < \".concat(i, \" && xTexelC\").concat(_n213, \"Ready == 0) {\\n                  xTexelC\").concat(_n213, \" = getX(batch, xR, xCOffset, d1);\\n\\n                  // Need to manually clear unused channels in case\\n                  // we're reading from recycled texture.\\n                  if (xCOffset + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n213, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n213, \"Ready = 1;\\n                }\\n              \"), $ += 1 === h && _r164 > 0 ? \"\\n                xC\".concat(_n213, \" = vec4(xTexelC\").concat(_n213 - 2, \".zw, xTexelC\").concat(_n213, \".xy);\\n                \") : \"\\n                  xCOffset = xC + 1 - 2;\\n\\n                  if (xCOffset >= 0 && xCOffset < \".concat(i, \") {\\n                    previous = getX(batch, xR, xCOffset, d1);\\n\\n                    // Need to manually clear unused channels in case\\n                    // we're reading from recycled texture.\\n                    if (xCOffset + 1 >= \").concat(i, \") {\\n                      previous.zw = vec2(0.0);\\n                    }\\n\\n                    xC\").concat(_n213, \" = vec4(previous.zw, xTexelC\").concat(_n213, \".xy);\\n                  } else {\\n                    xC\").concat(_n213, \" = vec4(0.0, 0.0, xTexelC\").concat(_n213, \".xy);\\n                  }\\n                  \")) : $ += \"\\n                if (xC >= 0 && xC < \".concat(i, \" && xTexelC\").concat(_n213, \"Ready == 0) {\\n                  xTexelC\").concat(_n213, \" = getX(batch, xR, xC, d1);\\n                  if (xC + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n213, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n213, \"Ready = 1;\\n                }\\n\\n                xC\").concat(_n213, \" = xTexelC\").concat(_n213, \";\\n                \"), _r164 + 1 < f)) {\n            var _e472 = u % 2 == 0 ? nearestLargerEven$1(h) : h;\n\n            h % 2 == 0 && u % 2 == 1 || h % 2 != 0 && u % 2 != 1 ? ($ += \"\\n                  xCOffset = xC + \".concat(u % 2, \" + \").concat(_e472, \";\\n\\n                  if (xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n213 + 1, \"Ready == 0) {\\n                    xTexelC\").concat(_n213 + 1, \" = getX(batch, xR, xCOffset, d1);\\n\\n                    // Need to manually clear unused channels in case\\n                    // we're reading from recycled texture.\\n                    if (xCOffset + 1 >= \").concat(i, \") {\\n                      xTexelC\").concat(_n213 + 1, \".zw = vec2(0.0);\\n                    }\\n                    xTexelC\").concat(_n213 + 1, \"Ready = 1;\\n                  }\\n                  \"), h > 1 && ($ += \"\\n                    xCOffset -= 2;\\n                    if (xCOffset >= 0 && xCOffset < \".concat(i, \" && xTexelC\").concat(_n213, \"Ready == 0) {\\n                      xTexelC\").concat(_n213, \" = getX(batch, xR, xCOffset, d1);\\n                      xTexelC\").concat(_n213, \"Ready = 1;\\n                    }\\n                    \")), $ += \"\\n                  xC\".concat(_n213 + 1, \" = vec4(xTexelC\").concat(_n213, \".zw, xTexelC\").concat(_n213 + 1, \".xy);\\n                  \")) : $ += 1 === _e472 ? \"\\n                    xC\".concat(_n213 + 1, \" = xTexelC\").concat(_n213, \";\\n                    \") : \"\\n                    xCOffset = xC + \".concat(_e472, \";\\n\\n                    if (xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n213 + 1, \"Ready == 0) {\\n                      xTexelC\").concat(_n213 + 1, \" = getX(batch, xR, xCOffset, d1);\\n                      if (xCOffset + 1 >= \").concat(i, \") {\\n                        xTexelC\").concat(_n213 + 1, \".zw = vec2(0.0);\\n                      }\\n                      xTexelC\").concat(_n213 + 1, \"Ready = 1;\\n                    }\\n\\n                    xC\").concat(_n213 + 1, \" = xTexelC\").concat(_n213 + 1, \";\\n                    \");\n          }\n        } else _r164 < f && (u % 2 == 1 ? ($ += \"\\n                xCOffset = xC + 1 - \".concat(p, \";\\n                if(xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n213, \"Ready == 0) {\\n                  xTexelC\").concat(_n213, \" = getX(batch, xR, xCOffset, d1);\\n                  // Need to manually clear unused channels in case\\n                  // we're reading from recycled texture.\\n                  if (xCOffset + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n213, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n213, \"Ready = 1;\\n                }\\n\\n                if(xC + 1 >= 0 && xC + 1 < \").concat(i, \" && xTexelC\").concat(_n213 + 1, \"Ready == 0) {\\n                  xTexelC\").concat(_n213 + 1, \" = getX(batch, xR, xC + 1, d1);\\n                  // Need to manually clear unused channels in case\\n                  // we're reading from recycled texture.\\n                  if (xC + 2 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n213 + 1, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n213 + 1, \"Ready = 1;\\n                }\\n\\n                xC\").concat(_n213, \" = vec4(xTexelC\").concat(_n213, \".zw, xTexelC\").concat(_n213 + 1, \".zw);\\n              \"), _r164 + 1 < f && ($ += \"\\n                  final = vec4(0.0);\\n                  xCOffset = xC + 1 + \".concat(p, \";\\n                  if(xCOffset >= 0 && xCOffset < \").concat(i, \") {\\n                    final = getX(batch, xR, xCOffset, d1);\\n                  }\\n                  xC\").concat(_n213 + 1, \" = vec4(xTexelC\").concat(_n213 + 1, \".xy, final.xy);\\n                \"))) : ($ += \"\\n                if(xC >= 0 && xC < \".concat(i, \" && xTexelC\").concat(_n213, \"Ready == 0) {\\n                  xTexelC\").concat(_n213, \" = getX(batch, xR, xC, d1);\\n                  if (xC + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n213, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n213, \"Ready = 1;\\n                }\\n\\n                xCOffset = xC + \").concat(p, \";\\n                if(xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n213 + 1, \"Ready == 0) {\\n                  xTexelC\").concat(_n213 + 1, \" = getX(batch, xR, xCOffset, d1);\\n                  if (xCOffset + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n213 + 1, \".zw = vec2(0.);\\n                  }\\n                  xTexelC\").concat(_n213 + 1, \"Ready = 1;\\n                }\\n\\n                xC\").concat(_n213, \" = vec4(\\n                  xTexelC\").concat(_n213, \".xy, xTexelC\").concat(_n213 + 1, \".xy);\\n              \"), _r164 + 1 < f && ($ += \"\\n                  xC\".concat(_n213 + 1, \" = vec4(xTexelC\").concat(_n213, \".zw, xTexelC\").concat(_n213 + 1, \".zw);\\n                \"))));\n\n        _n213 < f && ($ += \"\\n            wTexel = getW(\".concat(_e470, \", \").concat(_r164, \", d1, q);\\n            dotProd += xC\").concat(_n213, \" * vec4(wTexel.xz, wTexel.xz);\\n          \"), _r164 + 1 < f && ($ += \"\\n              wTexel = getW(\".concat(_e470, \", \").concat(_r164 + 1, \", d1, q);\\n              dotProd += xC\").concat(_n213 + 1, \" * vec4(wTexel.xz, wTexel.xz);\\n            \")));\n      }\n\n      $ += \"\\n        }\\n      \";\n    }\n\n    var y = \"\",\n        b = \"\";\n    n && (y = r ? \"vec4 activation(vec4 a) {\\n          vec4 b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(n, \"\\n        }\") : a ? \"vec4 activation(vec4 a) {\\n          vec4 b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(n, \"\\n        }\") : \"vec4 activation(vec4 x) {\\n          \".concat(n, \"\\n        }\"), b = \"result = activation(result);\");\n    var x = t ? \"result += getBiasAtOutCoords();\" : \"\";\n    t && this.variableNames.push(\"bias\"), r && this.variableNames.push(\"preluActivationWeights\"), a && this.variableNames.push(\"leakyreluAlpha\"), this.userCode = \"\\n      \".concat(y, \"\\n\\n      const ivec2 strides = ivec2(\").concat(c, \", \").concat(p, \");\\n      const ivec2 pads = ivec2(\").concat(l, \", \").concat(u, \");\\n\\n      void main() {\\n\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords.x;\\n        ivec2 xRCCorner = coords.yz * strides - pads;\\n        int d2 = coords.w;\\n        int d1 = d2 / \").concat(s, \";\\n        int q = d2 - d1 * \").concat(s, \";\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        //intialize dotProd with a small epsilon seems to reduce GPU accuracy loss.\\n        vec4 dotProd = vec4(0.000000000000001);\\n\\n        \").concat($, \"\\n\\n        vec4 result = dotProd - vec4(0.000000000000001);\\n        \").concat(x, \"\\n        \").concat(b, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction depthwiseConv2dNative$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l,\n    dimRoundingMode: u\n  } = r;\n  var c = l;\n  null == c && (c = [1, 1]), assert$6(eitherStridesOrDilationsAreOne$1(o, c), () => \"Error in depthwiseConv2d: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '\").concat(c, \"'\"));\n  var p = computeConv2DInfo$1(a.shape, s.shape, o, c, i, u, !0);\n  var d;\n  return d = env$1().getBool(\"WEBGL_PACK_DEPTHWISECONV\") && p.strideWidth <= 2 && p.outChannels / p.inChannels == 1 ? new DepthwiseConvPacked2DProgram$1(p) : new DepthwiseConv2DProgram$1(p), n.runWebGLProgram(d, [a, s], \"float32\");\n}\n\nvar depthwiseConv2dNativeConfig$2 = {\n  kernelName: DepthwiseConv2dNative$1,\n  backendName: \"webgl\",\n  kernelFunc: depthwiseConv2dNative$2\n};\n\nclass DepthwiseConv2DDerFilterProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"x\", \"dy\"], this.outputShape = e.filterShape, this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int wR = coords.x;\\n        int wC = coords.y;\\n        int d1 = coords.z;\\n        int dm = coords.w;\\n        int d2 = d1 * \".concat(e.outChannels / e.inChannels, \" + dm;\\n\\n        float dotProd = 0.0;\\n\\n        // TO DO: Vec4 over the batch size\\n        for (int b = 0; b < \").concat(e.batchSize, \"; b++) {\\n          for (int yR = 0; yR < \").concat(e.outHeight, \"; yR++) {\\n            int xR = wR + yR * \").concat(e.strideHeight, \" - \").concat(e.padInfo.top, \";\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int yC = 0; yC < \").concat(e.outWidth, \"; yC++) {\\n              int xC = wC + yC * \").concat(e.strideWidth, \" - \").concat(e.padInfo.left, \";\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              float dyValue = getDy(b, yR, yC, d2);\\n              float xValue = getX(b, xR, xC, d1);\\n              dotProd += (xValue * dyValue);\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass DepthwiseConv2DDerInputProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"W\"], this.outputShape = e.inShape;\n    var t = e.filterHeight,\n        n = e.filterWidth,\n        r = e.outChannels / e.inChannels;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d1 = coords[3];\\n        ivec2 dyCorner = coords.yz - pads;\\n        int dyRCorner = dyCorner.x;\\n        int dyCCorner = dyCorner.y;\\n\\n        float dotProd = 0.0;\\n\\n        for (int wR = 0; wR < \").concat(t, \"; wR++) {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          int wRPerm = \").concat(t, \" - 1 - wR;\\n\\n          for (int wC = 0; wC < \").concat(n, \"; wC++) {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            int wCPerm = \").concat(n, \" - 1 - wC;\\n\\n            // TO DO: Vec4 over the channelMul\\n            for (int dm = 0; dm < \").concat(r, \"; dm++) {\\n              int d2 = d1 * \").concat(r, \" + dm;\\n              float xValue = getDy(batch, idyR, idyC, d2);\\n              float wValue = getW(wRPerm, wCPerm, d1, dm);\\n              dotProd += xValue * wValue;\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction depthwiseConv2dNativeBackpropFilter$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r,\n      p = computeConv2DInfo$1(a.shape, c, o, i, l, u, !0),\n      d = new DepthwiseConv2DDerFilterProgram$1(p);\n  return n.runWebGLProgram(d, [a, s], \"float32\");\n}\n\nvar depthwiseConv2dNativeBackpropFilterConfig$2 = {\n  kernelName: DepthwiseConv2dNativeBackpropFilter$1,\n  backendName: \"webgl\",\n  kernelFunc: depthwiseConv2dNativeBackpropFilter$3\n};\n\nfunction depthwiseConv2dNativeBackpropInput$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    inputShape: c\n  } = r,\n      p = computeConv2DInfo$1(c, s.shape, o, i, l, u, !0),\n      d = new DepthwiseConv2DDerInputProgram$1(p);\n  return n.runWebGLProgram(d, [a, s], \"float32\");\n}\n\nvar depthwiseConv2dNativeBackpropInputConfig$2 = {\n  kernelName: DepthwiseConv2dNativeBackpropInput$1,\n  backendName: \"webgl\",\n  kernelFunc: depthwiseConv2dNativeBackpropInput$3\n};\n\nclass DiagProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"X\"], this.outputShape = [e, e], this.userCode = \"\\n      void main() {\\n          ivec2 coords = getOutputCoords();\\n          float val = coords[0] == coords[1] ? getX(coords[0]) : 0.0;\\n          setOutput(val);\\n      }\\n    \";\n  }\n\n}\n\nfunction diag$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t,\n      a = [...r.shape, ...r.shape],\n      s = sizeFromShape$1(r.shape),\n      o = reshape$4({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [s]\n    }\n  }),\n      i = new DiagProgram$1(s),\n      l = n.runWebGLProgram(i, [o], o.dtype),\n      u = reshape$4({\n    inputs: {\n      x: l\n    },\n    backend: n,\n    attrs: {\n      shape: a\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(l), u;\n}\n\nvar diagConfig$2 = {\n  kernelName: Diag$1,\n  backendName: \"webgl\",\n  kernelFunc: diag$3\n};\n\nclass Dilation2DProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var {\n      inHeight: t,\n      inWidth: n,\n      padInfo: r,\n      strideHeight: a,\n      strideWidth: s,\n      filterHeight: o,\n      filterWidth: i,\n      dilationHeight: l,\n      dilationWidth: u\n    } = e,\n        {\n      top: c,\n      left: p\n    } = r;\n    this.userCode = \"\\n      const ivec2 strides = ivec2(\".concat(a, \", \").concat(s, \");\\n      const ivec2 pads = ivec2(\").concat(c, \", \").concat(p, \");\\n      const float neg_infinity = -3.4e38;\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int d1 = coords.w;\\n        ivec2 outTopLeftCorner =\\n            coords.yz * strides - pads;\\n        int hBeg = outTopLeftCorner.x;\\n        int wBeg = outTopLeftCorner.y;\\n\\n        float curVal = neg_infinity;\\n        for (int h = 0; h < \").concat(o, \"; h++) {\\n          int hIn = hBeg + h * \").concat(l, \";\\n\\n          if (hIn >= 0 && hIn < \").concat(t, \") {\\n            for (int w = 0; w < \").concat(i, \"; w++) {\\n              int wIn = wBeg + w * \").concat(u, \";\\n\\n              if (wIn >= 0 && wIn < \").concat(n, \") {\\n                float xVal = getX(batch, hIn, wIn, d1);\\n                float wVal = getW(h, w, d1);\\n\\n                float val = xVal + wVal;\\n                if (val > curVal) {\\n                  curVal = val;\\n                }\\n              }\\n            }\\n          }\\n        }\\n\\n        float result = curVal;\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction dilation2D$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l\n  } = r,\n      u = computeDilation2DInfo$1(a.shape, s.shape, o, i, \"NHWC\", l);\n  var c;\n  var p = new Dilation2DProgram$1(u);\n  c = n.runWebGLProgram(p, [a, s], \"float32\");\n  var d = reshape$4({\n    inputs: {\n      x: c\n    },\n    backend: n,\n    attrs: {\n      shape: u.outShape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(c), d;\n}\n\nvar dilation2DConfig$1 = {\n  kernelName: Dilation2D$1,\n  backendName: \"webgl\",\n  kernelFunc: dilation2D$1\n};\n\nfunction einsum$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    equation: a\n  } = r,\n      s = t,\n      {\n    allDims: o,\n    summedDims: i,\n    idDims: l\n  } = decodeEinsumEquation$1(a, s.length);\n  checkEinsumDimSizes$1(o.length, l, s);\n  var {\n    path: u,\n    steps: c\n  } = getEinsumComputePath$1(i, l),\n      p = c.length;\n  var d = null,\n      h = o.length;\n  var m = [];\n\n  for (var _e473 = 0; _e473 < p; ++_e473) {\n    for (var _t338 of c[_e473]) {\n      var {\n        permutationIndices: _e474,\n        expandDims: _r165\n      } = getEinsumPermutation$1(h, l[_t338]);\n\n      var _a114 = void 0;\n\n      isIdentityPermutation$1(_e474) ? _a114 = s[_t338] : (_a114 = transpose$3({\n        inputs: {\n          x: s[_t338]\n        },\n        backend: n,\n        attrs: {\n          perm: _e474\n        }\n      }), m.push(_a114));\n\n      var _o62 = _a114.shape.slice();\n\n      for (var _e475 = 0; _e475 < _r165.length; ++_e475) {\n        _o62.splice(_r165[_e475], 0, 1);\n      }\n\n      arraysEqual$1(_a114.shape, _o62) || (_a114 = reshape$4({\n        inputs: {\n          x: _a114\n        },\n        backend: n,\n        attrs: {\n          shape: _o62\n        }\n      }), m.push(_a114)), null === d ? d = _a114 : (d = multiply$3({\n        inputs: {\n          a: _a114,\n          b: d\n        },\n        backend: n\n      }), m.push(d));\n    }\n\n    _e473 < p - 1 && (u[_e473] >= 0 && (d = sum$4({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        axis: u[_e473] - (o.length - h),\n        keepDims: !1\n      }\n    }), m.push(d)), h--);\n  }\n\n  for (var _e476 of m) {\n    _e476 !== d && n.disposeIntermediateTensorInfo(_e476);\n  }\n\n  return d;\n}\n\nvar einsumConfig$2 = {\n  kernelName: Einsum$1,\n  backendName: \"webgl\",\n  kernelFunc: einsum$3\n},\n    ELU$4 = \"return (x >= 0.0) ? x : (exp(x) - 1.0);\",\n    ELU_PACKED$1 = \"\\n  vec4 result;\\n\\n  result.r = (x.r >= 0.0) ? x.r : (exp(x.r) - 1.0);\\n  result.g = (x.g >= 0.0) ? x.g : (exp(x.g) - 1.0);\\n  result.b = (x.b >= 0.0) ? x.b : (exp(x.b) - 1.0);\\n  result.a = (x.a >= 0.0) ? x.a : (exp(x.a) - 1.0);\\n\\n  return result;\\n\",\n    elu$5 = unaryKernelFunc$2({\n  opSnippet: ELU$4,\n  packedOpSnippet: ELU_PACKED$1\n}),\n    eluConfig$2 = {\n  kernelName: Elu$3,\n  backendName: \"webgl\",\n  kernelFunc: elu$5\n},\n    ELU_DER$1 = \"return (b >= 1.0) ? a : a * (b + 1.0);\",\n    ELU_DER_PACKED$1 = \"\\n  vec4 bGTEZero = vec4(greaterThanEqual(b, vec4(0.)));\\n  return (bGTEZero * a) + ((vec4(1.0) - bGTEZero) * (a * (b + vec4(1.0))));\\n\",\n    eluGrad$2 = e => {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    dy: r,\n    y: a\n  } = t,\n      s = env$1().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram$1(ELU_DER_PACKED$1, r.shape, a.shape) : new BinaryOpProgram$1(ELU_DER$1, r.shape, a.shape);\n  return n.runWebGLProgram(s, [r, a], r.dtype);\n},\n    eluGradConfig$3 = {\n  kernelName: EluGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: eluGrad$2\n},\n    PACKED_EQUAL$1 = \"\\n  return vec4(equal(a, b));\\n\",\n    EQUAL$1 = \"return float(a == b);\",\n    equal$3 = binaryKernelFunc$2({\n  opSnippet: EQUAL$1,\n  packedOpSnippet: PACKED_EQUAL$1,\n  dtype: \"bool\",\n  cpuKernelImpl: equalImplCPU$1\n}),\n    equalConfig$2 = {\n  kernelName: Equal$1,\n  backendName: \"webgl\",\n  kernelFunc: equal$3\n},\n    ERF$1 = \"\\n  // Error function is calculated approximately with elementary function.\\n  // See \\\"Handbook of Mathematical Functions with Formulas,\\n  // Graphs, and Mathematical Tables\\\", Abramowitz and Stegun.\\n  float p = \".concat(ERF_P$1, \";\\n  float a1 = \").concat(ERF_A1$1, \";\\n  float a2 = \").concat(ERF_A2$1, \";\\n  float a3 = \").concat(ERF_A3$1, \";\\n  float a4 = \").concat(ERF_A4$1, \";\\n  float a5 = \").concat(ERF_A5$1, \";\\n\\n  float sign = sign(x);\\n  x = abs(x);\\n  float t = 1.0 / (1.0 + p * x);\\n  return sign * (1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*exp(-x*x));\\n\"),\n    erf$3 = unaryKernelFunc$2({\n  opSnippet: ERF$1\n}),\n    erfConfig$2 = {\n  kernelName: Erf$1,\n  backendName: \"webgl\",\n  kernelFunc: erf$3\n},\n    EXP$1 = \"return exp(x);\",\n    exp$3 = unaryKernelFunc$2({\n  opSnippet: EXP$1,\n  packedOpSnippet: EXP$1,\n  cpuKernelImpl: expImplCPU$1\n}),\n    expConfig$2 = {\n  kernelName: Exp$1,\n  backendName: \"webgl\",\n  kernelFunc: exp$3\n};\n\nfunction expandDims$4(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    dim: a\n  } = n,\n      {\n    input: s\n  } = t,\n      o = s.shape.length,\n      i = s.shape.slice();\n  var l = a;\n  return a < 0 && (assert$6(-(o + 1) <= a, () => \"Axis must be in the interval [\".concat(-(o + 1), \", \").concat(o, \"]\")), l = o + a + 1), i.splice(l, 0, 1), reshape$4({\n    inputs: {\n      x: s\n    },\n    backend: r,\n    attrs: {\n      shape: i\n    }\n  });\n}\n\nvar expandDimsConfig$2 = {\n  kernelName: ExpandDims$1,\n  backendName: \"webgl\",\n  kernelFunc: expandDims$4\n},\n    EXPM1$1 = \"return exp(x) - 1.0;\",\n    expm1$3 = unaryKernelFunc$2({\n  opSnippet: EXPM1$1,\n  packedOpSnippet: EXPM1$1,\n  cpuKernelImpl: expm1ImplCPU$1\n}),\n    expm1Config$2 = {\n  kernelName: Expm1$1,\n  backendName: \"webgl\",\n  kernelFunc: expm1$3\n};\n\nclass FFTProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"real\", \"imag\"];\n    var r = t[1];\n    this.outputShape = t;\n    var a = n ? \"2.0 * \".concat(Math.PI) : \"-2.0 * \".concat(Math.PI),\n        s = n ? \"\".concat(r, \".0\") : \"1.0\";\n    var o;\n    if (\"real\" === e) o = \"return real * expR - imag * expI;\";else {\n      if (\"imag\" !== e) throw new Error(\"FFT component must be either \\\"real\\\" or \\\"imag\\\", got \".concat(e, \".\"));\n      o = \"return real * expI + imag * expR;\";\n    }\n    this.userCode = \"\\n      const float exponentMultiplier = \".concat(a, \";\\n\\n      float unaryOpComplex(float real, float expR, float imag, float expI) {\\n        \").concat(o, \"\\n      }\\n\\n      float mulMatDFT(int batch, int index) {\\n        float indexRatio = float(index) / float(\").concat(r, \");\\n        float exponentMultiplierTimesIndexRatio =\\n            exponentMultiplier * indexRatio;\\n\\n        float result = 0.0;\\n\\n        for (int i = 0; i < \").concat(r, \"; i++) {\\n          // x = (-2|2 * PI / N) * index * i;\\n          float x = exponentMultiplierTimesIndexRatio * float(i);\\n          float expR = cos(x);\\n          float expI = sin(x);\\n          float real = getReal(batch, i);\\n          float imag = getImag(batch, i);\\n\\n          result +=\\n              unaryOpComplex(real, expR, imag, expI) / \").concat(s, \";\\n        }\\n\\n        return result;\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        setOutput(mulMatDFT(coords[0], coords[1]));\\n      }\\n    \");\n  }\n\n}\n\nfunction fftImpl$2(e, t, n) {\n  var r = n.texData.get(e.dataId),\n      a = sizeFromShape$1(e.shape),\n      s = e.shape[e.shape.length - 1],\n      o = reshape$4({\n    inputs: {\n      x: e\n    },\n    backend: n,\n    attrs: {\n      shape: [a / s, s]\n    }\n  }),\n      i = o.shape,\n      l = new FFTProgram$1(\"real\", i, t),\n      u = new FFTProgram$1(\"imag\", i, t),\n      c = [{\n    dataId: r.complexTensorInfos.real.dataId,\n    dtype: r.complexTensorInfos.real.dtype,\n    shape: i\n  }, {\n    dataId: r.complexTensorInfos.imag.dataId,\n    dtype: r.complexTensorInfos.imag.dtype,\n    shape: i\n  }],\n      p = n.runWebGLProgram(l, c, \"float32\"),\n      d = n.runWebGLProgram(u, c, \"float32\"),\n      h = complex$3({\n    inputs: {\n      real: p,\n      imag: d\n    },\n    backend: n\n  });\n  n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d);\n  var m = reshape$4({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      shape: e.shape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(h), m;\n}\n\nfunction fft$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return fftImpl$2(r, !1, n);\n}\n\nvar fftConfig$2 = {\n  kernelName: FFT$1,\n  backendName: \"webgl\",\n  kernelFunc: fft$3\n};\n\nclass FillProgram$1 {\n  constructor(e, t) {\n    this.outputShape = [], this.customUniforms = [{\n      name: \"value\",\n      type: \"float\"\n    }], this.variableNames = [\"x\"], this.outputShape = e, this.userCode = \"\\n      void main() {\\n        // Input can be obtained from uniform value.\\n        setOutput(value);\\n      }\\n    \";\n  }\n\n}\n\nfunction fill$3(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    shape: r,\n    value: a\n  } = n;\n  var {\n    dtype: s\n  } = n;\n\n  if (s = s || inferDtype$1(a), \"string\" === s) {\n    var _e477 = getArrayFromDType$1(s, sizeFromShape$1(r));\n\n    return _e477.fill(a), t.makeTensorInfo(r, s, _e477);\n  }\n\n  {\n    var _e478 = new FillProgram$1(r, a);\n\n    return t.runWebGLProgram(_e478, [], s, [[a]]);\n  }\n}\n\nvar fillConfig$2 = {\n  kernelName: Fill$1,\n  backendName: \"webgl\",\n  kernelFunc: fill$3\n};\n\nclass FlipLeftRightProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"Image\"], this.outputShape = [];\n    var t = e[2];\n    this.outputShape = e, this.userCode = \"\\n        void main() {\\n          ivec4 coords = getOutputCoords();\\n          int x = coords[2];\\n\\n          int coordX = \".concat(t, \" - x - 1;\\n          float outputValue;\\n          if(coordX >= 0 && coordX < \").concat(t, \") {\\n            outputValue = getImage(coords[0], coords[1], coordX, coords[3]);\\n          } else {\\n            outputValue = getImage(coords[0], coords[1], coords[2], coords[3]);\\n          }\\n          setOutput(outputValue);\\n        }\\n    \");\n  }\n\n}\n\nvar flipLeftRightConfig$2 = {\n  kernelName: FlipLeftRight$1,\n  backendName: \"webgl\",\n  kernelFunc: _ref22 => {\n    var {\n      inputs: e,\n      backend: t\n    } = _ref22;\n    var {\n      image: n\n    } = e,\n        r = t,\n        a = new FlipLeftRightProgram$1(n.shape);\n    return r.runWebGLProgram(a, [n], n.dtype);\n  }\n},\n    FLOOR$1 = \"return floor(x);\",\n    floor$3 = unaryKernelFunc$2({\n  opSnippet: FLOOR$1,\n  packedOpSnippet: FLOOR$1,\n  cpuKernelImpl: floorImplCPU$1\n}),\n    floorConfig$2 = {\n  kernelName: Floor$1,\n  backendName: \"webgl\",\n  kernelFunc: floor$3\n},\n    INT_DIV$1 = \"\\n  float s = sign(a) * sign(b);\\n  int ia = round(a);\\n  int ib = round(b);\\n  if (ib != 0) {\\n    // Windows (D3D) wants guaranteed non-zero int division at compile-time.\\n    return float(idiv(ia, ib, s));\\n  } else {\\n    return NAN;\\n  }\\n\",\n    INT_DIV_PACKED$1 = \"\\n  ivec4 ia = round(a);\\n  ivec4 ib = round(b);\\n  bvec4 cond = notEqual(ib, ivec4(0));\\n  ivec4 result = ivec4(0);\\n  vec4 s = sign(a) * sign(b);\\n\\n  // Windows (D3D) wants guaranteed non-zero int division at compile-time.\\n  if (cond[0]) {\\n    result[0] = idiv(ia[0], ib[0], s[0]);\\n  }\\n  if (cond[1]) {\\n    result[1] = idiv(ia[1], ib[1], s[1]);\\n  }\\n  if (cond[2]) {\\n    result[2] = idiv(ia[2], ib[2], s[2]);\\n  }\\n  if (cond[3]) {\\n    result[3] = idiv(ia[3], ib[3], s[3]);\\n  }\\n  return vec4(result);\\n\",\n    floorDiv$3 = binaryKernelFunc$2({\n  opSnippet: INT_DIV$1,\n  packedOpSnippet: INT_DIV_PACKED$1,\n  dtype: \"int32\"\n}),\n    floorDivConfig$2 = {\n  kernelName: FloorDiv$1,\n  backendName: \"webgl\",\n  kernelFunc: floorDiv$3\n};\n\nclass FromPixelsProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"];\n    var t = getGlslDifferences$1(),\n        [n, r] = e;\n    this.outputShape = e, this.userCode = \"\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n        int texR = coords[0];\\n        int texC = coords[1];\\n        int depth = coords[2];\\n        vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\".concat(r, \".0, \").concat(n, \".0);\\n\\n        vec4 values = \").concat(t.texture2D, \"(A, uv);\\n        float value;\\n        if (depth == 0) {\\n          value = values.r;\\n        } else if (depth == 1) {\\n          value = values.g;\\n        } else if (depth == 2) {\\n          value = values.b;\\n        } else if (depth == 3) {\\n          value = values.a;\\n        }\\n\\n        setOutput(floor(value * 255.0 + 0.5));\\n      }\\n    \");\n  }\n\n}\n\nclass FromPixelsPackedProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0;\n    var t = getGlslDifferences$1(),\n        [n, r] = e;\n    this.outputShape = e, this.userCode = \"\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n        int texR = coords[0];\\n        int texC = coords[1];\\n        int depth = coords[2];\\n\\n        vec4 result = vec4(0.);\\n\\n        for(int row=0; row<=1; row++) {\\n          for(int col=0; col<=1; col++) {\\n            texC = coords[1] + row;\\n            depth = coords[2] + col;\\n\\n            vec2 uv = (vec2(texC, texR) + halfCR) /\\n                       vec2(\".concat(r, \".0, \").concat(n, \".0);\\n            vec4 values = \").concat(t.texture2D, \"(A, uv);\\n            float value;\\n            if (depth == 0) {\\n              value = values.r;\\n            } else if (depth == 1) {\\n              value = values.g;\\n            } else if (depth == 2) {\\n              value = values.b;\\n            } else if (depth == 3) {\\n              value = values.a;\\n            }\\n\\n            result[row * 2 + col] = floor(value * 255.0 + 0.5);\\n          }\\n        }\\n\\n        \").concat(t.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nvar fromPixelsConfig$1 = {\n  kernelName: FromPixels$1,\n  backendName: \"webgl\",\n  kernelFunc: fromPixels$2\n};\nvar fromPixels2DContext$2;\n\nfunction fromPixels$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e;\n  var {\n    pixels: a\n  } = t;\n  var {\n    numChannels: s\n  } = r,\n      o = \"undefined\" != typeof HTMLVideoElement && a instanceof HTMLVideoElement,\n      i = \"undefined\" != typeof HTMLImageElement && a instanceof HTMLImageElement,\n      [l, u] = o ? [a.videoWidth, a.videoHeight] : [a.width, a.height],\n      c = [u, l],\n      p = [u, l, s];\n  (i || o) && (null == fromPixels2DContext$2 && (fromPixels2DContext$2 = document.createElement(\"canvas\").getContext(\"2d\")), fromPixels2DContext$2.canvas.width = l, fromPixels2DContext$2.canvas.height = u, fromPixels2DContext$2.drawImage(a, 0, 0, l, u), a = fromPixels2DContext$2.canvas);\n  var d = n.makeTensorInfo(c, \"int32\");\n  n.texData.get(d.dataId).usage = TextureUsage$1.PIXELS, n.gpgpu.uploadPixelDataToTexture(n.getTexture(d.dataId), a);\n  var h = env$1().getBool(\"WEBGL_PACK\") ? new FromPixelsPackedProgram$1(p) : new FromPixelsProgram$1(p),\n      m = n.runWebGLProgram(h, [d], \"int32\");\n  return n.disposeData(d.dataId), m;\n}\n\nfunction fusedConv2d$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dataFormat: c,\n    dilations: p,\n    dimRoundingMode: d,\n    activation: h,\n    leakyreluAlpha: m\n  } = r,\n      f = convertConv2DDataFormat$1(c),\n      g = computeConv2DInfo$1(a.shape, s.shape, l, p, u, d, !1, f);\n  var $;\n  var y = [];\n  if (1 !== g.filterHeight || 1 !== g.filterWidth || 1 !== g.dilationHeight || 1 !== g.dilationWidth || 1 !== g.strideHeight || 1 !== g.strideWidth || \"SAME\" !== g.padInfo.type && \"VALID\" !== g.padInfo.type) {\n    if (env$1().getBool(\"WEBGL_CONV_IM2COL\") && 1 === a.shape[0]) $ = conv2dWithIm2Row$1({\n      x: a,\n      filter: s,\n      convInfo: g,\n      backend: n,\n      bias: o,\n      activation: h,\n      preluActivationWeights: i,\n      leakyreluAlpha: m\n    });else {\n      var _e479 = null != o,\n          _t339 = null != i,\n          _r166 = \"leakyrelu\" === h,\n          _l32 = h ? mapActivationToShaderProgram$1(h, !1) : null,\n          _u29 = new Conv2DProgram$1(g, _e479, _l32, _t339, _r166),\n          _c19 = [a, s];\n\n      if (o && _c19.push(o), i && _c19.push(i), _r166) {\n        var _e480 = n.makeTensorInfo([], \"float32\", createScalarValue$1(m, \"float32\"));\n\n        _c19.push(_e480), y.push(_e480);\n      }\n\n      $ = n.runWebGLProgram(_u29, _c19, \"float32\");\n    }\n  } else $ = conv2dByMatMul$1({\n    x: a,\n    filter: s,\n    convInfo: g,\n    backend: n,\n    bias: o,\n    activation: h,\n    preluActivationWeights: i,\n    leakyreluAlpha: m\n  });\n  var b = reshape$4({\n    inputs: {\n      x: $\n    },\n    backend: n,\n    attrs: {\n      shape: g.outShape\n    }\n  });\n  return y.push($), y.forEach(e => n.disposeIntermediateTensorInfo(e)), b;\n}\n\nvar fusedConv2DConfig$2 = {\n  kernelName: FusedConv2D$1,\n  backendName: \"webgl\",\n  kernelFunc: fusedConv2d$1\n};\n\nfunction fusedDepthwiseConv2D$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dilations: c,\n    dimRoundingMode: p,\n    activation: d,\n    leakyreluAlpha: h\n  } = r,\n      m = [];\n  var f = c;\n  null == f && (f = [1, 1]), assert$6(eitherStridesOrDilationsAreOne$1(l, f), () => \"Error in depthwiseConv2d: Either strides or dilations must be 1. Got strides \".concat(l, \" and dilations '\").concat(f, \"'\"));\n  var g = computeConv2DInfo$1(a.shape, s.shape, l, f, u, p, !0),\n      $ = env$1().getBool(\"WEBGL_PACK_DEPTHWISECONV\") && g.strideWidth <= 2 && g.outChannels / g.inChannels == 1,\n      y = d ? mapActivationToShaderProgram$1(d, $) : null,\n      b = [a, s],\n      x = null != o,\n      v = null != i,\n      I = \"leakyrelu\" === d;\n\n  if (x && b.push(o), v && b.push(i), I) {\n    var _e481 = n.makeTensorInfo([], \"float32\", createScalarValue$1(h, \"float32\"));\n\n    b.push(_e481), m.push(_e481);\n  }\n\n  var C;\n  C = $ ? new DepthwiseConvPacked2DProgram$1(g, x, y, v, I) : new DepthwiseConv2DProgram$1(g, x, y, v, I);\n  var S = n.runWebGLProgram(C, b, \"float32\");\n  return m.forEach(e => n.disposeIntermediateTensorInfo(e)), S;\n}\n\nvar fusedDepthwiseConv2DConfig$2 = {\n  kernelName: FusedDepthwiseConv2D$1,\n  backendName: \"webgl\",\n  kernelFunc: fusedDepthwiseConv2D$2\n};\n\nclass GatherNDProgram$1 {\n  constructor(e, t, n) {\n    this.sliceDim = e, this.strides = t, this.variableNames = [\"x\", \"indices\"], this.outputShape = n;\n    var r = getCoordsDataType$1(t.length),\n        a = getCoordsDataType$1(n.length);\n    this.userCode = \"\\n        \".concat(r, \" strides = \").concat(r, \"(\").concat(this.strides, \");\\n         void main() {\\n          \").concat(a, \" coords = getOutputCoords();\\n          int flattenIndex = 0;\\n          for (int j = 0; j < \").concat(this.sliceDim, \"; j++) {\\n            int index = round(getIndices(coords[0], j));\\n            flattenIndex += index * \").concat(this.sliceDim > 1 ? \"strides[j]\" : \"strides\", \";\\n          }\\n          setOutput(getX(flattenIndex, coords[1]));\\n        }\\n      \");\n  }\n\n}\n\nfunction gatherNd$2(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    params: r,\n    indices: a\n  } = t,\n      s = a.shape,\n      o = s[s.length - 1],\n      i = sizeFromShape$1(r.shape),\n      [l, u, c, p] = prepareAndValidate$1(r, a),\n      d = reshape$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [u, o]\n    }\n  }),\n      h = reshape$4({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [sizeFromShape$1(r.shape) / c, c]\n    }\n  });\n\n  if (n.shouldExecuteOnCPU([r, a]) || \"string\" === r.dtype) {\n    var _e482 = n.readSync(a.dataId),\n        _t340 = n.bufferSync(r),\n        _s86 = gatherNdImplCPU$1(_e482, _t340, r.dtype, u, o, c, p, r.shape, i);\n\n    return n.makeTensorInfo(l, r.dtype, _s86.values);\n  }\n\n  var m = new GatherNDProgram$1(o, p, [u, c]),\n      f = n.runWebGLProgram(m, [h, d], h.dtype),\n      g = reshape$4({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  });\n  return n.disposeIntermediateTensorInfo(d), n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(f), g;\n}\n\nvar gatherNdConfig$2 = {\n  kernelName: GatherNd$1,\n  backendName: \"webgl\",\n  kernelFunc: gatherNd$2\n};\n\nclass GatherProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"A\", \"indices\"], this.outputShape = t, this.rank = t.length;\n    var n = getCoordsDataType$1(this.rank),\n        r = getSourceCoords$4(e);\n    this.userCode = \"\\n      void main() {\\n        \".concat(n, \" resRC = getOutputCoords();\\n        setOutput(getA(\").concat(r, \"));\\n      }\\n    \");\n  }\n\n}\n\nfunction getSourceCoords$4(e, t) {\n  var n = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\"],\n      r = [];\n\n  for (var _t341 = 0; _t341 < e.length; _t341++) {\n    r.push(2 === _t341 ? \"int(getIndices(resRC.x, resRC.z))\" : \"\".concat(n[_t341]));\n  }\n\n  return r.join();\n}\n\nfunction gatherV2$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    indices: s\n  } = t,\n      {\n    axis: o,\n    batchDims: i\n  } = r,\n      l = collectGatherOpShapeInfo$1(a, s, parseAxisParam$1(o, a.shape)[0], i),\n      u = sizeFromShape$1(s.shape),\n      c = [],\n      p = reshape$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [l.batchSize, l.outerSize, l.dimSize, l.sliceSize]\n    }\n  }),\n      d = reshape$4({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: [l.batchSize, u / l.batchSize]\n    }\n  });\n  c.push(p), c.push(d);\n  var h = [l.batchSize, l.outerSize, u / l.batchSize, l.sliceSize];\n\n  if (n.shouldExecuteOnCPU([a, s]) || \"string\" === a.dtype) {\n    var _e483 = n.bufferSync(d),\n        _t342 = n.bufferSync(p),\n        _r167 = gatherV2ImplCPU$1(_t342, _e483, h);\n\n    return c.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo(l.outputShape, _r167.dtype, _r167.values);\n  }\n\n  var m = new GatherProgram$1(p.shape, h),\n      f = n.runWebGLProgram(m, [p, d], p.dtype);\n  c.push(f);\n  var g = reshape$4({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: l.outputShape\n    }\n  });\n  return c.forEach(e => n.disposeIntermediateTensorInfo(e)), g;\n}\n\nvar gatherV2Config$2 = {\n  kernelName: GatherV2$1,\n  backendName: \"webgl\",\n  kernelFunc: gatherV2$2\n},\n    GREATER$1 = \"return float(a > b);\",\n    GREATER_PACKED$1 = \"\\n  return vec4(greaterThan(a, b));\\n\",\n    greater$4 = binaryKernelFunc$2({\n  opSnippet: GREATER$1,\n  packedOpSnippet: GREATER_PACKED$1,\n  cpuKernelImpl: greaterImplCPU$1,\n  dtype: \"bool\"\n}),\n    greaterConfig$2 = {\n  kernelName: Greater$1,\n  backendName: \"webgl\",\n  kernelFunc: greater$4\n},\n    GREATER_EQUAL$1 = \"return float(a >= b);\",\n    GREATER_EQUAL_PACKED$1 = \"\\n  return vec4(greaterThanEqual(a, b));\\n\",\n    greaterEqual$3 = binaryKernelFunc$2({\n  opSnippet: GREATER_EQUAL$1,\n  packedOpSnippet: GREATER_EQUAL_PACKED$1,\n  dtype: \"bool\",\n  cpuKernelImpl: greaterEqualImplCPU$1\n}),\n    greaterEqualConfig$2 = {\n  kernelName: GreaterEqual$1,\n  backendName: \"webgl\",\n  kernelFunc: greaterEqual$3\n};\n\nfunction ifft$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return fftImpl$2(r, !0, n);\n}\n\nvar ifftConfig$2 = {\n  kernelName: IFFT$1,\n  backendName: \"webgl\",\n  kernelFunc: ifft$3\n},\n    IS_FINITE$1 = \"return float(!isnan(x) && !isinf(x));\",\n    isFinite$4 = unaryKernelFunc$2({\n  opSnippet: IS_FINITE$1,\n  dtype: \"bool\"\n}),\n    isFiniteConfig$2 = {\n  kernelName: IsFinite$1,\n  backendName: \"webgl\",\n  kernelFunc: isFinite$4\n},\n    IS_INF$1 = \"return float(isinf(x));\",\n    isInf$3 = unaryKernelFunc$2({\n  opSnippet: IS_INF$1,\n  dtype: \"bool\"\n}),\n    isInfConfig$2 = {\n  kernelName: IsInf$1,\n  backendName: \"webgl\",\n  kernelFunc: isInf$3\n},\n    IS_NAN$1 = \"return float(isnan(x));\",\n    isNaN$4 = unaryKernelFunc$2({\n  opSnippet: IS_NAN$1,\n  dtype: \"bool\"\n}),\n    isNaNConfig$2 = {\n  kernelName: IsNan$1,\n  backendName: \"webgl\",\n  kernelFunc: isNaN$4\n},\n    LESS$1 = \"return float(a < b);\",\n    LESS_PACKED$1 = \"\\n  return vec4(lessThan(a, b));\\n\",\n    less$4 = binaryKernelFunc$2({\n  opSnippet: LESS$1,\n  packedOpSnippet: LESS_PACKED$1,\n  cpuKernelImpl: lessImplCPU$1,\n  dtype: \"bool\"\n}),\n    lessConfig$2 = {\n  kernelName: Less$1,\n  backendName: \"webgl\",\n  kernelFunc: less$4\n},\n    LESS_EQUAL$1 = \"return float(a <= b);\",\n    LESS_EQUAL_PACKED$1 = \"\\n  return vec4(lessThanEqual(a, b));\\n\",\n    lessEqual$3 = binaryKernelFunc$2({\n  opSnippet: LESS_EQUAL$1,\n  packedOpSnippet: LESS_EQUAL_PACKED$1,\n  cpuKernelImpl: lessEqualImplCPU$1,\n  dtype: \"bool\"\n}),\n    lessEqualConfig$2 = {\n  kernelName: LessEqual$1,\n  backendName: \"webgl\",\n  kernelFunc: lessEqual$3\n};\n\nfunction linSpace$2(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    num: s\n  } = n,\n      o = linSpaceImplCPU$1(r, a, s);\n  return t.makeTensorInfo([o.length], \"float32\", o);\n}\n\nvar linSpaceConfig$2 = {\n  kernelName: LinSpace$1,\n  backendName: \"webgl\",\n  kernelFunc: linSpace$2\n},\n    LOG$1 = \"if (x < 0.0) return NAN;\\n  return log(x);\",\n    LOG_PACKED$1 = \"\\n  vec4 result = log(x);\\n  vec4 isNaN = vec4(lessThan(x, vec4(0.0)));\\n  result.r = isNaN.r == 1.0 ? NAN : result.r;\\n  result.g = isNaN.g == 1.0 ? NAN : result.g;\\n  result.b = isNaN.b == 1.0 ? NAN : result.b;\\n  result.a = isNaN.a == 1.0 ? NAN : result.a;\\n\\n  return result;\\n\",\n    log$4 = unaryKernelFunc$2({\n  opSnippet: LOG$1,\n  packedOpSnippet: LOG_PACKED$1,\n  cpuKernelImpl: logImplCPU$1\n}),\n    logConfig$2 = {\n  kernelName: Log$1,\n  backendName: \"webgl\",\n  kernelFunc: log$4\n},\n    LOG1P$1 = \"return log(1.0 + x);\",\n    log1p$3 = unaryKernelFunc$2({\n  opSnippet: LOG1P$1\n}),\n    log1pConfig$2 = {\n  kernelName: Log1p$1,\n  backendName: \"webgl\",\n  kernelFunc: log1p$3\n},\n    LOGICAL_AND$1 = \"return float(a >= 1.0 && b >= 1.0);\",\n    LOGICAL_AND_PACKED$1 = \"\\n  return vec4(\\n    vec4(greaterThanEqual(a, vec4(1.0))) *\\n    vec4(greaterThanEqual(b, vec4(1.0))));\\n\",\n    logicalAnd$3 = binaryKernelFunc$2({\n  opSnippet: LOGICAL_AND$1,\n  packedOpSnippet: LOGICAL_AND_PACKED$1,\n  dtype: \"bool\"\n}),\n    logicalAndConfig$2 = {\n  kernelName: LogicalAnd$1,\n  backendName: \"webgl\",\n  kernelFunc: logicalAnd$3\n},\n    LOGICAL_NOT$1 = \"return float(!(x >= 1.0));\",\n    logicalNot$3 = unaryKernelFunc$2({\n  opSnippet: LOGICAL_NOT$1\n}),\n    logicalNotConfig$2 = {\n  kernelName: LogicalNot$1,\n  backendName: \"webgl\",\n  kernelFunc: logicalNot$3\n},\n    LOGICAL_OR$1 = \"return float(a >= 1.0 || b >= 1.0);\",\n    LOGICAL_OR_PACKED$1 = \"\\n  return min(\\n    vec4(greaterThanEqual(a, vec4(1.0))) +\\n    vec4(greaterThanEqual(b, vec4(1.0))),\\n    vec4(1.0));\\n\",\n    logicalOr$3 = binaryKernelFunc$2({\n  opSnippet: LOGICAL_OR$1,\n  packedOpSnippet: LOGICAL_OR_PACKED$1,\n  dtype: \"bool\"\n}),\n    logicalOrConfig$2 = {\n  kernelName: LogicalOr$1,\n  backendName: \"webgl\",\n  kernelFunc: logicalOr$3\n};\n\nclass LRNProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"x\"], this.outputShape = [];\n    var s = t,\n        o = e[3] - 1;\n    var i;\n    this.outputShape = e;\n    var l = \"float(\".concat(n, \") + float(\").concat(r, \") * sum\");\n    i = .5 === a ? \"inversesqrt(\".concat(l, \")\") : 1 === a ? \"1.0/(\".concat(l, \")\") : \"exp(log(\".concat(l, \") * float(-\").concat(a, \"));\"), this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int r = coords[1];\\n        int c = coords[2];\\n        int d = coords[3];\\n        float x = getX(b, r, c, d);\\n        float sum = 0.0;\\n        for (int j = -\".concat(s, \"; j <= \").concat(s, \"; j++) {\\n          int idx = d + j;\\n          if (idx >= 0 && idx <=  \").concat(o, \") {\\n            float z = getX(b, r, c, idx);\\n            sum += z * z;\\n          }\\n        }\\n        float val = x * \").concat(i, \";\\n        setOutput(val);\\n      }\\n    \");\n  }\n\n}\n\nclass LRNPackedProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"x\"], this.outputShape = [], this.packedInputs = !0, this.packedOutput = !0;\n    var s = t,\n        o = e[3] - 1;\n    var i;\n    this.outputShape = e;\n    var l = \"float(\".concat(n, \") + float(\").concat(r, \") * sum\");\n    i = .5 === a ? \"inversesqrt(\".concat(l, \")\") : 1 === a ? \"1.0/(\".concat(l, \")\") : \"exp(log(\".concat(l, \") * float(-\").concat(a, \"));\"), this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords.x;\\n        int r = coords.y;\\n        int c = coords.z;\\n        int d = coords.w;\\n\\n        bool hasNextCol = d < \".concat(this.outputShape[3], \";\\n        bool hasNextRow = c < \").concat(this.outputShape[2], \";\\n\\n        vec4 sum = vec4(0.);\\n        vec4 xFragAtOutputCoords = getX(b, r, c, d);\\n\\n        vec4 xAtOutputCoords = vec4(\\n          getChannel(xFragAtOutputCoords, vec2(c, d)),\\n          hasNextCol ?\\n            getChannel(xFragAtOutputCoords, vec2(c, d + 1)) : 0.0,\\n          hasNextRow ?\\n            getChannel(xFragAtOutputCoords , vec2(c + 1, d)) : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getChannel(xFragAtOutputCoords, vec2(c + 1, d + 1)) : 0.0\\n        );\\n\\n        int firstChannel = d - \").concat(s, \";\\n        vec2 cache = vec2(0.);\\n        if(firstChannel >= 0){\\n          vec4 firstChannelFrag = getX(b, r, c, firstChannel);\\n          cache.x = getChannel(firstChannelFrag, vec2(c, firstChannel));\\n            if(hasNextRow){\\n              cache.y = getChannel(firstChannelFrag, vec2(c + 1, firstChannel));\\n            }\\n        }\\n\\n        ivec2 depth = ivec2(d, d + 1);\\n        for (int j = - \").concat(s, \"; j <= \").concat(s, \"; j++) {\\n          ivec2 idx = depth + j;\\n          bvec2 aboveLowerBound = greaterThanEqual(idx, ivec2(0));\\n          bvec2 belowUpperBound = lessThanEqual(idx, ivec2(\").concat(o, \"));\\n\\n          bool depthInRange = aboveLowerBound.x && belowUpperBound.x;\\n          bool depthPlusOneInRange = aboveLowerBound.y && belowUpperBound.y;\\n\\n          if(depthInRange || depthPlusOneInRange){\\n            vec4 z = vec4(0.);\\n            vec4 xFragAtCurrentDepth;\\n            z.xz = cache.xy;\\n            if(depthPlusOneInRange && hasNextCol){\\n              xFragAtCurrentDepth = idx.y != d ?\\n                getX(b, r, c, idx.y) : xFragAtOutputCoords;\\n              z.y = getChannel(xFragAtCurrentDepth, vec2(c, idx.y));\\n              if(hasNextRow){\\n                z.w = getChannel(xFragAtCurrentDepth, vec2(c + 1, idx.y));\\n              }\\n            }\\n            cache.xy = z.yw;\\n            sum += z * z;\\n          }\\n        }\\n        vec4 result = xAtOutputCoords * \").concat(i, \";\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar lrn$1 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    depthRadius: s,\n    bias: o,\n    alpha: i,\n    beta: l\n  } = r,\n      u = env$1().getBool(\"WEBGL_PACK_NORMALIZATION\") ? new LRNPackedProgram$1(a.shape, s, o, i, l) : new LRNProgram$1(a.shape, s, o, i, l);\n  return n.runWebGLProgram(u, [a], a.dtype);\n},\n    LRNConfig$1 = {\n  kernelName: LRN$1,\n  backendName: \"webgl\",\n  kernelFunc: lrn$1\n};\n\nclass LRNGradProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"inputImage\", \"outputImage\", \"dy\"], this.outputShape = [], this.outputShape = e, this.depth = e[3], this.depthRadius = t, this.bias = n, this.alpha = r, this.beta = a, this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int r = coords[1];\\n        int c = coords[2];\\n\\n        float result = 0.0;\\n        for (int d = 0; d < \".concat(this.depth, \"; ++d) {\\n          int depthBegin = int(max(0.0, float(d - \").concat(t, \")));\\n          int depthEnd = int(min(float(\").concat(this.depth, \"),\\n              float(d + \").concat(t, \" + 1)));\\n\\n          const int MIN_DEPTH_BEGIN = 0;\\n          const int MAX_DEPTH_END = \").concat(this.depth, \";\\n\\n          float norm = 0.0;\\n          for (int k = MIN_DEPTH_BEGIN; k < MAX_DEPTH_END; ++k) {\\n            if (k < depthBegin){\\n              continue;\\n            }\\n            else if (k >= depthBegin && k < depthEnd) {\\n              norm += getInputImage(b, r, c, k) * getInputImage(b, r, c, k);\\n            }\\n            else {\\n              break;\\n            }\\n          }\\n\\n          norm = float(\").concat(r, \") * norm + float(\").concat(n, \");\\n\\n          for(int k = MIN_DEPTH_BEGIN; k < MAX_DEPTH_END; ++k){\\n            if (k < depthBegin){\\n              continue;\\n            }\\n            else if (k >= depthBegin && k < depthEnd){\\n              float dyi = -2.0 * float(\").concat(r, \")\\n                * float(\").concat(a, \")\\n                * getInputImage(b ,r ,c, k) * getOutputImage(b, r, c, d)\\n                / norm;\\n              if (k == d) {\\n                dyi += pow(norm, -1.0 * \").concat(a, \");\\n              }\\n              if (k == coords[3]) {\\n                dyi *= getDy(b, r, c, d);\\n                result += dyi;\\n              }\\n            }\\n            else {\\n              break;\\n            }\\n          }\\n      }\\n      setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar lrnGrad$1 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    y: s,\n    dy: o\n  } = t,\n      {\n    depthRadius: i,\n    bias: l,\n    alpha: u,\n    beta: c\n  } = r,\n      p = new LRNGradProgram$1(a.shape, i, l, u, c);\n  return n.runWebGLProgram(p, [a, s, o], a.dtype);\n},\n    LRNGradConfig$1 = {\n  kernelName: LRNGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: lrnGrad$1\n};\n\nfunction maxImpl$2(e, t, n, r) {\n  var a = sizeFromShape$1(t),\n      s = reshape$4({\n    inputs: {\n      x: e\n    },\n    attrs: {\n      shape: [sizeFromShape$1(e.shape) / a, a]\n    },\n    backend: r\n  }),\n      o = reduce$1(s, e.dtype, \"max\", r),\n      i = reshape$4({\n    inputs: {\n      x: o\n    },\n    attrs: {\n      shape: n\n    },\n    backend: r\n  });\n  return r.disposeIntermediateTensorInfo(s), r.disposeIntermediateTensorInfo(o), i;\n}\n\nfunction max$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reductionIndices: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam$1(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation$1(u, i),\n      p = null != c,\n      d = n.shouldExecuteOnCPU([a]);\n  var h = a;\n\n  if (p) {\n    if (d) {\n      var _e484 = n.texData.get(h.dataId).values,\n          _t343 = new Array(i);\n\n      for (var _e485 = 0; _e485 < _t343.length; _e485++) {\n        _t343[_e485] = a.shape[c[_e485]];\n      }\n\n      var _r168 = transposeImplCPU$1(_e484, a.shape, a.dtype, c, _t343);\n\n      h = n.makeTensorInfo(_t343, a.dtype), n.texData.get(h.dataId).values = _r168;\n    } else h = transposeImpl$2(a, c, n);\n\n    u = getInnerMostAxes$1(u.length, i);\n  }\n\n  assertAxesAreInnerMostDims$1(\"max\", u, i);\n  var [m, f] = computeOutAndReduceShapes$1(h.shape, u);\n  var g,\n      $ = m;\n\n  if (o && ($ = expandShapeToKeepDim$1(m, l)), d) {\n    var _e486 = n.texData.get(h.dataId),\n        _t344 = maxImplCPU$1(_e486.values, sizeFromShape$1(f), $, a.dtype);\n\n    g = n.makeTensorInfo($, a.dtype), n.texData.get(g.dataId).values = _t344;\n  } else g = maxImpl$2(h, f, $, n);\n\n  return p && n.disposeIntermediateTensorInfo(h), g;\n}\n\nvar maxConfig$2 = {\n  kernelName: Max$1,\n  backendName: \"webgl\",\n  kernelFunc: max$4\n},\n    MAXIMUM$1 = CHECK_NAN_SNIPPET$4 + \"\\n  return max(a, b);\\n\",\n    MAXIMUM_PACKED$1 = \"\\n  vec4 result = vec4(max(a, b));\\n  vec4 isNaN = min(vec4(isnan(a)) + vec4(isnan(b)), vec4(1.0));\\n  \" + CHECK_NAN_SNIPPET$3 + \"\\n  return result;\\n\",\n    maximum$4 = binaryKernelFunc$2({\n  opSnippet: MAXIMUM$1,\n  packedOpSnippet: MAXIMUM_PACKED$1,\n  cpuKernelImpl: maximumImplCPU$1\n}),\n    maximumConfig$2 = {\n  kernelName: Maximum$3,\n  backendName: \"webgl\",\n  kernelFunc: maximum$4\n};\n\nfunction maxPool$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex$2(a, \"maxPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$6(eitherStridesOrDilationsAreOne$1(o, 1), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo$1(a.shape, s, o, 1, i, l);\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual$1(u.inShape, u.outShape)) return identity$3({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var c = new Pool2DProgram$1(u, \"max\", !1);\n  return n.runWebGLProgram(c, [a], a.dtype);\n}\n\nvar maxPoolConfig$2 = {\n  kernelName: MaxPool$1,\n  backendName: \"webgl\",\n  kernelFunc: maxPool$3\n};\n\nfunction maxPool3d$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dimRoundingMode: u\n  } = r,\n      c = computePool3DInfo$1(a.shape, s, o, [1, 1, 1], i, u, l),\n      p = new Pool3DProgram$1(c, \"max\", !1);\n  return n.runWebGLProgram(p, [a], a.dtype);\n}\n\nvar maxPool3DConfig$2 = {\n  kernelName: MaxPool3D$1,\n  backendName: \"webgl\",\n  kernelFunc: maxPool3d$2\n};\n\nclass MaxPool2DBackpropProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"maxPos\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterHeight,\n        n = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n\\n        ivec2 dyRCCorner = coords.yz - pads;\\n        int dyRCorner = dyRCCorner.x;\\n        int dyCCorner = dyRCCorner.y;\\n\\n        // Convolve dy(?, ?, d) with pos mask(:, :, d) to get dx(xR, xC, d).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(t, \";\\n          wR += \").concat(e.dilationHeight, \") {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          for (int wC = 0; wC < \").concat(n, \"; wC++) {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            float dyValue = getDy(b, idyR, idyC, d);\\n            int maxPosValue = \").concat(t * n - 1, \" - int(getMaxPos(b, idyR, idyC, d));\\n\\n            // Get the current value, check it against the value from the\\n            // position matrix.\\n            int curPosValue = wR * \").concat(n, \" + wC;\\n            float mask = float(maxPosValue == curPosValue ? 1.0 : 0.0);\\n\\n            dotProd += dyValue * mask;\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass MaxPool3DBackpropProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"maxPos\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterDepth,\n        n = e.effectiveFilterHeight,\n        r = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec3 pads = ivec3(\".concat(t - 1 - e.padInfo.front, \", \").concat(n - 1 - e.padInfo.top, \", \").concat(r - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int ch = coords.u;\\n\\n        ivec3 dyCorner = ivec3(coords.y, coords.z, coords.w) - pads;\\n        int dyDCorner = dyCorner.x;\\n        int dyRCorner = dyCorner.y;\\n        int dyCCorner = dyCorner.z;\\n\\n        // Convolve dy(?, ?, ?, ch) with pos mask(:, :, :, d) to get\\n        // dx(xD, xR, xC, ch).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n\\n        for (int wD = 0; wD < \").concat(t, \";\\n           wD += \").concat(e.dilationDepth, \") {\\n          float dyD = float(dyDCorner + wD) / \").concat(e.strideDepth, \".0;\\n\\n          if (dyD < 0.0 || dyD >= \").concat(e.outDepth, \".0 || fract(dyD) > 0.0) {\\n            continue;\\n          }\\n          int idyD = int(dyD);\\n\\n          for (int wR = 0; wR < \").concat(n, \";\\n              wR += \").concat(e.dilationHeight, \") {\\n            float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n            if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 ||\\n                fract(dyR) > 0.0) {\\n              continue;\\n            }\\n            int idyR = int(dyR);\\n\\n            for (int wC = 0; wC < \").concat(r, \";\\n                wC += \").concat(e.dilationWidth, \") {\\n              float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n              if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                  fract(dyC) > 0.0) {\\n                continue;\\n              }\\n              int idyC = int(dyC);\\n\\n              float dyValue = getDy(batch, idyD, idyR, idyC, ch);\\n              int maxPosValue = \").concat(t * n * r - 1, \" -\\n                  int(getMaxPos(batch, idyD, idyR, idyC, ch));\\n\\n              // Get the current value, check it against the value from the\\n              // position matrix.\\n              int curPosValue =\\n                  wD * \").concat(n, \" * \").concat(r, \" +\\n                  wR * \").concat(r, \" + wC;\\n              float mask = float(maxPosValue == curPosValue ? 1.0 : 0.0);\\n\\n              dotProd += dyValue * mask;\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction maxPool3DGrad$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s,\n      {\n    filterSize: i,\n    strides: l,\n    pad: u,\n    dimRoundingMode: c\n  } = r,\n      p = computePool3DInfo$1(o.shape, i, l, [1, 1, 1], u, c),\n      d = new Pool3DProgram$1(p, \"max\", !0),\n      h = n.runWebGLProgram(d, [o], o.dtype),\n      m = new MaxPool3DBackpropProgram$1(p),\n      f = n.runWebGLProgram(m, [a, h], o.dtype);\n  return n.disposeIntermediateTensorInfo(h), f;\n}\n\nvar maxPoolGrad3DConfig$1 = {\n  kernelName: MaxPool3DGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: maxPool3DGrad$2\n};\n\nfunction maxPoolGrad$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s,\n    output: o\n  } = t,\n      i = s;\n  assertNotComplex$2([s, o], \"maxPoolGrad\");\n  var {\n    filterSize: l,\n    strides: u,\n    pad: c,\n    dimRoundingMode: p\n  } = r,\n      d = computePool2DInfo$1(i.shape, l, u, 1, c, p),\n      h = new Pool2DProgram$1(d, \"max\", !0),\n      m = n.runWebGLProgram(h, [i], i.dtype),\n      f = new MaxPool2DBackpropProgram$1(d),\n      g = n.runWebGLProgram(f, [a, m], i.dtype);\n  return n.disposeIntermediateTensorInfo(m), g;\n}\n\nvar maxPoolGradConfig$3 = {\n  kernelName: MaxPoolGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: maxPoolGrad$3\n};\n\nfunction maxPoolWithArgmaxImpl$2(e, t, n, r) {\n  var a = new Pool2DProgram$1(n, \"max\", !1);\n  var s = r.runWebGLProgram(a, [e], \"float32\");\n  return a = new Pool2DProgram$1(n, \"max\", !0, !0, t), [s, r.runWebGLProgram(a, [e], \"float32\")];\n}\n\nvar maxPoolWithArgmaxConfig$2 = {\n  kernelName: MaxPoolWithArgmax$1,\n  backendName: \"webgl\",\n  kernelFunc: _ref23 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref23;\n    var {\n      x: r\n    } = e,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o,\n      includeBatchInIndex: i\n    } = t,\n        l = n;\n    assert$6(4 === r.shape.length, () => \"Error in maxPool: input must be rank 4 but got rank \".concat(r.shape.length, \".\"));\n    var u = [1, 1];\n    assert$6(eitherStridesOrDilationsAreOne$1(s, u), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(s, \" and dilations '\").concat(u, \"'\"));\n    var c = computePool2DInfo$1(r.shape, a, s, u, o),\n        [p, d] = maxPoolWithArgmaxImpl$2(r, i, c, l);\n    return [p, d];\n  }\n};\n\nfunction meanImpl$1(e, t, n, r) {\n  var a = sizeFromShape$1(t),\n      s = reshape$4({\n    inputs: {\n      x: e\n    },\n    attrs: {\n      shape: [sizeFromShape$1(e.shape) / a, a]\n    },\n    backend: r\n  }),\n      o = reduce$1(s, \"float32\", \"mean\", r),\n      i = reshape$4({\n    inputs: {\n      x: o\n    },\n    attrs: {\n      shape: n\n    },\n    backend: r\n  });\n  return r.disposeIntermediateTensorInfo(s), r.disposeIntermediateTensorInfo(o), i;\n}\n\nvar meanConfig$2 = {\n  kernelName: Mean$1,\n  backendName: \"webgl\",\n  kernelFunc: _ref24 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref24;\n    var {\n      x: r\n    } = e,\n        {\n      keepDims: a,\n      axis: s\n    } = t,\n        o = n,\n        i = r.shape.length,\n        l = parseAxisParam$1(s, r.shape);\n    var u = l;\n    var c = getAxesPermutation$1(u, i),\n        p = null != c,\n        d = o.shouldExecuteOnCPU([r]),\n        h = [];\n    var m = r;\n\n    if (p) {\n      if (d) {\n        var _e487 = o.texData.get(m.dataId).values,\n            _t345 = new Array(i);\n\n        for (var _e488 = 0; _e488 < _t345.length; _e488++) {\n          _t345[_e488] = r.shape[c[_e488]];\n        }\n\n        var _n214 = transposeImplCPU$1(_e487, r.shape, r.dtype, c, _t345);\n\n        m = o.makeTensorInfo(_t345, r.dtype), o.texData.get(m.dataId).values = _n214;\n      } else m = transposeImpl$2(r, c, o);\n\n      h.push(m), u = getInnerMostAxes$1(u.length, i);\n    }\n\n    assertAxesAreInnerMostDims$1(\"sum\", u, i);\n    var [f, g] = computeOutAndReduceShapes$1(m.shape, u);\n    var $ = f;\n    a && ($ = expandShapeToKeepDim$1(f, l));\n    var y = meanImpl$1(m, g, $, o);\n\n    for (var _e489 of h) {\n      o.disposeIntermediateTensorInfo(_e489);\n    }\n\n    return y;\n  }\n};\n\nfunction min$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam$1(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation$1(u, i);\n  var p = a;\n  null != c && (p = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), u = getInnerMostAxes$1(u.length, a.shape.length)), assertAxesAreInnerMostDims$1(\"min\", u, i);\n  var [d, h] = computeOutAndReduceShapes$1(p.shape, u),\n      m = reshape$4({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, sizeFromShape$1(h)]\n    }\n  }),\n      f = reduce$1(m, m.dtype, \"min\", n);\n  var g;\n  return g = reshape$4(o ? {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: expandShapeToKeepDim$1(d, l)\n    }\n  } : {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  }), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), null != c && n.disposeIntermediateTensorInfo(p), g;\n}\n\nvar minConfig$2 = {\n  kernelName: Min$1,\n  backendName: \"webgl\",\n  kernelFunc: min$4\n},\n    MINIMUM$1 = CHECK_NAN_SNIPPET$4 + \"\\n  return min(a, b);\\n\",\n    MINIMUM_PACKED$1 = \"\\n  vec4 result = vec4(min(a, b));\\n  vec4 isNaN = min(vec4(isnan(a)) + vec4(isnan(b)), vec4(1.0));\\n  \" + CHECK_NAN_SNIPPET$3 + \"\\n  return result;\\n\",\n    minimum$4 = binaryKernelFunc$2({\n  opSnippet: MINIMUM$1,\n  packedOpSnippet: MINIMUM_PACKED$1,\n  cpuKernelImpl: minimumImplCPU$1\n}),\n    minimumConfig$2 = {\n  kernelName: Minimum$3,\n  backendName: \"webgl\",\n  kernelFunc: minimum$4\n};\n\nclass MirrorPadProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType$1(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = [\"coords[0]\", \"coords[1]\", \"coords[2]\", \"coords[3]\"].slice(0, r),\n        l = \"reflect\" === n ? 0 : 1;\n    this.userCode = 1 !== r ? \"\\n      \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outC = getOutputCoords();\\n        for (int i = 0; i < \").concat(r, \"; i++) {\\n          if (outC[i] < start[i]) {\\n            outC[i] = start[i] * 2 - outC[i] - \").concat(l, \";\\n          } else if(outC[i] >= end[i]) {\\n            outC[i] = (end[i] - 1) * 2 - outC[i] + \").concat(l, \";\\n          }\\n        }\\n        \").concat(a, \" coords = outC - start;\\n        setOutput(getX(\").concat(i, \"));\\n      }\\n    \") : \"\\n        int start = \".concat(s, \";\\n        int end = \").concat(o, \";\\n\\n        void main() {\\n          int outC = getOutputCoords();\\n          if (outC < start) {\\n            outC = start * 2 - outC - \").concat(l, \";\\n          } else if(outC >= end) {\\n            outC = (end - 1) * 2 - outC + \").concat(l, \";\\n          }\\n          setOutput(getX(outC - start));\\n        }\\n      \");\n  }\n\n}\n\nclass MirrorPadPackedProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType$1(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = getChannels$1(\"rc\", r),\n        l = getChannels$1(\"source\", r),\n        u = \"\".concat(i[r - 1], \" < \").concat(this.outputShape[r - 1]),\n        c = 1 === r ? \"source\" : \"vec2(\".concat(l.slice(-2).join(), \")\"),\n        p = \"reflect\" === n ? 0 : 1;\n    var d = \"\";\n\n    if (1 === r) {\n      var _e490 = \"\\n        \".concat(a, \" source = rc;\\n        if (source < start) {\\n          source = start * 2 - source - \").concat(p, \";\\n        } else if (source >= end) {\\n          source = (end - 1) * 2 - source + \").concat(p, \";\\n        }\\n        source -= start;\\n      \");\n\n      d = \"\\n        \".concat(a, \" rc = outputLoc;\\n        \").concat(_e490, \"\\n        result[0] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        \").concat(i[r - 1], \" += 1;\\n        if(\").concat(u, \") {\\n          \").concat(_e490, \"\\n          result[1] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        }\\n      \");\n    } else {\n      var _e491 = \"\\n        \".concat(a, \" source = rc;\\n        \").concat(a, \" lt = \").concat(a, \"(lessThan(source, start));\\n        \").concat(a, \" gte = \").concat(a, \"(greaterThanEqual(source, end));\\n        \").concat(a, \" orig = 1 - (lt + gte);\\n        source = orig * source +\\n                lt * (start * 2 - source - \").concat(p, \") +\\n                gte * ((end - 1) * 2 - source + \").concat(p, \");\\n        source -= start;\\n      \");\n\n      d = \"\\n        \".concat(a, \" rc = outputLoc;\\n        \").concat(_e491, \"\\n        result[0] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        \").concat(i[r - 1], \" += 1;\\n        if(\").concat(u, \") {\\n          \").concat(_e491, \"\\n          result[1] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        }\\n        rc = outputLoc;\\n        \").concat(i[r - 2], \" += 1;\\n        if(\").concat(i[r - 2], \" < \").concat(this.outputShape[r - 2], \") {\\n          \").concat(_e491, \"\\n          result[2] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n          \").concat(i[r - 1], \" += 1;\\n          if(\").concat(u, \") {\\n            \").concat(_e491, \"\\n            result[3] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n          }\\n        }\\n      \");\n    }\n\n    this.userCode = \"\\n      const \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      const \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outputLoc = getOutputCoords();\\n        vec4 result = vec4(0.);\\n        \").concat(d, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar mirrorPadKernelFunc$1 = _ref25 => {\n  var {\n    inputs: e,\n    backend: t,\n    attrs: n\n  } = _ref25;\n  var {\n    x: r\n  } = e,\n      {\n    paddings: a,\n    mode: s\n  } = n,\n      o = env$1().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new MirrorPadPackedProgram$1(r.shape, a, s) : new MirrorPadProgram$1(r.shape, a, s);\n  return t.runWebGLProgram(o, [r], r.dtype);\n},\n    mirrorPadConfig$2 = {\n  kernelName: MirrorPad$1,\n  backendName: \"webgl\",\n  kernelFunc: mirrorPadKernelFunc$1\n},\n    MOD$1 = \"if (b == 0.0) return NAN;\\n  return mod(a, b);\",\n    MOD_PACKED$1 = \"\\n  vec4 result = mod(a, b);\\n  vec4 isNaN = vec4(equal(b, vec4(0.0)));\\n  \" + CHECK_NAN_SNIPPET$3 + \"\\n  return result;\\n\",\n    mod$3 = binaryKernelFunc$2({\n  opSnippet: MOD$1,\n  packedOpSnippet: MOD_PACKED$1\n}),\n    modConfig$2 = {\n  kernelName: Mod$1,\n  backendName: \"webgl\",\n  kernelFunc: mod$3\n};\n\nclass MultinomialProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"probs\"], this.customUniforms = [{\n      name: \"seed\",\n      type: \"float\"\n    }], this.outputShape = [e, n], this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n\\n        float r = random(seed);\\n        float cdf = 0.0;\\n\\n        for (int i = 0; i < \".concat(t - 1, \"; i++) {\\n          cdf += getProbs(batch, i);\\n\\n          if (r < cdf) {\\n            setOutput(float(i));\\n            return;\\n          }\\n        }\\n\\n        // If no other event happened, last event happened.\\n        setOutput(float(\").concat(t - 1, \"));\\n      }\\n    \");\n  }\n\n}\n\nvar DIV$1 = \"\\nif (a == b) {\\n  return 1.0;\\n};\\nreturn a / b;\",\n    DIV_PACKED$1 = \"\\n  // vec4 one = vec4(equal(a, b));\\n  // return one + (vec4(1.0) - one) * a / b;\\n  vec4 result = a / b;\\n  if(a.x == b.x) {\\n    result.x = 1.;\\n  }\\n  if(a.y == b.y) {\\n    result.y = 1.;\\n  }\\n  if(a.z == b.z) {\\n    result.z = 1.;\\n  }\\n  if(a.w == b.w) {\\n    result.w = 1.;\\n  }\\n\\n  return result;\\n\",\n    realDiv$1 = binaryKernelFunc$2({\n  opSnippet: DIV$1,\n  packedOpSnippet: DIV_PACKED$1,\n  checkOutOfBounds: !0\n}),\n    realDivConfig$2 = {\n  kernelName: RealDiv$1,\n  backendName: \"webgl\",\n  kernelFunc: realDiv$1\n},\n    SUB$1 = \"return a - b;\",\n    sub$3 = binaryKernelFunc$2({\n  opSnippet: SUB$1,\n  packedOpSnippet: SUB$1,\n  supportsComplex: !0,\n  cpuKernelImpl: subImplCPU$1\n}),\n    subConfig$2 = {\n  kernelName: Sub$1,\n  backendName: \"webgl\",\n  kernelFunc: sub$3\n};\n\nfunction softmax$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    dim: s\n  } = r,\n      o = parseAxisParam$1([s], a.shape),\n      i = max$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      reductionIndices: o,\n      keepDims: !1\n    }\n  }),\n      l = expandShapeToKeepDim$1(i.shape, o),\n      u = reshape$4({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      c = sub$3({\n    inputs: {\n      a,\n      b: u\n    },\n    backend: n\n  }),\n      p = exp$3({\n    inputs: {\n      x: c\n    },\n    backend: n\n  }),\n      d = sum$4({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      axis: o,\n      keepDims: !1\n    }\n  }),\n      h = reshape$4({\n    inputs: {\n      x: d\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      m = realDiv$1({\n    inputs: {\n      a: p,\n      b: h\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(i), n.disposeIntermediateTensorInfo(u), n.disposeIntermediateTensorInfo(c), n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d), n.disposeIntermediateTensorInfo(h), m;\n}\n\nvar softmaxConfig$2 = {\n  kernelName: Softmax$5,\n  backendName: \"webgl\",\n  kernelFunc: softmax$4\n};\n\nfunction multinomial$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    numSamples: s,\n    seed: o,\n    normalized: i\n  } = r,\n      l = i ? a : softmax$4({\n    inputs: {\n      logits: a\n    },\n    backend: n,\n    attrs: {\n      dim: a.shape.length - 1\n    }\n  }),\n      u = new MultinomialProgram$1(l.shape[0], l.shape[1], s),\n      c = n.runWebGLProgram(u, [l], \"int32\", [[o]]);\n  return i || n.disposeIntermediateTensorInfo(l), c;\n}\n\nvar multinomialConfig$2 = {\n  kernelName: Multinomial$1,\n  backendName: \"webgl\",\n  kernelFunc: multinomial$3\n},\n    NEG$1 = \"return -x;\";\n\nfunction neg$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n\n  if (n.shouldExecuteOnCPU([r])) {\n    var _e492 = n.texData.get(r.dataId),\n        [_t346, _a115] = negImplCPU$1(_e492.values, r.shape, r.dtype);\n\n    return n.makeTensorInfo(_a115, r.dtype, _t346);\n  }\n\n  var a;\n  return a = env$1().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\") ? new UnaryOpPackedProgram$1(r.shape, NEG$1) : new UnaryOpProgram$1(r.shape, NEG$1), n.runWebGLProgram(a, [r], r.dtype);\n}\n\nvar negConfig$2 = {\n  kernelName: Neg$1,\n  backendName: \"webgl\",\n  kernelFunc: neg$3\n},\n    nonMaxSuppressionV3Impl$3 = nonMaxSuppressionV3Impl$5;\n\nfunction nonMaxSuppressionV3$2(e) {\n  warn$1(\"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead\");\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l\n  } = r,\n      u = n.readSync(a.dataId),\n      c = n.readSync(s.dataId),\n      {\n    selectedIndices: p\n  } = nonMaxSuppressionV3Impl$3(u, c, o, i, l);\n  return n.makeTensorInfo([p.length], \"int32\", new Int32Array(p));\n}\n\nvar nonMaxSuppressionV3Config$2 = {\n  kernelName: NonMaxSuppressionV3$1,\n  backendName: \"webgl\",\n  kernelFunc: nonMaxSuppressionV3$2\n},\n    nonMaxSuppressionV4Impl$3 = nonMaxSuppressionV4Impl$5;\n\nfunction nonMaxSuppressionV4$2(e) {\n  warn$1(\"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead\");\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    padToMaxOutputSize: u\n  } = r,\n      c = n.readSync(a.dataId),\n      p = n.readSync(s.dataId),\n      {\n    selectedIndices: d,\n    validOutputs: h\n  } = nonMaxSuppressionV4Impl$3(c, p, o, i, l, u);\n  return [n.makeTensorInfo([d.length], \"int32\", new Int32Array(d)), n.makeTensorInfo([], \"int32\", new Int32Array([h]))];\n}\n\nvar nonMaxSuppressionV4Config$2 = {\n  kernelName: NonMaxSuppressionV4$1,\n  backendName: \"webgl\",\n  kernelFunc: nonMaxSuppressionV4$2\n},\n    nonMaxSuppressionV5Impl$3 = nonMaxSuppressionV5Impl$5;\n\nfunction nonMaxSuppressionV5$2(e) {\n  warn$1(\"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead\");\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    softNmsSigma: u\n  } = r,\n      c = n.readSync(a.dataId),\n      p = n.readSync(s.dataId),\n      d = o,\n      h = i,\n      m = l,\n      f = u,\n      {\n    selectedIndices: g,\n    selectedScores: $\n  } = nonMaxSuppressionV5Impl$3(c, p, d, h, m, f);\n  return [n.makeTensorInfo([g.length], \"int32\", new Int32Array(g)), n.makeTensorInfo([$.length], \"float32\", new Float32Array($))];\n}\n\nvar nonMaxSuppressionV5Config$2 = {\n  kernelName: NonMaxSuppressionV5$1,\n  backendName: \"webgl\",\n  kernelFunc: nonMaxSuppressionV5$2\n};\n\nclass OneHotProgram$1 {\n  constructor(e, t, n, r) {\n    this.variableNames = [\"indices\"], this.outputShape = [e, t], this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int index = round(getIndices(coords.x));\\n        setOutput(mix(float(\".concat(r, \"), float(\").concat(n, \"),\\n                      float(index == coords.y)));\\n      }\\n    \");\n  }\n\n}\n\nvar oneHot$3 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a\n  } = t,\n      {\n    depth: s,\n    onValue: o,\n    offValue: i\n  } = r,\n      l = sizeFromShape$1(a.shape),\n      u = new OneHotProgram$1(l, s, o, i),\n      c = reshape$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [l]\n    }\n  }),\n      p = n.runWebGLProgram(u, [c], a.dtype);\n  n.disposeIntermediateTensorInfo(c);\n  var d = reshape$4({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [...a.shape, s]\n    }\n  });\n  return n.disposeIntermediateTensorInfo(p), d;\n},\n    oneHotConfig$2 = {\n  kernelName: OneHot$1,\n  backendName: \"webgl\",\n  kernelFunc: oneHot$3\n};\n\nfunction zerosLike$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n\n  if (\"complex64\" === r.dtype) {\n    var _e493 = real$3({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t347 = zerosLike$3({\n      inputs: {\n        x: _e493\n      },\n      backend: n\n    }),\n        a = imag$3({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike$3({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex$3({\n      inputs: {\n        real: _t347,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e493), n.disposeIntermediateTensorInfo(_t347), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill$3({\n    attrs: {\n      shape: r.shape,\n      dtype: r.dtype,\n      value: \"string\" === r.dtype ? \"\" : 0\n    },\n    backend: n\n  });\n}\n\nvar zerosLikeConfig$2 = {\n  kernelName: ZerosLike$1,\n  backendName: \"webgl\",\n  kernelFunc: zerosLike$3\n};\n\nfunction onesLike$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  if (\"string\" === r.dtype) throw new Error(\"onesLike is not supported under string dtype\");\n\n  if (\"complex64\" === r.dtype) {\n    var _e494 = real$3({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t348 = onesLike$3({\n      inputs: {\n        x: _e494\n      },\n      backend: n\n    }),\n        a = imag$3({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike$3({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex$3({\n      inputs: {\n        real: _t348,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e494), n.disposeIntermediateTensorInfo(_t348), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill$3({\n    attrs: {\n      shape: r.shape,\n      dtype: r.dtype,\n      value: 1\n    },\n    backend: n\n  });\n}\n\nvar onesLikeConfig$2 = {\n  kernelName: OnesLike$1,\n  backendName: \"webgl\",\n  kernelFunc: onesLike$3\n};\n\nfunction pack$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r;\n  if (1 === t.length) return expandDims$4({\n    inputs: {\n      input: t[0]\n    },\n    backend: n,\n    attrs: {\n      dim: a\n    }\n  });\n  var s = t[0].shape,\n      o = t[0].dtype;\n  t.forEach(e => {\n    assertShapesMatch$1(s, e.shape, \"All tensors passed to stack must have matching shapes\"), assert$6(o === e.dtype, () => \"All tensors passed to stack must have matching dtypes\");\n  });\n  var i = [],\n      l = concat$3({\n    inputs: t.map(e => {\n      var t = expandDims$4({\n        inputs: {\n          input: e\n        },\n        backend: n,\n        attrs: {\n          dim: a\n        }\n      });\n      return i.push(t), t;\n    }),\n    backend: n,\n    attrs: {\n      axis: a\n    }\n  });\n  return i.forEach(e => n.disposeIntermediateTensorInfo(e)), l;\n}\n\nvar packConfig$2 = {\n  kernelName: Pack$1,\n  backendName: \"webgl\",\n  kernelFunc: pack$2\n};\n\nclass PadProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.customUniforms = [{\n      name: \"value\",\n      type: \"float\"\n    }], this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType$1(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = [\"coords[0]\", \"coords[1]\", \"coords[2]\", \"coords[3]\"].slice(0, r);\n    this.userCode = 1 !== r ? \"\\n      \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outC = getOutputCoords();\\n        if (any(lessThan(outC, start)) || any(greaterThanEqual(outC, end))) {\\n          setOutput(value);\\n        } else {\\n          \").concat(a, \" coords = outC - start;\\n          setOutput(getX(\").concat(i, \"));\\n        }\\n      }\\n    \") : \"\\n        int start = \".concat(s, \";\\n        int end = \").concat(o, \";\\n\\n        void main() {\\n          int outC = getOutputCoords();\\n          if (outC < start || outC >= end) {\\n            setOutput(value);\\n          } else {\\n            setOutput(getX(outC - start));\\n          }\\n        }\\n      \");\n  }\n\n}\n\nclass PadPackedProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.packedInputs = !0, this.packedOutput = !0, this.customUniforms = [{\n      name: \"value\",\n      type: \"float\"\n    }], this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType$1(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = getChannels$1(\"rc\", r),\n        l = getChannels$1(\"source\", r),\n        u = \"\".concat(i[r - 1], \" < \").concat(this.outputShape[r - 1]),\n        c = 1 === r ? \"source\" : \"vec2(\".concat(l.slice(-2).join(), \")\"),\n        p = [\"\".concat(a, \" rc = outputLoc;\"), \"\".concat(i[r - 1], \" += 1;\\n       if(\").concat(u, \") {\\n      \"), 1 === r ? \"\" : \"}\\n       rc = outputLoc;\\n       \".concat(i[r - 2], \" += 1;\\n       if(\").concat(i[r - 2], \" < \").concat(this.outputShape[r - 2], \") {\"), 1 === r ? \"\" : \"  \".concat(i[r - 1], \" += 1;\\n         if(\").concat(u, \") {\")],\n        d = 1 === r ? \"rc < start || rc >= end\" : \"any(lessThan(rc, start)) || any(greaterThanEqual(rc, end))\";\n    var h = \"\";\n\n    for (var _e495 = 0, _t349 = 1 === r ? 2 : 4; _e495 < _t349; _e495++) {\n      h += \"\\n        \".concat(p[_e495], \"\\n        if (\").concat(d, \") {\\n          result[\").concat(_e495, \"] = float(value);\\n        } else {\\n          \").concat(a, \" source = rc - start;\\n          result[\").concat(_e495, \"] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        }\\n      \");\n    }\n\n    h += 1 === r ? \"} \" : \"}}\", this.userCode = \"\\n      const \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      const \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outputLoc = getOutputCoords();\\n        vec4 result = vec4(0.);\\n        \").concat(h, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar padV2$2 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    paddings: s,\n    constantValue: o\n  } = r,\n      i = env$1().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new PadPackedProgram$1(a.shape, s, o) : new PadProgram$1(a.shape, s, o);\n  return n.runWebGLProgram(i, [a], a.dtype, [[o]]);\n},\n    padV2Config$2 = {\n  kernelName: PadV2$1,\n  backendName: \"webgl\",\n  kernelFunc: padV2$2\n},\n    POW$1 = \"\\n  if(a < 0.0 && floor(b) < b){\\n    return NAN;\\n  }\\n  if (b == 0.0) {\\n    return 1.0;\\n  }\\n  return (round(mod(b, 2.0)) != 1) ?\\n      pow(abs(a), b) : sign(a) * pow(abs(a), b);\\n\",\n    POW_PACKED$1 = \"\\n  // isModRound1 has 1 for components with round(mod(b, 2.0)) == 1, 0 otherwise.\\n  vec4 isModRound1 = vec4(equal(round(mod(b, 2.0)), ivec4(1)));\\n  vec4 multiplier = sign(a) * isModRound1 + (vec4(1.0) - isModRound1);\\n  vec4 result = multiplier * pow(abs(a), b);\\n\\n  // Ensure that a^0 = 1, including 0^0 = 1 as this correspond to TF and JS\\n  bvec4 isExpZero = equal(b, vec4(0.0));\\n  result.r = isExpZero.r ? 1.0 : result.r;\\n  result.g = isExpZero.g ? 1.0 : result.g;\\n  result.b = isExpZero.b ? 1.0 : result.b;\\n  result.a = isExpZero.a ? 1.0 : result.a;\\n\\n  vec4 isNaN = vec4(lessThan(a, vec4(0.0))) * vec4(lessThan(floor(b), b));\\n  \" + CHECK_NAN_SNIPPET$3 + \"\\n  return result;\\n\",\n    pow$3 = binaryKernelFunc$2({\n  opSnippet: POW$1,\n  packedOpSnippet: POW_PACKED$1\n}),\n    powConfig$2 = {\n  kernelName: Pow$1,\n  backendName: \"webgl\",\n  kernelFunc: pow$3\n};\n\nfunction prod$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = [],\n      u = parseAxisParam$1(s, a.shape);\n  var c = u;\n  var p = getAxesPermutation$1(c, i);\n  var d,\n      h = a;\n\n  if (null != p && (h = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: p\n    }\n  }), c = getInnerMostAxes$1(c.length, i), l.push(h)), assertAxesAreInnerMostDims$1(\"prod\", c, i), n.shouldExecuteOnCPU([h])) {\n    var _e496 = n.texData.get(h.dataId).values,\n        {\n      outVals: _t350,\n      outShape: _r169,\n      outDtype: _a116\n    } = prodImplCPU$1(h.shape, h.dtype, _e496, c);\n    d = n.makeTensorInfo(_r169, _a116, _t350);\n  } else {\n    var [_e497, _t351] = computeOutAndReduceShapes$1(h.shape, c),\n        _r170 = sizeFromShape$1(_t351),\n        _s87 = reshape$4({\n      inputs: {\n        x: h\n      },\n      backend: n,\n      attrs: {\n        shape: [-1, _r170]\n      }\n    }),\n        _o63 = reduce$1(_s87, sumOutType$1(a.dtype), \"prod\", n);\n\n    d = reshape$4({\n      inputs: {\n        x: _o63\n      },\n      backend: n,\n      attrs: {\n        shape: _e497\n      }\n    }), l.push(_s87), l.push(_o63);\n  }\n\n  if (o) {\n    l.push(d);\n\n    var _e498 = expandShapeToKeepDim$1(d.shape, u);\n\n    d = reshape$4({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        shape: _e498\n      }\n    });\n  }\n\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), d;\n}\n\nvar prodConfig$2 = {\n  kernelName: Prod$1,\n  backendName: \"webgl\",\n  kernelFunc: prod$3\n},\n    range$5 = e => {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    step: s,\n    dtype: o\n  } = n,\n      i = rangeImplCPU$1(r, a, s, o);\n  return t.makeTensorInfo([i.length], o, i);\n},\n    rangeConfig$2 = {\n  kernelName: Range$1,\n  backendName: \"webgl\",\n  kernelFunc: range$5\n},\n    RECIPROCAL$1 = \"return 1.0 / x;\",\n    reciprocal$3 = unaryKernelFunc$2({\n  opSnippet: RECIPROCAL$1\n}),\n    reciprocalConfig$2 = {\n  kernelName: Reciprocal$1,\n  backendName: \"webgl\",\n  kernelFunc: reciprocal$3\n},\n    RELU$3 = CHECK_NAN_SNIPPET$5 + \"\\n  return (x < 0.0) ? 0.0 : x;\\n\",\n    RELU_PACKED$1 = \"\\n  vec4 result = x * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    relu$4 = unaryKernelFunc$2({\n  opSnippet: RELU$3,\n  packedOpSnippet: RELU_PACKED$1\n}),\n    reluConfig$2 = {\n  kernelName: Relu$3,\n  backendName: \"webgl\",\n  kernelFunc: relu$4\n},\n    RELU6$3 = CHECK_NAN_SNIPPET$5 + \"\\n  return (x < 0.0) ? 0.0 : min(6.0, x);\\n\",\n    RELU6_PACKED$1 = \"\\n  vec4 result = min(x, vec4(6.)) * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    relu6$3 = unaryKernelFunc$2({\n  opSnippet: RELU6$3,\n  packedOpSnippet: RELU6_PACKED$1\n}),\n    relu6Config$2 = {\n  kernelName: Relu6$3,\n  backendName: \"webgl\",\n  kernelFunc: relu6$3\n};\n\nclass ResizeBilinearProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"(vec2(yRC) + vec2(0.5)) * effectiveInputOverOutputRatioRC - vec2(0.5)\" : \"vec2(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec2 effectiveInputOverOutputRatioRC = vec2(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec2 inputShapeRC = vec2(\").concat(o, \".0, \").concat(i, \".0);\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        ivec2 yRC = coords.yz;\\n\\n        // Fractional source index.\\n        vec2 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the four integer indices.\\n        ivec2 sourceFloorRC = ivec2(max(sourceFracIndexRC, vec2(0.0)));\\n        ivec2 sourceCeilRC = ivec2(\\n          min(inputShapeRC - 1.0, ceil(sourceFracIndexRC)));\\n\\n        float topLeft = getA(b, sourceFloorRC.x, sourceFloorRC.y, d);\\n        float bottomLeft = getA(b, sourceCeilRC.x, sourceFloorRC.y, d);\\n        float topRight = getA(b, sourceFloorRC.x, sourceCeilRC.y, d);\\n        float bottomRight = getA(b, sourceCeilRC.x, sourceCeilRC.y, d);\\n\\n        vec2 fracRC = sourceFracIndexRC - vec2(sourceFloorRC);\\n\\n        float top = topLeft + (topRight - topLeft) * fracRC.y;\\n        float bottom = bottomLeft + (bottomRight - bottomLeft) * fracRC.y;\\n        float newValue = top + (bottom - top) * fracRC.x;\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nclass ResizeBilinearPackedProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"(vec3(yRC) + vec3(0.5)) * effectiveInputOverOutputRatioRC - vec3(0.5)\" : \"vec3(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec3 effectiveInputOverOutputRatioRC = vec3(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec3 inputShapeRC = vec3(\").concat(o, \".0, \").concat(i, \".0,\\n                                     \").concat(i, \".0);\\n\\n      float getAValue(int b, int r, int c, int d) {\\n        return getChannel(getA(b, r, c, d), vec2(c, d));\\n      }\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        // Calculate values for next column in yRC.z.\\n        ivec3 yRC = coords.yzz + ivec3(0, 0, 1);\\n\\n        // Fractional source index.\\n        vec3 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the four integer indices.\\n        ivec3 sourceFloorRC = ivec3(max(sourceFracIndexRC, vec3(0.0)));\\n        ivec3 sourceCeilRC = ivec3(\\n          min(inputShapeRC - 1.0, ceil(sourceFracIndexRC)));\\n\\n        // Should we calculate next column and row elements in 2x2 packed cell.\\n        bool hasNextCol = d < \").concat(l - 1, \";\\n        bool hasNextRow = coords.z < \").concat(n - 1, \";\\n\\n        // In parallel, construct four corners for all four components in\\n        // packed 2x2 cell.\\n        vec4 topLeft = vec4(\\n          getAValue(b, sourceFloorRC.x, sourceFloorRC.y, d),\\n          hasNextCol ? getAValue(b, sourceFloorRC.x, sourceFloorRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceFloorRC.x, sourceFloorRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceFloorRC.x, sourceFloorRC.z, d + 1) : 0.0);\\n\\n        vec4 bottomLeft = vec4(\\n          getAValue(b, sourceCeilRC.x, sourceFloorRC.y, d),\\n          hasNextCol ? getAValue(b, sourceCeilRC.x, sourceFloorRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceCeilRC.x, sourceFloorRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceCeilRC.x, sourceFloorRC.z, d + 1) : 0.0);\\n\\n        vec4 topRight = vec4(\\n          getAValue(b, sourceFloorRC.x, sourceCeilRC.y, d),\\n          hasNextCol ? getAValue(b, sourceFloorRC.x, sourceCeilRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceFloorRC.x, sourceCeilRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceFloorRC.x, sourceCeilRC.z, d + 1) : 0.0);\\n\\n        vec4 bottomRight = vec4(\\n          getAValue(b, sourceCeilRC.x, sourceCeilRC.y, d),\\n          hasNextCol ? getAValue(b, sourceCeilRC.x, sourceCeilRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceCeilRC.x, sourceCeilRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceCeilRC.x, sourceCeilRC.z, d + 1) : 0.0);\\n\\n        vec3 fracRC = sourceFracIndexRC - vec3(sourceFloorRC);\\n\\n        vec4 top = mix(topLeft, topRight, fracRC.yyzz);\\n        vec4 bottom = mix(bottomLeft, bottomRight, fracRC.yyzz);\\n        vec4 newValue = mix(top, bottom, fracRC.x);\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeBilinear$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r,\n      [l, u] = i,\n      c = env$1().getBool(\"WEBGL_PACK_IMAGE_OPERATIONS\") ? new ResizeBilinearPackedProgram$1(a.shape, l, u, s, o) : new ResizeBilinearProgram$1(a.shape, l, u, s, o);\n  return n.runWebGLProgram(c, [a], \"float32\");\n}\n\nvar resizeBilinearConfig$2 = {\n  kernelName: ResizeBilinear$1,\n  backendName: \"webgl\",\n  kernelFunc: resizeBilinear$3\n};\n\nclass ResizeBilinearBackpropProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"dy\"], this.outputShape = [], this.outputShape = t;\n    var [, r, a] = t,\n        [, s, o] = e,\n        i = [n && s > 1 ? r - 1 : r, n && o > 1 ? a - 1 : a],\n        l = [n && s > 1 ? s - 1 : s, n && o > 1 ? o - 1 : o],\n        u = i[0] / l[0],\n        c = i[1] / l[1],\n        p = 1 / u,\n        d = 1 / c,\n        h = 2 * Math.ceil(p) + 2,\n        m = 2 * Math.ceil(d) + 2;\n    this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        int r = coords[1];\\n        int c = coords[2];\\n\\n        float accumulator = 0.0;\\n\\n        const float heightScale = float(\".concat(u, \");\\n        const float widthScale = float(\").concat(c, \");\\n\\n        const float invHeightScale = float(\").concat(p, \");\\n        const float invWidthScale = float(\").concat(d, \");\\n\\n        const int winHeight = int(\").concat(h, \");\\n        const int winWidth = int(\").concat(m, \");\\n\\n        // Compute bounds for where in dy we will look\\n        float startRLerp = floor(float(r) * invHeightScale);\\n        int startDyR = int(startRLerp - float(winHeight / 2));\\n\\n        float startCLerp = floor(float(c) * invWidthScale);\\n        int startDyC = int(startCLerp - float(winWidth / 2));\\n\\n        // Loop over dy\\n        for (int dyROffset = 0; dyROffset < winHeight; dyROffset++) {\\n          int dyR = dyROffset + startDyR;\\n\\n          // Guard against the window exceeding the bounds of dy\\n          if (dyR < 0 || dyR >= \").concat(s, \") {\\n            continue;\\n          }\\n\\n          for (int dyCOffset = 0; dyCOffset < winWidth; dyCOffset++) {\\n            int dyC = dyCOffset + startDyC;\\n\\n            // Guard against the window exceeding the bounds of dy\\n            if (dyC < 0 || dyC >= \").concat(o, \") {\\n              continue;\\n            }\\n\\n            float dxR = float(dyR) * heightScale;\\n            int topDxRIndex = int(floor(dxR));\\n            int bottomDxRIndex = int(min(ceil(dxR), \").concat(r - 1, \".0));\\n            float dxRLerp = dxR - float(topDxRIndex);\\n            float inverseDxRLerp = 1.0 - dxRLerp;\\n\\n            float dxC = float(dyC) * widthScale;\\n            int leftDxCIndex = int(floor(dxC));\\n            int rightDxCIndex = int(min(ceil(dxC), \").concat(a - 1, \".0));\\n            float dxCLerp = dxC - float(leftDxCIndex);\\n            float inverseDxCLerp = 1.0 - dxCLerp;\\n\\n            if (r == topDxRIndex && c == leftDxCIndex) {\\n              // topLeft\\n              accumulator +=\\n                getDy(b, dyR, dyC, d) * inverseDxRLerp * inverseDxCLerp;\\n            }\\n\\n            if (r == topDxRIndex && c == rightDxCIndex) {\\n              // topRight\\n              accumulator += getDy(b, dyR, dyC, d) * inverseDxRLerp * dxCLerp;\\n            }\\n\\n            if (r == bottomDxRIndex && c == leftDxCIndex) {\\n              // bottomLeft\\n              accumulator += getDy(b, dyR, dyC, d) * dxRLerp * inverseDxCLerp;\\n            }\\n\\n            if (r == bottomDxRIndex && c == rightDxCIndex) {\\n              // bottomRight\\n              accumulator += getDy(b, dyR, dyC, d) * dxRLerp * dxCLerp;\\n            }\\n          }\\n        }\\n        // End loop over dy\\n\\n        setOutput(accumulator);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeBilinearGrad$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r,\n      i = new ResizeBilinearBackpropProgram$1(s.shape, a.shape, o);\n  return n.runWebGLProgram(i, [s], s.dtype);\n}\n\nvar resizeBilinearGradConfig$3 = {\n  kernelName: ResizeBilinearGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: resizeBilinearGrad$2\n};\n\nclass ResizeNearestNeighborProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"max((vec2(yRC) + vec2(0.5)) * effectiveInputOverOutputRatioRC, vec2(0.0))\" : \"vec2(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec2 effectiveInputOverOutputRatioRC = vec2(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec2 inputShapeRC = vec2(\").concat(o, \".0, \").concat(i, \".0);\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        ivec2 yRC = coords.yz;\\n\\n        // Fractional source index.\\n        vec2 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the coordinators of nearest neighbor point.\\n        ivec2 sourceNearestRC = ivec2(\\n          min(inputShapeRC - 1.0, floor(sourceFracIndexRC + \").concat(r ? \"0.5\" : \"0.0\", \")));\\n        float newValue = getA(b, sourceNearestRC.x, sourceNearestRC.y, d);\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nclass ResizeNearestNeighborPackedProgram$1 {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"max((vec3(yRC) + vec3(0.5)) * effectiveInputOverOutputRatioRC, vec3(0.0))\" : \"vec3(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec3 effectiveInputOverOutputRatioRC = vec3(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec3 inputShapeRC = vec3(\").concat(o, \".0, \").concat(i, \".0,\\n                                     \").concat(i, \".0);\\n\\n      float getAValue(int b, int r, int c, int d) {\\n        return getChannel(getA(b, r, c, d), vec2(c, d));\\n      }\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        // Calculate values for next column in yRC.z.\\n        ivec3 yRC = coords.yzz + ivec3(0, 0, 1);\\n\\n        // Fractional source index.\\n        vec3 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the coordinators of nearest neighbor point.\\n        ivec3 sourceNearestRC = ivec3(\\n          min(inputShapeRC - 1.0, floor(sourceFracIndexRC + \").concat(r ? \"0.5\" : \"0.0\", \")));\\n\\n        // Should we calculate next column and row elements in 2x2 packed cell.\\n        bool hasNextCol = d < \").concat(l - 1, \";\\n        bool hasNextRow = coords.z < \").concat(n - 1, \";\\n\\n        vec4 newValue = vec4(\\n          getAValue(b, sourceNearestRC.x, sourceNearestRC.y, d),\\n          hasNextCol ? getAValue(b, sourceNearestRC.x, sourceNearestRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceNearestRC.x, sourceNearestRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceNearestRC.x, sourceNearestRC.z, d + 1) : 0.0);\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeNearestNeighbor$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r,\n      [l, u] = i,\n      c = env$1().getBool(\"WEBGL_PACK_IMAGE_OPERATIONS\") ? new ResizeNearestNeighborPackedProgram$1(a.shape, l, u, s, o) : new ResizeNearestNeighborProgram$1(a.shape, l, u, s, o);\n  return n.runWebGLProgram(c, [a], a.dtype);\n}\n\nvar resizeNearestNeighborConfig$2 = {\n  kernelName: ResizeNearestNeighbor$1,\n  backendName: \"webgl\",\n  kernelFunc: resizeNearestNeighbor$3\n};\n\nclass ResizeNearestNeigborBackpropProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"dy\"], this.outputShape = [], this.outputShape = t;\n    var [, r, a] = t,\n        [, s, o] = e,\n        i = [n && s > 1 ? r - 1 : r, n && o > 1 ? a - 1 : a],\n        l = [n && s > 1 ? s - 1 : s, n && o > 1 ? o - 1 : o],\n        u = i[0] / l[0],\n        c = i[1] / l[1],\n        p = 1 / u,\n        d = 1 / c,\n        h = 2 * Math.ceil(p) + 2,\n        m = 2 * Math.ceil(d) + 2;\n    this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        int r = coords[1];\\n        int c = coords[2];\\n\\n        float accumulator = 0.0;\\n\\n        const float heightScale = float(\".concat(u, \");\\n        const float widthScale = float(\").concat(c, \");\\n\\n        const float invHeightScale = float(\").concat(p, \");\\n        const float invWidthScale = float(\").concat(d, \");\\n\\n        const int winHeight = int(\").concat(h, \");\\n        const int winWidth = int(\").concat(m, \");\\n\\n        // Compute bounds for where in dy we will look\\n        float startRLerp = floor(float(r) * invHeightScale);\\n        int startDyR = int(floor(startRLerp - float(winHeight / 2)));\\n\\n        float startCLerp = floor(float(c) * invWidthScale);\\n        int startDyC = int(floor(startCLerp - float(winWidth / 2)));\\n\\n        // Loop over dy\\n        for (int dyROffset = 0; dyROffset < winHeight; dyROffset++) {\\n          int dyR = dyROffset + startDyR;\\n\\n          // Guard against the window exceeding the bounds of dy\\n          if (dyR < 0 || dyR >= \").concat(s, \") {\\n            continue;\\n          }\\n\\n          for (int dyCOffset = 0; dyCOffset < winWidth; dyCOffset++) {\\n            int dyC = dyCOffset + startDyC;\\n\\n            // Guard against the window exceeding the bounds of dy\\n            if (dyC < 0 || dyC >= \").concat(o, \") {\\n              continue;\\n            }\\n\\n            float sourceFracRow =\\n              float(\").concat(i[0], \") *\\n                (float(dyR) / float(\").concat(l[0], \"));\\n\\n            float sourceFracCol =\\n                float(\").concat(i[1], \") *\\n                  (float(dyC) / float(\").concat(l[1], \"));\\n\\n            int sourceNearestRow = int(min(\\n                float(int(\").concat(r, \") - 1),\\n                \").concat(n, \" ? float(round(sourceFracRow)) :\\n                                  float(floor(sourceFracRow))));\\n\\n            int sourceNearestCol = int(min(\\n                float(int(\").concat(a, \") - 1),\\n                \").concat(n, \" ? float(round(sourceFracCol)) :\\n                                  float(floor(sourceFracCol))));\\n\\n            if (r == sourceNearestRow && c == sourceNearestCol) {\\n              accumulator += getDy(b, dyR, dyC, d);\\n            }\\n          }\\n        }\\n        // End loop over dy\\n\\n        setOutput(accumulator);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeNearestNeighborGrad$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r,\n      i = new ResizeNearestNeigborBackpropProgram$1(s.shape, a.shape, o);\n  return n.runWebGLProgram(i, [s], s.dtype);\n}\n\nvar resizeNearestNeighborGradConfig$3 = {\n  kernelName: ResizeNearestNeighborGrad$1,\n  backendName: \"webgl\",\n  kernelFunc: resizeNearestNeighborGrad$2\n};\n\nclass ReverseProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"x\"];\n    var n = e.length;\n    if (n > 4) throw new Error(\"WebGL backend: Reverse of rank-\".concat(n, \" tensor is not yet supported\"));\n    if (this.outputShape = e, 1 === n) return void (this.userCode = \"\\n        void main() {\\n          int coord = getOutputCoords();\\n          setOutput(getX(\".concat(e[0], \" - coord - 1));\\n        }\\n      \"));\n    var r = e.map((n, r) => (n => -1 !== t.indexOf(n) && 1 !== e[n] ? \"\".concat(e[n], \" - coords[\").concat(n, \"] - 1\") : \"coords[\".concat(n, \"]\"))(r)).join(\",\"),\n        a = getCoordsDataType$1(n);\n    this.userCode = \"\\n      void main() {\\n        \".concat(a, \" coords = getOutputCoords();\\n        setOutput(getX(\").concat(r, \"));\\n      }\\n    \");\n  }\n\n}\n\nclass ReversePackedProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"x\"], this.packedInputs = !0, this.packedOutput = !0;\n    var n = e.length;\n    if (n > 4) throw new Error(\"WebGL backend: Reverse of rank-\".concat(n, \" tensor is not yet supported\"));\n    this.outputShape = e;\n    var r = getChannels$1(\"rc\", n),\n        a = \"\".concat(r[n - 1], \" + 1 < \").concat(this.outputShape[n - 1]),\n        s = \"\".concat(r[n - 2], \" + 1 < \").concat(this.outputShape[n - 2]),\n        o = getCoordsDataType$1(n);\n\n    function i(n) {\n      var r = e.map((r, a) => function (n, r) {\n        return -1 !== t.indexOf(n) && 1 !== e[n] ? \"\".concat(e[n], \" - \").concat(r[n], \" - 1\") : \"\".concat(r[n]);\n      }(a, n));\n      return \"getChannel(getX(\".concat(r.join(\",\"), \"), vec2(\").concat(r.slice(-2).join(\",\"), \"))\");\n    }\n\n    this.userCode = 1 === n ? \"\\n        void main(){\\n          int rc = getOutputCoords();\\n          vec4 result = vec4(0.);\\n          result.r = getChannel(getX(\".concat(e[0], \" - rc - 1),\\n            \").concat(e[0], \" - rc - 1);\\n          if(\").concat(a, \"){\\n              result.g = getChannel(getX(\").concat(e[0], \" - (rc  + 1) - 1),\\n                \").concat(e[0], \" - (rc  + 1) - 1);\\n          }\\n          setOutput(result);\\n        }\\n      \") : \"\\n        void main() {\\n          \".concat(o, \" rc = getOutputCoords();\\n          vec4 result = vec4(0.);\\n          result.r = \").concat(function (e) {\n      return i(e);\n    }(r.slice()), \";\\n          if(\").concat(a, \"){\\n            result.g = \").concat(function (e) {\n      return e[n - 1] = \"(\" + e[n - 1] + \" + 1)\", i(e);\n    }(r.slice()), \";\\n          }\\n          if(\").concat(s, \") {\\n            result.b = \").concat(function (e) {\n      return e[n - 2] = \"(\" + e[n - 2] + \" + 1)\", i(e);\n    }(r.slice()), \";\\n            if(\").concat(a, \") {\\n              result.a = \").concat(function (e) {\n      return e[n - 1] = \"(\" + e[n - 1] + \" + 1)\", e[n - 2] = \"(\" + e[n - 2] + \" + 1)\", i(e);\n    }(r.slice()), \";\\n            }\\n          }\\n          setOutput(result);\\n        }\\n    \");\n  }\n\n}\n\nfunction reverse$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dims: s\n  } = r,\n      o = a.shape.length,\n      i = parseAxisParam$1(s, a.shape);\n  if (0 === o) return identity$3({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var l = env$1().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new ReversePackedProgram$1(a.shape, i) : new ReverseProgram$1(a.shape, i);\n  return n.runWebGLProgram(l, [a], a.dtype);\n}\n\nvar reverseConfig$2 = {\n  kernelName: Reverse$1,\n  backendName: \"webgl\",\n  kernelFunc: reverse$3\n};\n\nclass RotateProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"Image\"], this.outputShape = [], this.customUniforms = [{\n      name: \"params\",\n      type: \"vec4\"\n    }];\n    var n = e[1],\n        r = e[2];\n    this.outputShape = e;\n    var a = \"\";\n    a = \"number\" == typeof t ? \"float outputValue = \".concat(t.toFixed(2), \";\") : \"\\n        vec3 fill = vec3(\".concat(t.join(\",\"), \");\\n        float outputValue = fill[coords[3]];\"), this.userCode = \"\\n        void main() {\\n          ivec4 coords = getOutputCoords();\\n          int x = coords[2];\\n          int y = coords[1];\\n          float coordXFloat = (float(x) - params[0]) * params[3] -\\n            (float(y) - params[1]) * params[2];\\n          float coordYFloat = (float(x) - params[0]) * params[2] +\\n            (float(y) - params[1]) * params[3];\\n          int coordX = int(round(coordXFloat + params[0]));\\n          int coordY = int(round(coordYFloat + params[1]));\\n          \".concat(a, \"\\n          if(coordX >= 0 && coordX < \").concat(r, \" && coordY >= 0 && coordY < \").concat(n, \") {\\n            outputValue = getImage(coords[0], coordY, coordX, coords[3]);\\n          }\\n          setOutput(outputValue);\\n        }\\n    \");\n  }\n\n}\n\nvar rotateWithOffsetConfig$2 = {\n  kernelName: RotateWithOffset$1,\n  backendName: \"webgl\",\n  kernelFunc: _ref26 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref26;\n    var {\n      image: r\n    } = e,\n        {\n      radians: a,\n      fillValue: s,\n      center: o\n    } = t,\n        i = n,\n        l = new RotateProgram$1(r.shape, s),\n        [u, c] = getImageCenter$1(o, r.shape[1], r.shape[2]),\n        p = [[u, c, Math.sin(a), Math.cos(a)]];\n    return i.runWebGLProgram(l, [r], r.dtype, p);\n  }\n},\n    ROUND$1 = \"\\n  // OpenGL ES does not support round function.\\n  // The algorithm is based on banker's rounding.\\n  float base = floor(x);\\n  if ((x - base) < 0.5) {\\n    return floor(x);\\n  } else if ((x - base) > 0.5) {\\n    return ceil(x);\\n  } else {\\n    if (mod(base, 2.0) == 0.0) {\\n      return base;\\n    } else {\\n      return base + 1.0;\\n    }\\n  }\\n\",\n    round$4 = unaryKernelFunc$2({\n  opSnippet: ROUND$1\n}),\n    roundConfig$2 = {\n  kernelName: Round$1,\n  backendName: \"webgl\",\n  kernelFunc: round$4\n},\n    RSQRT$1 = \"return inversesqrt(x);\",\n    rsqrt$3 = unaryKernelFunc$2({\n  opSnippet: RSQRT$1,\n  cpuKernelImpl: rsqrtImplCPU$1\n}),\n    rsqrtConfig$2 = {\n  kernelName: Rsqrt$1,\n  backendName: \"webgl\",\n  kernelFunc: rsqrt$3\n};\n\nclass ScatterProgram$1 {\n  constructor(e, t, n, r, a, s) {\n    var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !0;\n    this.variableNames = [\"updates\", \"indices\", \"defaultValue\"], this.outputShape = s;\n    var i = getCoordsDataType$1(a.length),\n        l = getCoordsDataType$1(s.length);\n    var u = \"\";\n    1 === n ? u = \"i\" : 2 === n && (u = \"i, j\");\n    var c = \"\";\n    1 === r ? c = \"i\" : 2 === r && (c = \"i, coords[1]\"), this.userCode = \"\\n        \".concat(i, \" strides = \").concat(i, \"(\").concat(a, \");\\n\\n        void main() {\\n          \").concat(l, \" coords = getOutputCoords();\\n          float sum = 0.0;\\n          bool found = false;\\n          for (int i = 0; i < \").concat(e, \"; i++) {\\n            int flattenedIndex = 0;\\n            for (int j = 0; j < \").concat(t, \"; j++) {\\n              int index = round(getIndices(\").concat(u, \"));\\n              flattenedIndex += index * \").concat(t > 1 ? \"strides[j]\" : \"strides\", \";\\n            }\\n            if (flattenedIndex == coords[0]) {\\n              sum += getUpdates(\").concat(c, \");\\n              found = true;\\n            }\\n          }\\n          setOutput(mix(getDefaultValue(), sum, float(found)));\\n        }\\n      \");\n  }\n\n}\n\nfunction scatterNd$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a,\n    updates: s\n  } = t,\n      {\n    shape: o\n  } = r,\n      {\n    sliceRank: i,\n    numUpdates: l,\n    sliceSize: u,\n    strides: c,\n    outputSize: p\n  } = calculateShapes$1(s, a, o),\n      d = [p / u, u];\n  if (0 === p) return n.makeTensorInfo(o, a.dtype);\n  var h = reshape$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [l, i]\n    }\n  }),\n      m = reshape$4({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: [l, u]\n    }\n  }),\n      f = n.makeTensorInfo([], \"float32\", new Float32Array([0])),\n      g = new ScatterProgram$1(l, i, h.shape.length, m.shape.length, c, d),\n      $ = n.runWebGLProgram(g, [m, h, f], m.dtype),\n      y = reshape$4({\n    inputs: {\n      x: $\n    },\n    backend: n,\n    attrs: {\n      shape: o\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo($), n.disposeIntermediateTensorInfo(f), y;\n}\n\nvar scatterNdConfig$2 = {\n  kernelName: ScatterNd$1,\n  backendName: \"webgl\",\n  kernelFunc: scatterNd$2\n};\n\nclass SelectProgram$1 {\n  constructor(e, t, n) {\n    var r, a;\n    if (this.variableNames = [\"c\", \"a\", \"b\"], this.outputShape = t, n > 4) throw Error(\"Where for rank \".concat(n, \" is not yet supported\"));\n    if (1 === n) a = \"resRC\", r = \"resRC\";else {\n      var _n215 = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\"],\n          _s88 = [],\n          o = [];\n\n      for (var _r171 = 0; _r171 < t.length; _r171++) {\n        o.push(\"\".concat(_n215[_r171])), _r171 < e && _s88.push(\"\".concat(_n215[_r171]));\n      }\n\n      r = _s88.join(), a = o.join();\n    }\n    var s = getCoordsDataType$1(n);\n    this.userCode = \"\\n      void main() {\\n        \".concat(s, \" resRC = getOutputCoords();\\n        float cVal = getC(\").concat(r, \");\\n        if (cVal >= 1.0) {\\n          setOutput(getA(\").concat(a, \"));\\n        } else {\\n          setOutput(getB(\").concat(a, \"));\\n        }\\n      }\\n    \");\n  }\n\n}\n\nfunction select$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    condition: r,\n    t: a,\n    e: s\n  } = t,\n      o = new SelectProgram$1(r.shape.length, a.shape, a.shape.length);\n  return n.runWebGLProgram(o, [r, a, s], upcastType$1(a.dtype, s.dtype));\n}\n\nvar selectConfig$2 = {\n  kernelName: Select$1,\n  backendName: \"webgl\",\n  kernelFunc: select$3\n},\n    SELU$1 = \"\\n  // Stable and Attracting Fixed Point (0, 1) for Normalized Weights.\\n  // see: https://arxiv.org/abs/1706.02515\\n  float scaleAlpha = \".concat(SELU_SCALEALPHA$1, \";\\n  float scale = \").concat(SELU_SCALE$1, \";\\n  return (x >= 0.0) ? scale * x : scaleAlpha * (exp(x) - 1.0);\\n\"),\n    selu$3 = unaryKernelFunc$2({\n  opSnippet: SELU$1\n}),\n    seluConfig$2 = {\n  kernelName: Selu$3,\n  backendName: \"webgl\",\n  kernelFunc: selu$3\n},\n    SIGMOID$3 = \"return 1.0 / (1.0 + exp(-1.0 * x));\",\n    sigmoid$3 = unaryKernelFunc$2({\n  opSnippet: SIGMOID$3\n}),\n    sigmoidConfig$2 = {\n  kernelName: Sigmoid$3,\n  backendName: \"webgl\",\n  kernelFunc: sigmoid$3\n},\n    SIGN$1 = \"\\n  if (isnan(x)) { return 0.0; }\\n  return sign(x);\\n\",\n    sign$3 = unaryKernelFunc$2({\n  opSnippet: SIGN$1\n}),\n    signConfig$2 = {\n  kernelName: Sign$1,\n  backendName: \"webgl\",\n  kernelFunc: sign$3\n},\n    SIN$1 = CHECK_NAN_SNIPPET_UNARY$1 + \"\\n  return sin(x);\\n\",\n    sin$3 = unaryKernelFunc$2({\n  opSnippet: SIN$1\n}),\n    sinConfig$2 = {\n  kernelName: Sin$1,\n  backendName: \"webgl\",\n  kernelFunc: sin$3\n},\n    SINH$1 = \"\\n  float e2x = exp(x);\\n  return (e2x - 1.0 / e2x) / 2.0;\\n\",\n    sinh$3 = unaryKernelFunc$2({\n  opSnippet: SINH$1\n}),\n    sinhConfig$2 = {\n  kernelName: Sinh$1,\n  backendName: \"webgl\",\n  kernelFunc: sinh$3\n},\n    SOFTPLUS$1 = \"\\n  float epsilon = 1.1920928955078125e-7;\\n  float threshold = log(epsilon) + 2.0;\\n\\n  bool too_large = x > -threshold;\\n  bool too_small = x < threshold;\\n\\n  float result;\\n  float exp_x = exp(x);\\n\\n  if (too_large){\\n    result = x;\\n  }\\n  else if (too_small){\\n    result = exp_x;\\n  }\\n  else{\\n    result = log(exp_x + 1.0);\\n  }\\n  return result;\\n\",\n    softplus$3 = unaryKernelFunc$2({\n  opSnippet: SOFTPLUS$1\n}),\n    softplusConfig$2 = {\n  kernelName: Softplus$3,\n  backendName: \"webgl\",\n  kernelFunc: softplus$3\n},\n    spaceToBatchND$3 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    paddings: o\n  } = r;\n  assert$6(a.shape.length <= 4, () => \"spaceToBatchND for rank > 4 with a WebGL backend not implemented yet\");\n  var i = s.reduce((e, t) => e * t),\n      l = [[0, 0]];\n  l.push(...o);\n\n  for (var _e499 = 1 + s.length; _e499 < a.shape.length; ++_e499) {\n    l.push([0, 0]);\n  }\n\n  var u = [],\n      c = padV2$2({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      paddings: l,\n      constantValue: 0\n    }\n  }),\n      p = getReshaped$1(c.shape, s, i, !1),\n      d = getPermuted$1(p.length, s.length, !1),\n      h = getReshapedPermuted$1(c.shape, s, i, !1),\n      m = reshape$4({\n    inputs: {\n      x: c\n    },\n    backend: n,\n    attrs: {\n      shape: p\n    }\n  }),\n      f = transpose$3({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      perm: d\n    }\n  }),\n      g = reshape$4({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: h\n    }\n  });\n  return u.push(c), u.push(m), u.push(f), u.forEach(e => n.disposeIntermediateTensorInfo(e)), g;\n},\n    spaceToBatchNDConfig$2 = {\n  kernelName: SpaceToBatchND$1,\n  backendName: \"webgl\",\n  kernelFunc: spaceToBatchND$3\n};\n\nfunction sparseFillEmptyRows$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    indices: r,\n    values: a,\n    denseShape: s,\n    defaultValue: o\n  } = t;\n  if (1 !== s.shape.length) throw new Error(\"Dense shape must be a vector, saw:\\n         \".concat(s.shape));\n  if (2 !== r.shape.length) throw new Error(\"Indices must be a matrix, saw:\\n         \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Values must be a vector, saw:\\n         \".concat(a.shape));\n  if (0 !== o.shape.length) throw new Error(\"Default value must be a scalar, saw:\\n        \".concat(o.shape));\n  var i = n.readSync(r.dataId),\n      l = n.readSync(a.dataId),\n      u = n.readSync(s.dataId),\n      c = n.readSync(o.dataId)[0],\n      [p, d, h, m, f] = sparseFillEmptyRowsImplCPU$1(i, r.shape, r.dtype, l, a.dtype, u, c);\n  return [n.makeTensorInfo(d, r.dtype, p), n.makeTensorInfo([d[0]], a.dtype, h), n.makeTensorInfo([m.length], \"bool\", new Uint8Array(m.map(e => Number(e)))), n.makeTensorInfo([f.length], r.dtype, new Int32Array(f))];\n}\n\nvar sparseFillEmptyRowsConfig$2 = {\n  kernelName: SparseFillEmptyRows$1,\n  backendName: \"webgl\",\n  kernelFunc: sparseFillEmptyRows$3\n};\n\nfunction sparseReshape$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    inputIndices: r,\n    inputShape: a,\n    newShape: s\n  } = t;\n  if (2 !== r.shape.length) throw new Error(\"Input indices should be a matrix but received shape \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Input shape should be a vector but received shape \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Target shape should be a vector but received shape \".concat(s.shape));\n  var o = Array.from(n.readSync(a.dataId)),\n      i = n.readSync(r.dataId),\n      l = Array.from(n.readSync(s.dataId)),\n      [u, c, p] = sparseReshapeImplCPU$1(i, r.shape, r.dtype, o, l);\n  return [n.makeTensorInfo(c, r.dtype, u), n.makeTensorInfo([p.length], s.dtype, new Int32Array(p))];\n}\n\nvar sparseReshapeConfig$2 = {\n  kernelName: SparseReshape$1,\n  backendName: \"webgl\",\n  kernelFunc: sparseReshape$3\n};\n\nfunction sparseSegmentMean$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n              \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n              \".concat(s.shape));\n  var o = n.readSync(r.dataId),\n      i = n.readSync(a.dataId),\n      l = n.readSync(s.dataId),\n      [u, c] = sparseSegmentReductionImplCPU$1(o, r.shape, r.dtype, i, l, !0);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentMeanConfig$2 = {\n  kernelName: SparseSegmentMean$1,\n  backendName: \"webgl\",\n  kernelFunc: sparseSegmentMean$3\n};\n\nfunction sparseSegmentSum$3(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n             \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n             \".concat(s.shape));\n  var o = n.readSync(r.dataId),\n      i = n.readSync(a.dataId),\n      l = n.readSync(s.dataId),\n      [u, c] = sparseSegmentReductionImplCPU$1(o, r.shape, r.dtype, i, l);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentSumConfig$2 = {\n  kernelName: SparseSegmentSum$1,\n  backendName: \"webgl\",\n  kernelFunc: sparseSegmentSum$3\n};\n\nfunction sparseToDense$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    sparseIndices: a,\n    sparseValues: s,\n    defaultValue: o\n  } = t,\n      {\n    outputShape: i\n  } = r,\n      {\n    sliceRank: l,\n    numUpdates: u,\n    strides: c,\n    outputSize: p\n  } = calculateShapes$1(s, a, i),\n      d = new ScatterProgram$1(u, l, a.shape.length, s.shape.length, c, [p, 1], !1),\n      h = n.runWebGLProgram(d, [s, a, o], s.dtype),\n      m = reshape$4({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      shape: i\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), m;\n}\n\nvar sparseToDenseConfig$2 = {\n  kernelName: SparseToDense$1,\n  backendName: \"webgl\",\n  kernelFunc: sparseToDense$3\n};\n\nfunction splitV$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    numOrSizeSplits: s,\n    axis: o\n  } = r,\n      i = parseAxisParam$1(o, a.shape)[0],\n      l = prepareSplitSize$1(a, s, i),\n      u = new Array(a.shape.length).fill(0),\n      c = a.shape.slice();\n  return l.map(e => {\n    var t = [...c];\n    t[i] = e;\n    var r = slice$3({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        begin: u,\n        size: t\n      }\n    });\n    return u[i] += e, r;\n  });\n}\n\nvar splitVConfig$2 = {\n  kernelName: SplitV$1,\n  backendName: \"webgl\",\n  kernelFunc: splitV$2\n},\n    SQRT$1 = \"return sqrt(x);\",\n    sqrt$3 = unaryKernelFunc$2({\n  opSnippet: SQRT$1\n}),\n    sqrtConfig$2 = {\n  kernelName: Sqrt$1,\n  backendName: \"webgl\",\n  kernelFunc: sqrt$3\n},\n    SQUARE$1 = \"return x * x;\",\n    square$3 = unaryKernelFunc$2({\n  opSnippet: SQUARE$1\n}),\n    squareConfig$2 = {\n  kernelName: Square$1,\n  backendName: \"webgl\",\n  kernelFunc: square$3\n},\n    SQUARED_DIFFERENCE$1 = \"return (a - b) * (a - b);\",\n    squaredDifference$3 = binaryKernelFunc$2({\n  opSnippet: SQUARED_DIFFERENCE$1,\n  packedOpSnippet: SQUARED_DIFFERENCE$1\n}),\n    squaredDifferenceConfig$2 = {\n  kernelName: SquaredDifference$1,\n  backendName: \"webgl\",\n  kernelFunc: squaredDifference$3\n};\n\nfunction step$3(_ref27) {\n  var {\n    inputs: e,\n    attrs: t,\n    backend: n\n  } = _ref27;\n  var {\n    x: r\n  } = e,\n      a = new UnaryOpProgram$1(r.shape, CHECK_NAN_SNIPPET$5 + \"\\n    return x > 0.0 ? 1.0 : float(\".concat(t.alpha, \");\\n  \"));\n  return n.runWebGLProgram(a, [r], r.dtype);\n}\n\nvar stepConfig$2 = {\n  kernelName: Step$1,\n  backendName: \"webgl\",\n  kernelFunc: step$3\n};\n\nclass StridedSliceProgram$1 {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.outputShape = n;\n    var r = n.length,\n        a = getCoordsDataType$1(n.length),\n        s = getCoordsDataType$1(n.length);\n    var o = \"\";\n    if (1 === r) o = \"coords * strides + begin\";else {\n      var _e500 = 0;\n      o = n.map((t, r) => (_e500++, 1 === n.length ? \"coords * strides[\".concat(r, \"] + begin[\").concat(r, \"]\") : \"coords[\".concat(_e500 - 1, \"] * strides[\").concat(r, \"] + begin[\").concat(r, \"]\"))).join(\",\");\n    }\n    this.userCode = \"\\n      \".concat(a, \" begin = \").concat(a, \"(\").concat(e, \");\\n      \").concat(a, \" strides = \").concat(a, \"(\").concat(t, \");\\n\\n      void main() {\\n        \").concat(s, \" coords = getOutputCoords();\\n        setOutput(getX(\").concat(o, \"));\\n      }\\n    \");\n  }\n\n}\n\nfunction stridedSlice$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    end: o,\n    strides: i,\n    beginMask: l,\n    endMask: u,\n    ellipsisMask: c,\n    newAxisMask: p,\n    shrinkAxisMask: d\n  } = r,\n      {\n    nonStrided: h,\n    $begin: m,\n    $strides: f,\n    size: g,\n    newShape: $,\n    outShape: y\n  } = sliceInfo$1(a.shape, s, o, i, l, u, c, p, d),\n      b = reshape$4({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: $\n    }\n  });\n  var x;\n\n  if (h) {\n    var _e501 = slice$3({\n      inputs: {\n        x: b\n      },\n      backend: n,\n      attrs: {\n        begin: m,\n        size: g\n      }\n    });\n\n    x = reshape$4({\n      inputs: {\n        x: _e501\n      },\n      backend: n,\n      attrs: {\n        shape: y\n      }\n    }), n.disposeIntermediateTensorInfo(_e501);\n  } else if (y.some(e => 0 === e)) x = n.makeTensorInfo(y, a.dtype, []);else if (n.shouldExecuteOnCPU([b])) {\n    var _e502 = n.texData.get(b.dataId),\n        _t352 = buffer$1(b.shape, b.dtype, _e502.values),\n        _r172 = stridedSliceImplCPU$1(y, _t352, f, m);\n\n    x = n.makeTensorInfo(y, b.dtype, _r172.values);\n  } else {\n    var _e503 = new StridedSliceProgram$1(m, f, y);\n\n    x = n.runWebGLProgram(_e503, [b], b.dtype);\n  }\n\n  var v = reshape$4({\n    inputs: {\n      x\n    },\n    backend: n,\n    attrs: {\n      shape: y\n    }\n  });\n  return n.disposeIntermediateTensorInfo(b), n.disposeIntermediateTensorInfo(x), v;\n}\n\nvar stridedSliceConfig$2 = {\n  kernelName: StridedSlice$1,\n  backendName: \"webgl\",\n  kernelFunc: stridedSlice$3\n};\n\nfunction stringNGrams$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    separator: a,\n    nGramWidths: s,\n    leftPad: o,\n    rightPad: i,\n    padWidth: l,\n    preserveShortSequences: u\n  } = r,\n      {\n    data: c,\n    dataSplits: p\n  } = t,\n      d = n.readSync(c.dataId),\n      h = n.readSync(p.dataId),\n      [m, f] = stringNGramsImplCPU$1(d, h, a, s, o, i, l, u);\n  return [n.makeTensorInfo([m.length], \"string\", m), n.makeTensorInfo(p.shape, \"int32\", f)];\n}\n\nvar stringNGramsConfig$2 = {\n  kernelName: StringNGrams$1,\n  backendName: \"webgl\",\n  kernelFunc: stringNGrams$3\n};\n\nfunction stringSplit$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    skipEmpty: a\n  } = r,\n      {\n    input: s,\n    delimiter: o\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (1 !== s.shape.length) throw new Error(\"Input must be a vector, got shape: \".concat(s.shape));\n  if (0 !== o.shape.length) throw new Error(\"Delimiter must be a scalar, got shape: \".concat(o.shape));\n  var i = n.readSync(s.dataId),\n      l = n.readSync(o.dataId)[0],\n      [u, c, p] = stringSplitImplCPU$1(i, l, a),\n      d = c.length;\n  return [n.makeTensorInfo([d, 2], \"int32\", u), n.makeTensorInfo([d], \"string\", c), n.makeTensorInfo([2], \"int32\", new Int32Array(p))];\n}\n\nvar stringSplitConfig$2 = {\n  kernelName: StringSplit$1,\n  backendName: \"webgl\",\n  kernelFunc: stringSplit$3\n};\n\nfunction stringToHashBucketFast$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    numBuckets: a\n  } = r,\n      {\n    input: s\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (a <= 0) throw new Error(\"Number of buckets must be at least 1\");\n  var o = n.readSync(s.dataId),\n      i = stringToHashBucketFastImplCPU$1(o, a);\n  return n.makeTensorInfo(s.shape, \"int32\", i);\n}\n\nvar stringToHashBucketFastConfig$2 = {\n  kernelName: StringToHashBucketFast$1,\n  backendName: \"webgl\",\n  kernelFunc: stringToHashBucketFast$3\n},\n    TAN$1 = \"return tan(x);\",\n    tan$3 = unaryKernelFunc$2({\n  opSnippet: TAN$1\n}),\n    tanConfig$2 = {\n  kernelName: Tan$1,\n  backendName: \"webgl\",\n  kernelFunc: tan$3\n},\n    TANH$1 = \"\\n  float e2x = exp(-2.0 * abs(x));\\n  return sign(x) * (1.0 - e2x) / (1.0 + e2x);\\n\",\n    tanh$4 = unaryKernelFunc$2({\n  opSnippet: TANH$1\n}),\n    tanhConfig$2 = {\n  kernelName: Tanh$3,\n  backendName: \"webgl\",\n  kernelFunc: tanh$4\n};\n\nclass TileProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"A\"];\n    var n = new Array(e.length);\n\n    for (var _r173 = 0; _r173 < n.length; _r173++) {\n      n[_r173] = e[_r173] * t[_r173];\n    }\n\n    this.outputShape = n, this.rank = n.length;\n    var r = getCoordsDataType$1(this.rank),\n        a = getSourceCoords$3(e);\n    this.userCode = \"\\n      void main() {\\n        \".concat(r, \" resRC = getOutputCoords();\\n        setOutput(getA(\").concat(a, \"));\\n      }\\n    \");\n  }\n\n}\n\nfunction getSourceCoords$3(e) {\n  var t = e.length;\n  if (t > 5) throw Error(\"Tile for rank \".concat(t, \" is not yet supported\"));\n  if (1 === t) return \"imod(resRC, \".concat(e[0], \")\");\n  var n = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\", \"resRC.u\"],\n      r = [];\n\n  for (var _t353 = 0; _t353 < e.length; _t353++) {\n    r.push(\"imod(\".concat(n[_t353], \", \").concat(e[_t353], \")\"));\n  }\n\n  return r.join();\n}\n\nfunction tile$4(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reps: s\n  } = r;\n\n  if (\"string\" === a.dtype || a.shape.length > 5) {\n    var _e504 = n.readSync(a.dataId),\n        _t354 = \"string\" === a.dtype ? _e504.map(e => decodeString$1(e)) : _e504,\n        _r174 = buffer$1(a.shape, a.dtype, _t354),\n        _o64 = tileImplCPU$1(_r174, s);\n\n    return n.makeTensorInfo(_o64.shape, _o64.dtype, _o64.values);\n  }\n\n  var o = new TileProgram$1(a.shape, s);\n  return n.runWebGLProgram(o, [a], a.dtype);\n}\n\nvar tileConfig$2 = {\n  kernelName: Tile$1,\n  backendName: \"webgl\",\n  kernelFunc: tile$4\n};\n\nclass SwapProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"x\", \"indices\"], this.customUniforms = [{\n      name: \"n\",\n      type: \"int\"\n    }, {\n      name: \"firstPass\",\n      type: \"int\"\n    }, {\n      name: \"negativeInf\",\n      type: \"float\"\n    }, {\n      name: \"dir\",\n      type: \"int\"\n    }, {\n      name: \"inc\",\n      type: \"int\"\n    }], this.outputShape = e, this.userCode = \"\\n       void main() {\\n         ivec2 coords = getOutputCoords();\\n         int batch = coords[0];\\n         int elemIdx = coords[1];\\n\\n         // We compare elements pair-wise within a group of size 2 * inc.\\n         // The comparing rule for each group alternates between ascending\\n         // and descending. Within each group, we compare each pair at\\n         // positions i and i+inc. To decide whether an element at position i\\n         // is x0 or x1, we mod it by 2 * inc, if the result is smaller than\\n         // inc, it is in the first half of the group, we denote it as x0,\\n         // otherwise we denote it as x1.\\n         // For example, as shown in the Bitonic top K paper referenced above,\\n         // Figure5(a) shows that element[1] is in the\\n         // second half of the group when group size is 2, but it is in the\\n         // first half of the group when group size is 4.\\n\\n         bool isFirstInPair = imod(elemIdx, 2 * inc) < inc;\\n         int i = isFirstInPair ? elemIdx : elemIdx - inc;\\n\\n         int i0 = firstPass == 1 ? i : int(getIndices(batch, i));\\n         int i1 = firstPass == 1 ? i + inc : int(getIndices(batch, i + inc));\\n         float x0 = i0 < n ? getX(batch, i0) : negativeInf;\\n         float x1 = i1 < n ? getX(batch, i1) : negativeInf;\\n\\n         // Denotes which direction indices are in (ascending or descending).\\n         bool reverse = imod(elemIdx, 2 * dir) >= dir;\\n         bool isGreater = x0 > x1 || (x0 == x1 && i1 > i0);\\n         if (reverse == isGreater) { // Elements in opposite order of direction\\n           int iTemp = i0;\\n           i0 = i1;\\n           i1 = iTemp;\\n         }\\n         if (isFirstInPair) {\\n            setOutput(float(i0));\\n         } else {\\n            setOutput(float(i1));\\n         }\\n       }\\n     \";\n  }\n\n}\n\nclass MergeProgram$1 {\n  constructor(e) {\n    this.variableNames = [\"x\", \"indices\"], this.customUniforms = [{\n      name: \"n\",\n      type: \"int\"\n    }, {\n      name: \"firstPass\",\n      type: \"int\"\n    }, {\n      name: \"k\",\n      type: \"int\"\n    }], this.outputShape = e, this.userCode = \"\\n    void main() {\\n         // Takes max of indices (0, k), (1, k + 1), (2, k + 2) ...\\n         ivec2 coords = getOutputCoords();\\n         int batch = coords[0];\\n         int elemIdx = coords[1];\\n\\n         // The output size is half of the previous size.\\n         // If the previous sequence is | | | | _ _ _ _  | | | |  _ _ _ _ (k=4),\\n         // we only need to output the indices at positions |, the indices at\\n         // positions _ can be thrown away, see Figure5(b) After Phase 2\\n         // (Merge phase) in the Bitonic Top K paper referenced above.\\n         // For example, the paper shows we only need to output the orange bars.\\n         // The output sequence should look like this | | | | | | | |.\\n         // Because the sequence is halved, to map the output index back\\n         // to the previous sequence to find the corresponding value,\\n         // we need to double the index. When we double the index,\\n         // we basically interpolate a position, so 2i looks like\\n         // | _ | _ | _ | _ | _ | _ | _. We move the | to the first k position\\n         // of each 2k positions by - elemIdx % k. E.g. for output at\\n         // index 4,5,6,7, we want to get the corresponding element at\\n         // original index 8,9,10,11, for output at index 8,9,10,11,\\n         // we want to get the corresponding element at original index\\n         // 16,17,18,19, so on and so forth.\\n\\n         int i = elemIdx < k ? elemIdx : (elemIdx * 2 - imod(elemIdx, k));\\n         int i0 = firstPass == 1 ? i : int(getIndices(batch, i));\\n         int i1 = firstPass == 1 ? i + k : int(getIndices(batch, i + k));\\n\\n         float x0 = getX(batch, i0);\\n         float x1 = i1 < n ? getX(batch, i1) : x0;\\n\\n         setOutput(x0 >= x1 ? float(i0) : float(i1));\\n       }\\n     \";\n  }\n\n}\n\nfunction disposeIntermediateTensorInfoOrNull$1(e, t) {\n  null !== t && e.disposeIntermediateTensorInfo(t);\n}\n\nfunction roundUpToPow2$1(e) {\n  var t = 1;\n\n  for (; t < e;) {\n    t *= 2;\n  }\n\n  return t;\n}\n\nfunction topK$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    k: s,\n    sorted: o\n  } = r,\n      i = env$1().getNumber(\"TOPK_LAST_DIM_CPU_HANDOFF_SIZE_THRESHOLD\"),\n      l = env$1().getNumber(\"TOPK_K_CPU_HANDOFF_THRESHOLD\"),\n      u = a.shape,\n      c = u[u.length - 1];\n\n  if (n.shouldExecuteOnCPU([a]) || c < i || s > l) {\n    var _e505 = n.readSync(a.dataId),\n        [_t355, _r175] = topKImplCPU$1(_e505, u, a.dtype, s, o);\n\n    return [n.makeTensorInfo(_t355.shape, _t355.dtype, _t355.values), n.makeTensorInfo(_r175.shape, _r175.dtype, _r175.values)];\n  }\n\n  if (0 === s) return u[u.length - 1] = 0, [n.makeTensorInfo(u, a.dtype, []), n.makeTensorInfo(u, \"int32\", [])];\n  if (1 === c) return [a, fill$3({\n    attrs: {\n      shape: u,\n      dtype: \"int32\",\n      value: 0\n    },\n    backend: n\n  })];\n  var p = n.texData.get(a.dataId),\n      d = null !== p && p.isPacked,\n      h = d ? n.unpackTensor(a) : a,\n      m = sizeFromShape$1(u) / c,\n      f = reshape$4({\n    inputs: {\n      x: h\n    },\n    attrs: {\n      shape: [m, c]\n    },\n    backend: n\n  });\n  d && disposeIntermediateTensorInfoOrNull$1(n, h);\n  var g = roundUpToPow2$1(s),\n      $ = roundUpToPow2$1(c);\n  var y = null;\n\n  var b = () => null === y ? [f, f] : [f, y],\n      x = (e, t, r) => {\n    var a = b(),\n        s = new SwapProgram$1(r),\n        o = y;\n    y = n.runWebGLProgram(s, a, \"int32\", [[c], [null === y ? 1 : 0], [Number.NEGATIVE_INFINITY], [e], [t]]), disposeIntermediateTensorInfoOrNull$1(n, o);\n  };\n\n  for (var _e506 = 1; _e506 < g; _e506 *= 2) {\n    var _t356 = 2 * _e506;\n\n    for (var _n216 = _e506; _n216 >= 1; _n216 /= 2) {\n      x(_t356, _n216, [m, $]);\n    }\n  }\n\n  for (var _e507 = $; _e507 > g; _e507 /= 2) {\n    var _t357 = b(),\n        _r176 = new MergeProgram$1([m, _e507 / 2]),\n        _a117 = y;\n\n    y = n.runWebGLProgram(_r176, _t357, \"int32\", [[c], [null === y ? 1 : 0], [g]]), disposeIntermediateTensorInfoOrNull$1(n, _a117);\n\n    var _s89 = g / 2,\n        _o65 = 2 * _s89;\n\n    for (var _e508 = _s89; _e508 >= 1; _e508 /= 2) {\n      x(_o65, _e508, y.shape);\n    }\n  }\n\n  var v = y;\n  y = slice$3({\n    inputs: {\n      x: y\n    },\n    backend: n,\n    attrs: {\n      begin: 0,\n      size: [m, s]\n    }\n  }), disposeIntermediateTensorInfoOrNull$1(n, v);\n  var I = gatherV2$2({\n    inputs: {\n      x: f,\n      indices: y\n    },\n    backend: n,\n    attrs: {\n      axis: 1,\n      batchDims: 1\n    }\n  });\n  disposeIntermediateTensorInfoOrNull$1(n, f);\n  var C = u.slice(0, -1);\n  C.push(s), v = y, y = reshape$4({\n    inputs: {\n      x: y\n    },\n    attrs: {\n      shape: C\n    },\n    backend: n\n  }), disposeIntermediateTensorInfoOrNull$1(n, v);\n  var S = I;\n  return I = reshape$4({\n    inputs: {\n      x: I\n    },\n    attrs: {\n      shape: C\n    },\n    backend: n\n  }), disposeIntermediateTensorInfoOrNull$1(n, S), [I, y];\n}\n\nvar topKConfig$2 = {\n  kernelName: TopK$1,\n  backendName: \"webgl\",\n  kernelFunc: topK$2\n};\n\nclass TransformProgram$1 {\n  constructor(e, t, n, r, a, s) {\n    this.variableNames = [\"Image\", \"Transforms\"], this.outputShape = s;\n    var o = \"nearest\" === n ? 1 : 2;\n    var i;\n\n    switch (r) {\n      case \"constant\":\n        i = 1;\n        break;\n\n      case \"reflect\":\n        i = 2;\n        break;\n\n      case \"wrap\":\n        i = 3;\n        break;\n\n      case \"nearest\":\n        i = 4;\n        break;\n\n      default:\n        i = 1;\n    }\n\n    this.userCode = \"\\n            float mapCoord(float outCoord, float len) {\\n              float inCoord = outCoord;\\n              if(\".concat(i, \" == 2) {\\n                if (inCoord < 0.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz2 = 2.0 * len;\\n                    if (inCoord < sz2) {\\n                      inCoord = sz2 * float(int(float(-inCoord / sz2))) +\\n                      inCoord;\\n                    }\\n                    inCoord = inCoord < -len ? inCoord + sz2 : -inCoord - 1.0;\\n                  }\\n                } else if (inCoord > len - 1.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz2 = 2.0 * len;\\n                    inCoord -= sz2 * float(int(float(inCoord / sz2)));\\n                    if (inCoord >= len) {\\n                      inCoord = sz2 - inCoord - 1.0;\\n                    }\\n                  }\\n                }\\n                return clamp(inCoord, 0.0, len - 1.0);\\n              } else if (\").concat(i, \" == 3) {\\n                if (inCoord < 0.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz = len - 1.0;\\n                    inCoord += len * (float(int(float(-inCoord / sz))) + 1.0);\\n                  }\\n                } else if (inCoord > len - 1.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz = len - 1.0;\\n                    inCoord -= len * float(int(float(inCoord / sz)));\\n                  }\\n                }\\n                return clamp(inCoord, 0.0, len - 1.0);\\n              } else if (\").concat(i, \" == 4) {\\n                return clamp(outCoord, 0.0, len - 1.0);\\n              } else {\\n                return outCoord;\\n              }\\n            }\\n\\n            float readWithFillValue(int batch, int coordY, int coordX,\\n              int channel) {\\n              float outputValue;\\n              if (0 <= coordY && coordY < \").concat(e, \" && 0 <= coordX && coordX < \").concat(t, \") {\\n                  outputValue = getImage(batch, coordY, coordX, channel);\\n              } else {\\n                outputValue = float(\").concat(a, \");\\n              }\\n              return outputValue;\\n            }\\n\\n            void main() {\\n              ivec4 coords = getOutputCoords();\\n              float outputValue;\\n              int batch = coords[0];\\n              int x = coords[2];\\n              int y = coords[1];\\n              int channel = coords[3];\\n              float xf = float(x);\\n              float yf = float(y);\\n              float a1 = getTransforms(batch, 0);\\n              float a2 = getTransforms(batch, 1);\\n              float a3 = getTransforms(batch, 2);\\n              float b1 = getTransforms(batch, 3);\\n              float b2 = getTransforms(batch, 4);\\n              float b3 = getTransforms(batch, 5);\\n              float c1 = getTransforms(batch, 6);\\n              float c2 = getTransforms(batch, 7);\\n              float projection = c1 * xf + c2 * yf + 1.0;\\n              if (projection == 0.0) {\\n                outputValue = float(\").concat(a, \");\\n              } else {\\n                float inX = (a1 * xf + a2 * yf + a3) / projection;\\n                float inY = (b1 * xf + b2 * yf + b3) / projection;\\n                float mapX = mapCoord(inX, float(\").concat(t, \"));\\n                float mapY = mapCoord(inY, float(\").concat(e, \"));\\n\\n                if (\").concat(o, \" == 1) {\\n                  int coordY = int(round(mapY));\\n                  int coordX = int(round(mapX));\\n                  outputValue = readWithFillValue(batch, coordY, coordX,\\n                    channel);\\n                } else {\\n                  float yFloor = floor(mapY);\\n                  float xFloor = floor(mapX);\\n                  float yCeil = yFloor + 1.0;\\n                  float xCeil = xFloor + 1.0;\\n                  float valueYFloor = (xCeil - mapX) *\\n                  readWithFillValue(batch, int(yFloor), int(xFloor), channel) +\\n                  (mapX - xFloor) *\\n                  readWithFillValue(batch, int(yFloor), int(xCeil), channel);\\n                  float valueYCeil = (xCeil - mapX) *\\n                  readWithFillValue(batch, int(yCeil), int(xFloor), channel) +\\n                  (mapX - xFloor) *\\n                  readWithFillValue(batch, int(yCeil), int(xCeil), channel);\\n                  outputValue = (yCeil - mapY) * valueYFloor +\\n                  (mapY - yFloor) * valueYCeil;\\n                }\\n              }\\n              setOutput(outputValue);\\n            }\\n        \");\n  }\n\n}\n\nfunction transform$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    image: a,\n    transforms: s\n  } = t,\n      {\n    interpolation: o,\n    fillMode: i,\n    fillValue: l,\n    outputShape: u\n  } = r,\n      [c, p, d, h] = a.shape,\n      [m, f] = null != u ? u : [p, d],\n      g = new TransformProgram$1(p, d, o, i, l, [c, m, f, h]);\n  return n.runWebGLProgram(g, [a, s], \"float32\");\n}\n\nvar transformConfig$2 = {\n  kernelName: Transform$1,\n  backendName: \"webgl\",\n  kernelFunc: transform$3\n};\n\nfunction unique$4(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    axis: a\n  } = n,\n      {\n    x: s\n  } = t;\n  assertNotComplex$2(s, \"unique\"), console.warn(\"WARNING: \", \"UI might be locked temporarily as data is being downloaded\");\n  var o = r.readSync(s.dataId),\n      {\n    outputValues: i,\n    outputShape: l,\n    indices: u\n  } = uniqueImplCPU$1(o, a, s.shape, s.dtype);\n  return [r.makeTensorInfo(l, s.dtype, i), r.makeTensorInfo([u.length], \"int32\", u)];\n}\n\nvar uniqueConfig$2 = {\n  kernelName: Unique$1,\n  backendName: \"webgl\",\n  kernelFunc: unique$4\n};\n\nfunction unpack$2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    value: a\n  } = t;\n  var {\n    axis: s\n  } = r;\n  s < 0 && (s += a.shape.length);\n  var o = a,\n      i = o.shape.length,\n      l = a.shape[s],\n      u = new Array(i - 1);\n  var c = 0;\n\n  for (var _e509 = 0; _e509 < i; _e509++) {\n    _e509 !== s && (u[c++] = o.shape[_e509]);\n  }\n\n  var p = [],\n      d = new Array(i).fill(0),\n      h = o.shape.slice();\n  h[s] = 1;\n  var m = new Array(l);\n\n  for (var _e510 = 0; _e510 < m.length; _e510++) {\n    d[s] = _e510;\n\n    var _t358 = slice$3({\n      inputs: {\n        x: o\n      },\n      backend: n,\n      attrs: {\n        begin: d,\n        size: h\n      }\n    }),\n        _r177 = reshape$4({\n      inputs: {\n        x: _t358\n      },\n      backend: n,\n      attrs: {\n        shape: u\n      }\n    });\n\n    m[_e510] = _r177, p.push(_t358);\n  }\n\n  return p.forEach(e => n.disposeIntermediateTensorInfo(e)), m;\n}\n\nvar unpackConfig$2 = {\n  kernelName: Unpack$1,\n  backendName: \"webgl\",\n  kernelFunc: unpack$2\n};\n\nclass SegmentOpProgram$1 {\n  constructor(e, t) {\n    this.variableNames = [\"x\", \"segmentIds\"];\n    var n = e.windowSize,\n        r = e.batchSize,\n        a = e.inSize,\n        s = e.numSegments,\n        o = s * Math.ceil(a / n);\n    this.outputShape = [r, o];\n    var i = 4 * Math.floor(n / 4),\n        l = n % 4,\n        u = \"\\n        sumValue += dot(values, segFilter);\\n    \";\n    var c = \"\";\n    a % n > 0 && (c = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return initializationValue;\\n        }\\n      \"));\n    var p = \"\";\n    a % n > 0 && (p = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return -1.0;\\n        }\\n      \")), this.userCode = \"\\n      const float initializationValue = 0.0;\\n\\n      float getValue(int batch, int inIdx) {\\n        \".concat(c, \"\\n        return getX(batch, inIdx);\\n      }\\n\\n      float getSegmentIdAtIndex(int inIdx) {\\n        \").concat(p, \"\\n        return getSegmentIds(inIdx);\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = int(floor(float(outIdx) / float(\\n          \").concat(s, \")) * float(\").concat(n, \"));\\n        int currentSeg = int(mod(float(outIdx), float(\").concat(s, \")));\\n\\n        float sumValue = 0.0;\\n\\n        for (int i = 0; i < \").concat(i, \"; i += 4) {\\n          int inIdx = inOffset + i;\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            getValue(batch, inIdx + 3)\\n          );\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 1)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 2)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 3)) == currentSeg ? 1 : 0\\n          );\\n\\n          \").concat(u, \"\\n        }\\n\\n        int inIdx = inOffset + \").concat(i, \";\\n        if (\").concat(1 === l, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            initializationValue,\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          int inIdxSeg = int(getSegmentIdAtIndex(inIdx));\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            0,\\n            0,\\n            0\\n          );\\n\\n          \").concat(u, \"\\n        } else if (\").concat(2 === l, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 1)) == currentSeg ? 1 : 0,\\n              0,\\n              0\\n          );\\n\\n          \").concat(u, \"\\n        } else if (\").concat(3 === l, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            initializationValue\\n          );\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 1)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 2)) == currentSeg ? 1 : 0,\\n            0\\n          );\\n\\n          \").concat(u, \"\\n        }\\n        setOutput(sumValue);\\n      }\\n    \");\n  }\n\n}\n\nfunction unsortedSegmentSum$3(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    segmentIds: s\n  } = t,\n      {\n    numSegments: o\n  } = r,\n      i = a.shape.length,\n      l = [];\n  var u = 0;\n  var c = getAxesPermutation$1([u], i);\n  var p = a;\n  null != c && (p = transpose$3({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), l.push(p), u = getInnerMostAxes$1(1, i)[0]);\n  var d = computeOutShape$3(p.shape, u, o),\n      h = sizeFromShape$1([p.shape[u]]),\n      m = reshape$4({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, h]\n    }\n  });\n  l.push(m);\n\n  var f = sumOutType$1(a.dtype),\n      g = (e, t, r, a, s) => {\n    var o = e.shape[0],\n        i = e.shape[1],\n        u = segOpComputeOptimalWindowSize$1(i, s),\n        c = new SegmentOpProgram$1({\n      windowSize: u,\n      inSize: i,\n      batchSize: o,\n      numSegments: s\n    }, t),\n        p = n.compileAndRun(c, [e, r], a);\n    if (l.push(p), p.shape[1] === s) return p;\n    var d = range$5({\n      backend: n,\n      attrs: {\n        start: 0,\n        stop: s,\n        step: 1,\n        dtype: \"float32\"\n      }\n    }),\n        h = tile$4({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        reps: [i / u]\n      }\n    });\n    return l.push(d), l.push(h), g(p, t, h, a, s);\n  },\n      $ = reshape$4({\n    inputs: {\n      x: g(m, \"unsortedSegmentSum\", s, f, o)\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  });\n\n  var y = $;\n\n  if (null != c) {\n    l.push($);\n\n    var _e511 = getUndoAxesPermutation$1(c);\n\n    y = transpose$3({\n      inputs: {\n        x: y\n      },\n      backend: n,\n      attrs: {\n        perm: _e511\n      }\n    });\n  }\n\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), y;\n}\n\nvar unsortedSegmentSumConfig$2 = {\n  kernelName: UnsortedSegmentSum$1,\n  backendName: \"webgl\",\n  kernelFunc: unsortedSegmentSum$3\n},\n    kernelConfigs$2 = [LRNConfig$1, LRNGradConfig$1, _fusedMatMulConfig$2, absConfig$2, acosConfig$2, acoshConfig$2, addConfig$2, addNConfig$2, allConfig$2, anyConfig$2, argMaxConfig$2, argMinConfig$2, asinConfig$2, asinhConfig$2, atan2Config$2, atanConfig$2, atanhConfig$2, avgPool3DConfig$2, avgPoolConfig$2, avgPoolGrad3DConfig$1, avgPoolGradConfig$3, batchMatMulConfig$2, batchNormConfig$2, batchToSpaceNDConfig$2, bincountConfig$2, castConfig$2, ceilConfig$2, clipByValueConfig$1, complexAbsConfig$2, complexConfig$2, concatConfig$2, conv2DBackpropFilterConfig$2, conv2DBackpropInputConfig$2, conv2DConfig$2, conv3DBackpropFilterV2Config$2, conv3DBackpropInputConfig$1, conv3DConfig$2, cosConfig$2, coshConfig$2, cropAndResizeConfig$2, cumsumConfig$2, denseBincountConfig$2, depthToSpaceConfig$2, depthwiseConv2dNativeBackpropFilterConfig$2, depthwiseConv2dNativeBackpropInputConfig$2, depthwiseConv2dNativeConfig$2, diagConfig$2, dilation2DConfig$1, einsumConfig$2, eluConfig$2, eluGradConfig$3, equalConfig$2, erfConfig$2, expConfig$2, expandDimsConfig$2, expm1Config$2, fftConfig$2, fillConfig$2, flipLeftRightConfig$2, floorConfig$2, floorDivConfig$2, fromPixelsConfig$1, fusedConv2DConfig$2, fusedDepthwiseConv2DConfig$2, gatherNdConfig$2, gatherV2Config$2, greaterConfig$2, greaterEqualConfig$2, identityConfig$2, ifftConfig$2, imagConfig$2, isFiniteConfig$2, isInfConfig$2, isNaNConfig$2, leakyReluConfig$2, lessConfig$2, lessEqualConfig$2, linSpaceConfig$2, log1pConfig$2, logConfig$2, logicalAndConfig$2, logicalNotConfig$2, logicalOrConfig$2, maxConfig$2, maxPool3DConfig$2, maxPoolConfig$2, maxPoolGrad3DConfig$1, maxPoolGradConfig$3, maxPoolWithArgmaxConfig$2, maximumConfig$2, meanConfig$2, minConfig$2, minimumConfig$2, mirrorPadConfig$2, modConfig$2, multinomialConfig$2, multiplyConfig$2, negConfig$2, nonMaxSuppressionV3Config$2, nonMaxSuppressionV4Config$2, nonMaxSuppressionV5Config$2, notEqualConfig$2, oneHotConfig$2, onesLikeConfig$2, packConfig$2, padV2Config$2, powConfig$2, preluConfig$2, prodConfig$2, rangeConfig$2, realConfig$2, realDivConfig$2, reciprocalConfig$2, relu6Config$2, reluConfig$2, reshapeConfig$2, resizeBilinearConfig$2, resizeBilinearGradConfig$3, resizeNearestNeighborConfig$2, resizeNearestNeighborGradConfig$3, reverseConfig$2, rotateWithOffsetConfig$2, roundConfig$2, rsqrtConfig$2, scatterNdConfig$2, selectConfig$2, seluConfig$2, sigmoidConfig$2, signConfig$2, sinConfig$2, sinhConfig$2, sliceConfig$2, softmaxConfig$2, softplusConfig$2, spaceToBatchNDConfig$2, sparseFillEmptyRowsConfig$2, sparseReshapeConfig$2, sparseSegmentMeanConfig$2, sparseSegmentSumConfig$2, sparseToDenseConfig$2, splitVConfig$2, sqrtConfig$2, squareConfig$2, squaredDifferenceConfig$2, stepConfig$2, stridedSliceConfig$2, stringNGramsConfig$2, stringSplitConfig$2, stringToHashBucketFastConfig$2, subConfig$2, sumConfig$2, tanConfig$2, tanhConfig$2, tileConfig$2, topKConfig$2, transformConfig$2, transposeConfig$2, uniqueConfig$2, unpackConfig$2, unsortedSegmentSumConfig$2, zerosLikeConfig$2];\n\nfor (var _e512 of kernelConfigs$2) {\n  registerKernel$1(_e512);\n}\n\nvar version$8 = \"3.8.0\",\n    EPSILON_FLOAT32$1 = 1e-7,\n    EPSILON_FLOAT16$1 = 1e-4;\n\nclass DataStorage {\n  constructor(e, t) {\n    this.backend = e, this.dataMover = t, this.data = new WeakMap(), this.dataIdsCount = 0;\n  }\n\n  get(e) {\n    return this.data.has(e) || this.dataMover.moveData(this.backend, e), this.data.get(e);\n  }\n\n  set(e, t) {\n    this.dataIdsCount++, this.data.set(e, t);\n  }\n\n  has(e) {\n    return this.data.has(e);\n  }\n\n  delete(e) {\n    return this.dataIdsCount--, this.data.delete(e);\n  }\n\n  numDataIds() {\n    return this.dataIdsCount;\n  }\n\n}\n\nclass KernelBackend {\n  refCount(e) {\n    return notYetImplemented(\"refCount\");\n  }\n\n  incRef(e) {\n    return notYetImplemented(\"incRef\");\n  }\n\n  timerAvailable() {\n    return !0;\n  }\n\n  time(e) {\n    return notYetImplemented(\"time\");\n  }\n\n  read(e) {\n    return notYetImplemented(\"read\");\n  }\n\n  readSync(e) {\n    return notYetImplemented(\"readSync\");\n  }\n\n  numDataIds() {\n    return notYetImplemented(\"numDataIds\");\n  }\n\n  disposeData(e, t) {\n    return notYetImplemented(\"disposeData\");\n  }\n\n  write(e, t, n) {\n    return notYetImplemented(\"write\");\n  }\n\n  move(e, t, n, r, a) {\n    return notYetImplemented(\"move\");\n  }\n\n  memory() {\n    return notYetImplemented(\"memory\");\n  }\n\n  floatPrecision() {\n    return notYetImplemented(\"floatPrecision\");\n  }\n\n  epsilon() {\n    return 32 === this.floatPrecision() ? EPSILON_FLOAT32$1 : EPSILON_FLOAT16$1;\n  }\n\n  dispose() {\n    return notYetImplemented(\"dispose\");\n  }\n\n}\n\nfunction notYetImplemented(e) {\n  throw new Error(\"'\".concat(e, \"' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen\"));\n}\n\nfunction shuffle(e) {\n  var t = e.length,\n      n = 0;\n\n  for (; t > 0;) {\n    n = Math.random() * t | 0, t--, swap(e, t, n);\n  }\n}\n\nfunction shuffleCombo(e, t) {\n  if (e.length !== t.length) throw new Error(\"Array sizes must match to be shuffled together First array length was \".concat(e.length, \"Second array length was \").concat(t.length));\n  var n = e.length,\n      r = 0;\n\n  for (; n > 0;) {\n    r = Math.random() * n | 0, n--, swap(e, n, r), swap(t, n, r);\n  }\n}\n\nfunction clamp(e, t, n) {\n  return Math.max(e, Math.min(t, n));\n}\n\nfunction nearestLargerEven(e) {\n  return e % 2 == 0 ? e : e + 1;\n}\n\nfunction swap(e, t, n) {\n  var r = e[t];\n  e[t] = e[n], e[n] = r;\n}\n\nfunction sum$3(e) {\n  var t = 0;\n\n  for (var n = 0; n < e.length; n++) {\n    t += e[n];\n  }\n\n  return t;\n}\n\nfunction randUniform(e, t) {\n  var n = Math.random();\n  return t * n + (1 - n) * e;\n}\n\nfunction distSquared(e, t) {\n  var n = 0;\n\n  for (var r = 0; r < e.length; r++) {\n    var a = Number(e[r]) - Number(t[r]);\n    n += a * a;\n  }\n\n  return n;\n}\n\nfunction assert$4(e, t) {\n  if (!e) throw new Error(\"string\" == typeof t ? t : t());\n}\n\nfunction assertShapesMatch(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"\";\n  assert$4(arraysEqual(e, t), () => n + \" Shapes \".concat(e, \" and \").concat(t, \" must match\"));\n}\n\nfunction assertNonNull(e) {\n  assert$4(null != e, () => \"The input to the tensor constructor must be a non-null value.\");\n}\n\nfunction flatten$3(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : [];\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  if (null == t && (t = []), Array.isArray(e) || isTypedArray(e) && !n) for (var r = 0; r < e.length; ++r) {\n    flatten$3(e[r], t, n);\n  } else t.push(e);\n  return t;\n}\n\nfunction sizeFromShape(e) {\n  if (0 === e.length) return 1;\n  var t = e[0];\n\n  for (var n = 1; n < e.length; n++) {\n    t *= e[n];\n  }\n\n  return t;\n}\n\nfunction isScalarShape(e) {\n  return 0 === e.length;\n}\n\nfunction arraysEqual(e, t) {\n  if (e === t) return !0;\n  if (null == e || null == t) return !1;\n  if (e.length !== t.length) return !1;\n\n  for (var n = 0; n < e.length; n++) {\n    if (e[n] !== t[n]) return !1;\n  }\n\n  return !0;\n}\n\nfunction isInt(e) {\n  return e % 1 == 0;\n}\n\nfunction tanh$3(e) {\n  if (null != Math.tanh) return Math.tanh(e);\n  if (Infinity === e) return 1;\n  if (-Infinity === e) return -1;\n  {\n    var t = Math.exp(2 * e);\n    return (t - 1) / (t + 1);\n  }\n}\n\nfunction sizeToSquarishShape(e) {\n  var t = Math.ceil(Math.sqrt(e));\n  return [t, Math.ceil(e / t)];\n}\n\nfunction createShuffledIndices(e) {\n  var t = new Uint32Array(e);\n\n  for (var n = 0; n < e; ++n) {\n    t[n] = n;\n  }\n\n  return shuffle(t), t;\n}\n\nfunction rightPad(e, t) {\n  return t <= e.length ? e : e + \" \".repeat(t - e.length);\n}\n\nfunction repeatedTry(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e => 0;\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  return new Promise((r, a) => {\n    var s = 0;\n\n    var o = () => {\n      if (e()) return void r();\n      s++;\n      var i = t(s);\n      null != n && s >= n ? a() : setTimeout(o, i);\n    };\n\n    o();\n  });\n}\n\nfunction inferFromImplicitShape(e, t) {\n  var n = 1,\n      r = -1;\n\n  for (var _t359 = 0; _t359 < e.length; ++_t359) {\n    if (e[_t359] >= 0) n *= e[_t359];else if (-1 === e[_t359]) {\n      if (-1 !== r) throw Error(\"Shapes can only have 1 implicit size. Found -1 at dim \".concat(r, \" and dim \").concat(_t359));\n      r = _t359;\n    } else if (e[_t359] < 0) throw Error(\"Shapes can not be < 0. Found \".concat(e[_t359], \" at dim \").concat(_t359));\n  }\n\n  if (-1 === r) {\n    if (t > 0 && t !== n) throw Error(\"Size(\".concat(t, \") must match the product of shape \").concat(e));\n    return e;\n  }\n\n  if (0 === n) throw Error(\"Cannot infer the missing size in [\".concat(e, \"] when there are 0 elements\"));\n  if (t % n != 0) throw Error(\"The implicit shape can't be a fractional number. Got \".concat(t, \" / \").concat(n));\n  var a = e.slice();\n  return a[r] = t / n, a;\n}\n\nfunction parseAxisParam(e, t) {\n  var n = t.length;\n  return assert$4((e = null == e ? t.map((e, t) => t) : [].concat(e)).every(e => e >= -n && e < n), () => \"All values in axis param must be in range [-\".concat(n, \", \").concat(n, \") but got axis \").concat(e)), assert$4(e.every(e => isInt(e)), () => \"All values in axis param must be integers but got axis \".concat(e)), e.map(e => e < 0 ? n + e : e);\n}\n\nfunction squeezeShape(e, t) {\n  var n = [],\n      r = [],\n      a = null != t && Array.isArray(t) && 0 === t.length,\n      s = null == t || a ? null : parseAxisParam(t, e).sort();\n  var o = 0;\n\n  for (var _t360 = 0; _t360 < e.length; ++_t360) {\n    if (null != s) {\n      if (s[o] === _t360 && 1 !== e[_t360]) throw new Error(\"Can't squeeze axis \".concat(_t360, \" since its dim '\").concat(e[_t360], \"' is not 1\"));\n      (null == s[o] || s[o] > _t360) && 1 === e[_t360] && (n.push(e[_t360]), r.push(_t360)), s[o] <= _t360 && o++;\n    }\n\n    1 !== e[_t360] && (n.push(e[_t360]), r.push(_t360));\n  }\n\n  return {\n    newShape: n,\n    keptDims: r\n  };\n}\n\nfunction getTypedArrayFromDType(e, t) {\n  var n = null;\n  if (null == e || \"float32\" === e) n = new Float32Array(t);else if (\"int32\" === e) n = new Int32Array(t);else {\n    if (\"bool\" !== e) throw new Error(\"Unknown data type \".concat(e));\n    n = new Uint8Array(t);\n  }\n  return n;\n}\n\nfunction getArrayFromDType(e, t) {\n  var n = null;\n  if (null == e || \"float32\" === e) n = new Float32Array(t);else if (\"int32\" === e) n = new Int32Array(t);else if (\"bool\" === e) n = new Uint8Array(t);else {\n    if (\"string\" !== e) throw new Error(\"Unknown data type \".concat(e));\n    n = new Array(t);\n  }\n  return n;\n}\n\nfunction checkConversionForErrors(e, t) {\n  for (var n = 0; n < e.length; n++) {\n    var r = e[n];\n    if (isNaN(r) || !isFinite(r)) throw Error(\"A tensor of type \".concat(t, \" being uploaded contains \").concat(r, \".\"));\n  }\n}\n\nfunction isValidDtype(e) {\n  return \"bool\" === e || \"complex64\" === e || \"float32\" === e || \"int32\" === e || \"string\" === e;\n}\n\nfunction hasEncodingLoss(e, t) {\n  return !(\"complex64\" === t || \"float32\" === t && \"complex64\" !== e || \"int32\" === t && \"float32\" !== e && \"complex64\" !== e || \"bool\" === t && \"bool\" === e);\n}\n\nfunction isTypedArray(e) {\n  return e instanceof Float32Array || e instanceof Int32Array || e instanceof Uint8Array;\n}\n\nfunction bytesPerElement(e) {\n  if (\"float32\" === e || \"int32\" === e) return 4;\n  if (\"complex64\" === e) return 8;\n  if (\"bool\" === e) return 1;\n  throw new Error(\"Unknown dtype \".concat(e));\n}\n\nfunction bytesFromStringArray(e) {\n  if (null == e) return 0;\n  var t = 0;\n  return e.forEach(e => t += e.length), t;\n}\n\nfunction isString(e) {\n  return \"string\" == typeof e || e instanceof String;\n}\n\nfunction isBoolean(e) {\n  return \"boolean\" == typeof e;\n}\n\nfunction isNumber(e) {\n  return \"number\" == typeof e;\n}\n\nfunction inferDtype(e) {\n  return Array.isArray(e) ? inferDtype(e[0]) : e instanceof Float32Array ? \"float32\" : e instanceof Int32Array || e instanceof Uint8Array ? \"int32\" : isNumber(e) ? \"float32\" : isString(e) ? \"string\" : isBoolean(e) ? \"bool\" : \"float32\";\n}\n\nfunction isFunction(e) {\n  return !!(e && e.constructor && e.call && e.apply);\n}\n\nfunction nearestDivisor(e, t) {\n  for (var n = t; n < e; ++n) {\n    if (e % n == 0) return n;\n  }\n\n  return e;\n}\n\nfunction computeStrides(e) {\n  var t = e.length;\n  if (t < 2) return [];\n  var n = new Array(t - 1);\n  n[t - 2] = e[t - 1];\n\n  for (var r = t - 3; r >= 0; --r) {\n    n[r] = n[r + 1] * e[r + 1];\n  }\n\n  return n;\n}\n\nfunction createNestedArray(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = new Array();\n\n  if (1 === t.length) {\n    var s = t[0] * (r ? 2 : 1);\n\n    for (var _t361 = 0; _t361 < s; _t361++) {\n      a[_t361] = n[e + _t361];\n    }\n  } else {\n    var _s90 = t[0],\n        o = t.slice(1),\n        i = o.reduce((e, t) => e * t) * (r ? 2 : 1);\n\n    for (var _t362 = 0; _t362 < _s90; _t362++) {\n      a[_t362] = createNestedArray(e + _t362 * i, o, n, r);\n    }\n  }\n\n  return a;\n}\n\nfunction toNestedArray(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  if (0 === e.length) return t[0];\n  var r = e.reduce((e, t) => e * t) * (n ? 2 : 1);\n  if (0 === r) return [];\n  if (r !== t.length) throw new Error(\"[\".concat(e, \"] does not match the input size \").concat(t.length).concat(n ? \" for a complex tensor\" : \"\", \".\"));\n  return createNestedArray(0, e, t, n);\n}\n\nfunction makeOnesTypedArray(e, t) {\n  var n = makeZerosTypedArray(e, t);\n\n  for (var _e513 = 0; _e513 < n.length; _e513++) {\n    n[_e513] = 1;\n  }\n\n  return n;\n}\n\nfunction makeZerosTypedArray(e, t) {\n  if (null == t || \"float32\" === t || \"complex64\" === t) return new Float32Array(e);\n  if (\"int32\" === t) return new Int32Array(e);\n  if (\"bool\" === t) return new Uint8Array(e);\n  throw new Error(\"Unknown data type \".concat(t));\n}\n\nfunction makeZerosNestedTypedArray(e, t) {\n  var n = e.reduce((e, t) => e * t, 1);\n  if (null == t || \"float32\" === t) return toNestedArray(e, new Float32Array(n));\n  if (\"int32\" === t) return toNestedArray(e, new Int32Array(n));\n  if (\"bool\" === t) return toNestedArray(e, new Uint8Array(n));\n  throw new Error(\"Unknown data type \".concat(t));\n}\n\nfunction assertNonNegativeIntegerDimensions(e) {\n  e.forEach(t => {\n    assert$4(Number.isInteger(t) && t >= 0, () => \"Tensor must have a shape comprised of positive integers but got shape [\".concat(e, \"].\"));\n  });\n}\n\nfunction locToIndex(e, t, n) {\n  if (0 === t) return 0;\n  if (1 === t) return e[0];\n  var r = e[e.length - 1];\n\n  for (var _t363 = 0; _t363 < e.length - 1; ++_t363) {\n    r += n[_t363] * e[_t363];\n  }\n\n  return r;\n}\n\nfunction indexToLoc(e, t, n) {\n  if (0 === t) return [];\n  if (1 === t) return [e];\n  var r = new Array(t);\n\n  for (var _t364 = 0; _t364 < r.length - 1; ++_t364) {\n    r[_t364] = Math.floor(e / n[_t364]), e -= r[_t364] * n[_t364];\n  }\n\n  return r[r.length - 1] = e, r;\n}\n\nfunction isPromise(e) {\n  return e && e.then && \"function\" == typeof e.then;\n}\n\nvar TENSORFLOWJS_FLAGS_PREFIX = \"tfjsflags\";\n\nclass Environment {\n  constructor(e) {\n    this.global = e, this.flags = {}, this.flagRegistry = {}, this.urlFlags = {}, this.getQueryParams = getQueryParams, this.populateURLFlags();\n  }\n\n  setPlatform(e, t) {\n    null != this.platform && console.warn(\"Platform \".concat(this.platformName, \" has already been set. Overwriting the platform with \").concat(t, \".\")), this.platformName = e, this.platform = t;\n  }\n\n  registerFlag(e, t, n) {\n    if (this.flagRegistry[e] = {\n      evaluationFn: t,\n      setHook: n\n    }, null != this.urlFlags[e]) {\n      var _t365 = this.urlFlags[e];\n      console.warn(\"Setting feature override from URL \".concat(e, \": \").concat(_t365, \".\")), this.set(e, _t365);\n    }\n  }\n\n  getAsync(e) {\n    var _this76 = this;\n\n    return _asyncToGenerator(function* () {\n      return e in _this76.flags || (_this76.flags[e] = yield _this76.evaluateFlag(e)), _this76.flags[e];\n    })();\n  }\n\n  get(e) {\n    if (e in this.flags) return this.flags[e];\n    var t = this.evaluateFlag(e);\n    if (isPromise(t)) throw new Error(\"Flag \".concat(e, \" cannot be synchronously evaluated. Please use getAsync() instead.\"));\n    return this.flags[e] = t, this.flags[e];\n  }\n\n  getNumber(e) {\n    return this.get(e);\n  }\n\n  getBool(e) {\n    return this.get(e);\n  }\n\n  getFlags() {\n    return this.flags;\n  }\n\n  get features() {\n    return this.flags;\n  }\n\n  set(e, t) {\n    if (null == this.flagRegistry[e]) throw new Error(\"Cannot set flag \".concat(e, \" as it has not been registered.\"));\n    this.flags[e] = t, null != this.flagRegistry[e].setHook && this.flagRegistry[e].setHook(t);\n  }\n\n  evaluateFlag(e) {\n    if (null == this.flagRegistry[e]) throw new Error(\"Cannot evaluate flag '\".concat(e, \"': no evaluation function found.\"));\n    return this.flagRegistry[e].evaluationFn();\n  }\n\n  setFlags(e) {\n    this.flags = Object.assign({}, e);\n  }\n\n  reset() {\n    this.flags = {}, this.urlFlags = {}, this.populateURLFlags();\n  }\n\n  populateURLFlags() {\n    if (void 0 === this.global || void 0 === this.global.location || void 0 === this.global.location.search) return;\n    var e = this.getQueryParams(this.global.location.search);\n    TENSORFLOWJS_FLAGS_PREFIX in e && e[TENSORFLOWJS_FLAGS_PREFIX].split(\",\").forEach(e => {\n      var [t, n] = e.split(\":\");\n      this.urlFlags[t] = parseValue(t, n);\n    });\n  }\n\n}\n\nfunction getQueryParams(e) {\n  var t = {};\n  return e.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g, function (e) {\n    for (var _len5 = arguments.length, n = new Array(_len5 > 1 ? _len5 - 1 : 0), _key5 = 1; _key5 < _len5; _key5++) {\n      n[_key5 - 1] = arguments[_key5];\n    }\n\n    return decodeParam(t, n[0], n[1]), n.join(\"=\");\n  }), t;\n}\n\nfunction decodeParam(e, t, n) {\n  e[decodeURIComponent(t)] = decodeURIComponent(n || \"\");\n}\n\nfunction parseValue(e, t) {\n  if (\"true\" === (t = t.toLowerCase()) || \"false\" === t) return \"true\" === t;\n  if (\"\" + +t === t) return +t;\n  throw new Error(\"Could not parse value flag value \".concat(t, \" for flag \").concat(e, \".\"));\n}\n\nfunction env() {\n  return ENV$2;\n}\n\nvar ENV$2 = null,\n    globalNameSpace;\n\nfunction setEnvironmentGlobal(e) {\n  ENV$2 = e;\n}\n\nfunction getGlobalNamespace() {\n  if (null == globalNameSpace) {\n    var _e514;\n\n    if (\"undefined\" != typeof window) _e514 = window;else if (\"undefined\" != typeof global) _e514 = global;else if (\"undefined\" != typeof process) _e514 = process;else {\n      if (\"undefined\" == typeof self) throw new Error(\"Could not find a global object\");\n      _e514 = self;\n    }\n    globalNameSpace = _e514;\n  }\n\n  return globalNameSpace;\n}\n\nfunction getGlobalMap() {\n  var e = getGlobalNamespace();\n  return null == e._tfGlobals && (e._tfGlobals = new Map()), e._tfGlobals;\n}\n\nfunction getGlobal(e, t) {\n  var n = getGlobalMap();\n  if (n.has(e)) return n.get(e);\n  {\n    var r = t();\n    return n.set(e, r), n.get(e);\n  }\n}\n\nvar Abs = \"Abs\",\n    Acos = \"Acos\",\n    Acosh = \"Acosh\",\n    Add$1 = \"Add\",\n    AddN = \"AddN\",\n    All = \"All\",\n    Any = \"Any\",\n    ArgMax = \"ArgMax\",\n    ArgMin = \"ArgMin\",\n    Asin = \"Asin\",\n    Asinh = \"Asinh\",\n    Atan = \"Atan\",\n    Atanh = \"Atanh\",\n    Atan2 = \"Atan2\",\n    AvgPool = \"AvgPool\",\n    AvgPoolGrad = \"AvgPoolGrad\",\n    AvgPool3D = \"AvgPool3D\",\n    AvgPool3DGrad = \"AvgPool3DGrad\",\n    BatchMatMul = \"BatchMatMul\",\n    BatchToSpaceND = \"BatchToSpaceND\",\n    Bincount = \"Bincount\",\n    BroadcastTo = \"BroadcastTo\",\n    Cast = \"Cast\",\n    Ceil = \"Ceil\",\n    ClipByValue = \"ClipByValue\",\n    Complex = \"Complex\",\n    ComplexAbs = \"ComplexAbs\",\n    Concat = \"Concat\",\n    Conv2D$1 = \"Conv2D\",\n    Conv2DBackpropFilter = \"Conv2DBackpropFilter\",\n    Conv2DBackpropInput = \"Conv2DBackpropInput\",\n    Conv3D$1 = \"Conv3D\",\n    Conv3DBackpropFilterV2 = \"Conv3DBackpropFilterV2\",\n    Conv3DBackpropInputV2 = \"Conv3DBackpropInputV2\",\n    Cos = \"Cos\",\n    Cosh = \"Cosh\",\n    Cumsum = \"Cumsum\",\n    CropAndResize = \"CropAndResize\",\n    DenseBincount = \"DenseBincount\",\n    DepthToSpace = \"DepthToSpace\",\n    DepthwiseConv2dNative = \"DepthwiseConv2dNative\",\n    DepthwiseConv2dNativeBackpropFilter = \"DepthwiseConv2dNativeBackpropFilter\",\n    DepthwiseConv2dNativeBackpropInput = \"DepthwiseConv2dNativeBackpropInput\",\n    Diag = \"Diag\",\n    Dilation2D = \"Dilation2D\",\n    Dilation2DBackpropInput = \"Dilation2DBackpropInput\",\n    Dilation2DBackpropFilter = \"Dilation2DBackpropFilter\",\n    RealDiv = \"RealDiv\",\n    Einsum = \"Einsum\",\n    Elu$1 = \"Elu\",\n    EluGrad = \"EluGrad\",\n    Erf = \"Erf\",\n    Equal = \"Equal\",\n    Exp = \"Exp\",\n    ExpandDims = \"ExpandDims\",\n    Expm1 = \"Expm1\",\n    FFT = \"FFT\",\n    Fill = \"Fill\",\n    FlipLeftRight = \"FlipLeftRight\",\n    Floor = \"Floor\",\n    FloorDiv = \"FloorDiv\",\n    FusedBatchNorm = \"FusedBatchNorm\",\n    GatherV2 = \"GatherV2\",\n    GatherNd = \"GatherNd\",\n    Greater = \"Greater\",\n    GreaterEqual = \"GreaterEqual\",\n    Identity$1 = \"Identity\",\n    IFFT = \"IFFT\",\n    Imag = \"Imag\",\n    IsFinite = \"IsFinite\",\n    IsInf = \"IsInf\",\n    IsNan = \"IsNan\",\n    LeakyRelu = \"LeakyRelu\",\n    Less = \"Less\",\n    LessEqual = \"LessEqual\",\n    LinSpace = \"LinSpace\",\n    Log = \"Log\",\n    Log1p = \"Log1p\",\n    LogicalAnd = \"LogicalAnd\",\n    LogicalNot = \"LogicalNot\",\n    LogicalOr = \"LogicalOr\",\n    LogSoftmax$1 = \"LogSoftmax\",\n    LRN = \"LRN\",\n    LRNGrad = \"LRNGrad\",\n    Max = \"Max\",\n    Maximum$1 = \"Maximum\",\n    MaxPool = \"MaxPool\",\n    MaxPoolGrad = \"MaxPoolGrad\",\n    MaxPool3D = \"MaxPool3D\",\n    MaxPool3DGrad = \"MaxPool3DGrad\",\n    MaxPoolWithArgmax = \"MaxPoolWithArgmax\",\n    Mean = \"Mean\",\n    Min = \"Min\",\n    Minimum$1 = \"Minimum\",\n    MirrorPad = \"MirrorPad\",\n    Mod = \"Mod\",\n    Multinomial = \"Multinomial\",\n    Multiply$1 = \"Multiply\",\n    Neg = \"Neg\",\n    NotEqual = \"NotEqual\",\n    NonMaxSuppressionV3 = \"NonMaxSuppressionV3\",\n    NonMaxSuppressionV4 = \"NonMaxSuppressionV4\",\n    NonMaxSuppressionV5 = \"NonMaxSuppressionV5\",\n    OnesLike = \"OnesLike\",\n    OneHot = \"OneHot\",\n    Pack = \"Pack\",\n    PadV2 = \"PadV2\",\n    Pool = \"Pool\",\n    Pow = \"Pow\",\n    Prelu = \"Prelu\",\n    Prod = \"Prod\",\n    Range = \"Range\",\n    Real = \"Real\",\n    Reciprocal = \"Reciprocal\",\n    Relu$1 = \"Relu\",\n    Reshape$1 = \"Reshape\",\n    ResizeNearestNeighbor = \"ResizeNearestNeighbor\",\n    ResizeNearestNeighborGrad = \"ResizeNearestNeighborGrad\",\n    ResizeBilinear = \"ResizeBilinear\",\n    ResizeBilinearGrad = \"ResizeBilinearGrad\",\n    Relu6$1 = \"Relu6\",\n    Reverse = \"Reverse\",\n    Round = \"Round\",\n    Rsqrt = \"Rsqrt\",\n    ScatterNd = \"ScatterNd\",\n    Select = \"Select\",\n    Selu$1 = \"Selu\",\n    Slice = \"Slice\",\n    Sin = \"Sin\",\n    Sinh = \"Sinh\",\n    Sign = \"Sign\",\n    Sigmoid$1 = \"Sigmoid\",\n    Softplus$1 = \"Softplus\",\n    Sqrt = \"Sqrt\",\n    Sum = \"Sum\",\n    SpaceToBatchND = \"SpaceToBatchND\",\n    SplitV = \"SplitV\",\n    Softmax$2 = \"Softmax\",\n    SparseFillEmptyRows = \"SparseFillEmptyRows\",\n    SparseReshape = \"SparseReshape\",\n    SparseSegmentMean = \"SparseSegmentMean\",\n    SparseSegmentSum = \"SparseSegmentSum\",\n    SparseToDense = \"SparseToDense\",\n    SquaredDifference = \"SquaredDifference\",\n    Square = \"Square\",\n    StridedSlice = \"StridedSlice\",\n    StringNGrams = \"StringNGrams\",\n    StringSplit = \"StringSplit\",\n    StringToHashBucketFast = \"StringToHashBucketFast\",\n    Sub = \"Sub\",\n    Tan = \"Tan\",\n    Tanh$1 = \"Tanh\",\n    Tile = \"Tile\",\n    TopK = \"TopK\",\n    Transform = \"Transform\",\n    Transpose = \"Transpose\",\n    Unique = \"Unique\",\n    Unpack = \"Unpack\",\n    UnsortedSegmentSum = \"UnsortedSegmentSum\",\n    ZerosLike = \"ZerosLike\",\n    Step = \"Step\",\n    FromPixels = \"FromPixels\",\n    RotateWithOffset = \"RotateWithOffset\",\n    _FusedMatMul = \"_FusedMatMul\",\n    FusedConv2D = \"FusedConv2D\",\n    FusedDepthwiseConv2D = \"FusedDepthwiseConv2D\",\n    kernelRegistry = getGlobal(\"kernelRegistry\", () => new Map()),\n    gradRegistry = getGlobal(\"gradRegistry\", () => new Map());\n\nfunction getKernel(e, t) {\n  var n = makeKey(e, t);\n  return kernelRegistry.get(n);\n}\n\nfunction getGradient(e) {\n  return gradRegistry.get(e);\n}\n\nfunction getKernelsForBackend(e) {\n  var t = kernelRegistry.entries(),\n      n = [];\n\n  for (;;) {\n    var {\n      done: r,\n      value: a\n    } = t.next();\n    if (r) break;\n    var [s, o] = a,\n        [i] = s.split(\"_\");\n    i === e && n.push(o);\n  }\n\n  return n;\n}\n\nfunction registerKernel(e) {\n  var {\n    kernelName: t,\n    backendName: n\n  } = e,\n      r = makeKey(t, n);\n  kernelRegistry.has(r) && console.warn(\"The kernel '\".concat(t, \"' for backend '\").concat(n, \"' is already registered\")), kernelRegistry.set(r, e);\n}\n\nfunction registerGradient(e) {\n  var {\n    kernelName: t\n  } = e;\n  gradRegistry.has(t) && env().getBool(\"DEBUG\") && console.warn(\"Overriding the gradient for '\".concat(t, \"'\")), gradRegistry.set(t, e);\n}\n\nfunction unregisterKernel(e, t) {\n  var n = makeKey(e, t);\n  if (!kernelRegistry.has(n)) throw new Error(\"The kernel '\".concat(e, \"' for backend '\").concat(t, \"' is not registered\"));\n  kernelRegistry.delete(n);\n}\n\nfunction unregisterGradient(e) {\n  if (!gradRegistry.has(e)) throw new Error(\"The gradient '\".concat(e, \"' for backend is not registered\"));\n  gradRegistry.delete(e);\n}\n\nfunction copyRegisteredKernels(e, t) {\n  getKernelsForBackend(e).forEach(e => {\n    registerKernel(Object.assign({}, e, {\n      backendName: t\n    }));\n  });\n}\n\nfunction makeKey(e, t) {\n  return \"\".concat(t, \"_\").concat(e);\n}\n\nvar long = Long$1,\n    wasm = null;\n\ntry {\n  wasm = new WebAssembly.Instance(new WebAssembly.Module(new Uint8Array([0, 97, 115, 109, 1, 0, 0, 0, 1, 13, 2, 96, 0, 1, 127, 96, 4, 127, 127, 127, 127, 1, 127, 3, 7, 6, 0, 1, 1, 1, 1, 1, 6, 6, 1, 127, 1, 65, 0, 11, 7, 50, 6, 3, 109, 117, 108, 0, 1, 5, 100, 105, 118, 95, 115, 0, 2, 5, 100, 105, 118, 95, 117, 0, 3, 5, 114, 101, 109, 95, 115, 0, 4, 5, 114, 101, 109, 95, 117, 0, 5, 8, 103, 101, 116, 95, 104, 105, 103, 104, 0, 0, 10, 191, 1, 6, 4, 0, 35, 0, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 126, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 127, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 128, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 129, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11, 36, 1, 1, 126, 32, 0, 173, 32, 1, 173, 66, 32, 134, 132, 32, 2, 173, 32, 3, 173, 66, 32, 134, 132, 130, 34, 4, 66, 32, 135, 167, 36, 0, 32, 4, 167, 11])), {}).exports;\n} catch (e) {}\n\nfunction Long$1(e, t, n) {\n  this.low = 0 | e, this.high = 0 | t, this.unsigned = !!n;\n}\n\nfunction isLong(e) {\n  return !0 === (e && e.__isLong__);\n}\n\nObject.defineProperty(Long$1.prototype, \"__isLong__\", {\n  value: !0\n}), Long$1.isLong = isLong;\nvar INT_CACHE = {},\n    UINT_CACHE = {};\n\nfunction fromInt(e, t) {\n  var n, r, a;\n  return t ? (a = 0 <= (e >>>= 0) && e < 256) && (r = UINT_CACHE[e]) ? r : (n = fromBits(e, (0 | e) < 0 ? -1 : 0, !0), a && (UINT_CACHE[e] = n), n) : (a = -128 <= (e |= 0) && e < 128) && (r = INT_CACHE[e]) ? r : (n = fromBits(e, e < 0 ? -1 : 0, !1), a && (INT_CACHE[e] = n), n);\n}\n\nfunction fromNumber(e, t) {\n  if (isNaN(e)) return t ? UZERO : ZERO;\n\n  if (t) {\n    if (e < 0) return UZERO;\n    if (e >= TWO_PWR_64_DBL) return MAX_UNSIGNED_VALUE;\n  } else {\n    if (e <= -TWO_PWR_63_DBL) return MIN_VALUE;\n    if (e + 1 >= TWO_PWR_63_DBL) return MAX_VALUE;\n  }\n\n  return e < 0 ? fromNumber(-e, t).neg() : fromBits(e % TWO_PWR_32_DBL | 0, e / TWO_PWR_32_DBL | 0, t);\n}\n\nfunction fromBits(e, t, n) {\n  return new Long$1(e, t, n);\n}\n\nLong$1.fromInt = fromInt, Long$1.fromNumber = fromNumber, Long$1.fromBits = fromBits;\nvar pow_dbl = Math.pow;\n\nfunction fromString(e, t, n) {\n  if (0 === e.length) throw Error(\"empty string\");\n  if (\"NaN\" === e || \"Infinity\" === e || \"+Infinity\" === e || \"-Infinity\" === e) return ZERO;\n  if (\"number\" == typeof t ? (n = t, t = !1) : t = !!t, (n = n || 10) < 2 || 36 < n) throw RangeError(\"radix\");\n  var r;\n  if ((r = e.indexOf(\"-\")) > 0) throw Error(\"interior hyphen\");\n  if (0 === r) return fromString(e.substring(1), t, n).neg();\n\n  for (var a = fromNumber(pow_dbl(n, 8)), s = ZERO, o = 0; o < e.length; o += 8) {\n    var i = Math.min(8, e.length - o),\n        l = parseInt(e.substring(o, o + i), n);\n\n    if (i < 8) {\n      var u = fromNumber(pow_dbl(n, i));\n      s = s.mul(u).add(fromNumber(l));\n    } else s = (s = s.mul(a)).add(fromNumber(l));\n  }\n\n  return s.unsigned = t, s;\n}\n\nfunction fromValue(e, t) {\n  return \"number\" == typeof e ? fromNumber(e, t) : \"string\" == typeof e ? fromString(e, t) : fromBits(e.low, e.high, \"boolean\" == typeof t ? t : e.unsigned);\n}\n\nLong$1.fromString = fromString, Long$1.fromValue = fromValue;\nvar TWO_PWR_16_DBL = 65536,\n    TWO_PWR_24_DBL = 1 << 24,\n    TWO_PWR_32_DBL = TWO_PWR_16_DBL * TWO_PWR_16_DBL,\n    TWO_PWR_64_DBL = TWO_PWR_32_DBL * TWO_PWR_32_DBL,\n    TWO_PWR_63_DBL = TWO_PWR_64_DBL / 2,\n    TWO_PWR_24 = fromInt(TWO_PWR_24_DBL),\n    ZERO = fromInt(0);\nLong$1.ZERO = ZERO;\nvar UZERO = fromInt(0, !0);\nLong$1.UZERO = UZERO;\nvar ONE = fromInt(1);\nLong$1.ONE = ONE;\nvar UONE = fromInt(1, !0);\nLong$1.UONE = UONE;\nvar NEG_ONE = fromInt(-1);\nLong$1.NEG_ONE = NEG_ONE;\nvar MAX_VALUE = fromBits(-1, 2147483647, !1);\nLong$1.MAX_VALUE = MAX_VALUE;\nvar MAX_UNSIGNED_VALUE = fromBits(-1, -1, !0);\nLong$1.MAX_UNSIGNED_VALUE = MAX_UNSIGNED_VALUE;\nvar MIN_VALUE = fromBits(0, -2147483648, !1);\nLong$1.MIN_VALUE = MIN_VALUE;\nvar LongPrototype = Long$1.prototype;\nLongPrototype.toInt = function () {\n  return this.unsigned ? this.low >>> 0 : this.low;\n}, LongPrototype.toNumber = function () {\n  return this.unsigned ? (this.high >>> 0) * TWO_PWR_32_DBL + (this.low >>> 0) : this.high * TWO_PWR_32_DBL + (this.low >>> 0);\n}, LongPrototype.toString = function (e) {\n  if ((e = e || 10) < 2 || 36 < e) throw RangeError(\"radix\");\n  if (this.isZero()) return \"0\";\n\n  if (this.isNegative()) {\n    if (this.eq(MIN_VALUE)) {\n      var t = fromNumber(e),\n          n = this.div(t),\n          r = n.mul(t).sub(this);\n      return n.toString(e) + r.toInt().toString(e);\n    }\n\n    return \"-\" + this.neg().toString(e);\n  }\n\n  for (var a = fromNumber(pow_dbl(e, 6), this.unsigned), s = this, o = \"\";;) {\n    var i = s.div(a),\n        l = (s.sub(i.mul(a)).toInt() >>> 0).toString(e);\n    if ((s = i).isZero()) return l + o;\n\n    for (; l.length < 6;) {\n      l = \"0\" + l;\n    }\n\n    o = \"\" + l + o;\n  }\n}, LongPrototype.getHighBits = function () {\n  return this.high;\n}, LongPrototype.getHighBitsUnsigned = function () {\n  return this.high >>> 0;\n}, LongPrototype.getLowBits = function () {\n  return this.low;\n}, LongPrototype.getLowBitsUnsigned = function () {\n  return this.low >>> 0;\n}, LongPrototype.getNumBitsAbs = function () {\n  if (this.isNegative()) return this.eq(MIN_VALUE) ? 64 : this.neg().getNumBitsAbs();\n\n  for (var e = 0 != this.high ? this.high : this.low, t = 31; t > 0 && 0 == (e & 1 << t); t--) {\n    ;\n  }\n\n  return 0 != this.high ? t + 33 : t + 1;\n}, LongPrototype.isZero = function () {\n  return 0 === this.high && 0 === this.low;\n}, LongPrototype.eqz = LongPrototype.isZero, LongPrototype.isNegative = function () {\n  return !this.unsigned && this.high < 0;\n}, LongPrototype.isPositive = function () {\n  return this.unsigned || this.high >= 0;\n}, LongPrototype.isOdd = function () {\n  return 1 == (1 & this.low);\n}, LongPrototype.isEven = function () {\n  return 0 == (1 & this.low);\n}, LongPrototype.equals = function (e) {\n  return isLong(e) || (e = fromValue(e)), (this.unsigned === e.unsigned || this.high >>> 31 != 1 || e.high >>> 31 != 1) && this.high === e.high && this.low === e.low;\n}, LongPrototype.eq = LongPrototype.equals, LongPrototype.notEquals = function (e) {\n  return !this.eq(e);\n}, LongPrototype.neq = LongPrototype.notEquals, LongPrototype.ne = LongPrototype.notEquals, LongPrototype.lessThan = function (e) {\n  return this.comp(e) < 0;\n}, LongPrototype.lt = LongPrototype.lessThan, LongPrototype.lessThanOrEqual = function (e) {\n  return this.comp(e) <= 0;\n}, LongPrototype.lte = LongPrototype.lessThanOrEqual, LongPrototype.le = LongPrototype.lessThanOrEqual, LongPrototype.greaterThan = function (e) {\n  return this.comp(e) > 0;\n}, LongPrototype.gt = LongPrototype.greaterThan, LongPrototype.greaterThanOrEqual = function (e) {\n  return this.comp(e) >= 0;\n}, LongPrototype.gte = LongPrototype.greaterThanOrEqual, LongPrototype.ge = LongPrototype.greaterThanOrEqual, LongPrototype.compare = function (e) {\n  if (isLong(e) || (e = fromValue(e)), this.eq(e)) return 0;\n  var t = this.isNegative(),\n      n = e.isNegative();\n  return t && !n ? -1 : !t && n ? 1 : this.unsigned ? e.high >>> 0 > this.high >>> 0 || e.high === this.high && e.low >>> 0 > this.low >>> 0 ? -1 : 1 : this.sub(e).isNegative() ? -1 : 1;\n}, LongPrototype.comp = LongPrototype.compare, LongPrototype.negate = function () {\n  return !this.unsigned && this.eq(MIN_VALUE) ? MIN_VALUE : this.not().add(ONE);\n}, LongPrototype.neg = LongPrototype.negate, LongPrototype.add = function (e) {\n  isLong(e) || (e = fromValue(e));\n  var t = 0,\n      n = 0,\n      r = 0,\n      a = 0;\n  return r += (a += (65535 & this.low) + (65535 & e.low)) >>> 16, n += (r += (this.low >>> 16) + (e.low >>> 16)) >>> 16, t += (n += (65535 & this.high) + (65535 & e.high)) >>> 16, t += (this.high >>> 16) + (e.high >>> 16), fromBits((r &= 65535) << 16 | (a &= 65535), (t &= 65535) << 16 | (n &= 65535), this.unsigned);\n}, LongPrototype.subtract = function (e) {\n  return isLong(e) || (e = fromValue(e)), this.add(e.neg());\n}, LongPrototype.sub = LongPrototype.subtract, LongPrototype.multiply = function (e) {\n  if (this.isZero()) return ZERO;\n  if (isLong(e) || (e = fromValue(e)), wasm) return fromBits(wasm.mul(this.low, this.high, e.low, e.high), wasm.get_high(), this.unsigned);\n  if (e.isZero()) return ZERO;\n  if (this.eq(MIN_VALUE)) return e.isOdd() ? MIN_VALUE : ZERO;\n  if (e.eq(MIN_VALUE)) return this.isOdd() ? MIN_VALUE : ZERO;\n  if (this.isNegative()) return e.isNegative() ? this.neg().mul(e.neg()) : this.neg().mul(e).neg();\n  if (e.isNegative()) return this.mul(e.neg()).neg();\n  if (this.lt(TWO_PWR_24) && e.lt(TWO_PWR_24)) return fromNumber(this.toNumber() * e.toNumber(), this.unsigned);\n  var t = 65535 & this.high,\n      n = this.low >>> 16,\n      r = 65535 & this.low,\n      a = 65535 & e.high,\n      s = e.low >>> 16,\n      o = 65535 & e.low,\n      i = 0,\n      l = 0,\n      u = 0,\n      c = 0;\n  return u += (c += r * o) >>> 16, l += (u += n * o) >>> 16, u &= 65535, l += (u += r * s) >>> 16, i += (l += t * o) >>> 16, l &= 65535, i += (l += n * s) >>> 16, l &= 65535, i += (l += r * a) >>> 16, i += (this.high >>> 16) * o + t * s + n * a + r * (e.high >>> 16), fromBits((u &= 65535) << 16 | (c &= 65535), (i &= 65535) << 16 | (l &= 65535), this.unsigned);\n}, LongPrototype.mul = LongPrototype.multiply, LongPrototype.divide = function (e) {\n  if (isLong(e) || (e = fromValue(e)), e.isZero()) throw Error(\"division by zero\");\n  var t, n, r;\n  if (wasm) return this.unsigned || -2147483648 !== this.high || -1 !== e.low || -1 !== e.high ? fromBits((this.unsigned ? wasm.div_u : wasm.div_s)(this.low, this.high, e.low, e.high), wasm.get_high(), this.unsigned) : this;\n  if (this.isZero()) return this.unsigned ? UZERO : ZERO;\n\n  if (this.unsigned) {\n    if (e.unsigned || (e = e.toUnsigned()), e.gt(this)) return UZERO;\n    if (e.gt(this.shru(1))) return UONE;\n    r = UZERO;\n  } else {\n    if (this.eq(MIN_VALUE)) return e.eq(ONE) || e.eq(NEG_ONE) ? MIN_VALUE : e.eq(MIN_VALUE) ? ONE : (t = this.shr(1).div(e).shl(1)).eq(ZERO) ? e.isNegative() ? ONE : NEG_ONE : (n = this.sub(e.mul(t)), r = t.add(n.div(e)));\n    if (e.eq(MIN_VALUE)) return this.unsigned ? UZERO : ZERO;\n    if (this.isNegative()) return e.isNegative() ? this.neg().div(e.neg()) : this.neg().div(e).neg();\n    if (e.isNegative()) return this.div(e.neg()).neg();\n    r = ZERO;\n  }\n\n  for (n = this; n.gte(e);) {\n    t = Math.max(1, Math.floor(n.toNumber() / e.toNumber()));\n\n    for (var a = Math.ceil(Math.log(t) / Math.LN2), s = a <= 48 ? 1 : pow_dbl(2, a - 48), o = fromNumber(t), i = o.mul(e); i.isNegative() || i.gt(n);) {\n      i = (o = fromNumber(t -= s, this.unsigned)).mul(e);\n    }\n\n    o.isZero() && (o = ONE), r = r.add(o), n = n.sub(i);\n  }\n\n  return r;\n}, LongPrototype.div = LongPrototype.divide, LongPrototype.modulo = function (e) {\n  return isLong(e) || (e = fromValue(e)), wasm ? fromBits((this.unsigned ? wasm.rem_u : wasm.rem_s)(this.low, this.high, e.low, e.high), wasm.get_high(), this.unsigned) : this.sub(this.div(e).mul(e));\n}, LongPrototype.mod = LongPrototype.modulo, LongPrototype.rem = LongPrototype.modulo, LongPrototype.not = function () {\n  return fromBits(~this.low, ~this.high, this.unsigned);\n}, LongPrototype.and = function (e) {\n  return isLong(e) || (e = fromValue(e)), fromBits(this.low & e.low, this.high & e.high, this.unsigned);\n}, LongPrototype.or = function (e) {\n  return isLong(e) || (e = fromValue(e)), fromBits(this.low | e.low, this.high | e.high, this.unsigned);\n}, LongPrototype.xor = function (e) {\n  return isLong(e) || (e = fromValue(e)), fromBits(this.low ^ e.low, this.high ^ e.high, this.unsigned);\n}, LongPrototype.shiftLeft = function (e) {\n  return isLong(e) && (e = e.toInt()), 0 == (e &= 63) ? this : e < 32 ? fromBits(this.low << e, this.high << e | this.low >>> 32 - e, this.unsigned) : fromBits(0, this.low << e - 32, this.unsigned);\n}, LongPrototype.shl = LongPrototype.shiftLeft, LongPrototype.shiftRight = function (e) {\n  return isLong(e) && (e = e.toInt()), 0 == (e &= 63) ? this : e < 32 ? fromBits(this.low >>> e | this.high << 32 - e, this.high >> e, this.unsigned) : fromBits(this.high >> e - 32, this.high >= 0 ? 0 : -1, this.unsigned);\n}, LongPrototype.shr = LongPrototype.shiftRight, LongPrototype.shiftRightUnsigned = function (e) {\n  if (isLong(e) && (e = e.toInt()), 0 == (e &= 63)) return this;\n  var t = this.high;\n  return e < 32 ? fromBits(this.low >>> e | t << 32 - e, t >>> e, this.unsigned) : fromBits(32 === e ? t : t >>> e - 32, 0, this.unsigned);\n}, LongPrototype.shru = LongPrototype.shiftRightUnsigned, LongPrototype.shr_u = LongPrototype.shiftRightUnsigned, LongPrototype.toSigned = function () {\n  return this.unsigned ? fromBits(this.low, this.high, !1) : this;\n}, LongPrototype.toUnsigned = function () {\n  return this.unsigned ? this : fromBits(this.low, this.high, !0);\n}, LongPrototype.toBytes = function (e) {\n  return e ? this.toBytesLE() : this.toBytesBE();\n}, LongPrototype.toBytesLE = function () {\n  var e = this.high,\n      t = this.low;\n  return [255 & t, t >>> 8 & 255, t >>> 16 & 255, t >>> 24, 255 & e, e >>> 8 & 255, e >>> 16 & 255, e >>> 24];\n}, LongPrototype.toBytesBE = function () {\n  var e = this.high,\n      t = this.low;\n  return [e >>> 24, e >>> 16 & 255, e >>> 8 & 255, 255 & e, t >>> 24, t >>> 16 & 255, t >>> 8 & 255, 255 & t];\n}, Long$1.fromBytes = function (e, t, n) {\n  return n ? Long$1.fromBytesLE(e, t) : Long$1.fromBytesBE(e, t);\n}, Long$1.fromBytesLE = function (e, t) {\n  return new Long$1(e[0] | e[1] << 8 | e[2] << 16 | e[3] << 24, e[4] | e[5] << 8 | e[6] << 16 | e[7] << 24, t);\n}, Long$1.fromBytesBE = function (e, t) {\n  return new Long$1(e[4] << 24 | e[5] << 16 | e[6] << 8 | e[7], e[0] << 24 | e[1] << 16 | e[2] << 8 | e[3], t);\n};\nvar long$1 = long,\n    LongExports = /*#__PURE__*/Object.assign( /*#__PURE__*/Object.create(null), long, {\n  default: long$1\n});\nvar Long = long$1 || LongExports;\n\nfunction hexToLong(e) {\n  return Long.fromString(e, !0, 16);\n}\n\nvar k0 = hexToLong(\"c3a5c85c97cb3127\"),\n    k1 = hexToLong(\"b492b66fbe98f273\"),\n    k2 = hexToLong(\"9ae16a3b2f90404f\");\n\nfunction shiftMix(e) {\n  return e.xor(e.shru(47));\n}\n\nfunction fetch$2(e, t, n) {\n  var r = e.slice(t, t + n);\n  return Long.fromBytes(Array.from(r), !0, !0);\n}\n\nfunction fetch64(e, t) {\n  return fetch$2(e, t, 8);\n}\n\nfunction fetch32(e, t) {\n  return fetch$2(e, t, 4);\n}\n\nfunction rotate64(e, t) {\n  return 0 === t ? e : e.shru(t).or(e.shl(64 - t));\n}\n\nfunction hashLen16(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : hexToLong(\"9ddfea08eb382d69\");\n  var r = e.xor(t).mul(n);\n  r = r.xor(r.shru(47));\n  var a = t.xor(r).mul(n);\n  return a = a.xor(a.shru(47)), a = a.mul(n), a;\n}\n\nfunction weakHashLen32WithSeeds(e, t, n, r, a, s) {\n  a = a.add(e), s = rotate64(s.add(a).add(r), 21);\n  var o = a;\n  return a = (a = a.add(t)).add(n), s = s.add(rotate64(a, 44)), [a.add(r), s.add(o)];\n}\n\nfunction weakHashLen32WithSeedsStr(e, t, n, r) {\n  return weakHashLen32WithSeeds(fetch64(e, t), fetch64(e, t + 8), fetch64(e, t + 16), fetch64(e, t + 24), n, r);\n}\n\nfunction hashLen0to16(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n\n  if (t >= 8) {\n    var n = k2.add(2 * t),\n        r = fetch64(e, 0).add(k2),\n        a = fetch64(e, t - 8);\n    return hashLen16(rotate64(a, 37).mul(n).add(r), rotate64(r, 25).add(a).mul(n), n);\n  }\n\n  if (t >= 4) {\n    var _n217 = k2.add(2 * t);\n\n    return hashLen16(fetch32(e, 0).shl(3).add(t), fetch32(e, t - 4), _n217);\n  }\n\n  if (t > 0) {\n    var _n218 = t + (e[t - 1] << 2);\n\n    return shiftMix(k2.mul(e[0] + (e[t >> 1] << 8)).xor(k0.mul(_n218))).mul(k2);\n  }\n\n  return k2;\n}\n\nfunction hashLen17to32(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n  var n = k2.add(2 * t),\n      r = fetch64(e, 0).mul(k1),\n      a = fetch64(e, 8),\n      s = fetch64(e, t - 8).mul(n),\n      o = fetch64(e, t - 16).mul(k2);\n  return hashLen16(rotate64(r.add(a), 43).add(rotate64(s, 30)).add(o), r.add(rotate64(a.add(k2), 18)).add(s), n);\n}\n\nfunction hashLen33to64(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n  var n = k2.add(2 * t),\n      r = fetch64(e, 0).mul(k2),\n      a = fetch64(e, 8),\n      s = fetch64(e, t - 8).mul(n),\n      o = fetch64(e, t - 16).mul(k2),\n      i = rotate64(r.add(a), 43).add(rotate64(s, 30)).add(o),\n      l = hashLen16(i, r.add(rotate64(a.add(k2), 18)).add(s), n),\n      u = fetch64(e, 16).mul(n),\n      c = fetch64(e, 24),\n      p = i.add(fetch64(e, t - 32)).mul(n),\n      d = l.add(fetch64(e, t - 24)).mul(n);\n  return hashLen16(rotate64(u.add(c), 43).add(rotate64(p, 30)).add(d), u.add(rotate64(c.add(r), 18)).add(p), n);\n}\n\nfunction fingerPrint64(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : e.length;\n  var n = Long.fromNumber(81, !0);\n  if (t <= 32) return t <= 16 ? hashLen0to16(e, t) : hashLen17to32(e, t);\n  if (t <= 64) return hashLen33to64(e, t);\n  var r = n,\n      a = n.mul(k1).add(113),\n      s = shiftMix(a.mul(k2).add(113)).mul(k2),\n      o = [Long.UZERO, Long.UZERO],\n      i = [Long.UZERO, Long.UZERO];\n  r = r.mul(k2).add(fetch64(e, 0));\n  var l = 0;\n  var u = 64 * (t - 1 >> 6),\n      c = u + (t - 1 & 63) - 63;\n\n  do {\n    r = rotate64(r.add(a).add(o[0]).add(fetch64(e, l + 8)), 37).mul(k1), a = rotate64(a.add(o[1]).add(fetch64(e, l + 48)), 42).mul(k1), r = r.xor(i[1]), a = a.add(o[0]).add(fetch64(e, l + 40)), s = rotate64(s.add(i[0]), 33).mul(k1), o = weakHashLen32WithSeedsStr(e, l, o[1].mul(k1), r.add(i[0])), i = weakHashLen32WithSeedsStr(e, l + 32, s.add(i[1]), a.add(fetch64(e, l + 16))), [s, r] = [r, s], l += 64;\n  } while (l !== u);\n\n  var p = k1.add(s.and(255).shl(1));\n  return l = c, i[0] = i[0].add(t - 1 & 63), o[0] = o[0].add(i[0]), i[0] = i[0].add(o[0]), r = rotate64(r.add(a).add(o[0]).add(fetch64(e, l + 8)), 37).mul(p), a = rotate64(a.add(o[1]).add(fetch64(e, l + 48)), 42).mul(p), r = r.xor(i[1].mul(9)), a = a.add(o[0].mul(9).add(fetch64(e, l + 40))), s = rotate64(s.add(i[0]), 33).mul(p), o = weakHashLen32WithSeedsStr(e, l, o[1].mul(p), r.add(i[0])), i = weakHashLen32WithSeedsStr(e, l + 32, s.add(i[1]), a.add(fetch64(e, l + 16))), [s, r] = [r, s], hashLen16(hashLen16(o[0], i[0], p).add(shiftMix(a).mul(k0)).add(s), hashLen16(o[1], i[1], p).add(r), p);\n}\n\nfunction createScalarValue(e, t) {\n  return \"string\" === t ? encodeString(e) : toTypedArray([e], t);\n}\n\nfunction noConversionNeeded(e, t) {\n  return e instanceof Float32Array && \"float32\" === t || e instanceof Int32Array && \"int32\" === t || e instanceof Uint8Array && \"bool\" === t;\n}\n\nfunction toTypedArray(e, t) {\n  if (\"string\" === t) throw new Error(\"Cannot convert a string[] to a TypedArray\");\n  if (Array.isArray(e) && (e = flatten$3(e)), env().getBool(\"DEBUG\") && checkConversionForErrors(e, t), noConversionNeeded(e, t)) return e;\n  if (null == t || \"float32\" === t || \"complex64\" === t) return new Float32Array(e);\n  if (\"int32\" === t) return new Int32Array(e);\n\n  if (\"bool\" === t) {\n    var _t366 = new Uint8Array(e.length);\n\n    for (var n = 0; n < _t366.length; ++n) {\n      0 !== Math.round(e[n]) && (_t366[n] = 1);\n    }\n\n    return _t366;\n  }\n\n  throw new Error(\"Unknown data type \".concat(t));\n}\n\nfunction now() {\n  return env().platform.now();\n}\n\nfunction fetch$1(e, t) {\n  return env().platform.fetch(e, t);\n}\n\nfunction encodeString(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"utf-8\";\n  return t = t || \"utf-8\", env().platform.encode(e, t);\n}\n\nfunction decodeString(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"utf-8\";\n  return t = t || \"utf-8\", env().platform.decode(e, t);\n}\n\nvar util = {\n  __proto__: null,\n  createScalarValue,\n  toTypedArray,\n  now,\n  fetch: fetch$1,\n  encodeString,\n  decodeString,\n  shuffle,\n  shuffleCombo,\n  clamp,\n  nearestLargerEven,\n  swap,\n  sum: sum$3,\n  randUniform,\n  distSquared,\n  assert: assert$4,\n  assertShapesMatch,\n  assertNonNull,\n  flatten: flatten$3,\n  sizeFromShape,\n  isScalarShape,\n  arraysEqual,\n  isInt,\n  tanh: tanh$3,\n  sizeToSquarishShape,\n  createShuffledIndices,\n  rightPad,\n  repeatedTry,\n  inferFromImplicitShape,\n  parseAxisParam,\n  squeezeShape,\n  getTypedArrayFromDType,\n  getArrayFromDType,\n  checkConversionForErrors,\n  isValidDtype,\n  hasEncodingLoss,\n  isTypedArray,\n  bytesPerElement,\n  bytesFromStringArray,\n  isString,\n  isBoolean,\n  isNumber,\n  inferDtype,\n  isFunction,\n  nearestDivisor,\n  computeStrides,\n  toNestedArray,\n  makeOnesTypedArray,\n  makeZerosTypedArray,\n  makeZerosNestedTypedArray,\n  assertNonNegativeIntegerDimensions,\n  locToIndex,\n  indexToLoc,\n  isPromise,\n  hexToLong,\n  fingerPrint64\n};\n\nclass Profiler {\n  constructor(e, t) {\n    this.backendTimer = e, this.logger = t, null == t && (this.logger = new Logger());\n  }\n\n  profileKernel(e, t, n) {\n    var r;\n\n    var a = () => {\n      r = n();\n    };\n\n    var s;\n    var o = now();\n    if (this.backendTimer.timerAvailable()) s = this.backendTimer.time(a);else {\n      a();\n\n      for (var _e515 of r) {\n        _e515.dataSync();\n      }\n\n      s = Promise.resolve({\n        kernelMs: now() - o\n      });\n    }\n\n    if (env().getBool(\"CHECK_COMPUTATION_FOR_ERRORS\")) {\n      var _loop28 = function _loop28(_t367) {\n        var n = r[_t367];\n        n.data().then(t => {\n          checkComputationForErrors(t, n.dtype, e);\n        });\n      };\n\n      for (var _t367 = 0; _t367 < r.length; _t367++) {\n        _loop28(_t367);\n      }\n    }\n\n    return {\n      kernelName: e,\n      outputs: r,\n      inputs: t,\n      timeMs: s.then(e => e.kernelMs),\n      extraInfo: s.then(e => null != e.getExtraProfileInfo ? e.getExtraProfileInfo() : \"\")\n    };\n  }\n\n  logKernelProfile(e) {\n    var {\n      kernelName: t,\n      outputs: n,\n      timeMs: r,\n      inputs: a,\n      extraInfo: s\n    } = e;\n    n.forEach(e => {\n      Promise.all([e.data(), r, s]).then(n => {\n        this.logger.logKernelProfile(t, e, n[0], n[1], a, n[2]);\n      });\n    });\n  }\n\n}\n\nfunction checkComputationForErrors(e, t, n) {\n  if (\"float32\" !== t) return !1;\n\n  for (var _t368 = 0; _t368 < e.length; _t368++) {\n    var r = e[_t368];\n    if (isNaN(r) || !isFinite(r)) return console.warn(\"Found \".concat(r, \" in the result of '\").concat(n, \"'\")), !0;\n  }\n\n  return !1;\n}\n\nclass Logger {\n  logKernelProfile(e, t, n, r, a, s) {\n    var o = \"number\" == typeof r ? rightPad(\"\".concat(r, \"ms\"), 9) : r.error,\n        i = rightPad(e, 25),\n        l = t.rank,\n        u = t.size,\n        c = rightPad(t.shape.toString(), 14);\n    var p = \"\";\n\n    for (var _e516 in a) {\n      var _n219 = a[_e516];\n\n      if (null != _n219) {\n        var _r178 = _n219.shape || t.shape,\n            _a118 = _r178.length;\n\n        p += \"\".concat(_e516, \": \").concat(_a118, \"D \").concat(_a118 > 0 ? _r178 : \"\", \" \");\n      }\n    }\n\n    console.log(\"%c\".concat(i, \"\\t%c\").concat(o, \"\\t%c\").concat(l, \"D \").concat(c, \"\\t%c\").concat(u, \"\\t%c\").concat(p, \"\\t%c\").concat(s), \"font-weight:bold\", \"color:red\", \"color:blue\", \"color: orange\", \"color: green\", \"color: steelblue\");\n  }\n\n}\n\nfunction getFilteredNodesXToY(e, t, n) {\n  var r = {},\n      a = {};\n\n  for (var _e517 = 0; _e517 < t.length; _e517++) {\n    r[t[_e517].id] = !0;\n  }\n\n  for (var _n220 = 0; _n220 < e.length; _n220++) {\n    var _s91 = e[_n220],\n        _o66 = _s91.inputs;\n\n    for (var _e518 in _o66) {\n      var _n221 = _o66[_e518];\n\n      var _i41 = !1;\n\n      for (var _e519 = 0; _e519 < t.length; _e519++) {\n        if (r[_n221.id]) {\n          _s91.outputs.forEach(e => r[e.id] = !0), _i41 = !0, a[_s91.id] = !0;\n          break;\n        }\n      }\n\n      if (_i41) break;\n    }\n  }\n\n  var s = {};\n  s[n.id] = !0;\n  var o = {};\n\n  for (var _t369 = e.length - 1; _t369 >= 0; _t369--) {\n    var _n222 = e[_t369],\n        _r179 = _n222.inputs;\n\n    for (var _e520 = 0; _e520 < _n222.outputs.length; _e520++) {\n      if (s[_n222.outputs[_e520].id]) {\n        for (var _e521 in _r179) {\n          s[_r179[_e521].id] = !0, o[_n222.id] = !0;\n        }\n\n        break;\n      }\n    }\n  }\n\n  var i = [];\n\n  for (var _t370 = 0; _t370 < e.length; _t370++) {\n    var _n223 = e[_t370];\n\n    if (a[_n223.id] && o[_n223.id]) {\n      var _e522 = {};\n\n      for (var _t372 in _n223.inputs) {\n        var _a119 = _n223.inputs[_t372];\n        r[_a119.id] && (_e522[_t372] = _a119);\n      }\n\n      var _t371 = Object.assign({}, _n223);\n\n      _t371.inputs = _e522, _t371.outputs = _n223.outputs, i.push(_t371);\n    }\n  }\n\n  return i;\n}\n\nfunction backpropagateGradients(e, t, n, r) {\n  var _loop29 = function _loop29(a) {\n    var s = t[a],\n        o = [];\n    if (s.outputs.forEach(t => {\n      var n = e[t.id];\n      o.push(null != n ? n : null);\n    }), null == s.gradient) throw new Error(\"Cannot compute gradient: gradient function not found for \".concat(s.kernelName, \".\"));\n    var i = s.gradient(o);\n\n    var _loop30 = function _loop30(_t373) {\n      if (!(_t373 in i)) throw new Error(\"Cannot backprop through input \".concat(_t373, \". Available gradients found: \").concat(Object.keys(i), \".\"));\n      var a = n(() => i[_t373]());\n      if (\"float32\" !== a.dtype) throw new Error(\"Error in gradient for op \".concat(s.kernelName, \". The gradient of input \").concat(_t373, \" must have 'float32' dtype, but has '\").concat(a.dtype, \"'\"));\n      var o = s.inputs[_t373];\n      if (!arraysEqual(a.shape, o.shape)) throw new Error(\"Error in gradient for op \".concat(s.kernelName, \". The gradient of input '\").concat(_t373, \"' has shape '\").concat(a.shape, \"', which does not match the shape of the input '\").concat(o.shape, \"'\"));\n      if (null == e[o.id]) e[o.id] = a;else {\n        var _t374 = e[o.id];\n        e[o.id] = r(_t374, a), _t374.dispose();\n      }\n    };\n\n    for (var _t373 in s.inputs) {\n      _loop30(_t373);\n    }\n  };\n\n  for (var a = t.length - 1; a >= 0; a--) {\n    _loop29(a);\n  }\n}\n\nvar FORMAT_LIMIT_NUM_VALS = 20,\n    FORMAT_NUM_FIRST_LAST_VALS = 3,\n    FORMAT_NUM_SIG_DIGITS = 7;\n\nfunction tensorToString(e, t, n, r) {\n  var a = computeStrides(t),\n      s = computeMaxSizePerColumn(e, t, n, a),\n      o = t.length,\n      i = subTensorToString(e, t, n, a, s),\n      l = [\"Tensor\"];\n  return r && (l.push(\"  dtype: \".concat(n)), l.push(\"  rank: \".concat(o)), l.push(\"  shape: [\".concat(t, \"]\")), l.push(\"  values:\")), l.push(i.map(e => \"    \" + e).join(\"\\n\")), l.join(\"\\n\");\n}\n\nfunction computeMaxSizePerColumn(e, t, n, r) {\n  var a = sizeFromShape(t),\n      s = r[r.length - 1],\n      o = new Array(s).fill(0),\n      i = t.length,\n      l = \"complex64\" === n ? createComplexTuples(e) : e;\n  if (i > 1) for (var _e523 = 0; _e523 < a / s; _e523++) {\n    var _t375 = _e523 * s;\n\n    for (var _e524 = 0; _e524 < s; _e524++) {\n      o[_e524] = Math.max(o[_e524], valToString(l[_t375 + _e524], 0, n).length);\n    }\n  }\n  return o;\n}\n\nfunction valToString(e, t, n) {\n  var r;\n  return r = Array.isArray(e) ? \"\".concat(parseFloat(e[0].toFixed(FORMAT_NUM_SIG_DIGITS)), \" + \").concat(parseFloat(e[1].toFixed(FORMAT_NUM_SIG_DIGITS)), \"j\") : isString(e) ? \"'\".concat(e, \"'\") : \"bool\" === n ? boolNumToString(e) : parseFloat(e.toFixed(FORMAT_NUM_SIG_DIGITS)).toString(), rightPad(r, t);\n}\n\nfunction boolNumToString(e) {\n  return 0 === e ? \"false\" : \"true\";\n}\n\nfunction subTensorToString(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !0;\n  var o = \"complex64\" === n ? 2 : 1,\n      i = t[0],\n      l = t.length;\n  if (0 === l) return \"complex64\" === n ? [valToString(createComplexTuples(e)[0], 0, n)] : \"bool\" === n ? [boolNumToString(e[0])] : [e[0].toString()];\n\n  if (1 === l) {\n    if (i > FORMAT_LIMIT_NUM_VALS) {\n      var _t376 = Array.from(e.slice(0, FORMAT_NUM_FIRST_LAST_VALS * o)),\n          _r180 = Array.from(e.slice((i - FORMAT_NUM_FIRST_LAST_VALS) * o, i * o));\n\n      return \"complex64\" === n && (_t376 = createComplexTuples(_t376), _r180 = createComplexTuples(_r180)), [\"[\" + _t376.map((e, t) => valToString(e, a[t], n)).join(\", \") + \", ..., \" + _r180.map((e, t) => valToString(e, a[i - FORMAT_NUM_FIRST_LAST_VALS + t], n)).join(\", \") + \"]\"];\n    }\n\n    return [\"[\" + (\"complex64\" === n ? createComplexTuples(e) : Array.from(e)).map((e, t) => valToString(e, a[t], n)).join(\", \") + \"]\"];\n  }\n\n  var u = t.slice(1),\n      c = r.slice(1),\n      p = r[0] * o,\n      d = [];\n\n  if (i > FORMAT_LIMIT_NUM_VALS) {\n    for (var _t377 = 0; _t377 < FORMAT_NUM_FIRST_LAST_VALS; _t377++) {\n      var _r181 = _t377 * p;\n\n      d.push(...subTensorToString(e.slice(_r181, _r181 + p), u, n, c, a, !1));\n    }\n\n    d.push(\"...\");\n\n    for (var _t378 = i - FORMAT_NUM_FIRST_LAST_VALS; _t378 < i; _t378++) {\n      var _r182 = _t378 * p;\n\n      d.push(...subTensorToString(e.slice(_r182, _r182 + p), u, n, c, a, _t378 === i - 1));\n    }\n  } else for (var _t379 = 0; _t379 < i; _t379++) {\n    var _r183 = _t379 * p;\n\n    d.push(...subTensorToString(e.slice(_r183, _r183 + p), u, n, c, a, _t379 === i - 1));\n  }\n\n  var h = 2 === l ? \",\" : \"\";\n  d[0] = \"[\" + d[0] + h;\n\n  for (var _e525 = 1; _e525 < d.length - 1; _e525++) {\n    d[_e525] = \" \" + d[_e525] + h;\n  }\n\n  var m = \",\\n\";\n\n  for (var _e526 = 2; _e526 < l; _e526++) {\n    m += \"\\n\";\n  }\n\n  return d[d.length - 1] = \" \" + d[d.length - 1] + \"]\" + (s ? \"\" : m), d;\n}\n\nfunction createComplexTuples(e) {\n  var t = [];\n\n  for (var n = 0; n < e.length; n += 2) {\n    t.push([e[n], e[n + 1]]);\n  }\n\n  return t;\n}\n\nclass TensorBuffer {\n  constructor(e, t, n) {\n    if (this.dtype = t, this.shape = e.slice(), this.size = sizeFromShape(e), null != n) {\n      var _e527 = n.length;\n      assert$4(_e527 === this.size, () => \"Length of values '\".concat(_e527, \"' does not match the size inferred by the shape '\").concat(this.size, \"'.\"));\n    }\n\n    if (\"complex64\" === t) throw new Error(\"complex64 dtype TensorBuffers are not supported. Please create a TensorBuffer for the real and imaginary parts separately and call tf.complex(real, imag).\");\n    this.values = n || getArrayFromDType(t, this.size), this.strides = computeStrides(e);\n  }\n\n  set(e) {\n    for (var _len6 = arguments.length, t = new Array(_len6 > 1 ? _len6 - 1 : 0), _key6 = 1; _key6 < _len6; _key6++) {\n      t[_key6 - 1] = arguments[_key6];\n    }\n\n    0 === t.length && (t = [0]), assert$4(t.length === this.rank, () => \"The number of provided coordinates (\".concat(t.length, \") must match the rank (\").concat(this.rank, \")\"));\n    var n = this.locToIndex(t);\n    this.values[n] = e;\n  }\n\n  get() {\n    for (var _len7 = arguments.length, e = new Array(_len7), _key7 = 0; _key7 < _len7; _key7++) {\n      e[_key7] = arguments[_key7];\n    }\n\n    0 === e.length && (e = [0]);\n    var t = 0;\n\n    for (var _n224 of e) {\n      if (_n224 < 0 || _n224 >= this.shape[t]) throw new Error(\"Requested out of range element at \".concat(e, \".   Buffer shape=\").concat(this.shape));\n      t++;\n    }\n\n    var n = e[e.length - 1];\n\n    for (var _t380 = 0; _t380 < e.length - 1; ++_t380) {\n      n += this.strides[_t380] * e[_t380];\n    }\n\n    return this.values[n];\n  }\n\n  locToIndex(e) {\n    if (0 === this.rank) return 0;\n    if (1 === this.rank) return e[0];\n    var t = e[e.length - 1];\n\n    for (var n = 0; n < e.length - 1; ++n) {\n      t += this.strides[n] * e[n];\n    }\n\n    return t;\n  }\n\n  indexToLoc(e) {\n    if (0 === this.rank) return [];\n    if (1 === this.rank) return [e];\n    var t = new Array(this.shape.length);\n\n    for (var n = 0; n < t.length - 1; ++n) {\n      t[n] = Math.floor(e / this.strides[n]), e -= t[n] * this.strides[n];\n    }\n\n    return t[t.length - 1] = e, t;\n  }\n\n  get rank() {\n    return this.shape.length;\n  }\n\n  toTensor() {\n    return trackerFn().makeTensor(this.values, this.shape, this.dtype);\n  }\n\n}\n\nvar trackerFn = null,\n    opHandler$1 = null;\n\nfunction setTensorTracker(e) {\n  trackerFn = e;\n}\n\nfunction setOpHandler(e) {\n  opHandler$1 = e;\n}\n\nclass Tensor {\n  constructor(e, t, n, r) {\n    this.kept = !1, this.isDisposedInternal = !1, this.shape = e.slice(), this.dtype = t || \"float32\", this.size = sizeFromShape(e), this.strides = computeStrides(e), this.dataId = n, this.id = r, this.rankType = this.rank < 5 ? this.rank.toString() : \"higher\";\n  }\n\n  get rank() {\n    return this.shape.length;\n  }\n\n  buffer() {\n    var _this77 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this77.data();\n      return opHandler$1.buffer(_this77.shape, _this77.dtype, e);\n    })();\n  }\n\n  bufferSync() {\n    return opHandler$1.buffer(this.shape, this.dtype, this.dataSync());\n  }\n\n  array() {\n    var _this78 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this78.data();\n      return toNestedArray(_this78.shape, e, \"complex64\" === _this78.dtype);\n    })();\n  }\n\n  arraySync() {\n    return toNestedArray(this.shape, this.dataSync(), \"complex64\" === this.dtype);\n  }\n\n  data() {\n    var _this79 = this;\n\n    return _asyncToGenerator(function* () {\n      _this79.throwIfDisposed();\n\n      var e = trackerFn().read(_this79.dataId);\n\n      if (\"string\" === _this79.dtype) {\n        var t = yield e;\n\n        try {\n          return t.map(e => decodeString(e));\n        } catch (e) {\n          throw new Error(\"Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().\");\n        }\n      }\n\n      return e;\n    })();\n  }\n\n  dataSync() {\n    this.throwIfDisposed();\n    var e = trackerFn().readSync(this.dataId);\n    if (\"string\" === this.dtype) try {\n      return e.map(e => decodeString(e));\n    } catch (e) {\n      throw new Error(\"Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().\");\n    }\n    return e;\n  }\n\n  bytes() {\n    var _this80 = this;\n\n    return _asyncToGenerator(function* () {\n      _this80.throwIfDisposed();\n\n      var e = yield trackerFn().read(_this80.dataId);\n      return \"string\" === _this80.dtype ? e : new Uint8Array(e.buffer);\n    })();\n  }\n\n  dispose() {\n    this.isDisposed || (trackerFn().disposeTensor(this), this.isDisposedInternal = !0);\n  }\n\n  get isDisposed() {\n    return this.isDisposedInternal;\n  }\n\n  throwIfDisposed() {\n    if (this.isDisposed) throw new Error(\"Tensor is disposed.\");\n  }\n\n  print() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !1;\n    return opHandler$1.print(this, e);\n  }\n\n  clone() {\n    return this.throwIfDisposed(), opHandler$1.clone(this);\n  }\n\n  toString() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !1;\n    return tensorToString(this.dataSync(), this.shape, this.dtype, e);\n  }\n\n  cast(e) {\n    return this.throwIfDisposed(), opHandler$1.cast(this, e);\n  }\n\n  variable() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !0;\n    var t = arguments.length > 1 ? arguments[1] : undefined;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    return this.throwIfDisposed(), trackerFn().makeVariable(this, e, t, n);\n  }\n\n}\n\nfunction getGlobalTensorClass() {\n  return getGlobal(\"Tensor\", () => Tensor);\n}\n\nObject.defineProperty(Tensor, Symbol.hasInstance, {\n  value: e => !!e && null != e.data && null != e.dataSync && null != e.throwIfDisposed\n}), getGlobalTensorClass();\n\nclass Variable extends Tensor {\n  constructor(e, t, n, r) {\n    super(e.shape, e.dtype, e.dataId, r), this.trainable = t, this.name = n;\n  }\n\n  assign(e) {\n    if (e.dtype !== this.dtype) throw new Error(\"dtype of the new value (\".concat(e.dtype, \") and previous value (\").concat(this.dtype, \") must match\"));\n    if (!arraysEqual(e.shape, this.shape)) throw new Error(\"shape of the new value (\".concat(e.shape, \") and previous value (\").concat(this.shape, \") must match\"));\n    trackerFn().disposeTensor(this), this.dataId = e.dataId, trackerFn().incRef(this, null);\n  }\n\n  dispose() {\n    trackerFn().disposeVariable(this), this.isDisposedInternal = !0;\n  }\n\n}\n\nvar Rank, UpcastInt32AndMap, UpcastBoolAndMap, UpcastFloat32AndMap, UpcastComplex64AndMap;\nObject.defineProperty(Variable, Symbol.hasInstance, {\n  value: e => e instanceof Tensor && null != e.assign && e.assign instanceof Function\n}), function (e) {\n  e.R0 = \"R0\", e.R1 = \"R1\", e.R2 = \"R2\", e.R3 = \"R3\", e.R4 = \"R4\", e.R5 = \"R5\", e.R6 = \"R6\";\n}(Rank || (Rank = {})), function (e) {\n  e.float32 = \"float32\", e.int32 = \"int32\", e.bool = \"int32\", e.complex64 = \"complex64\";\n}(UpcastInt32AndMap || (UpcastInt32AndMap = {})), function (e) {\n  e.float32 = \"float32\", e.int32 = \"int32\", e.bool = \"bool\", e.complex64 = \"complex64\";\n}(UpcastBoolAndMap || (UpcastBoolAndMap = {})), function (e) {\n  e.float32 = \"float32\", e.int32 = \"float32\", e.bool = \"float32\", e.complex64 = \"complex64\";\n}(UpcastFloat32AndMap || (UpcastFloat32AndMap = {})), function (e) {\n  e.float32 = \"complex64\", e.int32 = \"complex64\", e.bool = \"complex64\", e.complex64 = \"complex64\";\n}(UpcastComplex64AndMap || (UpcastComplex64AndMap = {}));\nvar upcastTypeMap = {\n  float32: UpcastFloat32AndMap,\n  int32: UpcastInt32AndMap,\n  bool: UpcastBoolAndMap,\n  complex64: UpcastComplex64AndMap\n};\n\nfunction upcastType(e, t) {\n  if (\"string\" === e || \"string\" === t) {\n    if (\"string\" === e && \"string\" === t) return \"string\";\n    throw new Error(\"Can not upcast \".concat(e, \" with \").concat(t));\n  }\n\n  return upcastTypeMap[e][t];\n}\n\nfunction sumOutType(e) {\n  return upcastType(e, \"int32\");\n}\n\nfunction makeTypesMatch(e, t) {\n  if (e.dtype === t.dtype) return [e, t];\n  var n = upcastType(e.dtype, t.dtype);\n  return [e.cast(n), t.cast(n)];\n}\n\nfunction assertTypesMatch(e, t) {\n  assert$4(e.dtype === t.dtype, () => \"The dtypes of the first(\".concat(e.dtype, \") and second(\").concat(t.dtype, \") input must match\"));\n}\n\nfunction isTensorInList(e, t) {\n  return t.some(t => t.id === e.id);\n}\n\nfunction getTensorsInContainer(e) {\n  var t = [];\n  return walkTensorContainer(e, t, new Set()), t;\n}\n\nfunction walkTensorContainer(e, t, n) {\n  if (null == e) return;\n  if (e instanceof Tensor) return void t.push(e);\n  if (!isIterable$1(e)) return;\n  var r = e;\n\n  for (var _e528 in r) {\n    var a = r[_e528];\n    n.has(a) || (n.add(a), walkTensorContainer(a, t, n));\n  }\n}\n\nfunction isIterable$1(e) {\n  return Array.isArray(e) || \"object\" == typeof e;\n}\n\nvar tensor_util = {\n  __proto__: null,\n  makeTypesMatch,\n  assertTypesMatch,\n  isTensorInList,\n  getTensorsInContainer\n};\n\nfunction isRegisteredKernelInvocation(e) {\n  return null != e.kernelName;\n}\n\nclass EngineState {\n  constructor() {\n    this.registeredVariables = {}, this.nextTapeNodeId = 0, this.numBytes = 0, this.numTensors = 0, this.numStringTensors = 0, this.numDataBuffers = 0, this.gradientDepth = 0, this.kernelDepth = 0, this.scopeStack = [], this.numDataMovesStack = [], this.nextScopeId = 0, this.tensorInfo = new WeakMap(), this.profiling = !1, this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null,\n\n      get kernelNames() {\n        return Array.from(new Set(this.kernels.map(e => e.name)));\n      }\n\n    };\n  }\n\n  dispose() {\n    for (var _e529 in this.registeredVariables) {\n      this.registeredVariables[_e529].dispose();\n    }\n  }\n\n}\n\nclass Engine {\n  constructor(e) {\n    this.ENV = e, this.registry = {}, this.registryFactory = {}, this.pendingBackendInitId = 0, this.state = new EngineState();\n  }\n\n  ready() {\n    var _this81 = this;\n\n    return _asyncToGenerator(function* () {\n      if (null != _this81.pendingBackendInit) return _this81.pendingBackendInit.then(() => {});\n      if (null != _this81.backendInstance) return;\n\n      var e = _this81.getSortedBackends();\n\n      for (var t = 0; t < e.length; t++) {\n        var n = e[t];\n        if (yield _this81.initializeBackend(n).success) return void (yield _this81.setBackend(n));\n      }\n\n      throw new Error(\"Could not initialize any backends, all backend initializations failed.\");\n    })();\n  }\n\n  get backend() {\n    if (null != this.pendingBackendInit) throw new Error(\"Backend '\".concat(this.backendName, \"' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods\"));\n\n    if (null == this.backendInstance) {\n      var {\n        name: _e530,\n        asyncInit: t\n      } = this.initializeBackendsAndReturnBest();\n      if (t) throw new Error(\"The highest priority backend '\".concat(_e530, \"' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods\"));\n      this.setBackend(_e530);\n    }\n\n    return this.backendInstance;\n  }\n\n  backendNames() {\n    return Object.keys(this.registryFactory);\n  }\n\n  findBackend(e) {\n    if (!(e in this.registry)) {\n      if (!(e in this.registryFactory)) return null;\n      {\n        var {\n          asyncInit: t\n        } = this.initializeBackend(e);\n        if (t) return null;\n      }\n    }\n\n    return this.registry[e];\n  }\n\n  findBackendFactory(e) {\n    return e in this.registryFactory ? this.registryFactory[e].factory : null;\n  }\n\n  registerBackend(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n    return e in this.registryFactory ? (console.warn(\"\".concat(e, \" backend was already registered. Reusing existing backend factory.\")), !1) : (this.registryFactory[e] = {\n      factory: t,\n      priority: n\n    }, !0);\n  }\n\n  setBackend(e) {\n    var _this82 = this;\n\n    return _asyncToGenerator(function* () {\n      if (null == _this82.registryFactory[e]) throw new Error(\"Backend name '\".concat(e, \"' not found in registry\"));\n\n      if (_this82.backendName = e, null == _this82.registry[e]) {\n        _this82.backendInstance = null;\n\n        var {\n          success: t,\n          asyncInit: n\n        } = _this82.initializeBackend(e);\n\n        if (!(n ? yield t : t)) return !1;\n      }\n\n      return _this82.backendInstance = _this82.registry[e], _this82.setupRegisteredKernels(), _this82.profiler = new Profiler(_this82.backendInstance), !0;\n    })();\n  }\n\n  setupRegisteredKernels() {\n    getKernelsForBackend(this.backendName).forEach(e => {\n      null != e.setupFunc && e.setupFunc(this.backendInstance);\n    });\n  }\n\n  disposeRegisteredKernels(e) {\n    getKernelsForBackend(e).forEach(t => {\n      null != t.disposeFunc && t.disposeFunc(this.registry[e]);\n    });\n  }\n\n  initializeBackend(e) {\n    var t = this.registryFactory[e];\n    if (null == t) throw new Error(\"Cannot initialize backend \".concat(e, \", no registration found.\"));\n\n    try {\n      var n = t.factory();\n      if (!n || n instanceof KernelBackend || \"function\" != typeof n.then) return this.registry[e] = n, {\n        success: !0,\n        asyncInit: !1\n      };\n      {\n        var _t381 = ++this.pendingBackendInitId,\n            r = n.then(n => !(_t381 < this.pendingBackendInitId || (this.registry[e] = n, this.pendingBackendInit = null, 0))).catch(n => (_t381 < this.pendingBackendInitId || (this.pendingBackendInit = null, console.warn(\"Initialization of backend \".concat(e, \" failed\")), console.warn(n.stack || n.message)), !1));\n\n        return this.pendingBackendInit = r, {\n          success: r,\n          asyncInit: !0\n        };\n      }\n    } catch (t) {\n      return console.warn(\"Initialization of backend \".concat(e, \" failed\")), console.warn(t.stack || t.message), {\n        success: !1,\n        asyncInit: !1\n      };\n    }\n  }\n\n  removeBackend(e) {\n    if (!(e in this.registryFactory)) throw new Error(\"\".concat(e, \" backend not found in registry\"));\n    this.backendName === e && null != this.pendingBackendInit && this.pendingBackendInitId++, e in this.registry && (this.disposeRegisteredKernels(e), this.registry[e].dispose(), delete this.registry[e]), delete this.registryFactory[e], this.backendName === e && (this.pendingBackendInit = null, this.backendName = null, this.backendInstance = null);\n  }\n\n  getSortedBackends() {\n    if (0 === Object.keys(this.registryFactory).length) throw new Error(\"No backend found in registry.\");\n    return Object.keys(this.registryFactory).sort((e, t) => this.registryFactory[t].priority - this.registryFactory[e].priority);\n  }\n\n  initializeBackendsAndReturnBest() {\n    var e = this.getSortedBackends();\n\n    for (var t = 0; t < e.length; t++) {\n      var n = e[t],\n          {\n        success: r,\n        asyncInit: a\n      } = this.initializeBackend(n);\n      if (a || r) return {\n        name: n,\n        asyncInit: a\n      };\n    }\n\n    throw new Error(\"Could not initialize any backends, all backend initializations failed.\");\n  }\n\n  moveData(e, t) {\n    var n = this.state.tensorInfo.get(t),\n        r = n.backend,\n        a = this.readSync(t),\n        s = r.refCount(t);\n    r.disposeData(t, !0), n.backend = e, e.move(t, a, n.shape, n.dtype, s), this.shouldCheckForMemLeaks() && this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n  }\n\n  tidy(e, t) {\n    var n,\n        r = null;\n\n    if (null == t) {\n      if (\"function\" != typeof e) throw new Error(\"Please provide a function to tidy()\");\n      t = e;\n    } else {\n      if (\"string\" != typeof e && !(e instanceof String)) throw new Error(\"When calling with two arguments, the first argument to tidy() must be a string\");\n      if (\"function\" != typeof t) throw new Error(\"When calling with two arguments, the 2nd argument to tidy() must be a function\");\n      r = e;\n    }\n\n    return this.scopedRun(() => this.startScope(r), () => this.endScope(n), () => (n = t(), n instanceof Promise && console.error(\"Cannot return a Promise inside of tidy.\"), n));\n  }\n\n  scopedRun(e, t, n) {\n    e();\n\n    try {\n      var _e531 = n();\n\n      return t(), _e531;\n    } catch (e) {\n      throw t(), e;\n    }\n  }\n\n  nextTensorId() {\n    return Engine.nextTensorId++;\n  }\n\n  nextVariableId() {\n    return Engine.nextVariableId++;\n  }\n\n  clone(e) {\n    var t = ENGINE.runKernel(Identity$1, {\n      x: e\n    });\n    return this.addTapeNode(this.state.activeScope.name, {\n      x: e\n    }, [t], e => ({\n      x: () => ENGINE.runKernel(Cast, {\n        x: e\n      }, {\n        dtype: \"float32\"\n      })\n    }), [], {}), t;\n  }\n\n  runKernel(e, t, n) {\n    if (null == getKernel(e, this.backendName)) throw new Error(\"Kernel '\".concat(e, \"' not registered for backend '\").concat(this.backendName, \"'\"));\n    return this.runKernelFunc({\n      kernelName: e,\n      inputs: t,\n      attrs: n\n    });\n  }\n\n  shouldCheckForMemLeaks() {\n    return this.ENV.getBool(\"IS_TEST\");\n  }\n\n  checkKernelForMemLeak(e, t, n) {\n    var r = this.backend.numDataIds();\n    var a = 0;\n    n.forEach(e => {\n      a += \"complex64\" === e.dtype ? 3 : 1;\n    });\n    var s = r - t - a - this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    if (s > 0) throw new Error(\"Backend '\".concat(this.backendName, \"' has an internal memory leak (\").concat(s, \" data ids) after running '\").concat(e, \"'\"));\n  }\n\n  runKernelFunc(e) {\n    var t,\n        n = [];\n    var r = this.isTapeOn(),\n        a = this.state.numBytes,\n        s = this.state.numTensors;\n    var o, i;\n    this.shouldCheckForMemLeaks() && this.state.numDataMovesStack.push(0);\n    var l = isRegisteredKernelInvocation(e) ? e.kernelName : null != this.state.activeScope ? this.state.activeScope.name : \"\";\n\n    if (isRegisteredKernelInvocation(e)) {\n      var {\n        kernelName: _t382,\n        inputs: _a120,\n        attrs: _s92\n      } = e,\n          _l33 = getKernel(_t382, this.backendName);\n\n      assert$4(null != _l33, () => \"Cannot find registered kernel '\".concat(_t382, \"' for backend '\").concat(this.backendName, \"'\")), o = () => {\n        var e = this.backend.numDataIds();\n        i = _l33.kernelFunc({\n          inputs: _a120,\n          attrs: _s92,\n          backend: this.backend\n        });\n        var o = Array.isArray(i) ? i : [i];\n        this.shouldCheckForMemLeaks() && this.checkKernelForMemLeak(_t382, e, o);\n        var u = o.map(e => {\n          if (null != e.rank) return e;\n          var {\n            dataId: t,\n            shape: n,\n            dtype: r\n          } = e;\n          return this.makeTensorFromDataId(t, n, r);\n        });\n\n        if (r) {\n          var _e532 = this.getTensorsForGradient(_t382, _a120, u);\n\n          n = this.saveTensorsForBackwardMode(_e532);\n        }\n\n        return u;\n      };\n    } else {\n      var {\n        forwardFunc: _t383\n      } = e,\n          _a121 = e => {\n        r && (n = e.map(e => this.keep(this.clone(e))));\n      };\n\n      o = () => {\n        var e = this.backend.numDataIds();\n        i = this.tidy(() => _t383(this.backend, _a121));\n        var n = Array.isArray(i) ? i : [i];\n        return this.shouldCheckForMemLeaks() && this.checkKernelForMemLeak(l, e, n), n;\n      };\n    }\n\n    var {\n      inputs: u,\n      attrs: c\n    } = e,\n        p = isRegisteredKernelInvocation(e) ? null : e.backwardsFunc;\n    var d;\n    return this.scopedRun(() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n      this.ENV.getBool(\"DEBUG\") || this.state.profiling ? (d = this.profiler.profileKernel(l, u, () => o()), this.ENV.getBool(\"DEBUG\") && this.profiler.logKernelProfile(d), t = d.outputs) : t = o();\n    }), r && this.addTapeNode(l, u, t, p, n, c), this.state.profiling && this.state.activeProfile.kernels.push({\n      name: l,\n      bytesAdded: this.state.numBytes - a,\n      totalBytesSnapshot: this.state.numBytes,\n      tensorsAdded: this.state.numTensors - s,\n      totalTensorsSnapshot: this.state.numTensors,\n      inputShapes: Object.keys(u).map(e => null != u[e] ? u[e].shape : null),\n      outputShapes: t.map(e => e.shape),\n      kernelTimeMs: d.timeMs,\n      extraInfo: d.extraInfo\n    }), Array.isArray(i) ? t : t[0];\n  }\n\n  saveTensorsForBackwardMode(e) {\n    return e.map(e => this.keep(this.clone(e)));\n  }\n\n  getTensorsForGradient(e, t, n) {\n    var r = getGradient(e);\n\n    if (null != r) {\n      var _e533 = r.inputsToSave || [],\n          a = r.outputsToSave || [];\n\n      var s;\n      r.saveAllInputs ? (assert$4(Array.isArray(t), () => \"saveAllInputs is true, expected inputs to be an array.\"), s = Object.keys(t).map(e => t[e])) : s = _e533.map(e => t[e]);\n      var o = n.filter((e, t) => a[t]);\n      return s.concat(o);\n    }\n\n    return [];\n  }\n\n  makeTensor(e, t, n, r) {\n    if (null == e) throw new Error(\"Values passed to engine.makeTensor() are null\");\n    r = r || this.backend;\n    var a = e;\n    \"string\" === (n = n || \"float32\") && isString(e[0]) && (a = e.map(e => encodeString(e)));\n    var s = r.write(a, t, n),\n        o = new Tensor(t, n, s, this.nextTensorId());\n\n    if (this.trackTensor(o, r), \"string\" === n) {\n      var _e534 = this.state.tensorInfo.get(s),\n          _t384 = bytesFromStringArray(a);\n\n      this.state.numBytes += _t384 - _e534.bytes, _e534.bytes = _t384;\n    }\n\n    return o;\n  }\n\n  makeTensorFromDataId(e, t, n, r) {\n    var a = new Tensor(t, n = n || \"float32\", e, this.nextTensorId());\n    return this.trackTensor(a, r), a;\n  }\n\n  makeVariable(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    var r = arguments.length > 3 ? arguments[3] : undefined;\n    n = n || this.nextVariableId().toString(), null != r && r !== e.dtype && (e = e.cast(r));\n    var a = new Variable(e, t, n, this.nextTensorId());\n    if (null != this.state.registeredVariables[a.name]) throw new Error(\"Variable with name \".concat(a.name, \" was already registered\"));\n    return this.state.registeredVariables[a.name] = a, this.incRef(a, this.backend), a;\n  }\n\n  trackTensor(e, t) {\n    this.state.numTensors++, \"string\" === e.dtype && this.state.numStringTensors++;\n    var n = 0;\n    \"complex64\" !== e.dtype && \"string\" !== e.dtype && (n = e.size * bytesPerElement(e.dtype)), this.state.numBytes += n, this.state.tensorInfo.has(e.dataId) || (this.state.numDataBuffers++, this.state.tensorInfo.set(e.dataId, {\n      backend: t || this.backend,\n      dtype: e.dtype,\n      shape: e.shape,\n      bytes: n\n    })), e instanceof Variable || this.track(e);\n  }\n\n  incRef(e, t) {\n    this.trackTensor(e, t), this.backend.incRef(e.dataId);\n  }\n\n  removeDataId(e, t) {\n    this.state.tensorInfo.has(e) && this.state.tensorInfo.get(e).backend === t && (this.state.tensorInfo.delete(e), this.state.numDataBuffers--);\n  }\n\n  disposeTensor(e) {\n    if (!this.state.tensorInfo.has(e.dataId)) return;\n    var t = this.state.tensorInfo.get(e.dataId);\n\n    if (this.state.numTensors--, \"string\" === e.dtype && (this.state.numStringTensors--, this.state.numBytes -= t.bytes), \"complex64\" !== e.dtype && \"string\" !== e.dtype) {\n      var _t385 = e.size * bytesPerElement(e.dtype);\n\n      this.state.numBytes -= _t385;\n    }\n\n    t.backend.disposeData(e.dataId) && this.removeDataId(e.dataId, t.backend);\n  }\n\n  disposeVariables() {\n    for (var _e535 in this.state.registeredVariables) {\n      this.disposeVariable(this.state.registeredVariables[_e535]);\n    }\n  }\n\n  disposeVariable(e) {\n    this.disposeTensor(e), null != this.state.registeredVariables[e.name] && delete this.state.registeredVariables[e.name];\n  }\n\n  memory() {\n    var e = this.backend.memory();\n    return e.numTensors = this.state.numTensors, e.numDataBuffers = this.state.numDataBuffers, e.numBytes = this.state.numBytes, this.state.numStringTensors > 0 && (e.unreliable = !0, null == e.reasons && (e.reasons = []), e.reasons.push(\"Memory usage by string tensors is approximate (2 bytes per character)\")), e;\n  }\n\n  profile(e) {\n    var _this83 = this;\n\n    return _asyncToGenerator(function* () {\n      _this83.state.profiling = !0;\n      var t = _this83.state.numBytes,\n          n = _this83.state.numTensors;\n      _this83.state.activeProfile.kernels = [], _this83.state.activeProfile.result = yield e(), _this83.state.profiling = !1, _this83.state.activeProfile.peakBytes = Math.max(..._this83.state.activeProfile.kernels.map(e => e.totalBytesSnapshot)), _this83.state.activeProfile.newBytes = _this83.state.numBytes - t, _this83.state.activeProfile.newTensors = _this83.state.numTensors - n;\n\n      for (var _e536 of _this83.state.activeProfile.kernels) {\n        _e536.kernelTimeMs = yield _e536.kernelTimeMs, _e536.extraInfo = yield _e536.extraInfo;\n      }\n\n      return _this83.state.activeProfile;\n    })();\n  }\n\n  isTapeOn() {\n    return this.state.gradientDepth > 0 && 0 === this.state.kernelDepth;\n  }\n\n  addTapeNode(e, t, n, r, a, s) {\n    var o = {\n      id: this.state.nextTapeNodeId++,\n      kernelName: e,\n      inputs: t,\n      outputs: n,\n      saved: a\n    },\n        i = getGradient(e);\n    null != i && (r = i.gradFunc), null != r && (o.gradient = e => (e = e.map((e, t) => {\n      if (null == e) {\n        var _e537 = n[t],\n            _r184 = makeZerosTypedArray(_e537.size, _e537.dtype);\n\n        return this.makeTensor(_r184, _e537.shape, _e537.dtype);\n      }\n\n      return e;\n    }), r(e.length > 1 ? e : e[0], a, s))), this.state.activeTape.push(o);\n  }\n\n  keep(e) {\n    return e.kept = !0, e;\n  }\n\n  startTape() {\n    0 === this.state.gradientDepth && (this.state.activeTape = []), this.state.gradientDepth++;\n  }\n\n  endTape() {\n    this.state.gradientDepth--;\n  }\n\n  startScope(e) {\n    var t = {\n      track: [],\n      name: \"unnamed scope\",\n      id: this.state.nextScopeId++\n    };\n    e && (t.name = e), this.state.scopeStack.push(t), this.state.activeScope = t;\n  }\n\n  endScope(e) {\n    var t = getTensorsInContainer(e),\n        n = new Set(t.map(e => e.id));\n\n    for (var _e538 = 0; _e538 < this.state.activeScope.track.length; _e538++) {\n      var _t386 = this.state.activeScope.track[_e538];\n      _t386.kept || n.has(_t386.id) || _t386.dispose();\n    }\n\n    var r = this.state.scopeStack.pop();\n    this.state.activeScope = 0 === this.state.scopeStack.length ? null : this.state.scopeStack[this.state.scopeStack.length - 1], t.forEach(e => {\n      e.kept || e.scopeId !== r.id || this.track(e);\n    });\n  }\n\n  gradients(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    if (assert$4(t.length > 0, () => \"gradients() received an empty list of xs.\"), null != n && \"float32\" !== n.dtype) throw new Error(\"dy must have 'float32' dtype, but has '\".concat(n.dtype, \"'\"));\n    var a = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy(\"forward\", e));\n    assert$4(a instanceof Tensor, () => \"The result y returned by f() must be a tensor.\");\n    var s = getFilteredNodesXToY(this.state.activeTape, t, a);\n    if (!r && 0 === s.length && t.length > 0) throw new Error(\"Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.\");\n    return this.tidy(\"backward\", () => {\n      var e = {};\n      e[a.id] = null == n ? ones$2(a.shape) : n, backpropagateGradients(e, s, e => this.tidy(e), add$3);\n      var r = t.map(t => e[t.id]);\n      return 0 === this.state.gradientDepth && (this.state.activeTape.forEach(e => {\n        for (var _t387 of e.saved) {\n          _t387.dispose();\n        }\n      }), this.state.activeTape = null), {\n        value: a,\n        grads: r\n      };\n    });\n  }\n\n  customGrad(e) {\n    var _this84 = this;\n\n    return assert$4(isFunction(e), () => \"The f passed in customGrad(f) must be a function.\"), function () {\n      for (var _len8 = arguments.length, t = new Array(_len8), _key8 = 0; _key8 < _len8; _key8++) {\n        t[_key8] = arguments[_key8];\n      }\n\n      var n;\n      assert$4(t.every(e => e instanceof Tensor), () => \"The args passed in customGrad(f)(x1, x2,...) must all be tensors\");\n      var r = {};\n      return t.forEach((e, t) => {\n        r[t] = e;\n      }), _this84.runKernelFunc({\n        forwardFunc: (r, a) => (n = e(...t, a), assert$4(n.value instanceof Tensor, () => \"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor\"), assert$4(isFunction(n.gradFunc), () => \"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.\"), n.value),\n        backwardsFunc: (e, r) => {\n          var a = n.gradFunc(e, r),\n              s = Array.isArray(a) ? a : [a];\n          assert$4(s.length === t.length, () => \"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).\"), assert$4(s.every(e => e instanceof Tensor), () => \"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors.\");\n          var o = {};\n          return s.forEach((e, t) => {\n            o[t] = () => e;\n          }), o;\n        },\n        inputs: r\n      });\n    };\n  }\n\n  readSync(e) {\n    return this.state.tensorInfo.get(e).backend.readSync(e);\n  }\n\n  read(e) {\n    return this.state.tensorInfo.get(e).backend.read(e);\n  }\n\n  time(e) {\n    var _this85 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = now(),\n          n = yield _this85.backend.time(e);\n      return n.wallMs = now() - t, n;\n    })();\n  }\n\n  track(e) {\n    return null != this.state.activeScope && (e.scopeId = this.state.activeScope.id, this.state.activeScope.track.push(e)), e;\n  }\n\n  get registeredVariables() {\n    return this.state.registeredVariables;\n  }\n\n  reset() {\n    this.pendingBackendInitId++, this.state.dispose(), this.ENV.reset(), this.state = new EngineState();\n\n    for (var _e539 in this.registry) {\n      this.disposeRegisteredKernels(_e539), this.registry[_e539].dispose(), delete this.registry[_e539];\n    }\n\n    this.backendName = null, this.backendInstance = null, this.pendingBackendInit = null;\n  }\n\n}\n\nfunction ones$2(e) {\n  var t = makeOnesTypedArray(sizeFromShape(e), \"float32\");\n  return ENGINE.makeTensor(t, e, \"float32\");\n}\n\nfunction getOrMakeEngine() {\n  var e = getGlobalNamespace();\n\n  if (null == e._tfengine) {\n    var t = new Environment(e);\n    e._tfengine = new Engine(t);\n  }\n\n  return setEnvironmentGlobal(e._tfengine.ENV), setTensorTracker(() => e._tfengine), e._tfengine;\n}\n\nEngine.nextTensorId = 0, Engine.nextVariableId = 0;\nvar ENGINE = getOrMakeEngine();\n\nfunction add$3(e, t) {\n  return ENGINE.runKernel(Add$1, {\n    a: e,\n    b: t\n  });\n}\n\nfunction _isNavigatorDefined() {\n  return \"undefined\" != typeof navigator && null != navigator;\n}\n\nfunction isMobile(e) {\n  if (e || _isNavigatorDefined()) {\n    if (e || (e = navigator), \"ReactNative\" === e.product) return !0;\n    var t = e.userAgent || e.vendor || window.opera;\n    return /(android|bb\\d+|meego).+mobile|avantgo|bada\\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i.test(t) || /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\/|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\/)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\/(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\/|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\/|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-/i.test(t.substr(0, 4));\n  }\n\n  return !1;\n}\n\nfunction isBrowser() {\n  return \"undefined\" != typeof window && null != window.document || \"undefined\" != typeof WorkerGlobalScope;\n}\n\nvar device_util = {\n  __proto__: null,\n  isMobile,\n  isBrowser\n};\nvar ENV$1 = env();\n\nfunction inferShape(e, t) {\n  var n = e;\n  if (isTypedArray(e)) return \"string\" === t ? [] : [e.length];\n  if (!Array.isArray(e)) return [];\n  var r = [];\n\n  for (; Array.isArray(n) || isTypedArray(n) && \"string\" !== t;) {\n    r.push(n.length), n = n[0];\n  }\n\n  return Array.isArray(e) && env().getBool(\"TENSORLIKE_CHECK_SHAPE_CONSISTENCY\") && deepAssertShapeConsistency(e, r, []), r;\n}\n\nfunction deepAssertShapeConsistency(e, t, n) {\n  if (n = n || [], !Array.isArray(e) && !isTypedArray(e)) return void assert$4(0 === t.length, () => \"Element arr[\".concat(n.join(\"][\"), \"] is a primitive, but should be an array/TypedArray of \").concat(t[0], \" elements\"));\n  assert$4(t.length > 0, () => \"Element arr[\".concat(n.join(\"][\"), \"] should be a primitive, but is an array of \").concat(e.length, \" elements\")), assert$4(e.length === t[0], () => \"Element arr[\".concat(n.join(\"][\"), \"] should have \").concat(t[0], \" elements, but has \").concat(e.length, \" elements\"));\n  var r = t.slice(1);\n\n  for (var _t388 = 0; _t388 < e.length; ++_t388) {\n    deepAssertShapeConsistency(e[_t388], r, n.concat(_t388));\n  }\n}\n\nfunction assertDtype(e, t, n, r) {\n  if (\"string_or_numeric\" !== e) {\n    if (null == e) throw new Error(\"Expected dtype cannot be null.\");\n    if (\"numeric\" !== e && e !== t || \"numeric\" === e && \"string\" === t) throw new Error(\"Argument '\".concat(n, \"' passed to '\").concat(r, \"' must be \").concat(e, \" tensor, but got \").concat(t, \" tensor\"));\n  }\n}\n\nfunction convertToTensor(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"numeric\";\n  if (e instanceof Tensor) return assertDtype(r, e.dtype, t, n), e;\n  var a = inferDtype(e);\n  if (\"string\" !== a && [\"bool\", \"int32\", \"float32\"].indexOf(r) >= 0 && (a = r), assertDtype(r, a, t, n), null == e || !isTypedArray(e) && !Array.isArray(e) && \"number\" != typeof e && \"boolean\" != typeof e && \"string\" != typeof e) throw new Error(\"Argument '\".concat(t, \"' passed to '\").concat(n, \"' must be a Tensor or TensorLike, but got '\").concat(null == e ? \"null\" : e.constructor.name, \"'\"));\n  var s = inferShape(e, a);\n  isTypedArray(e) || Array.isArray(e) || (e = [e]);\n  var o = \"string\" !== a ? toTypedArray(e, a) : flatten$3(e, [], !0);\n  return ENGINE.makeTensor(o, s, a);\n}\n\nfunction convertToTensorArray(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"numeric\";\n  if (!Array.isArray(e)) throw new Error(\"Argument \".concat(t, \" passed to \").concat(n, \" must be a `Tensor[]` or `TensorLike[]`\"));\n  return e.map((e, a) => convertToTensor(e, \"\".concat(t, \"[\").concat(a, \"]\"), n, r));\n}\n\nENV$1.registerFlag(\"DEBUG\", () => !1, e => {\n  e && console.warn(\"Debugging mode is ON. The output of every math call will be downloaded to CPU and checked for NaNs. This significantly impacts performance.\");\n}), ENV$1.registerFlag(\"IS_BROWSER\", () => isBrowser()), ENV$1.registerFlag(\"IS_NODE\", () => \"undefined\" != typeof process && void 0 !== process.versions && void 0 !== process.versions.node), ENV$1.registerFlag(\"IS_CHROME\", () => \"undefined\" != typeof navigator && null != navigator && null != navigator.userAgent && /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor)), ENV$1.registerFlag(\"PROD\", () => !1), ENV$1.registerFlag(\"TENSORLIKE_CHECK_SHAPE_CONSISTENCY\", () => ENV$1.getBool(\"DEBUG\")), ENV$1.registerFlag(\"DEPRECATION_WARNINGS_ENABLED\", () => !0), ENV$1.registerFlag(\"IS_TEST\", () => !1), ENV$1.registerFlag(\"CHECK_COMPUTATION_FOR_ERRORS\", () => !0), ENV$1.registerFlag(\"WRAP_TO_IMAGEBITMAP\", () => !1);\nvar OP_SCOPE_SUFFIX = \"__op\";\n\nfunction op(e) {\n  var t = Object.keys(e);\n  if (1 !== t.length) throw new Error(\"Please provide an object with a single key (operation name) mapping to a function. Got an object with \".concat(t.length, \" keys.\"));\n  var n = t[0];\n  var r = e[n];\n  n.endsWith(\"_\") && (n = n.substring(0, n.length - 1)), n += OP_SCOPE_SUFFIX;\n\n  var a = function a() {\n    ENGINE.startScope(n);\n\n    try {\n      var _t389 = r(...arguments);\n\n      return isPromise(_t389) && console.error(\"Cannot return a Promise inside of tidy.\"), ENGINE.endScope(_t389), _t389;\n    } catch (e) {\n      throw ENGINE.endScope(null), e;\n    }\n  };\n\n  return Object.defineProperty(a, \"name\", {\n    value: n,\n    configurable: !0\n  }), a;\n}\n\nfunction complex_(e, t) {\n  var n = convertToTensor(e, \"real\", \"complex\"),\n      r = convertToTensor(t, \"imag\", \"complex\");\n  return assertShapesMatch(n.shape, r.shape, \"real and imag shapes, \".concat(n.shape, \" and \").concat(r.shape, \", must match in call to tf.complex().\")), ENGINE.runKernel(Complex, {\n    real: n,\n    imag: r\n  });\n}\n\nvar complex$2 = op({\n  complex_\n});\n\nfunction makeTensor(e, t, n, r) {\n  if (null == r && (r = inferDtype(e)), \"complex64\" === r) throw new Error(\"Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).\");\n  if (!isTypedArray(e) && !Array.isArray(e) && \"number\" != typeof e && \"boolean\" != typeof e && \"string\" != typeof e) throw new Error(\"values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray\");\n\n  if (null != t) {\n    assertNonNegativeIntegerDimensions(t);\n\n    var _e540 = sizeFromShape(t),\n        _r185 = sizeFromShape(n);\n\n    assert$4(_e540 === _r185, () => \"Based on the provided shape, [\".concat(t, \"], the tensor should have \").concat(_e540, \" values but has \").concat(_r185));\n\n    for (var _e541 = 0; _e541 < n.length; ++_e541) {\n      var _r186 = n[_e541],\n          a = _e541 !== n.length - 1 || _r186 !== sizeFromShape(t.slice(_e541));\n\n      assert$4(n[_e541] === t[_e541] || !a, () => \"Error creating a new Tensor. Inferred shape (\".concat(n, \") does not match the provided shape (\").concat(t, \"). \"));\n    }\n  }\n\n  return isTypedArray(e) || Array.isArray(e) || (e = [e]), t = t || n, e = \"string\" !== r ? toTypedArray(e, r) : flatten$3(e, [], !0), ENGINE.makeTensor(e, t, r);\n}\n\nfunction tensor(e, t, n) {\n  return makeTensor(e, t, inferShape(e, n), n);\n}\n\nvar DTYPE_VALUE_SIZE_MAP = {\n  float32: 4,\n  float16: 2,\n  int32: 4,\n  uint16: 2,\n  uint8: 1,\n  bool: 1,\n  complex64: 8\n},\n    NUM_BYTES_STRING_LENGTH = 4;\n\nfunction encodeWeights(_x51, _x52) {\n  return _encodeWeights.apply(this, arguments);\n}\n\nfunction _encodeWeights() {\n  _encodeWeights = _asyncToGenerator(function* (e, t) {\n    var n = [],\n        r = [],\n        a = Array.isArray(e) ? e.map(e => e.name) : Object.keys(e);\n\n    var _loop65 = function _loop65(s) {\n      var o = a[s],\n          i = Array.isArray(e) ? e[s].tensor : e[o];\n      if (\"float32\" !== i.dtype && \"int32\" !== i.dtype && \"bool\" !== i.dtype && \"string\" !== i.dtype && \"complex64\" !== i.dtype) throw new Error(\"Unsupported dtype in weight '\".concat(o, \"': \").concat(i.dtype));\n      var l = {\n        name: o,\n        shape: i.shape,\n        dtype: i.dtype\n      };\n\n      if (\"string\" === i.dtype) {\n        var _e1140 = new Promise( /*#__PURE__*/function () {\n          var _ref76 = _asyncToGenerator(function* (e) {\n            var t = yield i.bytes(),\n                n = t.reduce((e, t) => e + t.length, 0) + NUM_BYTES_STRING_LENGTH * t.length,\n                r = new Uint8Array(n);\n            var a = 0;\n\n            for (var _e1141 = 0; _e1141 < t.length; _e1141++) {\n              var _n454 = t[_e1141],\n                  _s225 = new Uint8Array(new Uint32Array([_n454.length]).buffer);\n\n              r.set(_s225, a), a += NUM_BYTES_STRING_LENGTH, r.set(_n454, a), a += _n454.length;\n            }\n\n            e(r);\n          });\n\n          return function (_x146) {\n            return _ref76.apply(this, arguments);\n          };\n        }());\n\n        r.push(_e1140);\n      } else r.push(i.data());\n\n      null != t && (l.group = t), n.push(l);\n    };\n\n    for (var s = 0; s < a.length; ++s) {\n      _loop65(s);\n    }\n\n    return {\n      data: concatenateTypedArrays(yield Promise.all(r)),\n      specs: n\n    };\n  });\n  return _encodeWeights.apply(this, arguments);\n}\n\nfunction decodeWeights(e, t) {\n  var n = {};\n  var r,\n      a = 0;\n\n  for (var s of t) {\n    var _t390 = s.name,\n        o = s.dtype,\n        i = s.shape,\n        l = sizeFromShape(i);\n    var u = void 0;\n\n    if (\"quantization\" in s) {\n      var _n225 = s.quantization;\n\n      if (\"uint8\" === _n225.dtype || \"uint16\" === _n225.dtype) {\n        if (!(\"min\" in _n225) || !(\"scale\" in _n225)) throw new Error(\"Weight \".concat(s.name, \" with quantization \").concat(_n225.dtype, \" doesn't have corresponding metadata min and scale.\"));\n      } else {\n        if (\"float16\" !== _n225.dtype) throw new Error(\"Weight \".concat(s.name, \" has unknown quantization dtype \").concat(_n225.dtype, \". Supported quantization dtypes are: 'uint8', 'uint16', and 'float16'.\"));\n        if (\"float32\" !== o) throw new Error(\"Weight \".concat(s.name, \" is quantized with \").concat(_n225.dtype, \" which only supports weights of type float32 not \").concat(o, \".\"));\n      }\n\n      var _i42 = DTYPE_VALUE_SIZE_MAP[_n225.dtype],\n          c = e.slice(a, a + l * _i42),\n          _p18 = \"uint8\" === _n225.dtype ? new Uint8Array(c) : new Uint16Array(c);\n\n      if (\"float32\" === o) {\n        if (\"uint8\" === _n225.dtype || \"uint16\" === _n225.dtype) {\n          u = new Float32Array(_p18.length);\n\n          for (var _e542 = 0; _e542 < _p18.length; _e542++) {\n            u[_e542] = _p18[_e542] * _n225.scale + _n225.min;\n          }\n        } else {\n          if (\"float16\" !== _n225.dtype) throw new Error(\"Unsupported quantization type \".concat(_n225.dtype, \" for weight type float32.\"));\n          void 0 === r && (r = getFloat16Decoder()), u = r(_p18);\n        }\n      } else {\n        if (\"int32\" !== o) throw new Error(\"Unsupported dtype in weight '\".concat(_t390, \"': \").concat(o));\n        if (\"uint8\" !== _n225.dtype && \"uint16\" !== _n225.dtype) throw new Error(\"Unsupported quantization type \".concat(_n225.dtype, \" for weight type int32.\"));\n        u = new Int32Array(_p18.length);\n\n        for (var _e543 = 0; _e543 < _p18.length; _e543++) {\n          u[_e543] = Math.round(_p18[_e543] * _n225.scale + _n225.min);\n        }\n      }\n      a += l * _i42;\n    } else if (\"string\" === o) {\n      var _t391 = sizeFromShape(s.shape);\n\n      u = [];\n\n      for (var _n226 = 0; _n226 < _t391; _n226++) {\n        var _t392 = new Uint32Array(e.slice(a, a + NUM_BYTES_STRING_LENGTH))[0];\n        a += NUM_BYTES_STRING_LENGTH;\n\n        var _n227 = new Uint8Array(e.slice(a, a + _t392));\n\n        u.push(_n227), a += _t392;\n      }\n    } else {\n      var _r187 = DTYPE_VALUE_SIZE_MAP[o],\n          _s93 = e.slice(a, a + l * _r187);\n\n      if (\"float32\" === o) u = new Float32Array(_s93);else if (\"int32\" === o) u = new Int32Array(_s93);else if (\"bool\" === o) u = new Uint8Array(_s93);else {\n        if (\"complex64\" !== o) throw new Error(\"Unsupported dtype in weight '\".concat(_t390, \"': \").concat(o));\n        {\n          u = new Float32Array(_s93);\n\n          var _e544 = new Float32Array(u.length / 2),\n              _r188 = new Float32Array(u.length / 2);\n\n          for (var _t393 = 0; _t393 < _e544.length; _t393++) {\n            _e544[_t393] = u[2 * _t393], _r188[_t393] = u[2 * _t393 + 1];\n          }\n\n          var _a122 = tensor(_e544, i, \"float32\"),\n              _o67 = tensor(_r188, i, \"float32\");\n\n          n[_t390] = complex$2(_a122, _o67), _a122.dispose(), _o67.dispose();\n        }\n      }\n      a += l * _r187;\n    }\n\n    \"complex64\" !== o && (n[_t390] = tensor(u, i, o));\n  }\n\n  return n;\n}\n\nfunction concatenateTypedArrays(e) {\n  if (null === e) throw new Error(\"Invalid input value: \".concat(JSON.stringify(e)));\n  var t = 0;\n  var n = [];\n  e.forEach(e => {\n    if (t += e.byteLength, n.push(e.byteLength === e.buffer.byteLength ? e : new e.constructor(e)), !(e instanceof Float32Array || e instanceof Int32Array || e instanceof Uint8Array)) throw new Error(\"Unsupported TypedArray subtype: \".concat(e.constructor.name));\n  });\n  var r = new Uint8Array(t);\n  var a = 0;\n  return n.forEach(e => {\n    r.set(new Uint8Array(e.buffer), a), a += e.byteLength;\n  }), r.buffer;\n}\n\nvar useNodeBuffer = \"undefined\" != typeof Buffer && (\"undefined\" == typeof Blob || \"undefined\" == typeof atob || \"undefined\" == typeof btoa);\n\nfunction stringByteLength(e) {\n  return useNodeBuffer ? Buffer.byteLength(e) : new Blob([e]).size;\n}\n\nfunction arrayBufferToBase64String(e) {\n  if (useNodeBuffer) return Buffer.from(e).toString(\"base64\");\n  var t = new Uint8Array(e);\n  var n = \"\";\n\n  for (var _e545 = 0, r = t.length; _e545 < r; _e545++) {\n    n += String.fromCharCode(t[_e545]);\n  }\n\n  return btoa(n);\n}\n\nfunction base64StringToArrayBuffer(e) {\n  if (useNodeBuffer) {\n    var _t394 = Buffer.from(e, \"base64\");\n\n    return _t394.buffer.slice(_t394.byteOffset, _t394.byteOffset + _t394.byteLength);\n  }\n\n  var t = atob(e),\n      n = new Uint8Array(t.length);\n\n  for (var _e546 = 0; _e546 < t.length; ++_e546) {\n    n.set([t.charCodeAt(_e546)], _e546);\n  }\n\n  return n.buffer;\n}\n\nfunction concatenateArrayBuffers(e) {\n  if (1 === e.length) return e[0];\n  var t = 0;\n  e.forEach(e => {\n    t += e.byteLength;\n  });\n  var n = new Uint8Array(t);\n  var r = 0;\n  return e.forEach(e => {\n    n.set(new Uint8Array(e), r), r += e.byteLength;\n  }), n.buffer;\n}\n\nfunction basename(e) {\n  for (e = e.trim(); e.endsWith(\"/\");) {\n    e = e.slice(0, e.length - 1);\n  }\n\n  var t = e.split(\"/\");\n  return t[t.length - 1];\n}\n\nfunction getModelJSONForModelArtifacts(e, t) {\n  var n = {\n    modelTopology: e.modelTopology,\n    format: e.format,\n    generatedBy: e.generatedBy,\n    convertedBy: e.convertedBy,\n    weightsManifest: t\n  };\n  return null != e.signature && (n.signature = e.signature), null != e.userDefinedMetadata && (n.userDefinedMetadata = e.userDefinedMetadata), null != e.modelInitializer && (n.modelInitializer = e.modelInitializer), null != e.trainingConfig && (n.trainingConfig = e.trainingConfig), n;\n}\n\nfunction getModelArtifactsForJSON(_x53, _x54) {\n  return _getModelArtifactsForJSON.apply(this, arguments);\n}\n\nfunction _getModelArtifactsForJSON() {\n  _getModelArtifactsForJSON = _asyncToGenerator(function* (e, t) {\n    var n = {\n      modelTopology: e.modelTopology,\n      format: e.format,\n      generatedBy: e.generatedBy,\n      convertedBy: e.convertedBy\n    };\n\n    if (null != e.trainingConfig && (n.trainingConfig = e.trainingConfig), null != e.weightsManifest) {\n      var [r, a] = yield t(e.weightsManifest);\n      n.weightSpecs = r, n.weightData = a;\n    }\n\n    return null != e.signature && (n.signature = e.signature), null != e.userDefinedMetadata && (n.userDefinedMetadata = e.userDefinedMetadata), null != e.modelInitializer && (n.modelInitializer = e.modelInitializer), n;\n  });\n  return _getModelArtifactsForJSON.apply(this, arguments);\n}\n\nfunction getModelArtifactsInfoForJSON(e) {\n  if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"Expected JSON model topology, received ArrayBuffer.\");\n  return {\n    dateSaved: new Date(),\n    modelTopologyType: \"JSON\",\n    modelTopologyBytes: null == e.modelTopology ? 0 : stringByteLength(JSON.stringify(e.modelTopology)),\n    weightSpecsBytes: null == e.weightSpecs ? 0 : stringByteLength(JSON.stringify(e.weightSpecs)),\n    weightDataBytes: null == e.weightData ? 0 : e.weightData.byteLength\n  };\n}\n\nfunction computeFloat16MantisaTable() {\n  var e = e => {\n    var t = e << 13,\n        n = 0;\n\n    for (; 0 == (8388608 & t);) {\n      n -= 8388608, t <<= 1;\n    }\n\n    return t &= -8388609, n += 947912704, t | n;\n  },\n      t = new Uint32Array(2048);\n\n  t[0] = 0;\n\n  for (var n = 1; n < 1024; n++) {\n    t[n] = e(n);\n  }\n\n  for (var _e547 = 1024; _e547 < 2048; _e547++) {\n    t[_e547] = 939524096 + (_e547 - 1024 << 13);\n  }\n\n  return t;\n}\n\nfunction computeFloat16ExponentTable() {\n  var e = new Uint32Array(64);\n  e[0] = 0, e[31] = 1199570944, e[32] = 2147483648, e[63] = 3347054592;\n\n  for (var t = 1; t < 31; t++) {\n    e[t] = t << 23;\n  }\n\n  for (var _t395 = 33; _t395 < 63; _t395++) {\n    e[_t395] = 2147483648 + (_t395 - 32 << 23);\n  }\n\n  return e;\n}\n\nfunction computeFloat16OffsetTable() {\n  var e = new Uint32Array(64);\n\n  for (var t = 0; t < 64; t++) {\n    e[t] = 1024;\n  }\n\n  return e[0] = e[32] = 0, e;\n}\n\nfunction getFloat16Decoder() {\n  var e = computeFloat16MantisaTable(),\n      t = computeFloat16ExponentTable(),\n      n = computeFloat16OffsetTable();\n  return r => {\n    var a = new ArrayBuffer(4 * r.length),\n        s = new Uint32Array(a);\n\n    for (var _a123 = 0; _a123 < r.length; _a123++) {\n      var o = r[_a123];\n      s[_a123] = e[n[o >> 10] + (1023 & o)] + t[o >> 10];\n    }\n\n    return new Float32Array(a);\n  };\n}\n\nclass IORouterRegistry {\n  constructor() {\n    this.saveRouters = [], this.loadRouters = [];\n  }\n\n  static getInstance() {\n    return null == IORouterRegistry.instance && (IORouterRegistry.instance = new IORouterRegistry()), IORouterRegistry.instance;\n  }\n\n  static registerSaveRouter(e) {\n    IORouterRegistry.getInstance().saveRouters.push(e);\n  }\n\n  static registerLoadRouter(e) {\n    IORouterRegistry.getInstance().loadRouters.push(e);\n  }\n\n  static getSaveHandlers(e) {\n    return IORouterRegistry.getHandlers(e, \"save\");\n  }\n\n  static getLoadHandlers(e, t) {\n    return IORouterRegistry.getHandlers(e, \"load\", t);\n  }\n\n  static getHandlers(e, t, n) {\n    var r = [];\n    return (\"load\" === t ? IORouterRegistry.getInstance().loadRouters : IORouterRegistry.getInstance().saveRouters).forEach(t => {\n      var a = t(e, n);\n      null !== a && r.push(a);\n    }), r;\n  }\n\n}\n\nvar registerSaveRouter = e => IORouterRegistry.registerSaveRouter(e),\n    registerLoadRouter = e => IORouterRegistry.registerLoadRouter(e),\n    getSaveHandlers = e => IORouterRegistry.getSaveHandlers(e),\n    getLoadHandlers = (e, t) => IORouterRegistry.getLoadHandlers(e, t),\n    DATABASE_NAME = \"tensorflowjs\",\n    DATABASE_VERSION = 1,\n    MODEL_STORE_NAME = \"models_store\",\n    INFO_STORE_NAME = \"model_info_store\";\n\nfunction getIndexedDBFactory() {\n  if (!env().getBool(\"IS_BROWSER\")) throw new Error(\"Failed to obtain IndexedDB factory because the current environmentis not a web browser.\");\n  var e = \"undefined\" == typeof window ? self : window,\n      t = e.indexedDB || e.mozIndexedDB || e.webkitIndexedDB || e.msIndexedDB || e.shimIndexedDB;\n  if (null == t) throw new Error(\"The current browser does not appear to support IndexedDB.\");\n  return t;\n}\n\nfunction setUpDatabase(e) {\n  var t = e.result;\n  t.createObjectStore(MODEL_STORE_NAME, {\n    keyPath: \"modelPath\"\n  }), t.createObjectStore(INFO_STORE_NAME, {\n    keyPath: \"modelPath\"\n  });\n}\n\nclass BrowserIndexedDB {\n  constructor(e) {\n    if (this.indexedDB = getIndexedDBFactory(), null == e || !e) throw new Error(\"For IndexedDB, modelPath must not be null, undefined or empty.\");\n    this.modelPath = e;\n  }\n\n  save(e) {\n    var _this86 = this;\n\n    return _asyncToGenerator(function* () {\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserLocalStorage.save() does not support saving model topology in binary formats yet.\");\n      return _this86.databaseAction(_this86.modelPath, e);\n    })();\n  }\n\n  load() {\n    var _this87 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this87.databaseAction(_this87.modelPath);\n    })();\n  }\n\n  databaseAction(e, t) {\n    return new Promise((e, n) => {\n      var r = this.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);\n      r.onupgradeneeded = () => setUpDatabase(r), r.onsuccess = () => {\n        var a = r.result;\n\n        if (null == t) {\n          var _t396 = a.transaction(MODEL_STORE_NAME, \"readonly\"),\n              _r189 = _t396.objectStore(MODEL_STORE_NAME).get(this.modelPath);\n\n          _r189.onsuccess = () => {\n            if (null == _r189.result) return a.close(), n(new Error(\"Cannot find model with path '\".concat(this.modelPath, \"' in IndexedDB.\")));\n            e(_r189.result.modelArtifacts);\n          }, _r189.onerror = e => (a.close(), n(_r189.error)), _t396.oncomplete = () => a.close();\n        } else {\n          var _r190 = getModelArtifactsInfoForJSON(t),\n              s = a.transaction(INFO_STORE_NAME, \"readwrite\");\n\n          var o = s.objectStore(INFO_STORE_NAME);\n          var i = o.put({\n            modelPath: this.modelPath,\n            modelArtifactsInfo: _r190\n          });\n          var l;\n          i.onsuccess = () => {\n            l = a.transaction(MODEL_STORE_NAME, \"readwrite\");\n            var i = l.objectStore(MODEL_STORE_NAME).put({\n              modelPath: this.modelPath,\n              modelArtifacts: t,\n              modelArtifactsInfo: _r190\n            });\n            i.onsuccess = () => e({\n              modelArtifactsInfo: _r190\n            }), i.onerror = e => {\n              o = s.objectStore(INFO_STORE_NAME);\n              var t = o.delete(this.modelPath);\n              t.onsuccess = () => (a.close(), n(i.error)), t.onerror = e => (a.close(), n(i.error));\n            };\n          }, i.onerror = e => (a.close(), n(i.error)), s.oncomplete = () => {\n            null == l ? a.close() : l.oncomplete = () => a.close();\n          };\n        }\n      }, r.onerror = e => n(r.error);\n    });\n  }\n\n}\n\nBrowserIndexedDB.URL_SCHEME = \"indexeddb://\";\n\nvar indexedDBRouter = e => env().getBool(\"IS_BROWSER\") && !Array.isArray(e) && e.startsWith(BrowserIndexedDB.URL_SCHEME) ? browserIndexedDB(e.slice(BrowserIndexedDB.URL_SCHEME.length)) : null;\n\nfunction browserIndexedDB(e) {\n  return new BrowserIndexedDB(e);\n}\n\nfunction maybeStripScheme$1(e) {\n  return e.startsWith(BrowserIndexedDB.URL_SCHEME) ? e.slice(BrowserIndexedDB.URL_SCHEME.length) : e;\n}\n\nIORouterRegistry.registerSaveRouter(indexedDBRouter), IORouterRegistry.registerLoadRouter(indexedDBRouter);\n\nclass BrowserIndexedDBManager {\n  constructor() {\n    this.indexedDB = getIndexedDBFactory();\n  }\n\n  listModels() {\n    var _this88 = this;\n\n    return _asyncToGenerator(function* () {\n      return new Promise((e, t) => {\n        var n = _this88.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);\n\n        n.onupgradeneeded = () => setUpDatabase(n), n.onsuccess = () => {\n          var r = n.result,\n              a = r.transaction(INFO_STORE_NAME, \"readonly\"),\n              s = a.objectStore(INFO_STORE_NAME).getAll();\n          s.onsuccess = () => {\n            var t = {};\n\n            for (var _e548 of s.result) {\n              t[_e548.modelPath] = _e548.modelArtifactsInfo;\n            }\n\n            e(t);\n          }, s.onerror = e => (r.close(), t(s.error)), a.oncomplete = () => r.close();\n        }, n.onerror = e => t(n.error);\n      });\n    })();\n  }\n\n  removeModel(e) {\n    var _this89 = this;\n\n    return _asyncToGenerator(function* () {\n      return e = maybeStripScheme$1(e), new Promise((t, n) => {\n        var r = _this89.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);\n\n        r.onupgradeneeded = () => setUpDatabase(r), r.onsuccess = () => {\n          var a = r.result,\n              s = a.transaction(INFO_STORE_NAME, \"readwrite\"),\n              o = s.objectStore(INFO_STORE_NAME),\n              i = o.get(e);\n          var l;\n          i.onsuccess = () => {\n            if (null == i.result) return a.close(), n(new Error(\"Cannot find model with path '\".concat(e, \"' in IndexedDB.\")));\n            {\n              var _r191 = o.delete(e),\n                  _s94 = () => {\n                l = a.transaction(MODEL_STORE_NAME, \"readwrite\");\n                var r = l.objectStore(MODEL_STORE_NAME).delete(e);\n                r.onsuccess = () => t(i.result.modelArtifactsInfo), r.onerror = e => n(i.error);\n              };\n\n              _r191.onsuccess = _s94, _r191.onerror = e => (_s94(), a.close(), n(i.error));\n            }\n          }, i.onerror = e => (a.close(), n(i.error)), s.oncomplete = () => {\n            null == l ? a.close() : l.oncomplete = () => a.close();\n          };\n        }, r.onerror = e => n(r.error);\n      });\n    })();\n  }\n\n}\n\nvar PATH_SEPARATOR = \"/\",\n    PATH_PREFIX = \"tensorflowjs_models\",\n    INFO_SUFFIX = \"info\",\n    MODEL_TOPOLOGY_SUFFIX = \"model_topology\",\n    WEIGHT_SPECS_SUFFIX = \"weight_specs\",\n    WEIGHT_DATA_SUFFIX = \"weight_data\",\n    MODEL_METADATA_SUFFIX = \"model_metadata\";\n\nfunction getModelKeys(e) {\n  return {\n    info: [PATH_PREFIX, e, INFO_SUFFIX].join(PATH_SEPARATOR),\n    topology: [PATH_PREFIX, e, MODEL_TOPOLOGY_SUFFIX].join(PATH_SEPARATOR),\n    weightSpecs: [PATH_PREFIX, e, WEIGHT_SPECS_SUFFIX].join(PATH_SEPARATOR),\n    weightData: [PATH_PREFIX, e, WEIGHT_DATA_SUFFIX].join(PATH_SEPARATOR),\n    modelMetadata: [PATH_PREFIX, e, MODEL_METADATA_SUFFIX].join(PATH_SEPARATOR)\n  };\n}\n\nfunction removeItems(e) {\n  for (var t of Object.values(e)) {\n    window.localStorage.removeItem(t);\n  }\n}\n\nfunction getModelPathFromKey(e) {\n  var t = e.split(PATH_SEPARATOR);\n  if (t.length < 3) throw new Error(\"Invalid key format: \".concat(e));\n  return t.slice(1, t.length - 1).join(PATH_SEPARATOR);\n}\n\nfunction maybeStripScheme(e) {\n  return e.startsWith(BrowserLocalStorage.URL_SCHEME) ? e.slice(BrowserLocalStorage.URL_SCHEME.length) : e;\n}\n\nclass BrowserLocalStorage {\n  constructor(e) {\n    if (!env().getBool(\"IS_BROWSER\") || \"undefined\" == typeof window || void 0 === window.localStorage) throw new Error(\"The current environment does not support local storage.\");\n    if (this.LS = window.localStorage, null == e || !e) throw new Error(\"For local storage, modelPath must not be null, undefined or empty.\");\n    this.modelPath = e, this.keys = getModelKeys(this.modelPath);\n  }\n\n  save(e) {\n    var _this90 = this;\n\n    return _asyncToGenerator(function* () {\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserLocalStorage.save() does not support saving model topology in binary formats yet.\");\n      {\n        var t = JSON.stringify(e.modelTopology),\n            n = JSON.stringify(e.weightSpecs),\n            r = getModelArtifactsInfoForJSON(e);\n\n        try {\n          return _this90.LS.setItem(_this90.keys.info, JSON.stringify(r)), _this90.LS.setItem(_this90.keys.topology, t), _this90.LS.setItem(_this90.keys.weightSpecs, n), _this90.LS.setItem(_this90.keys.weightData, arrayBufferToBase64String(e.weightData)), _this90.LS.setItem(_this90.keys.modelMetadata, JSON.stringify({\n            format: e.format,\n            generatedBy: e.generatedBy,\n            convertedBy: e.convertedBy,\n            signature: null != e.signature ? e.signature : void 0,\n            userDefinedMetadata: null != e.userDefinedMetadata ? e.userDefinedMetadata : void 0,\n            modelInitializer: null != e.modelInitializer ? e.modelInitializer : void 0,\n            trainingConfig: null != e.trainingConfig ? e.trainingConfig : void 0\n          })), {\n            modelArtifactsInfo: r\n          };\n        } catch (e) {\n          throw removeItems(_this90.keys), new Error(\"Failed to save model '\".concat(_this90.modelPath, \"' to local storage: size quota being exceeded is a possible cause of this failure: modelTopologyBytes=\").concat(r.modelTopologyBytes, \", weightSpecsBytes=\").concat(r.weightSpecsBytes, \", weightDataBytes=\").concat(r.weightDataBytes, \".\"));\n        }\n      }\n    })();\n  }\n\n  load() {\n    var _this91 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = JSON.parse(_this91.LS.getItem(_this91.keys.info));\n      if (null == e) throw new Error(\"In local storage, there is no model with name '\".concat(_this91.modelPath, \"'\"));\n      if (\"JSON\" !== e.modelTopologyType) throw new Error(\"BrowserLocalStorage does not support loading non-JSON model topology yet.\");\n      var t = {},\n          n = JSON.parse(_this91.LS.getItem(_this91.keys.topology));\n      if (null == n) throw new Error(\"In local storage, the topology of model '\".concat(_this91.modelPath, \"' is missing.\"));\n      t.modelTopology = n;\n      var r = JSON.parse(_this91.LS.getItem(_this91.keys.weightSpecs));\n      if (null == r) throw new Error(\"In local storage, the weight specs of model '\".concat(_this91.modelPath, \"' are missing.\"));\n      t.weightSpecs = r;\n\n      var a = _this91.LS.getItem(_this91.keys.modelMetadata);\n\n      if (null != a) {\n        var _e549 = JSON.parse(a);\n\n        t.format = _e549.format, t.generatedBy = _e549.generatedBy, t.convertedBy = _e549.convertedBy, null != _e549.signature && (t.signature = _e549.signature), null != _e549.userDefinedMetadata && (t.userDefinedMetadata = _e549.userDefinedMetadata), null != _e549.modelInitializer && (t.modelInitializer = _e549.modelInitializer), null != _e549.trainingConfig && (t.trainingConfig = _e549.trainingConfig);\n      }\n\n      var s = _this91.LS.getItem(_this91.keys.weightData);\n\n      if (null == s) throw new Error(\"In local storage, the binary weight values of model '\".concat(_this91.modelPath, \"' are missing.\"));\n      return t.weightData = base64StringToArrayBuffer(s), t;\n    })();\n  }\n\n}\n\nBrowserLocalStorage.URL_SCHEME = \"localstorage://\";\n\nvar localStorageRouter = e => env().getBool(\"IS_BROWSER\") && !Array.isArray(e) && e.startsWith(BrowserLocalStorage.URL_SCHEME) ? browserLocalStorage(e.slice(BrowserLocalStorage.URL_SCHEME.length)) : null;\n\nfunction browserLocalStorage(e) {\n  return new BrowserLocalStorage(e);\n}\n\nIORouterRegistry.registerSaveRouter(localStorageRouter), IORouterRegistry.registerLoadRouter(localStorageRouter);\n\nclass BrowserLocalStorageManager {\n  constructor() {\n    assert$4(env().getBool(\"IS_BROWSER\"), () => \"Current environment is not a web browser\"), assert$4(\"undefined\" == typeof window || void 0 !== window.localStorage, () => \"Current browser does not appear to support localStorage\"), this.LS = window.localStorage;\n  }\n\n  listModels() {\n    var _this92 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = {},\n          t = PATH_PREFIX + PATH_SEPARATOR,\n          n = PATH_SEPARATOR + INFO_SUFFIX;\n\n      for (var r = 0; r < _this92.LS.length; ++r) {\n        var a = _this92.LS.key(r);\n\n        a.startsWith(t) && a.endsWith(n) && (e[getModelPathFromKey(a)] = JSON.parse(_this92.LS.getItem(a)));\n      }\n\n      return e;\n    })();\n  }\n\n  removeModel(e) {\n    var _this93 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = getModelKeys(e = maybeStripScheme(e));\n      if (null == _this93.LS.getItem(t.info)) throw new Error(\"Cannot find model at path '\".concat(e, \"'\"));\n      var n = JSON.parse(_this93.LS.getItem(t.info));\n      return removeItems(t), n;\n    })();\n  }\n\n}\n\nvar URL_SCHEME_SUFFIX = \"://\";\n\nclass ModelStoreManagerRegistry {\n  constructor() {\n    this.managers = {};\n  }\n\n  static getInstance() {\n    return null == ModelStoreManagerRegistry.instance && (ModelStoreManagerRegistry.instance = new ModelStoreManagerRegistry()), ModelStoreManagerRegistry.instance;\n  }\n\n  static registerManager(e, t) {\n    assert$4(null != e, () => \"scheme must not be undefined or null.\"), e.endsWith(URL_SCHEME_SUFFIX) && (e = e.slice(0, e.indexOf(URL_SCHEME_SUFFIX))), assert$4(e.length > 0, () => \"scheme must not be an empty string.\");\n    var n = ModelStoreManagerRegistry.getInstance();\n    assert$4(null == n.managers[e], () => \"A model store manager is already registered for scheme '\".concat(e, \"'.\")), n.managers[e] = t;\n  }\n\n  static getManager(e) {\n    var t = this.getInstance().managers[e];\n    if (null == t) throw new Error(\"Cannot find model manager for scheme '\".concat(e, \"'\"));\n    return t;\n  }\n\n  static getSchemes() {\n    return Object.keys(this.getInstance().managers);\n  }\n\n}\n\nfunction parseURL(e) {\n  if (-1 === e.indexOf(URL_SCHEME_SUFFIX)) throw new Error(\"The url string provided does not contain a scheme. Supported schemes are: \".concat(ModelStoreManagerRegistry.getSchemes().join(\",\")));\n  return {\n    scheme: e.split(URL_SCHEME_SUFFIX)[0],\n    path: e.split(URL_SCHEME_SUFFIX)[1]\n  };\n}\n\nfunction cloneModelInternal(_x55, _x56) {\n  return _cloneModelInternal.apply(this, arguments);\n}\n\nfunction _cloneModelInternal() {\n  _cloneModelInternal = _asyncToGenerator(function* (e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    assert$4(e !== t, () => \"Old path and new path are the same: '\".concat(e, \"'\"));\n    var r = IORouterRegistry.getLoadHandlers(e);\n    assert$4(r.length > 0, () => \"Copying failed because no load handler is found for source URL \".concat(e, \".\")), assert$4(r.length < 2, () => \"Copying failed because more than one (\".concat(r.length, \") load handlers for source URL \").concat(e, \".\"));\n    var a = r[0],\n        s = IORouterRegistry.getSaveHandlers(t);\n    assert$4(s.length > 0, () => \"Copying failed because no save handler is found for destination URL \".concat(t, \".\")), assert$4(s.length < 2, () => \"Copying failed because more than one (\".concat(r.length, \") save handlers for destination URL \").concat(t, \".\"));\n    var o = s[0],\n        i = parseURL(e).scheme,\n        l = parseURL(e).path,\n        u = i === parseURL(e).scheme,\n        c = yield a.load();\n    n && u && (yield ModelStoreManagerRegistry.getManager(i).removeModel(l));\n    var p = yield o.save(c);\n    return n && !u && (yield ModelStoreManagerRegistry.getManager(i).removeModel(l)), p.modelArtifactsInfo;\n  });\n  return _cloneModelInternal.apply(this, arguments);\n}\n\nfunction listModels() {\n  return _listModels.apply(this, arguments);\n}\n\nfunction _listModels() {\n  _listModels = _asyncToGenerator(function* () {\n    var e = ModelStoreManagerRegistry.getSchemes(),\n        t = {};\n\n    for (var n of e) {\n      var _e1142 = yield ModelStoreManagerRegistry.getManager(n).listModels();\n\n      for (var r in _e1142) {\n        t[n + URL_SCHEME_SUFFIX + r] = _e1142[r];\n      }\n    }\n\n    return t;\n  });\n  return _listModels.apply(this, arguments);\n}\n\nfunction removeModel(_x57) {\n  return _removeModel.apply(this, arguments);\n}\n\nfunction _removeModel() {\n  _removeModel = _asyncToGenerator(function* (e) {\n    var t = parseURL(e);\n    return ModelStoreManagerRegistry.getManager(t.scheme).removeModel(t.path);\n  });\n  return _removeModel.apply(this, arguments);\n}\n\nfunction copyModel(_x58, _x59) {\n  return _copyModel.apply(this, arguments);\n}\n\nfunction _copyModel() {\n  _copyModel = _asyncToGenerator(function* (e, t) {\n    return cloneModelInternal(e, t, !1);\n  });\n  return _copyModel.apply(this, arguments);\n}\n\nfunction moveModel(_x60, _x61) {\n  return _moveModel.apply(this, arguments);\n}\n\nfunction _moveModel() {\n  _moveModel = _asyncToGenerator(function* (e, t) {\n    return cloneModelInternal(e, t, !0);\n  });\n  return _moveModel.apply(this, arguments);\n}\n\nclass PlatformBrowser {\n  fetch(e, t) {\n    return fetch(e, t);\n  }\n\n  now() {\n    return performance.now();\n  }\n\n  encode(e, t) {\n    if (\"utf-8\" !== t && \"utf8\" !== t) throw new Error(\"Browser's encoder only supports utf-8, but got \".concat(t));\n    return null == this.textEncoder && (this.textEncoder = new TextEncoder()), this.textEncoder.encode(e);\n  }\n\n  decode(e, t) {\n    return new TextDecoder(t).decode(e);\n  }\n\n}\n\nif (env().get(\"IS_BROWSER\")) {\n  env().setPlatform(\"browser\", new PlatformBrowser());\n\n  try {\n    ModelStoreManagerRegistry.registerManager(BrowserLocalStorage.URL_SCHEME, new BrowserLocalStorageManager());\n  } catch (e) {}\n\n  try {\n    ModelStoreManagerRegistry.registerManager(BrowserIndexedDB.URL_SCHEME, new BrowserIndexedDBManager());\n  } catch (e) {}\n}\n\nvar getNodeFetch = {\n  importFetch: () => require(\"node-fetch\")\n};\nvar systemFetch;\n\nclass PlatformNode {\n  constructor() {\n    this.util = require(\"util\"), this.textEncoder = new this.util.TextEncoder();\n  }\n\n  fetch(e, t) {\n    return null != env().global.fetch ? env().global.fetch(e, t) : (null == systemFetch && (systemFetch = getNodeFetch.importFetch()), systemFetch(e, t));\n  }\n\n  now() {\n    var e = process.hrtime();\n    return 1e3 * e[0] + e[1] / 1e6;\n  }\n\n  encode(e, t) {\n    if (\"utf-8\" !== t && \"utf8\" !== t) throw new Error(\"Node built-in encoder only supports utf-8, but got \".concat(t));\n    return this.textEncoder.encode(e);\n  }\n\n  decode(e, t) {\n    return 0 === e.length ? \"\" : new this.util.TextDecoder(t).decode(e);\n  }\n\n}\n\nfunction buffer(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  return t = t || \"float32\", assertNonNegativeIntegerDimensions(e), new TensorBuffer(e, t, n);\n}\n\nfunction cast_(e, t) {\n  var n = convertToTensor(e, \"x\", \"cast\");\n  if (!isValidDtype(t)) throw new Error(\"Failed to cast to unknown dtype \".concat(t));\n  if (\"string\" === t && \"string\" !== n.dtype || \"string\" !== t && \"string\" === n.dtype) throw new Error(\"Only strings can be casted to strings\");\n  return ENGINE.runKernel(Cast, {\n    x: n\n  }, {\n    dtype: t\n  });\n}\n\nenv().get(\"IS_NODE\") && env().setPlatform(\"node\", new PlatformNode());\nvar cast$3 = op({\n  cast_\n});\n\nfunction clone_(e) {\n  var t = convertToTensor(e, \"x\", \"clone\", \"string_or_numeric\");\n  return ENGINE.runKernel(Identity$1, {\n    x: t\n  });\n}\n\nvar clone = op({\n  clone_\n});\n\nfunction print(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  console.log(e.toString(t));\n}\n\ngetOrMakeEngine();\nvar opHandler = {\n  buffer,\n  cast: cast$3,\n  clone,\n  print\n};\nsetOpHandler(opHandler);\nvar DEFAULT_FILE_NAME_PREFIX = \"model\",\n    DEFAULT_JSON_EXTENSION_NAME = \".json\",\n    DEFAULT_WEIGHT_DATA_EXTENSION_NAME = \".weights.bin\";\n\nfunction defer(e) {\n  return new Promise(e => setTimeout(e)).then(e);\n}\n\nclass BrowserDownloads {\n  constructor(e) {\n    if (!env().getBool(\"IS_BROWSER\")) throw new Error(\"browserDownloads() cannot proceed because the current environment is not a browser.\");\n    e.startsWith(BrowserDownloads.URL_SCHEME) && (e = e.slice(BrowserDownloads.URL_SCHEME.length)), null != e && 0 !== e.length || (e = DEFAULT_FILE_NAME_PREFIX), this.modelJsonFileName = e + DEFAULT_JSON_EXTENSION_NAME, this.weightDataFileName = e + DEFAULT_WEIGHT_DATA_EXTENSION_NAME;\n  }\n\n  save(e) {\n    var _this94 = this;\n\n    return _asyncToGenerator(function* () {\n      if (\"undefined\" == typeof document) throw new Error(\"Browser downloads are not supported in this environment since `document` is not present\");\n      var t = window.URL.createObjectURL(new Blob([e.weightData], {\n        type: \"application/octet-stream\"\n      }));\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserDownloads.save() does not support saving model topology in binary formats yet.\");\n      {\n        var n = getModelJSONForModelArtifacts(e, [{\n          paths: [\"./\" + _this94.weightDataFileName],\n          weights: e.weightSpecs\n        }]),\n            r = window.URL.createObjectURL(new Blob([JSON.stringify(n)], {\n          type: \"application/json\"\n        })),\n            a = null == _this94.modelJsonAnchor ? document.createElement(\"a\") : _this94.modelJsonAnchor;\n\n        if (a.download = _this94.modelJsonFileName, a.href = r, yield defer(() => a.dispatchEvent(new MouseEvent(\"click\"))), null != e.weightData) {\n          var _e550 = null == _this94.weightDataAnchor ? document.createElement(\"a\") : _this94.weightDataAnchor;\n\n          _e550.download = _this94.weightDataFileName, _e550.href = t, yield defer(() => _e550.dispatchEvent(new MouseEvent(\"click\")));\n        }\n\n        return {\n          modelArtifactsInfo: getModelArtifactsInfoForJSON(e)\n        };\n      }\n    })();\n  }\n\n}\n\nBrowserDownloads.URL_SCHEME = \"downloads://\";\n\nclass BrowserFiles {\n  constructor(e) {\n    if (null == e || e.length < 1) throw new Error(\"When calling browserFiles, at least 1 file is required, but received \".concat(e));\n    this.jsonFile = e[0], this.weightsFiles = e.slice(1);\n  }\n\n  load() {\n    var _this95 = this;\n\n    return _asyncToGenerator(function* () {\n      return new Promise((e, t) => {\n        var n = new FileReader();\n        n.onload = n => {\n          var r = JSON.parse(n.target.result),\n              a = r.modelTopology;\n          if (null == a) return void t(new Error(\"modelTopology field is missing from file \".concat(_this95.jsonFile.name)));\n          if (null == r.weightsManifest) return void t(new Error(\"weightManifest field is missing from file \".concat(_this95.jsonFile.name)));\n          if (0 === _this95.weightsFiles.length) return void e({\n            modelTopology: a\n          });\n          var s = getModelArtifactsForJSON(r, e => _this95.loadWeights(e));\n          e(s);\n        }, n.onerror = e => t(\"Failed to read model topology and weights manifest JSON from file '\".concat(_this95.jsonFile.name, \"'. BrowserFiles supports loading Keras-style tf.Model artifacts only.\")), n.readAsText(_this95.jsonFile);\n      });\n    })();\n  }\n\n  loadWeights(e) {\n    var t = [],\n        n = [];\n\n    for (var _r192 of e) {\n      t.push(..._r192.weights), n.push(..._r192.paths);\n    }\n\n    var r = this.checkManifestAndWeightFiles(e),\n        a = n.map(e => this.loadWeightsFile(e, r[e]));\n    return Promise.all(a).then(e => [t, concatenateArrayBuffers(e)]);\n  }\n\n  loadWeightsFile(e, t) {\n    return new Promise((n, r) => {\n      var a = new FileReader();\n      a.onload = e => {\n        n(e.target.result);\n      }, a.onerror = t => r(\"Failed to weights data from file of path '\".concat(e, \"'.\")), a.readAsArrayBuffer(t);\n    });\n  }\n\n  checkManifestAndWeightFiles(e) {\n    var t = [],\n        n = this.weightsFiles.map(e => basename(e.name)),\n        r = {};\n\n    for (var a of e) {\n      a.paths.forEach(e => {\n        var a = basename(e);\n        if (-1 !== t.indexOf(a)) throw new Error(\"Duplicate file basename found in weights manifest: '\".concat(a, \"'\"));\n        if (t.push(a), -1 === n.indexOf(a)) throw new Error(\"Weight file with basename '\".concat(a, \"' is not provided.\"));\n        r[e] = this.weightsFiles[n.indexOf(a)];\n      });\n    }\n\n    if (t.length !== this.weightsFiles.length) throw new Error(\"Mismatch in the number of files in weights manifest (\".concat(t.length, \") and the number of weight files provided (\").concat(this.weightsFiles.length, \").\"));\n    return r;\n  }\n\n}\n\nvar browserDownloadsRouter = e => env().getBool(\"IS_BROWSER\") && !Array.isArray(e) && e.startsWith(BrowserDownloads.URL_SCHEME) ? browserDownloads(e.slice(BrowserDownloads.URL_SCHEME.length)) : null;\n\nfunction browserDownloads() {\n  var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : \"model\";\n  return new BrowserDownloads(e);\n}\n\nfunction browserFiles(e) {\n  return new BrowserFiles(e);\n}\n\nfunction monitorPromisesProgress(e, t, n, r) {\n  !function (e) {\n    assert$4(null != e && Array.isArray(e) && e.length > 0, () => \"promises must be a none empty array\");\n  }(e), function (e, t) {\n    assert$4(e >= 0 && e <= 1, () => \"Progress fraction must be in range [0, 1], but got startFraction \".concat(e)), assert$4(t >= 0 && t <= 1, () => \"Progress fraction must be in range [0, 1], but got endFraction \".concat(t)), assert$4(t >= e, () => \"startFraction must be no more than endFraction, but got startFraction \".concat(e, \" and endFraction \").concat(t));\n  }(n = null == n ? 0 : n, r = null == r ? 1 : r);\n  var a = 0;\n  return Promise.all(e.map(s => (s.then(s => {\n    var o = n + ++a / e.length * (r - n);\n    return t(o), s;\n  }), s)));\n}\n\nfunction loadWeightsAsArrayBuffer(_x62, _x63) {\n  return _loadWeightsAsArrayBuffer.apply(this, arguments);\n}\n\nfunction _loadWeightsAsArrayBuffer() {\n  _loadWeightsAsArrayBuffer = _asyncToGenerator(function* (e, t) {\n    null == t && (t = {});\n    var n = null == t.fetchFunc ? env().platform.fetch : t.fetchFunc,\n        r = e.map(e => n(e, t.requestInit, {\n      isBinary: !0\n    })),\n        a = (null == t.onProgress ? yield Promise.all(r) : yield monitorPromisesProgress(r, t.onProgress, 0, .5)).map(e => e.arrayBuffer());\n    return null == t.onProgress ? yield Promise.all(a) : yield monitorPromisesProgress(a, t.onProgress, .5, 1);\n  });\n  return _loadWeightsAsArrayBuffer.apply(this, arguments);\n}\n\nfunction loadWeights(_x64) {\n  return _loadWeights.apply(this, arguments);\n}\n\nfunction _loadWeights() {\n  _loadWeights = _asyncToGenerator(function* (e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"\";\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    var r = arguments.length > 3 ? arguments[3] : undefined;\n    return weightsLoaderFactory(e => loadWeightsAsArrayBuffer(e, {\n      requestInit: r\n    }))(e, t, n);\n  });\n  return _loadWeights.apply(this, arguments);\n}\n\nfunction weightsLoaderFactory(e) {\n  return /*#__PURE__*/function () {\n    var _ref28 = _asyncToGenerator(function* (t) {\n      var n = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"\";\n      var r = arguments.length > 2 ? arguments[2] : undefined;\n      var a = t.map(() => !1),\n          s = {},\n          o = null != r ? r.map(() => !1) : [],\n          i = [];\n\n      if (t.forEach((e, t) => {\n        var n = 0;\n        e.weights.forEach(e => {\n          var l = DTYPE_VALUE_SIZE_MAP[\"quantization\" in e ? e.quantization.dtype : e.dtype] * sizeFromShape(e.shape),\n              u = () => {\n            a[t] = !0, null == s[t] && (s[t] = []), s[t].push({\n              manifestEntry: e,\n              groupOffset: n,\n              sizeBytes: l\n            });\n          };\n\n          null != r ? r.forEach((t, n) => {\n            t === e.name && (u(), o[n] = !0);\n          }) : u(), i.push(e.name), n += l;\n        });\n      }), !o.every(e => e)) {\n        var _e551 = r.filter((e, t) => !o[t]);\n\n        throw new Error(\"Could not find weights in manifest with names: \".concat(_e551.join(\", \"), \". \\nManifest JSON has weights with names: \").concat(i.join(\", \"), \".\"));\n      }\n\n      var l = a.reduce((e, t, n) => (t && e.push(n), e), []),\n          u = [];\n      l.forEach(e => {\n        t[e].paths.forEach(e => {\n          var t = n + (n.endsWith(\"/\") ? \"\" : \"/\") + e;\n          u.push(t);\n        });\n      });\n      var c = yield e(u),\n          p = {};\n      var d = 0;\n      return l.forEach(e => {\n        var n = t[e].paths.length;\n        var r = 0;\n\n        for (var _e552 = 0; _e552 < n; _e552++) {\n          r += c[d + _e552].byteLength;\n        }\n\n        var a = new ArrayBuffer(r),\n            o = new Uint8Array(a);\n        var i = 0;\n\n        for (var _e553 = 0; _e553 < n; _e553++) {\n          var _t397 = new Uint8Array(c[d + _e553]);\n\n          o.set(_t397, i), i += _t397.byteLength;\n        }\n\n        s[e].forEach(e => {\n          var t = decodeWeights(a.slice(e.groupOffset, e.groupOffset + e.sizeBytes), [e.manifestEntry]);\n\n          for (var _e554 in t) {\n            p[_e554] = t[_e554];\n          }\n        }), d += n;\n      }), p;\n    });\n\n    return function (_x65) {\n      return _ref28.apply(this, arguments);\n    };\n  }();\n}\n\nIORouterRegistry.registerSaveRouter(browserDownloadsRouter);\nvar OCTET_STREAM_MIME_TYPE = \"application/octet-stream\",\n    JSON_TYPE = \"application/json\";\n\nclass HTTPRequest {\n  constructor(e, t) {\n    if (this.DEFAULT_METHOD = \"POST\", null == t && (t = {}), this.weightPathPrefix = t.weightPathPrefix, this.onProgress = t.onProgress, this.weightUrlConverter = t.weightUrlConverter, null != t.fetchFunc ? (assert$4(\"function\" == typeof t.fetchFunc, () => \"Must pass a function that matches the signature of `fetch` (see https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API)\"), this.fetch = t.fetchFunc) : this.fetch = env().platform.fetch, assert$4(null != e && e.length > 0, () => \"URL path for http must not be null, undefined or empty.\"), Array.isArray(e) && assert$4(2 === e.length, () => \"URL paths for http must have a length of 2, (actual length is \".concat(e.length, \").\")), this.path = e, null != t.requestInit && null != t.requestInit.body) throw new Error(\"requestInit is expected to have no pre-existing body, but has one.\");\n    this.requestInit = t.requestInit || {};\n  }\n\n  save(e) {\n    var _this96 = this;\n\n    return _asyncToGenerator(function* () {\n      if (e.modelTopology instanceof ArrayBuffer) throw new Error(\"BrowserHTTPRequest.save() does not support saving model topology in binary formats yet.\");\n      var t = Object.assign({\n        method: _this96.DEFAULT_METHOD\n      }, _this96.requestInit);\n      t.body = new FormData();\n      var n = getModelJSONForModelArtifacts(e, [{\n        paths: [\"./model.weights.bin\"],\n        weights: e.weightSpecs\n      }]);\n      t.body.append(\"model.json\", new Blob([JSON.stringify(n)], {\n        type: JSON_TYPE\n      }), \"model.json\"), null != e.weightData && t.body.append(\"model.weights.bin\", new Blob([e.weightData], {\n        type: OCTET_STREAM_MIME_TYPE\n      }), \"model.weights.bin\");\n      var r = yield _this96.fetch(_this96.path, t);\n      if (r.ok) return {\n        modelArtifactsInfo: getModelArtifactsInfoForJSON(e),\n        responses: [r]\n      };\n      throw new Error(\"BrowserHTTPRequest.save() failed due to HTTP response status \".concat(r.status, \".\"));\n    })();\n  }\n\n  load() {\n    var _this97 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this97.fetch(_this97.path, _this97.requestInit);\n      if (!e.ok) throw new Error(\"Request to \".concat(_this97.path, \" failed with status code \").concat(e.status, \". Please verify this URL points to the model JSON of the model to load.\"));\n      var t;\n\n      try {\n        t = yield e.json();\n      } catch (e) {\n        var _t398 = \"Failed to parse model JSON of response from \".concat(_this97.path, \".\");\n\n        throw _this97.path.endsWith(\".pb\") ? _t398 += \" Your path contains a .pb file extension. Support for .pb models have been removed in TensorFlow.js 1.0 in favor of .json models. You can re-convert your Python TensorFlow model using the TensorFlow.js 1.0 conversion scripts or you can convert your.pb models with the 'pb2json'NPM script in the tensorflow/tfjs-converter repository.\" : _t398 += \" Please make sure the server is serving valid JSON for this request.\", new Error(_t398);\n      }\n\n      if (null == t.modelTopology && null == t.weightsManifest) throw new Error(\"The JSON from HTTP path \".concat(_this97.path, \" contains neither model topology or manifest for weights.\"));\n      return getModelArtifactsForJSON(t, e => _this97.loadWeights(e));\n    })();\n  }\n\n  loadWeights(e) {\n    var _this98 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = Array.isArray(_this98.path) ? _this98.path[1] : _this98.path,\n          [n, r] = parseUrl(t),\n          a = _this98.weightPathPrefix || n,\n          s = [];\n\n      for (var _t399 of e) {\n        s.push(..._t399.weights);\n      }\n\n      var o = [],\n          i = [];\n\n      for (var _t400 of e) {\n        for (var _e555 of _t400.paths) {\n          null != _this98.weightUrlConverter ? i.push(_this98.weightUrlConverter(_e555)) : o.push(a + _e555 + r);\n        }\n      }\n\n      return _this98.weightUrlConverter && o.push(...(yield Promise.all(i))), [s, concatenateArrayBuffers(yield loadWeightsAsArrayBuffer(o, {\n        requestInit: _this98.requestInit,\n        fetchFunc: _this98.fetch,\n        onProgress: _this98.onProgress\n      }))];\n    })();\n  }\n\n}\n\nfunction parseUrl(e) {\n  var t = e.lastIndexOf(\"/\"),\n      n = e.lastIndexOf(\"?\");\n  return [e.substring(0, t) + \"/\", n > t ? e.substring(n) : \"\"];\n}\n\nfunction isHTTPScheme(e) {\n  return null != e.match(HTTPRequest.URL_SCHEME_REGEX);\n}\n\nHTTPRequest.URL_SCHEME_REGEX = /^https?:\\/\\//;\n\nvar httpRouter = (e, t) => {\n  if (\"undefined\" == typeof fetch && (null == t || null == t.fetchFunc)) return null;\n  {\n    var n = !0;\n    if (n = Array.isArray(e) ? e.every(e => isHTTPScheme(e)) : isHTTPScheme(e), n) return http(e, t);\n  }\n  return null;\n};\n\nfunction http(e, t) {\n  return new HTTPRequest(e, t);\n}\n\nfunction browserHTTPRequest(e, t) {\n  return http(e, t);\n}\n\nIORouterRegistry.registerSaveRouter(httpRouter), IORouterRegistry.registerLoadRouter(httpRouter);\n\nclass PassthroughLoader {\n  constructor(e) {\n    this.modelArtifacts = e;\n  }\n\n  load() {\n    var _this99 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this99.modelArtifacts;\n    })();\n  }\n\n}\n\nclass PassthroughSaver {\n  constructor(e) {\n    this.saveHandler = e;\n  }\n\n  save(e) {\n    var _this100 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this100.saveHandler(e);\n    })();\n  }\n\n}\n\nfunction fromMemory(e, t, n, r) {\n  return 1 === arguments.length ? null != e.modelTopology || null != e.weightSpecs ? new PassthroughLoader(e) : (console.warn(\"Please call tf.io.fromMemory() with only one argument. The argument should be of type ModelArtifacts. The multi-argument signature of tf.io.fromMemory() has been deprecated and will be removed in a future release.\"), new PassthroughLoader({\n    modelTopology: e\n  })) : (console.warn(\"Please call tf.io.fromMemory() with only one argument. The argument should be of type ModelArtifacts. The multi-argument signature of tf.io.fromMemory() has been deprecated and will be removed in a future release.\"), new PassthroughLoader({\n    modelTopology: e,\n    weightSpecs: t,\n    weightData: n,\n    trainingConfig: r\n  }));\n}\n\nfunction withSaveHandler(e) {\n  return new PassthroughSaver(e);\n}\n\nvar io = {\n  __proto__: null,\n  browserFiles,\n  browserHTTPRequest,\n  concatenateArrayBuffers,\n  decodeWeights,\n  encodeWeights,\n  fromMemory,\n  getLoadHandlers,\n  getModelArtifactsForJSON,\n  getModelArtifactsInfoForJSON,\n  getSaveHandlers,\n  http,\n  isHTTPScheme,\n  loadWeights,\n  registerLoadRouter,\n  registerSaveRouter,\n  weightsLoaderFactory,\n  withSaveHandler,\n  copyModel,\n  listModels,\n  moveModel,\n  removeModel\n};\n\nfunction matMul_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor(e, \"a\", \"matMul\"),\n      s = convertToTensor(t, \"b\", \"matMul\");\n  return [a, s] = makeTypesMatch(a, s), ENGINE.runKernel(BatchMatMul, {\n    a,\n    b: s\n  }, {\n    transposeA: n,\n    transposeB: r\n  });\n}\n\nvar matMul$1 = op({\n  matMul_\n});\n\nfunction oneHot_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  if (t < 2) throw new Error(\"Error in oneHot: depth must be >=2, but it is \".concat(t));\n  var a = convertToTensor(e, \"indices\", \"oneHot\", \"int32\");\n  return ENGINE.runKernel(OneHot, {\n    indices: a\n  }, {\n    depth: t,\n    onValue: n,\n    offValue: r\n  });\n}\n\nvar oneHot$2 = op({\n  oneHot_\n});\n\nfunction transpose_(e, t) {\n  var n = convertToTensor(e, \"x\", \"transpose\");\n  return null == t && (t = n.shape.map((e, t) => t).reverse()), assert$4(n.rank === t.length, () => \"Error in transpose: rank of input \".concat(n.rank, \" must match length of perm \").concat(t, \".\")), t.forEach(e => {\n    assert$4(e >= 0 && e < n.rank, () => \"All entries in 'perm' must be between 0 and \" + (n.rank - 1) + \" but got \".concat(t));\n  }), n.rank <= 1 ? n.clone() : ENGINE.runKernel(Transpose, {\n    x: n\n  }, {\n    perm: t\n  });\n}\n\nvar transpose$2 = op({\n  transpose_\n});\n\nfunction confusionMatrix_(e, t, n) {\n  var r = convertToTensor(e, \"labels\", \"confusionMatrix\"),\n      a = convertToTensor(t, \"predictions\", \"confusionMatrix\");\n  assert$4(null == n || n > 0 && Number.isInteger(n), () => \"If provided, numClasses must be a positive integer, but got \".concat(n)), assert$4(1 === r.rank, () => \"Expected the rank of labels to be 1, but got \".concat(r.rank)), assert$4(1 === a.rank, () => \"Expected the rank of predictions to be 1, but got \".concat(a.rank)), assert$4(r.shape[0] === a.shape[0], () => \"Mismatch in the number of examples: \".concat(r.shape[0], \" vs. \").concat(a.shape[0], \". Labels and predictions should have the same number of elements.\")), assert$4(n > 0 && Number.isInteger(n), () => \"numClasses is required to be a positive integer, but got \".concat(n));\n  var s = oneHot$2(cast$3(r, \"int32\"), n),\n      o = oneHot$2(cast$3(a, \"int32\"), n),\n      i = transpose$2(s),\n      l = matMul$1(i, o);\n  return cast$3(l, \"int32\");\n}\n\nvar confusionMatrix = op({\n  confusionMatrix_\n});\nvar math = {\n  __proto__: null,\n  confusionMatrix\n};\n\nfunction tensor3d(e, t, n) {\n  if (assertNonNull(e), null != t && 3 !== t.length) throw new Error(\"tensor3d() requires shape to have three numbers\");\n  var r = inferShape(e, n);\n  if (3 !== r.length && 1 !== r.length) throw new Error(\"tensor3d() requires values to be number[][][] or flat/TypedArray\");\n  if (1 === r.length && null == t) throw new Error(\"tensor3d() requires shape to be provided when `values` are a flat array\");\n  return makeTensor(e, t, r, n);\n}\n\nvar fromPixels2DContext$1;\n\nfunction fromPixels_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 3;\n  if (t > 4) throw new Error(\"Cannot construct Tensor with more than 4 channels from pixels.\");\n  if (null == e) throw new Error(\"pixels passed to tf.browser.fromPixels() can not be null\");\n  var n = !1,\n      r = !1,\n      a = !1,\n      s = !1,\n      o = !1,\n      i = !1;\n  if (e.data instanceof Uint8Array) n = !0;else if (\"undefined\" != typeof ImageData && e instanceof ImageData) r = !0;else if (\"undefined\" != typeof HTMLVideoElement && e instanceof HTMLVideoElement) a = !0;else if (\"undefined\" != typeof HTMLImageElement && e instanceof HTMLImageElement) s = !0;else if (null != e.getContext) o = !0;else {\n    if (!(\"undefined\" != typeof ImageBitmap && e instanceof ImageBitmap)) throw new Error(\"pixels passed to tf.browser.fromPixels() must be either an HTMLVideoElement, HTMLImageElement, HTMLCanvasElement, ImageData in browser, or OffscreenCanvas, ImageData in webworker or {data: Uint32Array, width: number, height: number}, but was \".concat(e.constructor.name));\n    i = !0;\n  }\n\n  if (a) {\n    var _t401 = 2;\n    if (a && e.readyState < _t401) throw new Error(\"The video element has not loaded data yet. Please wait for `loadeddata` event on the <video> element.\");\n  }\n\n  if (null != getKernel(FromPixels, ENGINE.backendName)) return ENGINE.runKernel(FromPixels, {\n    pixels: e\n  }, {\n    numChannels: t\n  });\n  var [l, u] = a ? [e.videoWidth, e.videoHeight] : [e.width, e.height];\n  var c, p;\n  if (o ? c = e.getContext(\"2d\").getImageData(0, 0, l, u).data : r || n ? c = e.data : (s || a || i) && (null == fromPixels2DContext$1 && (fromPixels2DContext$1 = document.createElement(\"canvas\").getContext(\"2d\")), fromPixels2DContext$1.canvas.width = l, fromPixels2DContext$1.canvas.height = u, fromPixels2DContext$1.drawImage(e, 0, 0, l, u), c = fromPixels2DContext$1.getImageData(0, 0, l, u).data), 4 === t) p = new Int32Array(c);else {\n    var _e556 = l * u;\n\n    p = new Int32Array(_e556 * t);\n\n    for (var _n228 = 0; _n228 < _e556; _n228++) {\n      for (var _e557 = 0; _e557 < t; ++_e557) {\n        p[_n228 * t + _e557] = c[4 * _n228 + _e557];\n      }\n    }\n  }\n  return tensor3d(p, [u, l, t], \"int32\");\n}\n\nfunction isPixelData(e) {\n  return null != e && e.data instanceof Uint8Array;\n}\n\nfunction isImageBitmapFullySupported() {\n  return \"undefined\" != typeof window && \"undefined\" != typeof ImageBitmap && window.hasOwnProperty(\"createImageBitmap\");\n}\n\nfunction isNonEmptyPixels(e) {\n  return null != e && 0 !== e.width && 0 !== e.height;\n}\n\nfunction canWrapPixelsToImageBitmap(e) {\n  return isImageBitmapFullySupported() && !(e instanceof ImageBitmap) && isNonEmptyPixels(e) && !isPixelData(e);\n}\n\nfunction fromPixelsAsync(_x66) {\n  return _fromPixelsAsync.apply(this, arguments);\n}\n\nfunction _fromPixelsAsync() {\n  _fromPixelsAsync = _asyncToGenerator(function* (e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 3;\n    var n = null;\n\n    if (env().getBool(\"WRAP_TO_IMAGEBITMAP\") && canWrapPixelsToImageBitmap(e)) {\n      var _t772;\n\n      try {\n        _t772 = yield createImageBitmap(e, {\n          premultiplyAlpha: \"none\"\n        });\n      } catch (e) {\n        _t772 = null;\n      }\n\n      n = null != _t772 && _t772.width === e.width && _t772.height === e.height ? _t772 : e;\n    } else n = e;\n\n    return fromPixels_(n, t);\n  });\n  return _fromPixelsAsync.apply(this, arguments);\n}\n\nfunction toPixels(_x67, _x68) {\n  return _toPixels.apply(this, arguments);\n}\n\nfunction _toPixels() {\n  _toPixels = _asyncToGenerator(function* (e, t) {\n    var n = convertToTensor(e, \"img\", \"toPixels\");\n\n    if (!(e instanceof Tensor)) {\n      var _e1143 = n;\n      n = cast$3(_e1143, \"int32\"), _e1143.dispose();\n    }\n\n    if (2 !== n.rank && 3 !== n.rank) throw new Error(\"toPixels only supports rank 2 or 3 tensors, got rank \".concat(n.rank, \".\"));\n    var [r, a] = n.shape.slice(0, 2),\n        s = 2 === n.rank ? 1 : n.shape[2];\n    if (s > 4 || 2 === s) throw new Error(\"toPixels only supports depth of size 1, 3 or 4 but got \".concat(s));\n    if (\"float32\" !== n.dtype && \"int32\" !== n.dtype) throw new Error(\"Unsupported type for toPixels: \".concat(n.dtype, \". Please use float32 or int32 tensors.\"));\n    var o = yield n.data(),\n        i = \"float32\" === n.dtype ? 255 : 1,\n        l = new Uint8ClampedArray(a * r * 4);\n\n    for (var _e1144 = 0; _e1144 < r * a; ++_e1144) {\n      var _t773 = [0, 0, 0, 255];\n\n      for (var _r448 = 0; _r448 < s; _r448++) {\n        var _a319 = o[_e1144 * s + _r448];\n\n        if (\"float32\" === n.dtype) {\n          if (_a319 < 0 || _a319 > 1) throw new Error(\"Tensor values for a float32 Tensor must be in the range [0 - 1] but encountered \".concat(_a319, \".\"));\n        } else if (\"int32\" === n.dtype && (_a319 < 0 || _a319 > 255)) throw new Error(\"Tensor values for a int32 Tensor must be in the range [0 - 255] but encountered \".concat(_a319, \".\"));\n\n        1 === s ? (_t773[0] = _a319 * i, _t773[1] = _a319 * i, _t773[2] = _a319 * i) : _t773[_r448] = _a319 * i;\n      }\n\n      var _r447 = 4 * _e1144;\n\n      l[_r447 + 0] = Math.round(_t773[0]), l[_r447 + 1] = Math.round(_t773[1]), l[_r447 + 2] = Math.round(_t773[2]), l[_r447 + 3] = Math.round(_t773[3]);\n    }\n\n    if (null != t) {\n      t.width = a, t.height = r;\n\n      var _e1145 = t.getContext(\"2d\"),\n          _n455 = new ImageData(l, a, r);\n\n      _e1145.putImageData(_n455, 0, 0);\n    }\n\n    return n !== e && n.dispose(), l;\n  });\n  return _toPixels.apply(this, arguments);\n}\n\nvar fromPixels$1 = op({\n  fromPixels_\n});\nvar browser = {\n  __proto__: null,\n  fromPixelsAsync,\n  toPixels,\n  fromPixels: fromPixels$1\n};\n\nfunction prepareAndValidate(e, t) {\n  var n = e.shape.length,\n      r = t.shape.length;\n  if (n < 1) throw new Error(\"tf.gatherND() expects the input to be rank 1 or higher, but the rank was \".concat(n, \".\"));\n  if (r < 1) throw new Error(\"tf.gatherND() expects the indices to be rank 1 or higher, but the rank was \".concat(r, \".\"));\n  if (\"int32\" !== t.dtype) throw new Error(\"tf.gatherND() expects the indices to be int32 type, but the dtype was \".concat(t.dtype, \".\"));\n  if (t.shape[r - 1] > n) throw new Error(\"index innermost dimension length must be <= tensor rank; saw: \".concat(t.shape[r - 1], \" vs. \").concat(n));\n  if (0 === sizeFromShape(e.shape)) throw new Error(\"Requested more than 0 entries, but input is empty. Input shape: \".concat(e.shape, \".\"));\n  var a = t.shape,\n      s = a[a.length - 1];\n  var o = 1;\n\n  for (var _e558 = 0; _e558 < a.length - 1; ++_e558) {\n    o *= a[_e558];\n  }\n\n  var i = e.shape,\n      l = a.slice();\n  l.pop();\n  var u = 1;\n\n  for (var _e559 = s; _e559 < n; ++_e559) {\n    u *= i[_e559], l.push(i[_e559]);\n  }\n\n  var c = [...computeStrides(e.shape).map(e => e / u), 1].slice(0, s);\n  return [l, o, u, c];\n}\n\nvar gather_nd_util = {\n  __proto__: null,\n  prepareAndValidate\n};\n\nfunction validateUpdateShape(e, t, n) {\n  var r = t.rank > 1 ? t.shape[t.rank - 1] : 1,\n      a = t.rank > 1 ? t.rank - 1 : 1,\n      s = \"Must have updates.shape = indices.shape[:batchDim] + shape[sliceDim:], got updates.shape: \".concat(n.shape, \", indices.shape: \").concat(t.shape, \", shape: \").concat(e, \", sliceDim: \").concat(r, \", and batchDim: \").concat(a, \".\");\n  if (n.rank < a) throw new Error(s + \" update.rank < \".concat(a, \". \"));\n  if (e.length < r + (n.rank - a)) throw new Error(s + \" Output shape length < \".concat(r + (n.rank - a)));\n  if (n.rank !== a + e.length - r) throw new Error(s + \" update.rank != \" + (a + e.length - r));\n\n  for (var _e560 = 0; _e560 < a; ++_e560) {\n    if (n.shape[_e560] !== t.shape[_e560]) throw new Error(s + \" updates.shape[\".concat(_e560, \"] (\").concat(n.shape[_e560], \") != indices.shape[\").concat(_e560, \"] (\").concat(t.shape[_e560], \").\"));\n  }\n\n  for (var _t402 = 0; _t402 < n.rank - a; ++_t402) {\n    if (n.shape[_t402 + a] !== e[_t402 + r]) throw new Error(s + \" updates.shape[\".concat(_t402 + a, \"] (\").concat(n.shape[_t402 + a], \") != shape[\").concat(_t402 + a, \"] (\").concat(e[_t402 + a], \")\"));\n  }\n}\n\nfunction validateInput$1(e, t, n) {\n  if (t.rank < 1) throw new Error(\"tf.scatterND() expects the indices to be rank 1 or higher, but the rank was \".concat(t.rank, \".\"));\n  if (e.rank < 1) throw new Error(\"tf.scatterND() expects the updates to be rank 1 or higher, but the rank was \".concat(e.rank, \".\"));\n  if (\"int32\" !== t.dtype) throw new Error(\"The dtype of 'indices' should be int32, but got dtype: \".concat(t.dtype));\n  if (n.length < 1) throw new Error(\"Output rank must be greater or equal to 1, but got shape: \".concat(n));\n\n  if (0 === n.length) {\n    if (0 === t.size) throw new Error(\"Indices specified for empty output. indices shape: \".concat(t.shape));\n    if (0 === e.size) throw new Error(\"Updates specified for empty output. updates shape: \".concat(e.shape));\n  }\n\n  validateUpdateShape(n, t, e);\n}\n\nfunction calculateShapes(e, t, n) {\n  var r = t.shape.length,\n      a = r > 1 ? t.shape[r - 1] : 1,\n      s = n.length;\n  var o = 1;\n\n  for (var _e561 = a; _e561 < s; ++_e561) {\n    o *= n[_e561];\n  }\n\n  var i = a < 1 ? 1 : a;\n  return {\n    sliceRank: a,\n    numUpdates: sizeFromShape(t.shape) / i,\n    sliceSize: o,\n    strides: [...computeStrides(n.slice(0, a)), 1],\n    outputSize: sizeFromShape(n)\n  };\n}\n\nvar scatter_nd_util = {\n  __proto__: null,\n  validateUpdateShape,\n  validateInput: validateInput$1,\n  calculateShapes\n};\n\nfunction assertParamsValid(e, t, n) {\n  var r = e.shape.length;\n  assert$4(r === t.length, () => \"Error in slice\".concat(r, \"D: Length of begin \").concat(t, \" must match the rank of the array (\").concat(r, \").\")), assert$4(r === n.length, () => \"Error in slice\".concat(r, \"D: Length of size \").concat(n, \" must match the rank of the array (\").concat(r, \").\"));\n\n  var _loop31 = function _loop31(a) {\n    assert$4(t[a] + n[a] <= e.shape[a], () => \"Error in slice\".concat(r, \"D: begin[\").concat(a, \"] + size[\").concat(a, \"] (\").concat(t[a] + n[a], \") would overflow input.shape[\").concat(a, \"] (\").concat(e.shape[a], \")\"));\n  };\n\n  for (var a = 0; a < r; ++a) {\n    _loop31(a);\n  }\n}\n\nfunction maskToAxes(e) {\n  var t = [];\n  var n = 0;\n\n  for (; e > 0;) {\n    1 & e && t.push(n), e /= 2, n++;\n  }\n\n  return t;\n}\n\nfunction computeOutShape$2(e, t, n) {\n  var r = [];\n\n  for (var a = 0; a < e.length; a++) {\n    r[a] = Math.ceil((t[a] - e[a]) / n[a]);\n  }\n\n  return r;\n}\n\nfunction stridesWithElidedDims(e, t, n, r) {\n  var a = [...e];\n\n  for (var _e562 = a.length; _e562 < r.length; _e562++) {\n    a.push(1);\n  }\n\n  for (var _e563 = 0; _e563 < n; _e563++) {\n    0 === _e563 ? a[t] = 1 : (a.splice(t, 0, 1), a.pop());\n  }\n\n  return a;\n}\n\nfunction unnormalizeAxis(e, t, n) {\n  return n <= e ? n : n - (t - 1);\n}\n\nfunction getElidedAxes(e, t) {\n  var n = [];\n\n  for (var r = 0; r < e; r++) {\n    n.push(t + r);\n  }\n\n  return n;\n}\n\nfunction getNormalizedAxes(e, t, n, r, a, s, o, i, l) {\n  var u = e.length;\n  var c = new Array(u),\n      p = new Array(u),\n      d = new Array(u);\n\n  if (t.length && n > 0) {\n    var _l34 = t[0],\n        _u30 = n + 1;\n\n    c = startIndicesWithElidedDims(o, _l34, _u30, r, e), p = stopIndicesWithElidedDims(i, _l34, _u30, a, e), d = stridesWithElidedDims(s, _l34, _u30, e);\n  } else for (var _t403 = 0; _t403 < u; _t403++) {\n    c[_t403] = startForAxis(o, r, s, e, _t403, l), p[_t403] = stopForAxis(i, a, s, e, _t403, l), d[_t403] = stridesForAxis(s, _t403, l);\n  }\n\n  return {\n    begin: c,\n    end: p,\n    strides: d\n  };\n}\n\nfunction startIndicesWithElidedDims(e, t, n, r, a) {\n  var s = [...a],\n      o = getElidedAxes(n, t);\n\n  for (var _a124 = 0; _a124 < s.length; _a124++) {\n    if (o.indexOf(_a124) > -1) s[_a124] = 0;else {\n      var _o68 = unnormalizeAxis(t, n, _a124);\n\n      var i = r[_o68];\n      e & 1 << _o68 && (i = 0), s[_a124] = i;\n    }\n  }\n\n  return s;\n}\n\nfunction stopIndicesWithElidedDims(e, t, n, r, a) {\n  var s = [...a],\n      o = getElidedAxes(n, t);\n\n  for (var _a125 = 0; _a125 < s.length; _a125++) {\n    if (o.indexOf(_a125) > -1) s[_a125] = Number.MAX_SAFE_INTEGER;else {\n      var _o69 = unnormalizeAxis(t, n, _a125);\n\n      var i = r[_o69];\n      e & 1 << _o69 && (i = Number.MAX_SAFE_INTEGER), s[_a125] = i;\n    }\n  }\n\n  for (var _e564 = 0; _e564 < s.length; _e564++) {\n    var _t404 = a[_e564];\n    s[_e564] < 0 && (s[_e564] += _t404), s[_e564] = clamp(0, s[_e564], a[_e564]);\n  }\n\n  return s;\n}\n\nfunction stridesForAxis(e, t, n) {\n  var r = e[t];\n  return (n & 1 << t || null == r) && (r = 1), r;\n}\n\nfunction startForAxis(e, t, n, r, a, s) {\n  var o = t[a];\n  (e & 1 << a || s & 1 << a || null == o) && (o = (n[a] || 1) > 0 ? Number.MIN_SAFE_INTEGER : Number.MAX_SAFE_INTEGER);\n  var i = r[a];\n  return o < 0 && (o += i), o = clamp(0, o, i - 1), o;\n}\n\nfunction stopForAxis(e, t, n, r, a, s) {\n  var o = t[a];\n  var i = n[a] || 1;\n  (e & 1 << a || s & 1 << a || null == o) && (o = i > 0 ? Number.MAX_SAFE_INTEGER : Number.MIN_SAFE_INTEGER);\n  var l = r[a];\n  return o < 0 && (o += l), o = i > 0 ? clamp(0, o, l) : clamp(-1, o, l - 1), o;\n}\n\nfunction isSliceContinous(e, t, n) {\n  var r = n.length;\n\n  for (var _e565 = 0; _e565 < n.length; _e565++) {\n    if (n[_e565] > 1) {\n      r = _e565;\n      break;\n    }\n  }\n\n  for (var a = r + 1; a < n.length; a++) {\n    if (t[a] > 0 || n[a] !== e[a]) return !1;\n  }\n\n  return !0;\n}\n\nfunction computeFlatOffset(e, t) {\n  var n = e.length > 0 ? e[e.length - 1] : 1;\n\n  for (var r = 0; r < e.length - 1; r++) {\n    n += e[r] * t[r];\n  }\n\n  return n;\n}\n\nfunction parseSliceParams(e, t, n) {\n  var r;\n  var a = e.shape.length;\n  var s;\n  return r = \"number\" == typeof t ? [t, ...new Array(a - 1).fill(0)] : t.length < a ? t.concat(new Array(a - t.length).fill(0)) : t.slice(), r.forEach(e => {\n    assert$4(-1 !== e, () => \"slice() does not support negative begin indexing.\");\n  }), s = null == n ? new Array(a).fill(-1) : \"number\" == typeof n ? [n, ...new Array(a - 1).fill(-1)] : n.length < a ? n.concat(new Array(a - n.length).fill(-1)) : n, s = s.map((t, n) => t >= 0 ? t : (assert$4(-1 === t, () => \"Negative size values should be exactly -1 but got \".concat(t, \" for the slice() size at index \").concat(n, \".\")), e.shape[n] - r[n])), [r, s];\n}\n\nfunction sliceInfo(e, t, n, r, a, s, o, i, l) {\n  var u = t.slice(),\n      c = n.slice(),\n      p = r;\n  null == r && (p = new Array(u.length));\n  var d = maskToAxes(o);\n  if (d.length > 1) throw new Error(\"Multiple ellipses in slice is not allowed.\");\n  if (0 !== o && 0 !== i) throw new Error(\"Using both ellipsisMask and newAxisMask is not yet supported.\");\n  if (0 !== o && 0 !== l) throw new Error(\"Using both ellipsisMask and shrinkAxisMask is not yet supported.\");\n  var h = e.length - u.length,\n      m = maskToAxes(i),\n      f = e.slice();\n  m.forEach(e => {\n    u[e] = 0, c[e] = 1, f.splice(e, 0, 1);\n  });\n  var {\n    begin: g,\n    end: $,\n    strides: y\n  } = getNormalizedAxes(f, d, h, u, c, p, a, s, o);\n  u = g, c = $, p = y;\n  var b = maskToAxes(l);\n  b.forEach(e => {\n    c[e] = u[e] + 1, p[e] = 1;\n  });\n  var x = computeOutShape$2(u, c, p),\n      v = x.filter((e, t) => -1 === b.indexOf(t));\n  return {\n    nonStrided: p.every(e => 1 === e),\n    $begin: u,\n    $end: c,\n    $strides: p,\n    size: x,\n    newShape: f,\n    outShape: v\n  };\n}\n\nvar slice_util = {\n  __proto__: null,\n  assertParamsValid,\n  maskToAxes,\n  computeOutShape: computeOutShape$2,\n  stridesWithElidedDims,\n  getNormalizedAxes,\n  startIndicesWithElidedDims,\n  stopIndicesWithElidedDims,\n  stridesForAxis,\n  startForAxis,\n  stopForAxis,\n  isSliceContinous,\n  computeFlatOffset,\n  parseSliceParams,\n  sliceInfo\n};\n\nclass Serializable {\n  getClassName() {\n    return this.constructor.className;\n  }\n\n  static fromConfig(e, t) {\n    return new e(t);\n  }\n\n}\n\nclass SerializationMap {\n  constructor() {\n    this.classNameMap = {};\n  }\n\n  static getMap() {\n    return null == SerializationMap.instance && (SerializationMap.instance = new SerializationMap()), SerializationMap.instance;\n  }\n\n  static register(e) {\n    SerializationMap.getMap().classNameMap[e.className] = [e, e.fromConfig];\n  }\n\n}\n\nfunction registerClass(e) {\n  assert$4(null != e.className, () => \"Class being registered does not have the static className property defined.\"), assert$4(\"string\" == typeof e.className, () => \"className is required to be a string, but got type \" + typeof e.className), assert$4(e.className.length > 0, () => \"Class being registered has an empty-string as its className, which is disallowed.\"), SerializationMap.register(e);\n}\n\nvar serialization = {\n  __proto__: null,\n  Serializable,\n  SerializationMap,\n  registerClass\n};\nvar TEST_EPSILON_FLOAT32 = .001,\n    TEST_EPSILON_FLOAT16 = .1;\n\nfunction expectArraysClose(e, t, n) {\n  return null == n && (n = testEpsilon()), expectArraysPredicate(e, t, (e, t) => areClose(e, t, n));\n}\n\nfunction testEpsilon() {\n  return 32 === ENGINE.backend.floatPrecision() ? TEST_EPSILON_FLOAT32 : TEST_EPSILON_FLOAT16;\n}\n\nfunction expectArraysPredicate(e, t, n) {\n  var r = !0;\n\n  if ((isTypedArray(e) || isTypedArray(t)) && (r = !1), isTypedArray(e) && isTypedArray(t) && (r = !0), r) {\n    var _n229 = e.constructor.name,\n        _r193 = t.constructor.name;\n    if (_n229 !== _r193) throw new Error(\"Arrays are of different type. Actual: \".concat(_n229, \". Expected: \").concat(_r193));\n  }\n\n  if (Array.isArray(e) && Array.isArray(t)) {\n    var _n230 = inferShape(e),\n        _r194 = inferShape(t);\n\n    if (!arraysEqual(_n230, _r194)) throw new Error(\"Arrays have different shapes. Actual: [\".concat(_n230, \"]. Expected: [\").concat(_r194, \"]\"));\n  }\n\n  var a = isTypedArray(e) ? e : flatten$3(e),\n      s = isTypedArray(t) ? t : flatten$3(t);\n  if (a.length !== s.length) throw new Error(\"Arrays have different lengths actual: \".concat(a.length, \" vs expected: \").concat(s.length, \".\\nActual:   \").concat(a, \".\\nExpected: \").concat(s, \".\"));\n\n  for (var _e566 = 0; _e566 < s.length; ++_e566) {\n    var _t405 = a[_e566],\n        _r195 = s[_e566];\n    if (!n(_t405, _r195)) throw new Error(\"Arrays differ: actual[\".concat(_e566, \"] = \").concat(_t405, \", expected[\").concat(_e566, \"] = \").concat(_r195, \".\\nActual:   \").concat(a, \".\\nExpected: \").concat(s, \".\"));\n  }\n}\n\nfunction expectPromiseToFail(e, t) {\n  e().then(() => t.fail(), () => t());\n}\n\nfunction expectArraysEqual(e, t) {\n  var n = \"string\" == typeof t || \"number\" == typeof t || \"boolean\" == typeof t ? [t] : t;\n  return isString(e) || isString(e[0]) || isString(t) || isString(t[0]) ? expectArraysPredicate(e, n, (e, t) => e == t) : expectArraysPredicate(e, t, (e, t) => areClose(e, t, 0));\n}\n\nfunction expectNumbersClose(e, t, n) {\n  if (null == n && (n = testEpsilon()), !areClose(e, t, n)) throw new Error(\"Numbers differ: actual === \".concat(e, \", expected === \").concat(t));\n}\n\nfunction areClose(e, t, n) {\n  return !isFinite(e) && !isFinite(t) || !(isNaN(e) || isNaN(t) || Math.abs(e - t) > n);\n}\n\nfunction expectValuesInRange(e, t, n) {\n  for (var r = 0; r < e.length; r++) {\n    if (e[r] < t || e[r] > n) throw new Error(\"Value out of range:\".concat(e[r], \" low: \").concat(t, \", high: \").concat(n));\n  }\n}\n\nfunction expectArrayBuffersEqual(e, t) {\n  expect(new Float32Array(e)).toEqual(new Float32Array(t));\n}\n\nfunction encodeStrings(e) {\n  for (var t = 0; t < e.length; t++) {\n    var n = e[t];\n    Array.isArray(n) ? encodeStrings(n) : e[t] = encodeString(n);\n  }\n\n  return e;\n}\n\nvar test_util = {\n  __proto__: null,\n  TEST_EPSILON_FLOAT16,\n  expectArraysClose,\n  testEpsilon,\n  expectPromiseToFail,\n  expectArraysEqual,\n  expectNumbersClose,\n  expectValuesInRange,\n  expectArrayBuffersEqual,\n  encodeStrings\n};\nvar version$7 = \"3.8.0\";\n\nfunction enableProdMode() {\n  env().set(\"PROD\", !0);\n}\n\nfunction enableDebugMode() {\n  env().set(\"DEBUG\", !0);\n}\n\nfunction disableDeprecationWarnings() {\n  env().set(\"DEPRECATION_WARNINGS_ENABLED\", !1), console.warn(\"TensorFlow.js deprecation warnings have been disabled.\");\n}\n\nfunction deprecationWarn(e) {\n  env().getBool(\"DEPRECATION_WARNINGS_ENABLED\") && console.warn(e + \" You can disable deprecation warnings with tf.disableDeprecationWarnings().\");\n}\n\nfunction disposeVariables() {\n  ENGINE.disposeVariables();\n}\n\nfunction engine() {\n  return ENGINE;\n}\n\nfunction memory() {\n  return ENGINE.memory();\n}\n\nfunction profile(e) {\n  return ENGINE.profile(e);\n}\n\nfunction tidy(e, t) {\n  return ENGINE.tidy(e, t);\n}\n\nfunction dispose(e) {\n  getTensorsInContainer(e).forEach(e => e.dispose());\n}\n\nfunction keep(e) {\n  return ENGINE.keep(e);\n}\n\nfunction time(e) {\n  return ENGINE.time(e);\n}\n\nfunction setBackend(e) {\n  return ENGINE.setBackend(e);\n}\n\nfunction ready() {\n  return ENGINE.ready();\n}\n\nfunction getBackend() {\n  return ENGINE.backendName;\n}\n\nfunction removeBackend(e) {\n  ENGINE.removeBackend(e);\n}\n\nfunction findBackend(e) {\n  return ENGINE.findBackend(e);\n}\n\nfunction findBackendFactory(e) {\n  return ENGINE.findBackendFactory(e);\n}\n\nfunction registerBackend(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  return ENGINE.registerBackend(e, t, n);\n}\n\nfunction backend() {\n  return ENGINE.backend;\n}\n\nfunction setPlatform(e, t) {\n  env().setPlatform(e, t);\n}\n\nfunction add_(e, t) {\n  var n = convertToTensor(e, \"a\", \"add\"),\n      r = convertToTensor(t, \"b\", \"add\");\n  return [n, r] = makeTypesMatch(n, r), ENGINE.runKernel(Add$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar add$2 = op({\n  add_\n});\n\nfunction floorDiv_(e, t) {\n  var n = convertToTensor(e, \"a\", \"floorDiv\"),\n      r = convertToTensor(t, \"b\", \"floorDiv\");\n  return [n, r] = makeTypesMatch(n, r), ENGINE.runKernel(FloorDiv, {\n    a: n,\n    b: r\n  });\n}\n\nvar floorDiv$2 = op({\n  floorDiv_\n});\n\nfunction div_(e, t) {\n  var n = convertToTensor(e, \"a\", \"div\"),\n      r = convertToTensor(t, \"b\", \"div\");\n  return [n, r] = makeTypesMatch(n, r), \"int32\" === n.dtype && \"int32\" === r.dtype ? floorDiv$2(n, r) : ENGINE.runKernel(RealDiv, {\n    a: n,\n    b: r\n  }, {});\n}\n\nvar div$1 = op({\n  div_\n});\n\nfunction mul_(e, t) {\n  var n = convertToTensor(e, \"a\", \"mul\"),\n      r = convertToTensor(t, \"b\", \"mul\");\n  return [n, r] = makeTypesMatch(n, r), ENGINE.runKernel(Multiply$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar mul = op({\n  mul_\n});\n\nfunction abs_(e) {\n  var t = convertToTensor(e, \"x\", \"abs\");\n  return ENGINE.runKernel(\"complex64\" === t.dtype ? ComplexAbs : Abs, {\n    x: t\n  });\n}\n\nvar abs$2 = op({\n  abs_\n});\n\nfunction acos_(e) {\n  var t = convertToTensor(e, \"x\", \"acos\");\n  return ENGINE.runKernel(Acos, {\n    x: t\n  });\n}\n\nvar acos$2 = op({\n  acos_\n});\n\nfunction acosh_(e) {\n  var t = convertToTensor(e, \"x\", \"acosh\");\n  return ENGINE.runKernel(Acosh, {\n    x: t\n  });\n}\n\nvar acosh$2 = op({\n  acosh_\n});\n\nfunction addN_(e) {\n  assert$4(Array.isArray(e), () => \"The argument passed to tf.addN() must be a list of tensors\"), assert$4(e.length >= 1, () => \"Must pass at least one tensor to tf.addN(), but got \".concat(e.length));\n  var t = e.map((e, t) => convertToTensor(e, \"tensors\".concat(t), \"addN\")),\n      n = t[0];\n  return t.forEach(e => {\n    if (e.dtype !== n.dtype) throw new Error(\"All tensors passed to tf.addN() must have the same dtype\");\n  }), t.forEach(e => {\n    if (!arraysEqual(e.shape, n.shape)) throw new Error(\"All tensors passed to tf.addN() must have the same shape\");\n  }), ENGINE.runKernel(AddN, t);\n}\n\nvar addN$2 = op({\n  addN_\n});\n\nfunction all_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"all\", \"bool\");\n  return ENGINE.runKernel(All, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar all$2 = op({\n  all_\n});\n\nfunction any_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"any\", \"bool\");\n  return ENGINE.runKernel(Any, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar any$2 = op({\n  any_\n});\n\nfunction argMax_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor(e, \"x\", \"argMax\");\n  return ENGINE.runKernel(ArgMax, {\n    x: n\n  }, {\n    axis: t\n  });\n}\n\nvar argMax$2 = op({\n  argMax_\n});\n\nfunction argMin_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor(e, \"x\", \"argMin\");\n  return ENGINE.runKernel(ArgMin, {\n    x: n\n  }, {\n    axis: t\n  });\n}\n\nvar argMin$2 = op({\n  argMin_\n});\n\nfunction asin_(e) {\n  var t = convertToTensor(e, \"x\", \"asin\");\n  return ENGINE.runKernel(Asin, {\n    x: t\n  });\n}\n\nvar asin$2 = op({\n  asin_\n});\n\nfunction asinh_(e) {\n  var t = convertToTensor(e, \"x\", \"asinh\");\n  return ENGINE.runKernel(Asinh, {\n    x: t\n  });\n}\n\nvar asinh$2 = op({\n  asinh_\n});\n\nfunction atan_(e) {\n  var t = convertToTensor(e, \"x\", \"atan\");\n  return ENGINE.runKernel(Atan, {\n    x: t\n  });\n}\n\nvar atan$2 = op({\n  atan_\n});\n\nfunction atan2_(e, t) {\n  var n = convertToTensor(e, \"a\", \"atan2\"),\n      r = convertToTensor(t, \"b\", \"atan2\");\n  return [n, r] = makeTypesMatch(n, r), ENGINE.runKernel(Atan2, {\n    a: n,\n    b: r\n  });\n}\n\nvar atan2$2 = op({\n  atan2_\n});\n\nfunction atanh_(e) {\n  var t = convertToTensor(e, \"x\", \"atanh\");\n  return ENGINE.runKernel(Atanh, {\n    x: t\n  });\n}\n\nvar atanh$2 = op({\n  atanh_\n});\n\nfunction computeDilation2DInfo(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NHWC\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  return computeConv2DInfo(e, [...t, e[3]], n, s, r, null, null, convertConv2DDataFormat(a));\n}\n\nfunction computePool2DInfo(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"channelsLast\";\n  var [i, l] = parseTupleParam(t);\n  var u;\n  if (\"channelsLast\" === o) u = [i, l, e[3], e[3]];else {\n    if (\"channelsFirst\" !== o) throw new Error(\"Unknown dataFormat \".concat(o));\n    u = [i, l, e[1], e[1]];\n  }\n  return computeConv2DInfo(e, u, n, r, a, s, !1, o);\n}\n\nfunction computePool3DInfo(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"NDHWC\";\n  var [i, l, u] = parse3TupleParam(t);\n  var c, p;\n  if (\"NDHWC\" === o) p = \"channelsLast\", c = [i, l, u, e[4], e[4]];else {\n    if (\"NCDHW\" !== o) throw new Error(\"Unknown dataFormat \".concat(o));\n    p = \"channelsFirst\", c = [i, l, u, e[1], e[1]];\n  }\n  return computeConv3DInfo(e, c, n, r, a, !1, p, s);\n}\n\nfunction computeConv2DInfo(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !1;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : \"channelsLast\";\n  var [l, u, c, p] = [-1, -1, -1, -1];\n  if (\"channelsLast\" === i) [l, u, c, p] = e;else {\n    if (\"channelsFirst\" !== i) throw new Error(\"Unknown dataFormat \".concat(i));\n    [l, p, u, c] = e;\n  }\n  var [d, h,, m] = t,\n      [f, g] = parseTupleParam(n),\n      [$, y] = parseTupleParam(r),\n      b = getEffectiveFilterSize(d, $),\n      x = getEffectiveFilterSize(h, y),\n      {\n    padInfo: v,\n    outHeight: I,\n    outWidth: C\n  } = getPadAndOutInfo(a, u, c, f, g, b, x, s, i),\n      S = o ? m * p : m;\n  var k;\n  return \"channelsFirst\" === i ? k = [l, S, I, C] : \"channelsLast\" === i && (k = [l, I, C, S]), {\n    batchSize: l,\n    dataFormat: i,\n    inHeight: u,\n    inWidth: c,\n    inChannels: p,\n    outHeight: I,\n    outWidth: C,\n    outChannels: S,\n    padInfo: v,\n    strideHeight: f,\n    strideWidth: g,\n    filterHeight: d,\n    filterWidth: h,\n    effectiveFilterHeight: b,\n    effectiveFilterWidth: x,\n    dilationHeight: $,\n    dilationWidth: y,\n    inShape: e,\n    outShape: k,\n    filterShape: t\n  };\n}\n\nfunction computeConv3DInfo(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"channelsLast\";\n  var i = arguments.length > 7 ? arguments[7] : undefined;\n  var [l, u, c, p, d] = [-1, -1, -1, -1, -1];\n  if (\"channelsLast\" === o) [l, u, c, p, d] = e;else {\n    if (\"channelsFirst\" !== o) throw new Error(\"Unknown dataFormat \".concat(o));\n    [l, d, u, c, p] = e;\n  }\n  var [h, m, f,, g] = t,\n      [$, y, b] = parse3TupleParam(n),\n      [x, v, I] = parse3TupleParam(r),\n      C = getEffectiveFilterSize(h, x),\n      S = getEffectiveFilterSize(m, v),\n      k = getEffectiveFilterSize(f, I),\n      {\n    padInfo: T,\n    outDepth: N,\n    outHeight: w,\n    outWidth: E\n  } = get3DPadAndOutInfo(a, u, c, p, $, y, b, C, S, k, i),\n      A = s ? g * d : g;\n  var D;\n  return \"channelsFirst\" === o ? D = [l, A, N, w, E] : \"channelsLast\" === o && (D = [l, N, w, E, A]), {\n    batchSize: l,\n    dataFormat: o,\n    inDepth: u,\n    inHeight: c,\n    inWidth: p,\n    inChannels: d,\n    outDepth: N,\n    outHeight: w,\n    outWidth: E,\n    outChannels: A,\n    padInfo: T,\n    strideDepth: $,\n    strideHeight: y,\n    strideWidth: b,\n    filterDepth: h,\n    filterHeight: m,\n    filterWidth: f,\n    effectiveFilterDepth: C,\n    effectiveFilterHeight: S,\n    effectiveFilterWidth: k,\n    dilationDepth: x,\n    dilationHeight: v,\n    dilationWidth: I,\n    inShape: e,\n    outShape: D,\n    filterShape: t\n  };\n}\n\nfunction computeOutputShape2D(e, t, n, r, a) {\n  null == r && (r = computeDefaultPad(e, t, n));\n  var s = e[1];\n  return [round$3((e[0] - t + 2 * r) / n + 1, a), round$3((s - t + 2 * r) / n + 1, a)];\n}\n\nfunction computeOutputShape4D(e, t, n, r, a, s) {\n  null == a && (a = computeDefaultPad(e, t, r));\n  var o = e[1],\n      i = e[2];\n  return [round$3((e[0] - t + 2 * a) / r + 1, s), round$3((o - t + 2 * a) / r + 1, s), round$3((i - t + 2 * a) / r + 1, s), n];\n}\n\nfunction computeDefaultPad(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = getEffectiveFilterSize(t, r);\n  return Math.floor((e[0] * (n - 1) - n + a) / 2);\n}\n\nfunction parseTupleParam(e) {\n  return \"number\" == typeof e ? [e, e, e] : 2 === e.length ? [e[0], e[1], 1] : e;\n}\n\nfunction parse3TupleParam(e) {\n  return \"number\" == typeof e ? [e, e, e] : e;\n}\n\nfunction getEffectiveFilterSize(e, t) {\n  return t <= 1 ? e : e + (e - 1) * (t - 1);\n}\n\nfunction getPadAndOutInfo(e, t, n, r, a, s, o, i, l) {\n  var u, c, p;\n\n  if (\"number\" == typeof e) {\n    u = {\n      top: e,\n      bottom: e,\n      left: e,\n      right: e,\n      type: 0 === e ? \"VALID\" : \"NUMBER\"\n    };\n\n    var _a126 = computeOutputShape2D([t, n], s, r, e, i);\n\n    c = _a126[0], p = _a126[1];\n  } else if (\"same\" === e) {\n    c = Math.ceil(t / r), p = Math.ceil(n / a);\n\n    var _e567 = Math.max(0, (c - 1) * r + s - t),\n        _i43 = Math.max(0, (p - 1) * a + o - n),\n        _l35 = Math.floor(_e567 / 2),\n        d = _e567 - _l35,\n        h = Math.floor(_i43 / 2);\n\n    u = {\n      top: _l35,\n      bottom: d,\n      left: h,\n      right: _i43 - h,\n      type: \"SAME\"\n    };\n  } else if (\"valid\" === e) u = {\n    top: 0,\n    bottom: 0,\n    left: 0,\n    right: 0,\n    type: \"VALID\"\n  }, c = Math.ceil((t - s + 1) / r), p = Math.ceil((n - o + 1) / a);else {\n    if (\"object\" != typeof e) throw Error(\"Unknown padding parameter: \".concat(e));\n    {\n      var _d12 = \"channelsLast\" === l ? e[1][0] : e[2][0],\n          _h13 = \"channelsLast\" === l ? e[1][1] : e[2][1],\n          m = \"channelsLast\" === l ? e[2][0] : e[3][0],\n          f = \"channelsLast\" === l ? e[2][1] : e[3][1];\n\n      u = {\n        top: _d12,\n        bottom: _h13,\n        left: m,\n        right: f,\n        type: 0 === _d12 && 0 === _h13 && 0 === m && 0 === f ? \"VALID\" : \"EXPLICIT\"\n      }, c = round$3((t - s + _d12 + _h13) / r + 1, i), p = round$3((n - o + m + f) / a + 1, i);\n    }\n  }\n\n  return {\n    padInfo: u,\n    outHeight: c,\n    outWidth: p\n  };\n}\n\nfunction get3DPadAndOutInfo(e, t, n, r, a, s, o, i, l, u, c) {\n  var p, d, h, m;\n\n  if (\"number\" == typeof e) {\n    p = {\n      top: e,\n      bottom: e,\n      left: e,\n      right: e,\n      front: e,\n      back: e,\n      type: 0 === e ? \"VALID\" : \"NUMBER\"\n    };\n\n    var _s95 = computeOutputShape4D([t, n, r, 1], i, 1, a, e, c);\n\n    d = _s95[0], h = _s95[1], m = _s95[2];\n  } else if (\"same\" === e) {\n    d = Math.ceil(t / a), h = Math.ceil(n / s), m = Math.ceil(r / o);\n\n    var _e568 = (d - 1) * a + i - t,\n        _c20 = (h - 1) * s + l - n,\n        f = (m - 1) * o + u - r,\n        g = Math.floor(_e568 / 2),\n        $ = _e568 - g,\n        y = Math.floor(_c20 / 2),\n        b = _c20 - y,\n        x = Math.floor(f / 2);\n\n    p = {\n      top: y,\n      bottom: b,\n      left: x,\n      right: f - x,\n      front: g,\n      back: $,\n      type: \"SAME\"\n    };\n  } else {\n    if (\"valid\" !== e) throw Error(\"Unknown padding parameter: \".concat(e));\n    p = {\n      top: 0,\n      bottom: 0,\n      left: 0,\n      right: 0,\n      front: 0,\n      back: 0,\n      type: \"VALID\"\n    }, d = Math.ceil((t - i + 1) / a), h = Math.ceil((n - l + 1) / s), m = Math.ceil((r - u + 1) / o);\n  }\n\n  return {\n    padInfo: p,\n    outDepth: d,\n    outHeight: h,\n    outWidth: m\n  };\n}\n\nfunction round$3(e, t) {\n  if (!t) return Math.trunc(e);\n\n  switch (t) {\n    case \"round\":\n      return Math.round(e);\n\n    case \"ceil\":\n      return Math.ceil(e);\n\n    case \"floor\":\n      return Math.floor(e);\n\n    default:\n      throw new Error(\"Unknown roundingMode \".concat(t));\n  }\n}\n\nfunction tupleValuesAreOne(e) {\n  var [t, n, r] = parseTupleParam(e);\n  return 1 === t && 1 === n && 1 === r;\n}\n\nfunction eitherStridesOrDilationsAreOne(e, t) {\n  return tupleValuesAreOne(e) || tupleValuesAreOne(t);\n}\n\nfunction convertConv2DDataFormat(e) {\n  if (\"NHWC\" === e) return \"channelsLast\";\n  if (\"NCHW\" === e) return \"channelsFirst\";\n  throw new Error(\"Unknown dataFormat \".concat(e));\n}\n\nfunction reshape_(e, t) {\n  var n = convertToTensor(e, \"x\", \"reshape\", \"string_or_numeric\");\n  return ENGINE.runKernel(Reshape$1, {\n    x: n\n  }, {\n    shape: t\n  });\n}\n\nvar reshape$3 = op({\n  reshape_\n});\n\nfunction avgPool_(e, t, n, r, a) {\n  var s = convertToTensor(e, \"x\", \"avgPool\", \"float32\");\n  assert$4(eitherStridesOrDilationsAreOne(n, 1), () => \"Error in avgPool: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '1'\"));\n  var o = s,\n      i = !1;\n  3 === s.rank && (i = !0, o = reshape$3(s, [1, s.shape[0], s.shape[1], s.shape[2]])), assert$4(4 === o.rank, () => \"Error in avgPool: x must be rank 4 but got rank \".concat(o.rank, \".\")), null != a && assert$4(isInt(r), () => \"Error in avgPool: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var l = ENGINE.runKernel(AvgPool, {\n    x: o\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a\n  });\n  return l = cast$3(l, s.dtype), i ? reshape$3(l, [l.shape[1], l.shape[2], l.shape[3]]) : l;\n}\n\nvar avgPool$2 = op({\n  avgPool_\n});\n\nfunction avgPool3d_(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NDHWC\";\n  var o = convertToTensor(e, \"x\", \"avgPool3d\", \"float32\");\n  var i = o,\n      l = !1;\n  4 === o.rank && (l = !0, i = reshape$3(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]])), assert$4(5 === i.rank, () => \"Error in avgPool3d: x must be rank 5 but got rank \".concat(i.rank, \".\")), assert$4(\"NDHWC\" === s, () => \"Error in avgPool3d: Only NDHWC is currently supported, but got dataFormat of \".concat(s)), null != a && assert$4(isInt(r), () => \"Error in avgPool3d: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var u = ENGINE.runKernel(AvgPool3D, {\n    x: i\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a,\n    dataFormat: s\n  });\n  return u = cast$3(u, i.dtype), l ? reshape$3(u, [u.shape[1], u.shape[2], u.shape[3], u.shape[4]]) : u;\n}\n\nvar avgPool3d$1 = op({\n  avgPool3d_\n});\n\nfunction concat_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  assert$4(e.length >= 1, () => \"Pass at least one tensor to concat\");\n  var n = convertToTensorArray(e, \"tensors\", \"concat\", \"string_or_numeric\");\n  return \"complex64\" === n[0].dtype && n.forEach(e => {\n    if (\"complex64\" !== e.dtype) throw new Error(\"Cannot concatenate complex64 tensors with a tensor\\n          with dtype \".concat(e.dtype, \". \"));\n  }), 1 === n.length ? clone(n[0]) : ENGINE.runKernel(Concat, n, {\n    axis: t\n  });\n}\n\nvar concat$2 = op({\n  concat_\n});\n\nfunction sigmoid_(e) {\n  var t = convertToTensor(e, \"x\", \"sigmoid\");\n  return ENGINE.runKernel(Sigmoid$1, {\n    x: t\n  });\n}\n\nvar sigmoid$2 = op({\n  sigmoid_\n});\n\nfunction slice_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"slice\", \"string_or_numeric\");\n  if (0 === r.rank) throw new Error(\"Slicing scalar is not possible\");\n  return ENGINE.runKernel(Slice, {\n    x: r\n  }, {\n    begin: t,\n    size: n\n  });\n}\n\nvar slice$2 = op({\n  slice_\n});\n\nfunction tanh_(e) {\n  var t = convertToTensor(e, \"x\", \"tanh\");\n  return ENGINE.runKernel(Tanh$1, {\n    x: t\n  });\n}\n\nvar tanh$2 = op({\n  tanh_\n});\n\nfunction basicLSTMCell_(e, t, n, r, a, s) {\n  var o = convertToTensor(e, \"forgetBias\", \"basicLSTMCell\"),\n      i = convertToTensor(t, \"lstmKernel\", \"basicLSTMCell\"),\n      l = convertToTensor(n, \"lstmBias\", \"basicLSTMCell\"),\n      u = convertToTensor(r, \"data\", \"basicLSTMCell\"),\n      c = convertToTensor(a, \"c\", \"basicLSTMCell\"),\n      p = convertToTensor(s, \"h\", \"basicLSTMCell\"),\n      d = concat$2([u, p], 1),\n      h = matMul$1(d, i),\n      m = add$2(h, l),\n      f = m.shape[1] / 4,\n      g = [m.shape[0], f],\n      $ = slice$2(m, [0, 0], g),\n      y = slice$2(m, [0, f], g),\n      b = slice$2(m, [0, 2 * f], g),\n      x = slice$2(m, [0, 3 * f], g),\n      v = add$2(mul(sigmoid$2($), tanh$2(y)), mul(c, sigmoid$2(add$2(o, b))));\n  return [v, mul(tanh$2(v), sigmoid$2(x))];\n}\n\nvar basicLSTMCell = op({\n  basicLSTMCell_\n});\n\nfunction batchToSpaceND_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"batchToSpaceND\"),\n      a = t.reduce((e, t) => e * t);\n  return assert$4(r.rank >= 1 + t.length, () => \"input rank is \".concat(r.rank, \" but should be > than blockShape.length \").concat(t.length)), assert$4(n.length === t.length, () => \"crops.length is \".concat(n.length, \" but should be equal to blockShape.length  \").concat(t.length)), assert$4(r.shape[0] % a == 0, () => \"input tensor batch is \".concat(r.shape[0], \" but is not divisible by the product of the elements of blockShape \").concat(t.join(\" * \"), \" === \").concat(a)), ENGINE.runKernel(BatchToSpaceND, {\n    x: r\n  }, {\n    blockShape: t,\n    crops: n\n  });\n}\n\nvar batchToSpaceND$2 = op({\n  batchToSpaceND_\n});\n\nfunction xAs4D(e) {\n  var t;\n  return t = 0 === e.rank || 1 === e.rank ? reshape$3(e, [1, 1, 1, e.size]) : 2 === e.rank ? reshape$3(e, [1, 1, e.shape[0], e.shape[1]]) : 3 === e.rank ? reshape$3(e, [1, e.shape[0], e.shape[1], e.shape[2]]) : e, t;\n}\n\nfunction batchNorm_(e, t, n, r, a, s) {\n  null == s && (s = .001);\n  var o = convertToTensor(e, \"x\", \"batchNorm\"),\n      i = convertToTensor(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor(n, \"variance\", \"batchNorm\");\n  var u, c;\n  null != a && (u = convertToTensor(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor(r, \"offset\", \"batchNorm\")), assert$4(i.rank === l.rank, () => \"Batch normalization gradient requires mean and variance to have equal ranks.\"), assert$4(null == c || i.rank === c.rank, () => \"Batch normalization gradient requires mean and offset to have equal ranks.\"), assert$4(null == u || i.rank === u.rank, () => \"Batch normalization gradient requires mean and scale to have equal ranks.\");\n  var p = xAs4D(o),\n      d = ENGINE.runKernel(FusedBatchNorm, {\n    x: p,\n    scale: u,\n    offset: c,\n    mean: i,\n    variance: l\n  }, {\n    varianceEpsilon: s\n  });\n  return reshape$3(d, o.shape);\n}\n\nvar batchNorm$2 = op({\n  batchNorm_\n});\n\nfunction batchNorm2d_(e, t, n, r, a, s) {\n  var o = convertToTensor(e, \"x\", \"batchNorm\"),\n      i = convertToTensor(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor(n, \"variance\", \"batchNorm\");\n  var u, c;\n  return null != a && (u = convertToTensor(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor(r, \"offset\", \"batchNorm\")), assert$4(2 === o.rank, () => \"Error in batchNorm2D: x must be rank 2 but got rank \".concat(o.rank, \".\")), assert$4(2 === i.rank || 1 === i.rank, () => \"Error in batchNorm2D: mean must be rank 2 or rank 1 but got rank \".concat(i.rank, \".\")), assert$4(2 === l.rank || 1 === l.rank, () => \"Error in batchNorm2D: variance must be rank 2 or rank 1 but got rank \".concat(l.rank, \".\")), null != u && assert$4(2 === u.rank || 1 === u.rank, () => \"Error in batchNorm2D: scale must be rank 2 or rank 1 but got rank \".concat(u.rank, \".\")), null != c && assert$4(2 === c.rank || 1 === c.rank, () => \"Error in batchNorm2D: offset must be rank 2 or rank 1 but got rank \".concat(c.rank, \".\")), batchNorm$2(o, i, l, c, u, s);\n}\n\nvar batchNorm2d = op({\n  batchNorm2d_\n});\n\nfunction batchNorm3d_(e, t, n, r, a, s) {\n  var o = convertToTensor(e, \"x\", \"batchNorm\"),\n      i = convertToTensor(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor(n, \"variance\", \"batchNorm\");\n  var u, c;\n  return null != a && (u = convertToTensor(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor(r, \"offset\", \"batchNorm\")), assert$4(3 === o.rank, () => \"Error in batchNorm3D: x must be rank 3 but got rank \".concat(o.rank, \".\")), assert$4(3 === i.rank || 1 === i.rank, () => \"Error in batchNorm3D: mean must be rank 3 or rank 1 but got rank \".concat(i.rank, \".\")), assert$4(3 === l.rank || 1 === l.rank, () => \"Error in batchNorm3D: variance must be rank 3 or rank 1 but got rank \".concat(l.rank, \".\")), null != u && assert$4(3 === u.rank || 1 === u.rank, () => \"Error in batchNorm3D: scale must be rank 3 or rank 1 but got rank \".concat(u.rank, \".\")), null != c && assert$4(3 === c.rank || 1 === c.rank, () => \"Error in batchNorm3D: offset must be rank 3 or rank 1 but got rank \".concat(c.rank, \".\")), batchNorm$2(o, i, l, c, u, s);\n}\n\nvar batchNorm3d = op({\n  batchNorm3d_\n});\n\nfunction batchNorm4d_(e, t, n, r, a, s) {\n  var o = convertToTensor(e, \"x\", \"batchNorm\"),\n      i = convertToTensor(t, \"mean\", \"batchNorm\"),\n      l = convertToTensor(n, \"variance\", \"batchNorm\");\n  var u, c;\n  return null != a && (u = convertToTensor(a, \"scale\", \"batchNorm\")), null != r && (c = convertToTensor(r, \"offset\", \"batchNorm\")), assert$4(4 === o.rank, () => \"Error in batchNorm4D: x must be rank 4 but got rank \".concat(o.rank, \".\")), assert$4(4 === i.rank || 1 === i.rank, () => \"Error in batchNorm4D: mean must be rank 4 or rank 1 but got rank \".concat(i.rank, \".\")), assert$4(4 === l.rank || 1 === l.rank, () => \"Error in batchNorm4D: variance must be rank 4 or rank 1 but got rank \".concat(l.rank, \".\")), null != u && assert$4(4 === u.rank || 1 === u.rank, () => \"Error in batchNorm4D: scale must be rank 4 or rank 1 but got rank \".concat(u.rank, \".\")), null != c && assert$4(4 === c.rank || 1 === c.rank, () => \"Error in batchNorm4D: offset must be rank 4 or rank 1 but got rank \".concat(c.rank, \".\")), batchNorm$2(o, i, l, c, u, s);\n}\n\nvar batchNorm4d = op({\n  batchNorm4d_\n});\n\nfunction bincount_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"bincount\"),\n      a = convertToTensor(t, \"weights\", \"bincount\");\n  return assert$4(\"int32\" === r.dtype, () => \"Error in bincount: input dtype must be int32, but got \".concat(r.dtype)), assert$4(n >= 0, () => \"size must be non-negative, but got \".concat(n, \".\")), assert$4(a.size === r.size || 0 === a.size, () => \"Error in bincount: weights must have the same size as input or0-length, but got input shape: \".concat(r.shape, \", weights shape: \").concat(a.shape, \".\")), ENGINE.runKernel(Bincount, {\n    x: r,\n    weights: a\n  }, {\n    size: n\n  });\n}\n\nvar bincount$2 = op({\n  bincount_\n});\n\nfunction broadcastTo_(e, t) {\n  var n = convertToTensor(e, \"broadcastTo\", \"x\");\n  var r = n.shape;\n  if (t.some(e => !(e > 0) || e % 1 != 0)) throw new Error(\"broadcastTo(): Invalid broadcast shape [\".concat(t, \"].\"));\n  if (t.length < n.rank) throw new Error(\"broadcastTo(): shape.length=\".concat(t.length, \" < input.rank=\").concat(n.rank, \".\"));\n\n  if (t.length > n.rank) {\n    var _e569 = n.shape.slice();\n\n    for (; _e569.length < t.length;) {\n      _e569.unshift(1);\n    }\n\n    n = reshape$3(n, _e569);\n  }\n\n  var a = n.shape,\n      s = Array.from(t);\n\n  for (var _e570 = t.length - 1; _e570 >= 0; _e570--) {\n    if (a[_e570] === t[_e570]) s[_e570] = 1;else if (1 !== n.shape[_e570]) throw new Error(\"broadcastTo(): [\".concat(r, \"] cannot be broadcast to [\").concat(t, \"].\"));\n  }\n\n  return 0 === s.map((e, t) => e > 1 ? t : -1).filter(e => e >= 0).length ? clone(n) : ENGINE.runKernel(Tile, {\n    x: n\n  }, {\n    reps: s\n  });\n}\n\nvar broadcastTo = op({\n  broadcastTo_\n});\n\nfunction ceil_(e) {\n  var t = convertToTensor(e, \"x\", \"ceil\");\n  return ENGINE.runKernel(Ceil, {\n    x: t\n  });\n}\n\nvar ceil$2 = op({\n  ceil_\n});\n\nfunction clipByValue_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"clipByValue\");\n  return assert$4(t <= n, () => \"Error in clip: min (\".concat(t, \") must be less than or equal to max (\").concat(n, \").\")), ENGINE.runKernel(ClipByValue, {\n    x: r\n  }, {\n    clipValueMin: t,\n    clipValueMax: n\n  });\n}\n\nvar clipByValue$1 = op({\n  clipByValue_\n});\n\nfunction concat1d_(e) {\n  return concat$2(e, 0);\n}\n\nvar concat1d = op({\n  concat1d_\n});\n\nfunction concat2d_(e, t) {\n  return concat$2(e, t);\n}\n\nvar concat2d = op({\n  concat2d_\n});\n\nfunction concat3d_(e, t) {\n  return concat$2(e, t);\n}\n\nvar concat3d = op({\n  concat3d_\n});\n\nfunction concat4d_(e, t) {\n  return concat$2(e, t);\n}\n\nvar concat4d = op({\n  concat4d_\n});\n\nfunction conv2d_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NHWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = convertToTensor(e, \"x\", \"conv2d\"),\n      l = convertToTensor(t, \"filter\", \"conv2d\");\n  var u = i,\n      c = !1;\n  3 === i.rank && (c = !0, u = reshape$3(i, [1, i.shape[0], i.shape[1], i.shape[2]])), assert$4(4 === u.rank, () => \"Error in conv2d: input must be rank 4, but got rank \".concat(u.rank, \".\")), assert$4(4 === l.rank, () => \"Error in conv2d: filter must be rank 4, but got rank \".concat(l.rank, \".\")), null != o && assert$4(isInt(r), () => \"Error in conv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\"));\n  var p = \"NHWC\" === a ? u.shape[3] : u.shape[1];\n  assert$4(p === l.shape[2], () => \"Error in conv2d: depth of input (\".concat(p, \") must match input depth for filter \").concat(l.shape[2], \".\")), assert$4(eitherStridesOrDilationsAreOne(n, s), () => \"Error in conv2D: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '\").concat(s, \"'\"));\n  var d = ENGINE.runKernel(Conv2D$1, {\n    x: u,\n    filter: l\n  }, {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s,\n    dimRoundingMode: o\n  });\n  return c ? reshape$3(d, [d.shape[1], d.shape[2], d.shape[3]]) : d;\n}\n\nvar conv2d$3 = op({\n  conv2d_\n});\n\nfunction conv1d_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1;\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = convertToTensor(e, \"x\", \"conv1d\"),\n      l = convertToTensor(t, \"filter\", \"conv1d\");\n  var u = i,\n      c = !1;\n  2 === i.rank && (c = !0, u = reshape$3(i, [1, i.shape[0], i.shape[1]])), assert$4(3 === u.rank, () => \"Error in conv1d: input must be rank 3, but got rank \".concat(u.rank, \".\")), assert$4(3 === l.rank, () => \"Error in conv1d: filter must be rank 3, but got rank \".concat(l.rank, \".\")), null != o && assert$4(isInt(r), () => \"Error in conv1d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\")), assert$4(u.shape[2] === l.shape[1], () => \"Error in conv1d: depth of input (\".concat(u.shape[2], \") must match input depth for filter \").concat(l.shape[1], \".\")), assert$4(eitherStridesOrDilationsAreOne(n, s), () => \"Error in conv1D: Either stride or dilation must be 1. Got stride \".concat(n, \" and dilation '\").concat(s, \"'\")), assert$4(\"NWC\" === a, () => \"Error in conv1d: got dataFormat of \".concat(a, \" but only NWC is currently supported.\"));\n  var p = reshape$3(l, [1, l.shape[0], l.shape[1], l.shape[2]]),\n      d = reshape$3(u, [u.shape[0], 1, u.shape[1], u.shape[2]]),\n      h = conv2d$3(d, p, [1, n], r, \"NHWC\", [1, s], o);\n  return reshape$3(h, c ? [h.shape[2], h.shape[3]] : [h.shape[0], h.shape[2], h.shape[3]]);\n}\n\nvar conv1d$1 = op({\n  conv1d_\n});\n\nfunction conv2DBackpropInput_(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NHWC\";\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  assert$4(e.length === t.rank, () => \"Length of inShape (\".concat(e.length, \") and rank of dy (\").concat(t.rank, \") must match\"));\n  var i = e,\n      l = t,\n      u = !1;\n  3 === t.rank && (u = !0, l = reshape$3(t, [1, t.shape[0], t.shape[1], t.shape[2]]), i = [1, e[0], e[1], e[2]]), assert$4(4 === i.length, () => \"Error in conv2dDerInput: inShape must be length 4, but got length \".concat(i.length, \".\")), assert$4(4 === l.rank, () => \"Error in conv2dDerInput: dy must be rank 4, but got rank \".concat(l.rank)), assert$4(4 === n.rank, () => \"Error in conv2dDerInput: filter must be rank 4, but got rank \".concat(n.rank));\n  var c = \"NHWC\" === s ? i[3] : i[1],\n      p = \"NHWC\" === s ? l.shape[3] : l.shape[1];\n  assert$4(c === n.shape[2], () => \"Error in conv2dDerInput: depth of input (\".concat(c, \") must match input depth for filter \").concat(n.shape[2], \".\")), assert$4(p === n.shape[3], () => \"Error in conv2dDerInput: depth of output (\".concat(p, \") must match output depth for filter \").concat(n.shape[3], \".\")), null != o && assert$4(isInt(a), () => \"Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(a, \".\"));\n  var d = ENGINE.runKernel(Conv2DBackpropInput, {\n    dy: l,\n    filter: n\n  }, {\n    strides: r,\n    pad: a,\n    dataFormat: s,\n    dimRoundingMode: o,\n    inputShape: i\n  });\n  return u ? reshape$3(d, [d.shape[1], d.shape[2], d.shape[3]]) : d;\n}\n\nvar conv2DBackpropInput$2 = op({\n  conv2DBackpropInput_\n});\n\nfunction conv2dTranspose_(e, t, n, r, a, s) {\n  var o = convertToTensor(e, \"x\", \"conv2dTranspose\"),\n      i = convertToTensor(t, \"filter\", \"conv2dTranspose\");\n  return conv2DBackpropInput$2(n, o, i, r, a, \"NHWC\", s);\n}\n\nvar conv2dTranspose$1 = op({\n  conv2dTranspose_\n});\n\nfunction conv3d_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NDHWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1, 1];\n  var o = convertToTensor(e, \"x\", \"conv3d\"),\n      i = convertToTensor(t, \"filter\", \"conv3d\");\n  var l = o,\n      u = !1;\n  4 === o.rank && (u = !0, l = reshape$3(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]])), assert$4(5 === l.rank, () => \"Error in conv3d: input must be rank 5, but got rank \".concat(l.rank, \".\")), assert$4(5 === i.rank, () => \"Error in conv3d: filter must be rank 5, but got rank \".concat(i.rank, \".\")), assert$4(l.shape[4] === i.shape[3], () => \"Error in conv3d: depth of input (\".concat(l.shape[4], \") must match input depth for filter \").concat(i.shape[3], \".\")), assert$4(eitherStridesOrDilationsAreOne(n, s), () => \"Error in conv3D: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '\").concat(s, \"'\")), assert$4(\"NDHWC\" === a, () => \"Error in conv3d: got dataFormat of \".concat(a, \" but only NDHWC is currently supported.\"));\n  var c = ENGINE.runKernel(Conv3D$1, {\n    x: l,\n    filter: i\n  }, {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s\n  });\n  return u ? reshape$3(c, [c.shape[1], c.shape[2], c.shape[3], c.shape[4]]) : c;\n}\n\nvar conv3d$1 = op({\n  conv3d_\n});\n\nfunction conv3DBackpropInput_(e, t, n, r, a) {\n  assert$4(e.length === t.rank, () => \"Length of inShape (\".concat(e.length, \") and rank of dy (\").concat(t.rank, \") must match\"));\n  var s = e,\n      o = t,\n      i = !1;\n  4 === t.rank && (i = !0, o = reshape$3(t, [1, t.shape[0], t.shape[1], t.shape[2], t.shape[3]]), s = [1, e[0], e[1], e[2], e[3]]);\n  var l = s[4],\n      u = o.shape[4];\n  assert$4(5 === s.length, () => \"Error in conv3dDerInput: inShape must be length 5, but got length \".concat(s.length, \".\")), assert$4(5 === o.rank, () => \"Error in conv3dDerInput: dy must be rank 5, but got rank \".concat(o.rank)), assert$4(5 === n.rank, () => \"Error in conv3dDerInput: filter must be rank 5, but got rank \".concat(n.rank)), assert$4(l === n.shape[3], () => \"Error in conv3dDerInput: depth of input (\".concat(l, \") must match input depth for filter \").concat(n.shape[3], \".\")), assert$4(u === n.shape[4], () => \"Error in conv3dDerInput: depth of output (\".concat(u, \") must match output depth for filter \").concat(n.shape[4], \".\"));\n  var c = ENGINE.runKernel(Conv3DBackpropInputV2, {\n    dy: o,\n    filter: n\n  }, {\n    pad: a,\n    strides: r,\n    inputShape: s\n  });\n  return i ? reshape$3(c, [c.shape[1], c.shape[2], c.shape[3], c.shape[4]]) : c;\n}\n\nvar conv3DBackpropInput$1 = op({\n  conv3DBackpropInput_\n});\n\nfunction conv3dTranspose_(e, t, n, r, a) {\n  var s = convertToTensor(e, \"x\", \"conv3dTranspose\"),\n      o = convertToTensor(t, \"filter\", \"conv3dTranspose\");\n  return conv3DBackpropInput$1(n, s, o, r, a);\n}\n\nvar conv3dTranspose$1 = op({\n  conv3dTranspose_\n});\n\nfunction cos_(e) {\n  var t = convertToTensor(e, \"x\", \"cos\");\n  return ENGINE.runKernel(Cos, {\n    x: t\n  });\n}\n\nvar cos$2 = op({\n  cos_\n});\n\nfunction cosh_(e) {\n  var t = convertToTensor(e, \"x\", \"cosh\");\n  return ENGINE.runKernel(Cosh, {\n    x: t\n  });\n}\n\nvar cosh$2 = op({\n  cosh_\n});\n\nfunction cumsum_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor(e, \"x\", \"cumsum\");\n  return ENGINE.runKernel(Cumsum, {\n    x: a\n  }, {\n    axis: t,\n    exclusive: n,\n    reverse: r\n  });\n}\n\nvar cumsum$2 = op({\n  cumsum_\n});\n\nfunction denseBincount_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor(e, \"x\", \"denseBincount\"),\n      s = convertToTensor(t, \"weights\", \"denseBincount\");\n  return assert$4(\"int32\" === a.dtype, () => \"Error in denseBincount: input dtype must be int32, but got \".concat(a.dtype)), assert$4(a.rank <= 2, () => \"Error in denseBincount: input must be at most rank 2, but got rank \".concat(a.rank, \".\")), assert$4(n >= 0, () => \"size must be non-negative, but got \".concat(n, \".\")), assert$4(s.size === a.size || 0 === s.size, () => \"Error in denseBincount: weights must have the same shape as x or 0-length, but got x shape: \".concat(a.shape, \", weights shape: \").concat(s.shape, \".\")), ENGINE.runKernel(DenseBincount, {\n    x: a,\n    weights: s\n  }, {\n    size: n,\n    binaryOutput: r\n  });\n}\n\nvar denseBincount$2 = op({\n  denseBincount_\n});\n\nfunction depthToSpace_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"NHWC\";\n  var r = convertToTensor(e, \"x\", \"depthToSpace\"),\n      a = \"NHWC\" === n ? r.shape[1] : r.shape[2],\n      s = \"NHWC\" === n ? r.shape[2] : r.shape[3],\n      o = \"NHWC\" === n ? r.shape[3] : r.shape[1];\n  return assert$4(a * t >= 0, () => \"Negative dimension size caused by overflow when multiplying\\n    \".concat(a, \" and \").concat(t, \"  for depthToSpace with input shape\\n    \").concat(r.shape)), assert$4(s * t >= 0, () => \"Negative dimension size caused by overflow when multiplying\\n    \".concat(s, \" and \").concat(t, \" for depthToSpace with input shape\\n        \").concat(r.shape)), assert$4(o % (t * t) == 0, () => \"Dimension size must be evenly divisible by \".concat(t * t, \" but is \").concat(o, \" for depthToSpace with input shape \").concat(r.shape)), ENGINE.runKernel(DepthToSpace, {\n    x: r\n  }, {\n    blockSize: t,\n    dataFormat: n\n  });\n}\n\nvar depthToSpace$2 = op({\n  depthToSpace_\n});\n\nfunction depthwiseConv2d_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"NHWC\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = convertToTensor(e, \"x\", \"depthwiseConv2d\"),\n      l = convertToTensor(t, \"filter\", \"depthwiseConv2d\");\n  var u = i,\n      c = !1;\n  3 === i.rank && (c = !0, u = reshape$3(i, [1, i.shape[0], i.shape[1], i.shape[2]])), assert$4(4 === u.rank, () => \"Error in depthwiseConv2d: input must be rank 4, but got rank \".concat(u.rank, \".\")), assert$4(4 === l.rank, () => \"Error in depthwiseConv2d: filter must be rank 4, but got rank \".concat(l.rank, \".\")), assert$4(u.shape[3] === l.shape[2], () => \"Error in depthwiseConv2d: number of input channels (\".concat(u.shape[3], \") must match the inChannels dimension in filter \").concat(l.shape[2], \".\")), null != o && assert$4(isInt(r), () => \"Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\"));\n  var p = ENGINE.runKernel(DepthwiseConv2dNative, {\n    x: u,\n    filter: l\n  }, {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s,\n    dimRoundingMode: o\n  });\n  return c ? reshape$3(p, [p.shape[1], p.shape[2], p.shape[3]]) : p;\n}\n\nvar depthwiseConv2d$3 = op({\n  depthwiseConv2d_\n});\n\nfunction diag_(e) {\n  var t = convertToTensor(e, \"x\", \"diag\");\n  return ENGINE.runKernel(Diag, {\n    x: t\n  });\n}\n\nvar diag$2 = op({\n  diag_\n});\n\nfunction dilation2d_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : [1, 1];\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NHWC\";\n  var o = convertToTensor(e, \"x\", \"dilation2d\"),\n      i = convertToTensor(t, \"filter\", \"dilation2d\");\n  assert$4(3 === o.rank || 4 === o.rank, () => \"Error in dilation2d: input must be rank 3 or 4, but got rank \".concat(o.rank, \".\")), assert$4(3 === i.rank, () => \"Error in dilation2d: filter must be rank 3, but got rank \".concat(i.rank, \".\")), assert$4(\"NHWC\" === s, () => \"Error in dilation2d: Only NHWC is currently supported, but got dataFormat of \".concat(s));\n  var l = o,\n      u = !1;\n  3 === o.rank && (l = reshape$3(o, [1, o.shape[0], o.shape[1], o.shape[2]]), u = !0);\n  var c = ENGINE.runKernel(Dilation2D, {\n    x: l,\n    filter: i\n  }, {\n    strides: n,\n    pad: r,\n    dilations: a\n  });\n  return u ? reshape$3(c, [c.shape[1], c.shape[2], c.shape[3]]) : c;\n}\n\nvar dilation2d = op({\n  dilation2d_\n});\n\nfunction getBroadcastDims$1(e, t) {\n  var n = e.length,\n      r = [];\n\n  for (var a = 0; a < n; a++) {\n    var s = n - 1 - a,\n        o = e[s] || 1;\n    (t[t.length - 1 - a] || 1) > 1 && 1 === o && r.unshift(s);\n  }\n\n  return r;\n}\n\nfunction getReductionAxes(e, t) {\n  var n = [];\n\n  for (var r = 0; r < t.length; r++) {\n    var a = e[e.length - r - 1],\n        s = t.length - r - 1,\n        o = t[s];\n    (null == a || 1 === a && o > 1) && n.unshift(s);\n  }\n\n  return n;\n}\n\nfunction assertAndGetBroadcastShape(e, t) {\n  var n = [],\n      r = Math.max(e.length, t.length);\n\n  for (var a = 0; a < r; a++) {\n    var _r196 = e[e.length - a - 1];\n    null == _r196 && (_r196 = 1);\n    var s = t[t.length - a - 1];\n    if (null == s && (s = 1), 1 === _r196) n.unshift(s);else if (1 === s) n.unshift(_r196);else {\n      if (_r196 !== s) throw Error(\"Operands could not be broadcast together with shapes \".concat(e, \" and \").concat(t, \".\"));\n      n.unshift(_r196);\n    }\n  }\n\n  return n;\n}\n\nfunction equal_(e, t) {\n  var n = convertToTensor(e, \"a\", \"equal\", \"string_or_numeric\"),\n      r = convertToTensor(t, \"b\", \"equal\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch(n, r), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(Equal, {\n    a: n,\n    b: r\n  });\n}\n\nvar equal$2 = op({\n  equal_\n});\n\nfunction where_(e, t, n) {\n  var r = convertToTensor(t, \"a\", \"where\"),\n      a = convertToTensor(n, \"b\", \"where\"),\n      s = convertToTensor(e, \"condition\", \"where\", \"bool\"),\n      o = assertAndGetBroadcastShape(assertAndGetBroadcastShape(s.shape, r.shape), a.shape),\n      i = broadcastTo(s, o),\n      l = broadcastTo(r, o),\n      u = broadcastTo(a, o);\n  return ENGINE.runKernel(Select, {\n    condition: i,\n    t: l,\n    e: u\n  });\n}\n\nvar where = op({\n  where_\n});\n\nfunction zerosLike_(e) {\n  var t = convertToTensor(e, \"x\", \"zerosLike\");\n  return ENGINE.runKernel(ZerosLike, {\n    x: t\n  });\n}\n\nvar zerosLike$2 = op({\n  zerosLike_\n});\n\nfunction divNoNan_(e, t) {\n  var n = convertToTensor(e, \"a\", \"div\"),\n      r = convertToTensor(t, \"b\", \"div\");\n  [n, r] = makeTypesMatch(n, r);\n  var a = div$1(n, r),\n      s = zerosLike$2(a),\n      o = equal$2(r, s);\n  return where(o, s, a);\n}\n\nvar divNoNan = op({\n  divNoNan_\n});\n\nfunction dot_(e, t) {\n  var n = convertToTensor(e, \"t1\", \"dot\"),\n      r = convertToTensor(t, \"t2\", \"dot\");\n  assert$4(!(1 !== n.rank && 2 !== n.rank || 1 !== r.rank && 2 !== r.rank), () => \"Error in dot: inputs must all be rank 1 or 2, but got ranks \".concat(n.rank, \" and \").concat(r.rank, \".\"));\n  var a = 1 === n.rank ? n.size : n.shape[1],\n      s = 1 === r.rank ? r.size : r.shape[0];\n\n  if (assert$4(a === s, () => \"Error in dot: inner dimensions of inputs must match, but got \".concat(a, \" and \").concat(s, \".\")), 1 === n.rank && 1 === r.rank) {\n    var _e571 = reshape$3(n, [1, -1]),\n        _t406 = reshape$3(r, [-1, 1]),\n        _a127 = matMul$1(_e571, _t406);\n\n    return reshape$3(_a127, []);\n  }\n\n  if (1 === n.rank && 2 === r.rank) {\n    var _e572 = reshape$3(n, [1, -1]),\n        _t407 = reshape$3(r, [r.shape[0], r.shape[1]]),\n        _a128 = matMul$1(_e572, _t407);\n\n    return reshape$3(_a128, [_a128.size]);\n  }\n\n  if (2 === n.rank && 1 === r.rank) {\n    var _e573 = reshape$3(r, [-1, 1]),\n        _t408 = matMul$1(n, _e573);\n\n    return reshape$3(_t408, [_t408.size]);\n  }\n\n  {\n    var _e574 = reshape$3(r, [r.shape[0], r.shape[1]]);\n\n    return matMul$1(n, _e574);\n  }\n}\n\nvar dot$2 = op({\n  dot_\n});\n\nfunction einsum_(e) {\n  for (var _len9 = arguments.length, t = new Array(_len9 > 1 ? _len9 - 1 : 0), _key9 = 1; _key9 < _len9; _key9++) {\n    t[_key9 - 1] = arguments[_key9];\n  }\n\n  var n = t.map((e, t) => convertToTensor(e, \"tensors\".concat(t), \"einsum\"));\n  return ENGINE.runKernel(Einsum, n, {\n    equation: e\n  });\n}\n\nvar einsum$2 = op({\n  einsum_\n});\n\nfunction elu_(e) {\n  var t = convertToTensor(e, \"x\", \"elu\");\n  return ENGINE.runKernel(Elu$1, {\n    x: t\n  });\n}\n\nvar elu$4 = op({\n  elu_\n});\n\nfunction erf_(e) {\n  var t = convertToTensor(e, \"x\", \"erf\");\n  return assert$4(\"int32\" === t.dtype || \"float32\" === t.dtype, () => \"Input dtype must be `int32` or `float32`.\"), \"int32\" === t.dtype && (t = cast$3(t, \"float32\")), ENGINE.runKernel(Erf, {\n    x: t\n  });\n}\n\nvar erf$2 = op({\n  erf_\n});\n\nfunction exp_(e) {\n  var t = convertToTensor(e, \"x\", \"exp\");\n  return ENGINE.runKernel(Exp, {\n    x: t\n  });\n}\n\nvar exp$2 = op({\n  exp_\n});\n\nfunction expandDims_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor(e, \"x\", \"expandDims\", \"string_or_numeric\");\n  return assert$4(t <= n.rank, () => \"Axis must be <= rank of the tensor\"), ENGINE.runKernel(ExpandDims, {\n    input: n\n  }, {\n    dim: t\n  });\n}\n\nvar expandDims$3 = op({\n  expandDims_\n});\n\nfunction expm1_(e) {\n  var t = convertToTensor(e, \"x\", \"expm1\");\n  return ENGINE.runKernel(Expm1, {\n    x: t\n  });\n}\n\nvar expm1$2 = op({\n  expm1_\n});\n\nfunction tile_(e, t) {\n  var n = convertToTensor(e, \"x\", \"tile\", \"string_or_numeric\");\n  return assert$4(n.rank === t.length, () => \"Error in transpose: rank of input \".concat(n.rank, \" must match length of reps \").concat(t, \".\")), ENGINE.runKernel(Tile, {\n    x: n\n  }, {\n    reps: t\n  });\n}\n\nvar tile$3 = op({\n  tile_\n});\n\nfunction eye_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"float32\";\n  null == t && (t = e);\n  var a = buffer([e, t], r),\n      s = e <= t ? e : t;\n\n  for (var _e575 = 0; _e575 < s; ++_e575) {\n    a.set(1, _e575, _e575);\n  }\n\n  var o = reshape$3(a.toTensor(), [e, t]);\n  if (null == n) return o;\n  if (1 === n.length) return tile$3(expandDims$3(o, 0), [n[0], 1, 1]);\n  if (2 === n.length) return tile$3(expandDims$3(expandDims$3(o, 0), 0), [n[0], n[1], 1, 1]);\n  if (3 === n.length) return tile$3(expandDims$3(expandDims$3(expandDims$3(o, 0), 0), 0), [n[0], n[1], n[2], 1, 1]);\n  throw new Error(\"eye() currently supports only 1D and 2D batchShapes, but received \".concat(n.length, \"D.\"));\n}\n\nvar eye = op({\n  eye_\n});\n\nfunction fill$2(e, t, n) {\n  return ENGINE.runKernel(Fill, {}, {\n    shape: e,\n    value: t,\n    dtype: n\n  });\n}\n\nfunction floor_(e) {\n  var t = convertToTensor(e, \"x\", \"floor\");\n  return ENGINE.runKernel(Floor, {\n    x: t\n  });\n}\n\nvar floor$2 = op({\n  floor_\n});\n\nfunction gather_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var a = convertToTensor(e, \"x\", \"gather\"),\n      s = convertToTensor(t, \"indices\", \"gather\", \"int32\");\n  return ENGINE.runKernel(GatherV2, {\n    x: a,\n    indices: s\n  }, {\n    axis: n,\n    batchDims: r\n  });\n}\n\nvar gather$1 = op({\n  gather_\n});\n\nfunction greater_(e, t) {\n  var n = convertToTensor(e, \"a\", \"greater\", \"string_or_numeric\"),\n      r = convertToTensor(t, \"b\", \"greater\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch(n, r), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(Greater, {\n    a: n,\n    b: r\n  });\n}\n\nvar greater$3 = op({\n  greater_\n});\n\nfunction greaterEqual_(e, t) {\n  var n = convertToTensor(e, \"a\", \"greaterEqual\", \"string_or_numeric\"),\n      r = convertToTensor(t, \"b\", \"greaterEqual\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch(n, r), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(GreaterEqual, {\n    a: n,\n    b: r\n  });\n}\n\nvar greaterEqual$2 = op({\n  greaterEqual_\n});\n\nfunction imag_(e) {\n  var t = convertToTensor(e, \"input\", \"imag\");\n  return ENGINE.runKernel(Imag, {\n    input: t\n  });\n}\n\nvar imag$2 = op({\n  imag_\n});\n\nfunction isFinite_(e) {\n  var t = convertToTensor(e, \"x\", \"isFinite\");\n  return ENGINE.runKernel(IsFinite, {\n    x: t\n  });\n}\n\nvar isFinite$3 = op({\n  isFinite_\n});\n\nfunction isInf_(e) {\n  var t = convertToTensor(e, \"x\", \"isInf\");\n  return ENGINE.runKernel(IsInf, {\n    x: t\n  });\n}\n\nvar isInf$2 = op({\n  isInf_\n});\n\nfunction isNaN_(e) {\n  var t = convertToTensor(e, \"x\", \"isNaN\");\n  return ENGINE.runKernel(IsNan, {\n    x: t\n  });\n}\n\nvar isNaN$3 = op({\n  isNaN_\n});\n\nfunction leakyRelu_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .2;\n  var n = convertToTensor(e, \"x\", \"leakyRelu\");\n  return ENGINE.runKernel(LeakyRelu, {\n    x: n\n  }, {\n    alpha: t\n  });\n}\n\nvar leakyRelu$2 = op({\n  leakyRelu_\n});\n\nfunction less_(e, t) {\n  var n = convertToTensor(e, \"a\", \"less\", \"string_or_numeric\"),\n      r = convertToTensor(t, \"b\", \"less\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch(n, r), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(Less, {\n    a: n,\n    b: r\n  });\n}\n\nvar less$3 = op({\n  less_\n});\n\nfunction lessEqual_(e, t) {\n  var n = convertToTensor(e, \"a\", \"lessEqual\", \"string_or_numeric\"),\n      r = convertToTensor(t, \"b\", \"lessEqual\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch(n, r), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(LessEqual, {\n    a: n,\n    b: r\n  });\n}\n\nvar lessEqual$2 = op({\n  lessEqual_\n});\n\nfunction linspace(e, t, n) {\n  if (n <= 0) throw new Error(\"The number of values should be positive.\");\n  return ENGINE.runKernel(LinSpace, {}, {\n    start: e,\n    stop: t,\n    num: n\n  });\n}\n\nfunction localResponseNormalization_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 5;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .5;\n  var s = convertToTensor(e, \"x\", \"localResponseNormalization\");\n  assert$4(4 === s.rank || 3 === s.rank, () => \"Error in localResponseNormalization: x must be rank 3 or 4 but got\\n               rank \".concat(s.rank, \".\")), assert$4(isInt(t), () => \"Error in localResponseNormalization: depthRadius must be an integer but got depthRadius \".concat(t, \".\"));\n  var o = s,\n      i = !1;\n  3 === s.rank && (i = !0, o = reshape$3(s, [1, s.shape[0], s.shape[1], s.shape[2]]));\n  var l = ENGINE.runKernel(LRN, {\n    x: o\n  }, {\n    depthRadius: t,\n    bias: n,\n    alpha: r,\n    beta: a\n  });\n  return i ? reshape$3(l, [l.shape[1], l.shape[2], l.shape[3]]) : l;\n}\n\nvar localResponseNormalization = op({\n  localResponseNormalization_\n});\n\nfunction log_(e) {\n  var t = convertToTensor(e, \"x\", \"log\");\n  return ENGINE.runKernel(Log, {\n    x: t\n  });\n}\n\nvar log$3 = op({\n  log_\n});\n\nfunction log1p_(e) {\n  var t = convertToTensor(e, \"x\", \"log1p\");\n  return ENGINE.runKernel(Log1p, {\n    x: t\n  });\n}\n\nvar log1p$2 = op({\n  log1p_\n});\n\nfunction grad(e) {\n  return assert$4(isFunction(e), () => \"The f passed in grad(f) must be a function\"), (t, n) => {\n    var r = convertToTensor(t, \"x\", \"tf.grad\", \"string_or_numeric\"),\n        a = null != n ? convertToTensor(n, \"dy\", \"tf.grad\") : null;\n    return ENGINE.tidy(() => {\n      var {\n        value: t,\n        grads: n\n      } = ENGINE.gradients(() => e(r), [r], a);\n      return null != a && assertShapesMatch(t.shape, a.shape, \"The shape of dy passed in grad(f)(x, dy) must match the shape returned by f(x)\"), checkGrads(n), n[0];\n    });\n  };\n}\n\nfunction grads(e) {\n  return assert$4(isFunction(e), () => \"The f passed in grads(f) must be a function\"), (t, n) => {\n    assert$4(Array.isArray(t), () => \"The args passed in grads(f)(args) must be an array of `Tensor`s or `TensorLike`s\");\n    var r = convertToTensorArray(t, \"args\", \"tf.grads\", \"string_or_numeric\"),\n        a = null != n ? convertToTensor(n, \"dy\", \"tf.grads\") : null;\n    return ENGINE.tidy(() => {\n      var {\n        value: t,\n        grads: n\n      } = ENGINE.gradients(() => e(...r), r, a);\n      return null != a && assertShapesMatch(t.shape, a.shape, \"The shape of dy passed in grads(f)([x1,...], dy) must match the shape returned by f([x1,...])\"), checkGrads(n), n;\n    });\n  };\n}\n\nfunction valueAndGrad(e) {\n  return assert$4(isFunction(e), () => \"The f passed in valueAndGrad(f) must be a function\"), (t, n) => {\n    assert$4(t instanceof Tensor, () => \"The x passed in valueAndGrad(f)(x) must be a tensor\"), assert$4(null == n || n instanceof Tensor, () => \"The dy passed in valueAndGrad(f)(x, dy) must be a tensor\");\n    var {\n      grads: r,\n      value: a\n    } = ENGINE.gradients(() => e(t), [t], n);\n    return checkGrads(r), {\n      grad: r[0],\n      value: a\n    };\n  };\n}\n\nfunction valueAndGrads(e) {\n  return assert$4(isFunction(e), () => \"The f passed in valueAndGrads(f) must be a function\"), (t, n) => {\n    assert$4(Array.isArray(t) && t.every(e => e instanceof Tensor), () => \"The args passed in valueAndGrads(f)(args) must be array of tensors\"), assert$4(null == n || n instanceof Tensor, () => \"The dy passed in valueAndGrads(f)(args, dy) must be a tensor\");\n    var r = ENGINE.gradients(() => e(...t), t, n);\n    return null != n && assertShapesMatch(r.value.shape, n.shape, \"The shape of dy passed in valueAndGrads(f)([x1,...], dy) must match the shape returned by f([x1,...])\"), checkGrads(r.grads), r;\n  };\n}\n\nfunction variableGrads(e, t) {\n  assert$4(isFunction(e), () => \"The f passed in variableGrads(f) must be a function\"), assert$4(null == t || Array.isArray(t) && t.every(e => e instanceof Variable), () => \"The varList passed in variableGrads(f, varList) must be an array of variables\");\n  var n = null != t;\n\n  if (!n) {\n    t = [];\n\n    for (var _e576 in ENGINE.registeredVariables) {\n      t.push(ENGINE.registeredVariables[_e576]);\n    }\n  }\n\n  var r = n ? t.filter(e => !e.trainable) : null,\n      a = t.length;\n  assert$4((t = t.filter(e => e.trainable)).length > 0, () => \"variableGrads() expects at least one of the input variables to be trainable, but none of the \".concat(a, \" variables is trainable.\"));\n  var {\n    value: s,\n    grads: o\n  } = ENGINE.gradients(e, t, null, !0);\n  assert$4(o.some(e => null != e), () => \"Cannot find a connection between any variable and the result of the loss function y=f(x). Please make sure the operations that use variables are inside the function f passed to minimize().\"), assert$4(0 === s.rank, () => \"The f passed in variableGrads(f) must return a scalar, but it returned a rank-\".concat(s.rank, \" tensor\"));\n  var i = {};\n  return t.forEach((e, t) => {\n    null != o[t] && (i[e.name] = o[t]);\n  }), null != r && r.forEach(e => i[e.name] = null), {\n    value: s,\n    grads: i\n  };\n}\n\nfunction customGrad(e) {\n  return ENGINE.customGrad(e);\n}\n\nfunction checkGrads(e) {\n  if (e.filter(e => null == e).length > 0) throw new Error(\"Cannot compute gradient of y=f(x) with respect to x. Make sure that\\n    the f you passed encloses all operations that lead from x to y.\");\n}\n\nfunction neg_(e) {\n  var t = convertToTensor(e, \"x\", \"neg\");\n  return ENGINE.runKernel(Neg, {\n    x: t\n  });\n}\n\nvar neg$2 = op({\n  neg_\n});\n\nfunction softplus_(e) {\n  var t = convertToTensor(e, \"x\", \"softplus\");\n  return ENGINE.runKernel(Softplus$1, {\n    x: t\n  });\n}\n\nvar softplus$2 = op({\n  softplus_\n});\n\nfunction logSigmoid_(e) {\n  var t = convertToTensor(e, \"x\", \"logSigmoid\");\n  return customGrad(e => ({\n    value: neg$2(softplus$2(neg$2(e))),\n    gradFunc: t => mul(t, sigmoid$2(neg$2(e)))\n  }))(t);\n}\n\nvar logSigmoid = op({\n  logSigmoid_\n});\n\nfunction max_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"max\");\n  return ENGINE.runKernel(Max, {\n    x: r\n  }, {\n    reductionIndices: t,\n    keepDims: n\n  });\n}\n\nvar max$3 = op({\n  max_\n});\n\nfunction sub_(e, t) {\n  var n = convertToTensor(e, \"a\", \"sub\"),\n      r = convertToTensor(t, \"b\", \"sub\");\n  return [n, r] = makeTypesMatch(n, r), ENGINE.runKernel(Sub, {\n    a: n,\n    b: r\n  });\n}\n\nvar sub$2 = op({\n  sub_\n});\n\nfunction sum_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"sum\");\n  return \"bool\" === r.dtype && (r = cast$3(r, \"int32\")), ENGINE.runKernel(Sum, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar sum$2 = op({\n  sum_\n});\n\nfunction logSoftmax_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n = convertToTensor(e, \"logits\", \"logSoftmax\");\n  if (-1 === t && (t = n.rank - 1), t !== n.rank - 1) throw Error(\"Log Softmax along a non-last dimension is not yet supported. Logits was rank \".concat(n.rank, \" and axis was \").concat(t));\n  return customGrad((e, n) => {\n    var r = max$3(e, t, !0),\n        a = sub$2(e, r),\n        s = sub$2(cast$3(a, \"float32\"), log$3(sum$2(exp$2(a), t, !0)));\n    return n([s]), {\n      value: s,\n      gradFunc: (e, n) => {\n        var [r] = n,\n            a = exp$2(r);\n        return sub$2(e, mul(sum$2(e, t, !0), a));\n      }\n    };\n  })(n);\n}\n\nvar logSoftmax = op({\n  logSoftmax_\n});\n\nfunction axesAreInnerMostDims(e, t) {\n  for (var n = 0; n < e.length; ++n) {\n    if (e[e.length - n - 1] !== t - 1 - n) return !1;\n  }\n\n  return !0;\n}\n\nfunction combineLocations(e, t, n) {\n  var r = e.length + t.length,\n      a = [];\n  var s = 0,\n      o = 0;\n\n  for (var i = 0; i < r; i++) {\n    -1 === n.indexOf(i) ? a.push(e[s++]) : a.push(t[o++]);\n  }\n\n  return a;\n}\n\nfunction computeOutAndReduceShapes(e, t) {\n  var n = [],\n      r = e.length;\n\n  for (var a = 0; a < r; a++) {\n    -1 === t.indexOf(a) && n.push(e[a]);\n  }\n\n  return [n, t.map(t => e[t])];\n}\n\nfunction expandShapeToKeepDim(e, t) {\n  return combineLocations(e, t.map(e => 1), t);\n}\n\nfunction assertAxesAreInnerMostDims(e, t, n) {\n  assert$4(axesAreInnerMostDims(t, n), () => \"\".concat(e, \" supports only inner-most axes for now. Got axes \").concat(t, \" and rank-\").concat(n, \" input.\"));\n}\n\nfunction getAxesPermutation(e, t) {\n  if (axesAreInnerMostDims(e, t)) return null;\n  var n = [];\n\n  for (var r = 0; r < t; ++r) {\n    -1 === e.indexOf(r) && n.push(r);\n  }\n\n  return e.forEach(e => n.push(e)), n;\n}\n\nfunction getUndoAxesPermutation(e) {\n  return e.map((e, t) => [t, e]).sort((e, t) => e[1] - t[1]).map(e => e[0]);\n}\n\nfunction getInnerMostAxes(e, t) {\n  var n = [];\n\n  for (var r = t - e; r < t; ++r) {\n    n.push(r);\n  }\n\n  return n;\n}\n\nfunction logSumExp_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"logSumExp\"),\n      a = parseAxisParam(t, r.shape),\n      s = max$3(r, a, !0),\n      o = sub$2(r, s),\n      i = exp$2(o),\n      l = sum$2(i, a),\n      u = log$3(l),\n      c = add$2(reshape$3(s, u.shape), u);\n\n  if (n) {\n    var _e577 = expandShapeToKeepDim(c.shape, a);\n\n    return reshape$3(c, _e577);\n  }\n\n  return c;\n}\n\nvar logSumExp = op({\n  logSumExp_\n});\n\nfunction logicalAnd_(e, t) {\n  var n = convertToTensor(e, \"a\", \"logicalAnd\", \"bool\"),\n      r = convertToTensor(t, \"b\", \"logicalAnd\", \"bool\");\n  return assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(LogicalAnd, {\n    a: n,\n    b: r\n  });\n}\n\nvar logicalAnd$2 = op({\n  logicalAnd_\n});\n\nfunction logicalNot_(e) {\n  var t = convertToTensor(e, \"x\", \"logicalNot\", \"bool\");\n  return ENGINE.runKernel(LogicalNot, {\n    x: t\n  });\n}\n\nvar logicalNot$2 = op({\n  logicalNot_\n});\n\nfunction logicalOr_(e, t) {\n  var n = convertToTensor(e, \"a\", \"logicalOr\", \"bool\"),\n      r = convertToTensor(t, \"b\", \"logicalOr\", \"bool\");\n  return assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(LogicalOr, {\n    a: n,\n    b: r\n  });\n}\n\nvar logicalOr$2 = op({\n  logicalOr_\n});\n\nfunction logicalXor_(e, t) {\n  var n = convertToTensor(e, \"a\", \"logicalXor\", \"bool\"),\n      r = convertToTensor(t, \"b\", \"logicalXor\", \"bool\");\n  return assertAndGetBroadcastShape(n.shape, r.shape), logicalAnd$2(logicalOr$2(e, t), logicalNot$2(logicalAnd$2(e, t)));\n}\n\nvar logicalXor = op({\n  logicalXor_\n});\n\nfunction maxPool_(e, t, n, r, a) {\n  var s = convertToTensor(e, \"x\", \"maxPool\");\n  var o = s,\n      i = !1;\n  3 === s.rank && (i = !0, o = reshape$3(s, [1, s.shape[0], s.shape[1], s.shape[2]])), assert$4(4 === o.rank, () => \"Error in maxPool: input must be rank 4 but got rank \".concat(o.rank, \".\")), assert$4(eitherStridesOrDilationsAreOne(n, 1), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '1'\")), null != a && assert$4(isInt(r), () => \"Error in maxPool: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var l = ENGINE.runKernel(MaxPool, {\n    x: o\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a\n  });\n  return i ? reshape$3(l, [l.shape[1], l.shape[2], l.shape[3]]) : l;\n}\n\nvar maxPool$2 = op({\n  maxPool_\n});\n\nfunction maxPool3d_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : [1, 1, 1];\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NDHWC\";\n  var o = convertToTensor(e, \"x\", \"maxPool3d\");\n  var i = o,\n      l = !1;\n  4 === o.rank && (l = !0, i = reshape$3(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]])), assert$4(5 === i.rank, () => \"Error in maxPool3d: x must be rank 5 but got rank \".concat(i.rank, \".\")), assert$4(\"NDHWC\" === s, () => \"Error in maxPool3d: Only NDHWC is currently supported, but got dataFormat of \".concat(s)), null != a && assert$4(isInt(r), () => \"Error in maxPool3d: pad must be an integer when using, dimRoundingMode \".concat(a, \" but got pad \").concat(r, \".\"));\n  var u = ENGINE.runKernel(MaxPool3D, {\n    x: i\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    dimRoundingMode: a,\n    dataFormat: s\n  });\n  return l ? reshape$3(u, [u.shape[1], u.shape[2], u.shape[3], u.shape[4]]) : u;\n}\n\nvar maxPool3d$1 = op({\n  maxPool3d_\n});\n\nfunction maxPoolWithArgmax_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n  var s = convertToTensor(e, \"x\", \"maxPoolWithArgmax\"),\n      o = ENGINE.runKernel(MaxPoolWithArgmax, {\n    x: s\n  }, {\n    filterSize: t,\n    strides: n,\n    pad: r,\n    includeBatchInIndex: a\n  });\n  return {\n    result: o[0],\n    indexes: o[1]\n  };\n}\n\nvar maxPoolWithArgmax = op({\n  maxPoolWithArgmax_\n});\n\nfunction maximum_(e, t) {\n  var n = convertToTensor(e, \"a\", \"maximum\"),\n      r = convertToTensor(t, \"b\", \"maximum\");\n  return [n, r] = makeTypesMatch(n, r), \"bool\" === n.dtype && (n = cast$3(n, \"int32\"), r = cast$3(r, \"int32\")), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(Maximum$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar maximum$3 = op({\n  maximum_\n});\n\nfunction mean_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"mean\");\n  return ENGINE.runKernel(Mean, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar mean$1 = op({\n  mean_\n});\n\nfunction zeros$2(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n\n  if (\"complex64\" === t) {\n    var _t409 = zeros$2(e, \"float32\"),\n        _n231 = zeros$2(e, \"float32\");\n\n    return complex$2(_t409, _n231);\n  }\n\n  var n = makeZerosTypedArray(sizeFromShape(e), t);\n  return ENGINE.makeTensor(n, e, t);\n}\n\nfunction ones$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n\n  if (\"complex64\" === t) {\n    var _t410 = ones$1(e, \"float32\"),\n        _n232 = zeros$2(e, \"float32\");\n\n    return complex$2(_t410, _n232);\n  }\n\n  var n = makeOnesTypedArray(sizeFromShape(e), t);\n  return ENGINE.makeTensor(n, e, t);\n}\n\nfunction meshgrid(e, t) {\n  var {\n    indexing: n = \"xy\"\n  } = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n  if (\"xy\" !== n && \"ij\" !== n) throw new TypeError(\"\".concat(n, \" is not a valid third argument to meshgrid\"));\n  if (void 0 === e) return [];\n  var r = convertToTensor(e, \"x\", \"meshgrid\", e instanceof Tensor ? e.dtype : \"float32\");\n  if (void 0 === t) return [r];\n  var a = convertToTensor(t, \"y\", \"meshgrid\", t instanceof Tensor ? t.dtype : \"float32\");\n  var s = sizeFromShape(r.shape),\n      o = sizeFromShape(a.shape);\n  return \"xy\" === n ? (r = reshape$3(r, [1, -1]), a = reshape$3(a, [-1, 1]), [matMul$1(ones$1([o, 1], r.dtype), r), matMul$1(a, ones$1([1, s], a.dtype))]) : (r = reshape$3(r, [-1, 1]), a = reshape$3(a, [1, -1]), [matMul$1(r, ones$1([1, o], r.dtype)), matMul$1(ones$1([s, 1], a.dtype), a)]);\n}\n\nfunction min_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"min\");\n  return ENGINE.runKernel(Min, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar min$3 = op({\n  min_\n});\n\nfunction minimum_(e, t) {\n  var n = convertToTensor(e, \"a\", \"minimum\"),\n      r = convertToTensor(t, \"b\", \"minimum\");\n  return [n, r] = makeTypesMatch(n, r), \"bool\" === n.dtype && (n = cast$3(n, \"int32\"), r = cast$3(r, \"int32\")), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(Minimum$1, {\n    a: n,\n    b: r\n  });\n}\n\nvar minimum$3 = op({\n  minimum_\n});\n\nfunction mirrorPad_(e, t, n) {\n  assert$4(\"reflect\" === n || \"symmetric\" === n, () => \"Invalid mode. Mode must be either reflect or symmetric. Got \".concat(n, \".\"));\n  var r = convertToTensor(e, \"x\", \"mirrorPad\");\n  if (0 === r.rank) throw new Error(\"mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad\");\n  assert$4(t.length === r.rank, () => \"Padding doesn't match input. Must be \".concat(r.rank, \". Got \").concat(t.length, \".\"));\n  var a = \"reflect\" === n ? 1 : 0;\n\n  var _loop32 = function _loop32(_e578) {\n    assert$4(2 === t[_e578].length, () => \"Invalid number of paddings. Must be length of 2 each.\"), assert$4(t[_e578][0] >= 0 && t[_e578][0] <= r.shape[_e578] - a && t[_e578][1] >= 0 && t[_e578][1] <= r.shape[_e578] - a, () => \"Padding in dimension \".concat(_e578, \" cannot be greater than or equal to \").concat(r.shape[_e578] - a, \" or less than 0 for input of shape \").concat(r.shape));\n  };\n\n  for (var _e578 = 0; _e578 < r.rank; _e578++) {\n    _loop32(_e578);\n  }\n\n  return ENGINE.runKernel(MirrorPad, {\n    x: r\n  }, {\n    paddings: t,\n    mode: n\n  });\n}\n\nvar mirrorPad$1 = op({\n  mirrorPad_\n});\n\nfunction mod_(e, t) {\n  var n = convertToTensor(e, \"a\", \"mod\"),\n      r = convertToTensor(t, \"b\", \"mod\");\n  return [n, r] = makeTypesMatch(n, r), ENGINE.runKernel(Mod, {\n    a: n,\n    b: r\n  });\n}\n\nvar mod$2 = op({\n  mod_\n});\n\nfunction square_(e) {\n  var t = convertToTensor(e, \"x\", \"square\");\n  return ENGINE.runKernel(\"Square\", {\n    x: t\n  }, {});\n}\n\nvar square$2 = op({\n  square_\n});\n\nfunction moments_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = parseAxisParam(t, (e = convertToTensor(e, \"x\", \"moments\")).shape),\n      a = mean$1(e, r, n);\n  var s = a.shape;\n  n || (s = expandShapeToKeepDim(a.shape, r));\n  var o = square$2(sub$2(cast$3(e, \"float32\"), reshape$3(a, s)));\n  return {\n    mean: a,\n    variance: mean$1(o, r, n)\n  };\n}\n\nvar moments = op({\n  moments_\n});\n\nfunction multiRNNCell_(e, t, n, r) {\n  var a = convertToTensor(t, \"data\", \"multiRNNCell\"),\n      s = convertToTensorArray(n, \"c\", \"multiRNNCell\"),\n      o = convertToTensorArray(r, \"h\", \"multiRNNCell\");\n  var i = a;\n  var l = [];\n\n  for (var _t411 = 0; _t411 < e.length; _t411++) {\n    var _n233 = e[_t411](i, s[_t411], o[_t411]);\n\n    l.push(_n233[0]), l.push(_n233[1]), i = _n233[1];\n  }\n\n  var u = [],\n      c = [];\n\n  for (var _e579 = 0; _e579 < l.length; _e579 += 2) {\n    u.push(l[_e579]), c.push(l[_e579 + 1]);\n  }\n\n  return [u, c];\n}\n\nvar multiRNNCell = op({\n  multiRNNCell_\n});\n\nfunction multinomial_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor(e, \"logits\", \"multinomial\"),\n      s = a.size,\n      o = a.rank;\n  if (s < 2) throw new Error(\"Error in multinomial: you need at least 2 outcomes, but got \".concat(s, \".\"));\n  if (o > 2) throw new Error(\"Rank of probabilities must be 1 or 2, but is \".concat(o));\n  n = n || Math.random();\n  var i = 1 === o ? reshape$3(a, [1, -1]) : a,\n      l = ENGINE.runKernel(Multinomial, {\n    logits: i\n  }, {\n    numSamples: t,\n    seed: n,\n    normalized: r\n  });\n  return 1 === o ? reshape$3(l, [l.size]) : l;\n}\n\nvar multinomial$2 = op({\n  multinomial_\n});\n\nfunction notEqual_(e, t) {\n  var n = convertToTensor(e, \"a\", \"notEqual\", \"string_or_numeric\"),\n      r = convertToTensor(t, \"b\", \"notEqual\", \"string_or_numeric\");\n  return [n, r] = makeTypesMatch(n, r), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(NotEqual, {\n    a: n,\n    b: r\n  });\n}\n\nvar notEqual$2 = op({\n  notEqual_\n});\n\nfunction onesLike_(e) {\n  var t = convertToTensor(e, \"x\", \"onesLike\");\n  return ENGINE.runKernel(OnesLike, {\n    x: t\n  });\n}\n\nvar onesLike$2 = op({\n  onesLike_\n});\n\nfunction outerProduct_(e, t) {\n  var n = convertToTensor(e, \"v1\", \"outerProduct\"),\n      r = convertToTensor(t, \"v2\", \"outerProduct\");\n  assert$4(1 === n.rank && 1 === r.rank, () => \"Error in outerProduct: inputs must be rank 1, but got ranks \".concat(n.rank, \" and \").concat(r.rank, \".\"));\n  var a = reshape$3(n, [-1, 1]),\n      s = reshape$3(r, [1, -1]);\n  return matMul$1(a, s);\n}\n\nvar outerProduct = op({\n  outerProduct_\n});\n\nfunction pad_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = convertToTensor(e, \"x\", \"pad\");\n  if (0 === r.rank) throw new Error(\"pad(scalar) is not defined. Pass non-scalar to pad\");\n  return ENGINE.runKernel(PadV2, {\n    x: r\n  }, {\n    paddings: t,\n    constantValue: n\n  });\n}\n\nvar pad = op({\n  pad_\n});\n\nfunction pad1d_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  return assert$4(2 === t.length, () => \"Invalid number of paddings. Must be length of 2.\"), pad(e, [t], n);\n}\n\nvar pad1d = op({\n  pad1d_\n});\n\nfunction pad2d_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  return assert$4(2 === t.length && 2 === t[0].length && 2 === t[1].length, () => \"Invalid number of paddings. Must be length of 2 each.\"), pad(e, t, n);\n}\n\nvar pad2d = op({\n  pad2d_\n});\n\nfunction pad3d_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  return assert$4(3 === t.length && 2 === t[0].length && 2 === t[1].length && 2 === t[2].length, () => \"Invalid number of paddings. Must be length of 2 each.\"), pad(e, t, n);\n}\n\nvar pad3d = op({\n  pad3d_\n});\n\nfunction pad4d_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  return assert$4(4 === t.length && 2 === t[0].length && 2 === t[1].length && 2 === t[2].length && 2 === t[3].length, () => \"Invalid number of paddings. Must be length of 2 each.\"), pad(e, t, n);\n}\n\nvar pad4d = op({\n  pad4d_\n});\n\nfunction spaceToBatchND_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"spaceToBatchND\");\n  return assert$4(r.rank >= 1 + t.length, () => \"input rank \".concat(r.rank, \" should be > than [blockShape] \").concat(t.length)), assert$4(n.length === t.length, () => \"paddings.shape[0] \".concat(n.length, \" must be equal to [blockShape] \").concat(t.length)), assert$4(r.shape.reduce((e, r, a) => a > 0 && a <= t.length ? e && (r + n[a - 1][0] + n[a - 1][1]) % t[a - 1] == 0 : e, !0), () => \"input spatial dimensions \".concat(r.shape.slice(1), \" with paddings \").concat(n.toString(), \" must be divisible by blockShapes \").concat(t.toString())), ENGINE.runKernel(SpaceToBatchND, {\n    x: r\n  }, {\n    blockShape: t,\n    paddings: n\n  });\n}\n\nvar spaceToBatchND$2 = op({\n  spaceToBatchND_\n});\n\nfunction pool_(e, t, n, r, a, s) {\n  null == a && (a = [1, 1]), null == s && (s = 1), 0 === r && (r = \"valid\");\n  var o = convertToTensor(e, \"x\", \"maxPool\");\n  var i = o,\n      l = !1;\n  3 === o.rank && (l = !0, i = reshape$3(o, [1, o.shape[0], o.shape[1], o.shape[2]])), assert$4(eitherStridesOrDilationsAreOne(s, a), () => \"Error in pool: Either strides or dilations must be 1. Got strides \".concat(s, \" and dilations '\").concat(a, \"'\"));\n  var u = computePool2DInfo(i.shape, t, s, a, r),\n      c = [u.dilationHeight, u.dilationWidth];\n  var p;\n  p = \"same\" === r ? withSpaceToBatchBasePaddings([u.filterHeight, u.filterWidth], c) : [[0, 0], [0, 0]];\n  var d = 1 === c[0] && 1 === c[1],\n      [h, m] = requiredSpaceToBatchPaddings([u.inHeight, u.inWidth], c, p),\n      f = d ? r : \"valid\",\n      g = d ? i : spaceToBatchND$2(i, c, h),\n      $ = (\"avg\" === n ? () => avgPool$2(g, t, s, f) : () => maxPool$2(g, t, s, f))(),\n      y = d ? $ : batchToSpaceND$2($, c, m);\n  return l ? reshape$3(y, [y.shape[1], y.shape[2], y.shape[3]]) : y;\n}\n\nfunction requiredSpaceToBatchPaddings(e, t, n) {\n  var r = n.map(e => e[0]),\n      a = n.map(e => e[1]),\n      s = e.concat(r, a),\n      o = t.map((e, t) => (e - s[t] % e) % e),\n      i = a.map((e, t) => e + o[t]);\n  return [t.map((e, t) => [r[t], i[t]]), t.map((e, t) => [0, o[t]])];\n}\n\nfunction withSpaceToBatchBasePaddings(e, t) {\n  var n = e.map((e, n) => e + (e - 1) * (t[n] - 1)).map(e => e - 1),\n      r = n.map(e => Math.floor(e / 2)),\n      a = n.map((e, t) => e - r[t]);\n  return n.map((e, t) => [r[t], a[t]]);\n}\n\nvar pool$1 = op({\n  pool_\n});\n\nfunction pow_(e, t) {\n  var n = convertToTensor(e, \"base\", \"pow\"),\n      r = convertToTensor(t, \"exp\", \"pow\");\n  return [n, r] = makeTypesMatch(n, r), ENGINE.runKernel(Pow, {\n    a: n,\n    b: r\n  });\n}\n\nvar pow$2 = op({\n  pow_\n});\n\nfunction prelu_(e, t) {\n  var n = convertToTensor(e, \"x\", \"prelu\"),\n      r = convertToTensor(t, \"alpha\", \"prelu\");\n  return ENGINE.runKernel(Prelu, {\n    x: n,\n    alpha: r\n  });\n}\n\nvar prelu$3 = op({\n  prelu_\n});\n\nfunction prod_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = convertToTensor(e, \"x\", \"prod\");\n  return \"bool\" === r.dtype && (r = cast$3(r, \"int32\")), ENGINE.runKernel(Prod, {\n    x: r\n  }, {\n    axis: t,\n    keepDims: n\n  });\n}\n\nvar prod$2 = op({\n  prod_\n});\n\nfunction rand_(e, t, n) {\n  var r = sizeFromShape(e);\n  var a = null;\n  if (null == n || \"float32\" === n) a = new Float32Array(r);else if (\"int32\" === n) a = new Int32Array(r);else {\n    if (\"bool\" !== n) throw new Error(\"Unknown data type \".concat(n));\n    a = new Uint8Array(r);\n  }\n\n  for (var _e580 = 0; _e580 < r; _e580++) {\n    a[_e580] = t();\n  }\n\n  return ENGINE.makeTensor(a, e, n);\n}\n\nvar rand = op({\n  rand_\n});\nvar alea = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t,\n          n = this,\n          r = (t = 4022871197, function (e) {\n        e = e.toString();\n\n        for (var n = 0; n < e.length; n++) {\n          var r = .02519603282416938 * (t += e.charCodeAt(n));\n          r -= t = r >>> 0, t = (r *= t) >>> 0, t += 4294967296 * (r -= t);\n        }\n\n        return 2.3283064365386963e-10 * (t >>> 0);\n      });\n      n.next = function () {\n        var e = 2091639 * n.s0 + 2.3283064365386963e-10 * n.c;\n        return n.s0 = n.s1, n.s1 = n.s2, n.s2 = e - (n.c = 0 | e);\n      }, n.c = 1, n.s0 = r(\" \"), n.s1 = r(\" \"), n.s2 = r(\" \"), n.s0 -= r(e), n.s0 < 0 && (n.s0 += 1), n.s1 -= r(e), n.s1 < 0 && (n.s1 += 1), n.s2 -= r(e), n.s2 < 0 && (n.s2 += 1), r = null;\n    }\n\n    function a(e, t) {\n      return t.c = e.c, t.s0 = e.s0, t.s1 = e.s1, t.s2 = e.s2, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = n.next;\n      return o.int32 = function () {\n        return 4294967296 * n.next() | 0;\n      }, o.double = function () {\n        return o() + 11102230246251565e-32 * (2097152 * o() | 0);\n      }, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.alea = s;\n  }(0, e);\n}),\n    xor128 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this,\n          n = \"\";\n      t.x = 0, t.y = 0, t.z = 0, t.w = 0, t.next = function () {\n        var e = t.x ^ t.x << 11;\n        return t.x = t.y, t.y = t.z, t.z = t.w, t.w ^= t.w >>> 19 ^ e ^ e >>> 8;\n      }, e === (0 | e) ? t.x = e : n += e;\n\n      for (var r = 0; r < n.length + 64; r++) {\n        t.x ^= 0 | n.charCodeAt(r), t.next();\n      }\n    }\n\n    function a(e, t) {\n      return t.x = e.x, t.y = e.y, t.z = e.z, t.w = e.w, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xor128 = s;\n  }(0, e);\n}),\n    xorwow = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this,\n          n = \"\";\n      t.next = function () {\n        var e = t.x ^ t.x >>> 2;\n        return t.x = t.y, t.y = t.z, t.z = t.w, t.w = t.v, (t.d = t.d + 362437 | 0) + (t.v = t.v ^ t.v << 4 ^ e ^ e << 1) | 0;\n      }, t.x = 0, t.y = 0, t.z = 0, t.w = 0, t.v = 0, e === (0 | e) ? t.x = e : n += e;\n\n      for (var r = 0; r < n.length + 64; r++) {\n        t.x ^= 0 | n.charCodeAt(r), r == n.length && (t.d = t.x << 10 ^ t.x >>> 4), t.next();\n      }\n    }\n\n    function a(e, t) {\n      return t.x = e.x, t.y = e.y, t.z = e.z, t.w = e.w, t.v = e.v, t.d = e.d, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xorwow = s;\n  }(0, e);\n}),\n    xorshift7 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this;\n      t.next = function () {\n        var e,\n            n,\n            r = t.x,\n            a = t.i;\n        return e = r[a], n = (e ^= e >>> 7) ^ e << 24, n ^= (e = r[a + 1 & 7]) ^ e >>> 10, n ^= (e = r[a + 3 & 7]) ^ e >>> 3, n ^= (e = r[a + 4 & 7]) ^ e << 7, e = r[a + 7 & 7], r[a] = n ^= (e ^= e << 13) ^ e << 9, t.i = a + 1 & 7, n;\n      }, function (e, t) {\n        var n,\n            r = [];\n        if (t === (0 | t)) r[0] = t;else for (t = \"\" + t, n = 0; n < t.length; ++n) {\n          r[7 & n] = r[7 & n] << 15 ^ t.charCodeAt(n) + r[n + 1 & 7] << 13;\n        }\n\n        for (; r.length < 8;) {\n          r.push(0);\n        }\n\n        for (n = 0; n < 8 && 0 === r[n]; ++n) {\n          ;\n        }\n\n        for (8 == n && (r[7] = -1), e.x = r, e.i = 0, n = 256; n > 0; --n) {\n          e.next();\n        }\n      }(t, e);\n    }\n\n    function a(e, t) {\n      return t.x = e.x.slice(), t.i = e.i, t;\n    }\n\n    function s(e, t) {\n      null == e && (e = +new Date());\n\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (s.x && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xorshift7 = s;\n  }(0, e);\n}),\n    xor4096 = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this;\n      t.next = function () {\n        var e,\n            n,\n            r = t.w,\n            a = t.X,\n            s = t.i;\n        return t.w = r = r + 1640531527 | 0, n = a[s + 34 & 127], e = a[s = s + 1 & 127], n ^= n << 13, e ^= e << 17, n = a[s] = (n ^= n >>> 15) ^ (e ^= e >>> 12), t.i = s, n + (r ^ r >>> 16) | 0;\n      }, function (e, t) {\n        var n,\n            r,\n            a,\n            s,\n            o,\n            i = [],\n            l = 128;\n\n        for (t === (0 | t) ? (r = t, t = null) : (t += \"\\0\", r = 0, l = Math.max(l, t.length)), a = 0, s = -32; s < l; ++s) {\n          t && (r ^= t.charCodeAt((s + 32) % t.length)), 0 === s && (o = r), r ^= r << 10, r ^= r >>> 15, r ^= r << 4, r ^= r >>> 13, s >= 0 && (a = 0 == (n = i[127 & s] ^= r + (o = o + 1640531527 | 0)) ? a + 1 : 0);\n        }\n\n        for (a >= 128 && (i[127 & (t && t.length || 0)] = -1), a = 127, s = 512; s > 0; --s) {\n          r = i[a + 34 & 127], n = i[a = a + 1 & 127], r ^= r << 13, n ^= n << 17, i[a] = (r ^= r >>> 15) ^ (n ^= n >>> 12);\n        }\n\n        e.w = o, e.X = i, e.i = a;\n      }(t, e);\n    }\n\n    function a(e, t) {\n      return t.i = e.i, t.w = e.w, t.X = e.X.slice(), t;\n    }\n\n    function s(e, t) {\n      null == e && (e = +new Date());\n\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (s.X && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.xor4096 = s;\n  }(0, e);\n}),\n    tychei = createCommonjsModule(function (e) {\n  !function (e, t, n) {\n    function r(e) {\n      var t = this,\n          n = \"\";\n      t.next = function () {\n        var e = t.b,\n            n = t.c,\n            r = t.d,\n            a = t.a;\n        return e = e << 25 ^ e >>> 7 ^ n, n = n - r | 0, r = r << 24 ^ r >>> 8 ^ a, a = a - e | 0, t.b = e = e << 20 ^ e >>> 12 ^ n, t.c = n = n - r | 0, t.d = r << 16 ^ n >>> 16 ^ a, t.a = a - e | 0;\n      }, t.a = 0, t.b = 0, t.c = -1640531527, t.d = 1367130551, e === Math.floor(e) ? (t.a = e / 4294967296 | 0, t.b = 0 | e) : n += e;\n\n      for (var r = 0; r < n.length + 20; r++) {\n        t.b ^= 0 | n.charCodeAt(r), t.next();\n      }\n    }\n\n    function a(e, t) {\n      return t.a = e.a, t.b = e.b, t.c = e.c, t.d = e.d, t;\n    }\n\n    function s(e, t) {\n      var n = new r(e),\n          s = t && t.state,\n          o = function o() {\n        return (n.next() >>> 0) / 4294967296;\n      };\n\n      return o.double = function () {\n        do {\n          var e = ((n.next() >>> 11) + (n.next() >>> 0) / 4294967296) / (1 << 21);\n        } while (0 === e);\n\n        return e;\n      }, o.int32 = n.next, o.quick = o, s && (\"object\" == typeof s && a(s, n), o.state = function () {\n        return a(n, {});\n      }), o;\n    }\n\n    t && t.exports ? t.exports = s : this.tychei = s;\n  }(0, e);\n}),\n    seedrandom$1 = createCommonjsModule(function (e) {\n  !function (t, n) {\n    var r,\n        a = this,\n        s = 256,\n        o = n.pow(s, 6),\n        i = n.pow(2, 52),\n        l = 2 * i,\n        u = 255;\n\n    function c(e, u, c) {\n      var g = [],\n          $ = m(h((u = 1 == u ? {\n        entropy: !0\n      } : u || {}).entropy ? [e, f(t)] : null == e ? function () {\n        try {\n          var e;\n          return r && (e = r.randomBytes) ? e = e(s) : (e = new Uint8Array(s), (a.crypto || a.msCrypto).getRandomValues(e)), f(e);\n        } catch (e) {\n          var n = a.navigator,\n              o = n && n.plugins;\n          return [+new Date(), a, o, a.screen, f(t)];\n        }\n      }() : e, 3), g),\n          y = new p(g),\n          b = function b() {\n        for (var e = y.g(6), t = o, n = 0; e < i;) {\n          e = (e + n) * s, t *= s, n = y.g(1);\n        }\n\n        for (; e >= l;) {\n          e /= 2, t /= 2, n >>>= 1;\n        }\n\n        return (e + n) / t;\n      };\n\n      return b.int32 = function () {\n        return 0 | y.g(4);\n      }, b.quick = function () {\n        return y.g(4) / 4294967296;\n      }, b.double = b, m(f(y.S), t), (u.pass || c || function (e, t, r, a) {\n        return a && (a.S && d(a, y), e.state = function () {\n          return d(y, {});\n        }), r ? (n.random = e, t) : e;\n      })(b, $, \"global\" in u ? u.global : this == n, u.state);\n    }\n\n    function p(e) {\n      var t,\n          n = e.length,\n          r = this,\n          a = 0,\n          o = r.i = r.j = 0,\n          i = r.S = [];\n\n      for (n || (e = [n++]); a < s;) {\n        i[a] = a++;\n      }\n\n      for (a = 0; a < s; a++) {\n        i[a] = i[o = u & o + e[a % n] + (t = i[a])], i[o] = t;\n      }\n\n      (r.g = function (e) {\n        for (var t, n = 0, a = r.i, o = r.j, i = r.S; e--;) {\n          t = i[a = u & a + 1], n = n * s + i[u & (i[a] = i[o = u & o + t]) + (i[o] = t)];\n        }\n\n        return r.i = a, r.j = o, n;\n      })(s);\n    }\n\n    function d(e, t) {\n      return t.i = e.i, t.j = e.j, t.S = e.S.slice(), t;\n    }\n\n    function h(e, t) {\n      var n,\n          r = [],\n          a = typeof e;\n      if (t && \"object\" == a) for (n in e) {\n        try {\n          r.push(h(e[n], t - 1));\n        } catch (e) {}\n      }\n      return r.length ? r : \"string\" == a ? e : e + \"\\0\";\n    }\n\n    function m(e, t) {\n      for (var n, r = e + \"\", a = 0; a < r.length;) {\n        t[u & a] = u & (n ^= 19 * t[u & a]) + r.charCodeAt(a++);\n      }\n\n      return f(t);\n    }\n\n    function f(e) {\n      return String.fromCharCode.apply(0, e);\n    }\n\n    if (n.seedrandom = c, m(n.random(), t), e.exports) {\n      e.exports = c;\n\n      try {\n        r = _nodeResolve_empty$1;\n      } catch (e) {}\n    }\n  }([], Math);\n});\nseedrandom$1.alea = alea, seedrandom$1.xor128 = xor128, seedrandom$1.xorwow = xorwow, seedrandom$1.xorshift7 = xorshift7, seedrandom$1.xor4096 = xor4096, seedrandom$1.tychei = tychei;\nvar seedrandom = seedrandom$1;\n\nclass MPRandGauss {\n  constructor(e, t, n, r, a) {\n    this.mean = e, this.stdDev = t, this.dtype = n, this.nextVal = NaN, this.truncated = r, this.truncated && (this.upper = this.mean + 2 * this.stdDev, this.lower = this.mean - 2 * this.stdDev);\n    var s = a || Math.random();\n    this.random = seedrandom.alea(s.toString());\n  }\n\n  nextValue() {\n    if (!isNaN(this.nextVal)) {\n      var _e581 = this.nextVal;\n      return this.nextVal = NaN, _e581;\n    }\n\n    var e,\n        t,\n        n = !1;\n\n    for (; !n;) {\n      var r = void 0,\n          a = void 0,\n          s = void 0;\n\n      do {\n        r = 2 * this.random() - 1, a = 2 * this.random() - 1, s = r * r + a * a;\n      } while (s >= 1 || 0 === s);\n\n      var o = Math.sqrt(-2 * Math.log(s) / s);\n      e = this.mean + this.stdDev * r * o, t = this.mean + this.stdDev * a * o, this.truncated && !this.isValidTruncated(e) || (n = !0);\n    }\n\n    return this.truncated && !this.isValidTruncated(t) || (this.nextVal = this.convertValue(t)), this.convertValue(e);\n  }\n\n  convertValue(e) {\n    return null == this.dtype || \"float32\" === this.dtype ? e : Math.round(e);\n  }\n\n  isValidTruncated(e) {\n    return e <= this.upper && e >= this.lower;\n  }\n\n}\n\nclass RandGamma {\n  constructor(e, t, n, r) {\n    this.alpha = e, this.beta = 1 / t, this.dtype = n;\n    var a = r || Math.random();\n    this.randu = seedrandom.alea(a.toString()), this.randn = new MPRandGauss(0, 1, n, !1, this.randu()), this.d = e < 1 ? e + 2 / 3 : e - 1 / 3, this.c = 1 / Math.sqrt(9 * this.d);\n  }\n\n  nextValue() {\n    var e, t, n, r, a, s;\n\n    for (;;) {\n      do {\n        r = this.randn.nextValue(), s = 1 + this.c * r;\n      } while (s <= 0);\n\n      if (s *= s * s, e = r * r, t = 1 - .331 * e * e, n = .5 * e + this.d * (1 - s + Math.log(s)), a = this.randu(), a < t || Math.log(a) < n) break;\n    }\n\n    return s *= 1 / this.beta * this.d, this.alpha < 1 && (s *= Math.pow(this.randu(), 1 / this.alpha)), this.convertValue(s);\n  }\n\n  convertValue(e) {\n    return \"float32\" === this.dtype ? e : Math.round(e);\n  }\n\n}\n\nclass UniformRandom {\n  constructor() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    var r = arguments.length > 3 ? arguments[3] : undefined;\n    if (this.canReturnFloat = () => null == this.dtype || \"float32\" === this.dtype, this.min = e, this.range = t - e, this.dtype = n, null == r && (r = Math.random()), \"number\" == typeof r && (r = r.toString()), !this.canReturnFloat() && this.range <= 1) throw new Error(\"The difference between \".concat(e, \" - \").concat(t, \" <= 1 and dtype is not float\"));\n    this.random = seedrandom.alea(r);\n  }\n\n  convertValue(e) {\n    return this.canReturnFloat() ? e : Math.round(e);\n  }\n\n  nextValue() {\n    return this.convertValue(this.min + this.range * this.random());\n  }\n\n}\n\nfunction randomGamma_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"float32\";\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  if (null == n && (n = 1), null == r && (r = \"float32\"), \"float32\" !== r && \"int32\" !== r) throw new Error(\"Unsupported data type \".concat(r));\n  var s = new RandGamma(t, n, r, a),\n      o = buffer(e, r);\n\n  for (var _e582 = 0; _e582 < o.values.length; _e582++) {\n    o.values[_e582] = s.nextValue();\n  }\n\n  return o.toTensor();\n}\n\nvar randomGamma = op({\n  randomGamma_\n});\n\nfunction randomNormal_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  if (null != r && \"bool\" === r) throw new Error(\"Unsupported data type \".concat(r));\n  var s = new MPRandGauss(t, n, r, !1, a),\n      o = buffer(e, r);\n\n  for (var _e583 = 0; _e583 < o.values.length; _e583++) {\n    o.values[_e583] = s.nextValue();\n  }\n\n  return o.toTensor();\n}\n\nvar randomNormal$2 = op({\n  randomNormal_\n});\n\nfunction randomUniform_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"float32\";\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = buffer(e, r),\n      o = new UniformRandom(t, n, null, a);\n\n  for (var _e584 = 0; _e584 < s.values.length; _e584++) {\n    s.values[_e584] = o.nextValue();\n  }\n\n  return s.toTensor();\n}\n\nvar randomUniform$1 = op({\n  randomUniform_\n});\n\nfunction range$4(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"float32\";\n  if (0 === n) throw new Error(\"Cannot have a step of zero\");\n  return ENGINE.runKernel(Range, {}, {\n    start: e,\n    stop: t,\n    step: n,\n    dtype: r\n  });\n}\n\nfunction real_(e) {\n  var t = convertToTensor(e, \"input\", \"real\");\n  return ENGINE.runKernel(Real, {\n    input: t\n  });\n}\n\nvar real$2 = op({\n  real_\n});\n\nfunction reciprocal_(e) {\n  var t = convertToTensor(e, \"x\", \"reciprocal\");\n  return ENGINE.runKernel(Reciprocal, {\n    x: t\n  });\n}\n\nvar reciprocal$2 = op({\n  reciprocal_\n});\n\nfunction relu_(e) {\n  var t = convertToTensor(e, \"x\", \"relu\");\n  return ENGINE.runKernel(Relu$1, {\n    x: t\n  });\n}\n\nvar relu$3 = op({\n  relu_\n});\n\nfunction relu6_(e) {\n  var t = convertToTensor(e, \"x\", \"relu6\");\n  return ENGINE.runKernel(Relu6$1, {\n    x: t\n  });\n}\n\nvar relu6$2 = op({\n  relu6_\n});\n\nfunction reverse_(e, t) {\n  var n = convertToTensor(e, \"x\", \"reverse\");\n  return ENGINE.runKernel(Reverse, {\n    x: n\n  }, {\n    dims: t\n  });\n}\n\nvar reverse$2 = op({\n  reverse_\n});\n\nfunction reverse1d_(e) {\n  var t = convertToTensor(e, \"x\", \"reverse\");\n  return assert$4(1 === t.rank, () => \"Error in reverse1D: x must be rank 1 but got rank \".concat(t.rank, \".\")), reverse$2(t, 0);\n}\n\nvar reverse1d = op({\n  reverse1d_\n});\n\nfunction reverse2d_(e, t) {\n  var n = convertToTensor(e, \"x\", \"reverse\");\n  return assert$4(2 === n.rank, () => \"Error in reverse2D: x must be rank 2 but got rank \".concat(n.rank, \".\")), reverse$2(n, t);\n}\n\nvar reverse2d = op({\n  reverse2d_\n});\n\nfunction reverse3d_(e, t) {\n  var n = convertToTensor(e, \"x\", \"reverse\");\n  return assert$4(3 === n.rank, () => \"Error in reverse3D: x must be rank 3 but got rank \".concat(n.rank, \".\")), reverse$2(n, t);\n}\n\nvar reverse3d = op({\n  reverse3d_\n});\n\nfunction reverse4d_(e, t) {\n  var n = convertToTensor(e, \"x\", \"reverse\");\n  return assert$4(4 === n.rank, () => \"Error in reverse4D: x must be rank 4 but got rank \".concat(n.rank, \".\")), reverse$2(n, t);\n}\n\nvar reverse4d = op({\n  reverse4d_\n});\n\nfunction round_(e) {\n  var t = convertToTensor(e, \"x\", \"round\");\n  return ENGINE.runKernel(Round, {\n    x: t\n  });\n}\n\nvar round$2 = op({\n  round_\n});\n\nfunction rsqrt_(e) {\n  var t = convertToTensor(e, \"x\", \"rsqrt\");\n  return ENGINE.runKernel(Rsqrt, {\n    x: t\n  });\n}\n\nvar rsqrt$2 = op({\n  rsqrt_\n});\n\nfunction scalar(e, t) {\n  if ((isTypedArray(e) && \"string\" !== t || Array.isArray(e)) && \"complex64\" !== t) throw new Error(\"Error creating a new Scalar: value must be a primitive (number|boolean|string)\");\n  if (\"string\" === t && isTypedArray(e) && !(e instanceof Uint8Array)) throw new Error(\"When making a scalar from encoded string, the value must be `Uint8Array`.\");\n  return makeTensor(e, [], [], t);\n}\n\nfunction selu_(e) {\n  var t = convertToTensor(e, \"x\", \"selu\");\n  return ENGINE.runKernel(Selu$1, {\n    x: t\n  });\n}\n\nvar selu$2 = op({\n  selu_\n});\n\nfunction separableConv2d_(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : \"NHWC\";\n  var i = convertToTensor(e, \"x\", \"separableConv2d\"),\n      l = convertToTensor(t, \"depthwiseFilter\", \"separableConv2d\"),\n      u = convertToTensor(n, \"pointwiseFilter\", \"separableConv2d\");\n  var c = i,\n      p = !1;\n  if (3 === i.rank && (p = !0, c = reshape$3(i, [1, i.shape[0], i.shape[1], i.shape[2]])), \"NCHW\" === o) throw new Error(\"separableConv2d currently does not support dataFormat NCHW; only NHWC is supported\");\n  assert$4(4 === c.rank, () => \"Error in separableConv2d: input must be rank 4, but got rank \".concat(c.rank, \".\")), assert$4(4 === l.rank, () => \"Error in separableConv2d: depthwise filter must be rank 4, but got rank \".concat(l.rank, \".\")), assert$4(4 === u.rank, () => \"Error in separableConv2d: pointwise filter must be rank 4, but got rank \".concat(l.rank, \".\")), assert$4(1 === u.shape[0], () => \"Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got \".concat(u.shape[0], \".\")), assert$4(1 === u.shape[1], () => \"Error in separableConv2d: the second dimension of pointwise filter must be 1, but got \".concat(u.shape[1], \".\"));\n  var d = l.shape[2],\n      h = l.shape[3];\n  assert$4(u.shape[2] === d * h, () => \"Error in separableConv2d: the third dimension of pointwise filter must be \".concat(d * h, \", but got \").concat(u.shape[2], \".\"));\n  var m = depthwiseConv2d$3(c, l, r, a, o, s),\n      f = conv2d$3(m, u, 1, \"valid\", o);\n  return p ? reshape$3(f, [f.shape[1], f.shape[2], f.shape[3]]) : f;\n}\n\nvar separableConv2d$1 = op({\n  separableConv2d_\n});\n\nfunction setdiff1dAsync_(_x69, _x70) {\n  return _setdiff1dAsync_.apply(this, arguments);\n}\n\nfunction _setdiff1dAsync_() {\n  _setdiff1dAsync_ = _asyncToGenerator(function* (e, t) {\n    var n = convertToTensor(e, \"x\", \"setdiff1d\"),\n        r = convertToTensor(t, \"y\", \"setdiff1d\");\n    assert$4(n.dtype === r.dtype, () => \"x and y should have the same dtype, but got x (\".concat(n.dtype, \") and y (\").concat(r.dtype, \").\")), assert$4(1 === n.rank, () => \"x should be 1D tensor, but got x (\".concat(n.shape, \").\")), assert$4(1 === r.rank, () => \"y should be 1D tensor, but got y (\".concat(r.shape, \").\"));\n    var a = yield n.data(),\n        s = yield r.data(),\n        o = new Set(s);\n    var i = 0;\n\n    for (var _e1146 = 0; _e1146 < a.length; _e1146++) {\n      o.has(a[_e1146]) || i++;\n    }\n\n    var l = new TensorBuffer([i], n.dtype),\n        u = new TensorBuffer([i], \"int32\");\n\n    for (var _e1147 = 0, _t774 = 0; _e1147 < a.length; _e1147++) {\n      o.has(a[_e1147]) || (l.values[_t774] = a[_e1147], u.values[_t774] = _e1147, _t774++);\n    }\n\n    return [l.toTensor(), u.toTensor()];\n  });\n  return _setdiff1dAsync_.apply(this, arguments);\n}\n\nvar setdiff1dAsync = setdiff1dAsync_;\n\nfunction sign_(e) {\n  var t = convertToTensor(e, \"x\", \"sign\");\n  return ENGINE.runKernel(Sign, {\n    x: t\n  });\n}\n\nvar sign$2 = op({\n  sign_\n});\n\nfunction sin_(e) {\n  var t = convertToTensor(e, \"x\", \"sin\");\n  return ENGINE.runKernel(Sin, {\n    x: t\n  });\n}\n\nvar sin$2 = op({\n  sin_\n});\n\nfunction sinh_(e) {\n  var t = convertToTensor(e, \"x\", \"sinh\");\n  return ENGINE.runKernel(Sinh, {\n    x: t\n  });\n}\n\nvar sinh$2 = op({\n  sinh_\n});\n\nfunction slice1d_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"slice1d\");\n  return assert$4(1 === r.rank, () => \"slice1d expects a rank-1 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$2(r, [t], [n]);\n}\n\nvar slice1d = op({\n  slice1d_\n});\n\nfunction slice2d_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"slice2d\");\n  return assert$4(2 === r.rank, () => \"slice2d expects a rank-2 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$2(r, t, n);\n}\n\nvar slice2d = op({\n  slice2d_\n});\n\nfunction slice3d_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"slice3d\");\n  return assert$4(3 === r.rank, () => \"slice3d expects a rank-3 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$2(r, t, n);\n}\n\nvar slice3d = op({\n  slice3d_\n});\n\nfunction slice4d_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"slice4d\");\n  return assert$4(4 === r.rank, () => \"slice4d expects a rank-4 tensor, but got a rank-\".concat(r.rank, \" tensor\")), slice$2(r, t, n);\n}\n\nvar slice4d = op({\n  slice4d_\n});\n\nfunction softmax_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n = convertToTensor(e, \"logits\", \"softmax\", \"float32\");\n  if (-1 === t && (t = n.rank - 1), t !== n.rank - 1) throw Error(\"Softmax along a non-last dimension is not yet supported. Logits was rank \".concat(n.rank, \" and dim was \").concat(t));\n  return ENGINE.runKernel(Softmax$2, {\n    logits: n\n  }, {\n    dim: t\n  });\n}\n\nvar softmax$3 = op({\n  softmax_\n});\n\nfunction fft_(e) {\n  return assert$4(\"complex64\" === e.dtype, () => \"The dtype for tf.spectral.fft() must be complex64 but got \".concat(e.dtype, \".\")), ENGINE.runKernel(FFT, {\n    input: e\n  });\n}\n\nvar fft$2 = op({\n  fft_\n});\n\nfunction ifft_(e) {\n  return assert$4(\"complex64\" === e.dtype, () => \"The dtype for tf.spectral.ifft() must be complex64 but got \".concat(e.dtype, \".\")), ENGINE.runKernel(IFFT, {\n    input: e\n  });\n}\n\nvar ifft$2 = op({\n  ifft_\n});\n\nfunction irfft_(e) {\n  var t = e.shape[e.shape.length - 1],\n      n = e.size / t;\n  var r;\n\n  if (t <= 2) {\n    var a = reshape$3(e, [n, t]);\n    r = ifft$2(a);\n  } else {\n    var _a129 = [n, 2 * (t - 1)],\n        s = reshape$3(real$2(e), [n, t]),\n        o = reshape$3(imag$2(e), [n, t]),\n        i = reverse$2(slice$2(s, [0, 1], [n, t - 2]), 1),\n        l = mul(reverse$2(slice$2(o, [0, 1], [n, t - 2]), 1), scalar(-1)),\n        u = concat$2([s, i], 1),\n        c = concat$2([o, l], 1),\n        _p19 = reshape$3(complex$2(u, c), [_a129[0], _a129[1]]);\n\n    r = ifft$2(_p19);\n  }\n\n  if (r = real$2(r), 3 === e.rank && 0 !== e.shape[0]) {\n    var _t412 = r,\n        _n234 = e.shape[0];\n    r = reshape$3(r, [_n234, r.shape[0] / _n234, r.shape[1]]), _t412.dispose();\n  }\n\n  return r;\n}\n\nvar irfft = op({\n  irfft_\n});\n\nfunction split_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = convertToTensor(e, \"x\", \"split\");\n  return ENGINE.runKernel(SplitV, {\n    x: r\n  }, {\n    numOrSizeSplits: t,\n    axis: n\n  });\n}\n\nvar split$2 = op({\n  split_\n});\n\nfunction rfft_(e, t) {\n  assert$4(\"float32\" === e.dtype, () => \"The dtype for rfft() must be real value but got \".concat(e.dtype));\n  var n = e.shape[e.shape.length - 1];\n  var r = e.size / n;\n  var a;\n\n  if (null != t && t < n) {\n    var _r197 = e.shape.map(e => 0),\n        _s96 = e.shape.map(e => e);\n\n    _s96[e.shape.length - 1] = t, a = slice$2(e, _r197, _s96), n = t;\n  } else if (null != t && t > n) {\n    var _r198 = e.shape.map(e => e);\n\n    _r198[e.shape.length - 1] = t - n, a = concat$2([e, zeros$2(_r198)], e.shape.length - 1), n = t;\n  } else a = e;\n\n  var s = zerosLike$2(a),\n      o = reshape$3(complex$2(a, s), [r, n]),\n      i = fft$2(o),\n      l = Math.floor(n / 2) + 1,\n      u = real$2(i),\n      c = imag$2(i),\n      p = split$2(u, [l, n - l], u.shape.length - 1),\n      d = split$2(c, [l, n - l], c.shape.length - 1),\n      h = a.shape.slice();\n  return h[a.shape.length - 1] = l, reshape$3(complex$2(p[0], d[0]), h);\n}\n\nvar rfft = op({\n  rfft_\n});\n\nfunction sqrt_(e) {\n  var t = convertToTensor(e, \"x\", \"sqrt\");\n  return ENGINE.runKernel(Sqrt, {\n    x: t\n  });\n}\n\nvar sqrt$2 = op({\n  sqrt_\n});\n\nfunction squaredDifference_(e, t) {\n  var n = convertToTensor(e, \"a\", \"squaredDifference\"),\n      r = convertToTensor(t, \"b\", \"squaredDifference\");\n  return [n, r] = makeTypesMatch(n, r), assertAndGetBroadcastShape(n.shape, r.shape), ENGINE.runKernel(SquaredDifference, {\n    a: n,\n    b: r\n  }, {});\n}\n\nvar squaredDifference$2 = op({\n  squaredDifference_\n});\n\nfunction squeeze_(e, t) {\n  var n = convertToTensor(e, \"x\", \"squeeze\");\n  return reshape$3(n, squeezeShape(n.shape, t).newShape);\n}\n\nvar squeeze = op({\n  squeeze_\n});\n\nfunction stack_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensorArray(e, \"tensors\", \"stack\", \"string_or_numeric\");\n  return assert$4(n.length >= 1, () => \"Pass at least one tensor to tf.stack\"), n.length > 0 && assert$4(t <= n[0].rank, () => \"Axis must be <= rank of the tensor\"), ENGINE.runKernel(Pack, n, {\n    axis: t\n  });\n}\n\nvar stack = op({\n  stack_\n});\n\nfunction step_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor(e, \"x\", \"step\");\n  return ENGINE.runKernel(Step, {\n    x: n\n  }, {\n    alpha: t\n  });\n}\n\nvar step$2 = op({\n  step_\n});\n\nfunction stridedSlice_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : 0;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : 0;\n  var l = arguments.length > 8 && arguments[8] !== undefined ? arguments[8] : 0;\n  var u = convertToTensor(e, \"x\", \"stridedSlice\", \"string_or_numeric\");\n  return ENGINE.runKernel(StridedSlice, {\n    x: u\n  }, {\n    begin: t,\n    end: n,\n    strides: r,\n    beginMask: a,\n    endMask: s,\n    ellipsisMask: o,\n    newAxisMask: i,\n    shrinkAxisMask: l\n  });\n}\n\nvar stridedSlice$2 = op({\n  stridedSlice_\n});\n\nfunction tan_(e) {\n  var t = convertToTensor(e, \"x\", \"tan\");\n  return ENGINE.runKernel(Tan, {\n    x: t\n  });\n}\n\nvar tan$2 = op({\n  tan_\n});\n\nfunction tensor1d(e, t) {\n  assertNonNull(e);\n  var n = inferShape(e, t);\n  if (1 !== n.length) throw new Error(\"tensor1d() requires values to be a flat/TypedArray\");\n  return makeTensor(e, null, n, t);\n}\n\nfunction tensor2d(e, t, n) {\n  if (assertNonNull(e), null != t && 2 !== t.length) throw new Error(\"tensor2d() requires shape to have two numbers\");\n  var r = inferShape(e, n);\n  if (2 !== r.length && 1 !== r.length) throw new Error(\"tensor2d() requires values to be number[][] or flat/TypedArray\");\n  if (1 === r.length && null == t) throw new Error(\"tensor2d() requires shape to be provided when `values` are a flat/TypedArray\");\n  return makeTensor(e, t, r, n);\n}\n\nfunction tensor4d(e, t, n) {\n  if (assertNonNull(e), null != t && 4 !== t.length) throw new Error(\"tensor4d() requires shape to have four numbers\");\n  var r = inferShape(e, n);\n  if (4 !== r.length && 1 !== r.length) throw new Error(\"tensor4d() requires values to be number[][][][] or flat/TypedArray\");\n  if (1 === r.length && null == t) throw new Error(\"tensor4d() requires shape to be provided when `values` are a flat array\");\n  return makeTensor(e, t, r, n);\n}\n\nfunction tensor5d(e, t, n) {\n  if (assertNonNull(e), null != t && 5 !== t.length) throw new Error(\"tensor5d() requires shape to have five numbers\");\n  var r = inferShape(e, n);\n  if (5 !== r.length && 1 !== r.length) throw new Error(\"tensor5d() requires values to be number[][][][][] or flat/TypedArray\");\n  if (1 === r.length && null == t) throw new Error(\"tensor5d() requires shape to be provided when `values` are a flat array\");\n  return makeTensor(e, t, r, n);\n}\n\nfunction tensor6d(e, t, n) {\n  if (assertNonNull(e), null != t && 6 !== t.length) throw new Error(\"tensor6d() requires shape to have six numbers\");\n  var r = inferShape(e, n);\n  if (6 !== r.length && 1 !== r.length) throw new Error(\"tensor6d() requires values to be number[][][][][][] or flat/TypedArray\");\n  if (1 === r.length && null == t) throw new Error(\"tensor6d() requires shape to be provided when `values` are a flat array\");\n  return makeTensor(e, t = t || r, r, n);\n}\n\nfunction topk_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n  var r = convertToTensor(e, \"x\", \"topk\");\n  if (0 === r.rank) throw new Error(\"topk() expects the input to be of rank 1 or higher\");\n  var a = r.shape[r.shape.length - 1];\n  if (t < 0) throw new Error(\"'k' passed to topk() must be >= 0 but got \".concat(t));\n  if (t > a) throw new Error(\"'k' passed to topk() must be <= the last dimension (\".concat(a, \") but got \").concat(t));\n  var s = {\n    x: r\n  },\n      o = {\n    k: t,\n    sorted: n\n  },\n      [i, l] = ENGINE.runKernel(TopK, s, o);\n  return {\n    values: i,\n    indices: l\n  };\n}\n\nvar topk = op({\n  topk_\n});\n\nfunction truncatedNormal_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  if (null != r && \"bool\" === r) throw new Error(\"Unsupported data type $ { dtype }\");\n  var s = new MPRandGauss(t, n, r, !0, a),\n      o = buffer(e, r);\n\n  for (var _e585 = 0; _e585 < o.values.length; _e585++) {\n    o.values[_e585] = s.nextValue();\n  }\n\n  return o.toTensor();\n}\n\nvar truncatedNormal$1 = op({\n  truncatedNormal_\n});\n\nfunction unique_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor(e, \"x\", \"unique\", \"string_or_numeric\");\n  assert$4(n.rank > 0, () => \"The input tensor must be at least 1D\");\n  var r = {\n    x: n\n  },\n      a = {\n    axis: t\n  },\n      [s, o] = ENGINE.runKernel(Unique, r, a);\n  return {\n    values: s,\n    indices: o\n  };\n}\n\nvar unique$3 = op({\n  unique_\n});\n\nfunction unsortedSegmentSum_(e, t, n) {\n  var r = convertToTensor(e, \"x\", \"unsortedSegmentSum\"),\n      a = convertToTensor(t, \"segmentIds\", \"unsortedSegmentSum\", \"int32\");\n  return assert$4(isInt(n), () => \"numSegments must be of dtype int\"), ENGINE.runKernel(UnsortedSegmentSum, {\n    x: r,\n    segmentIds: a\n  }, {\n    numSegments: n\n  });\n}\n\nvar unsortedSegmentSum$2 = op({\n  unsortedSegmentSum_\n});\n\nfunction unstack_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = convertToTensor(e, \"x\", \"unstack\", \"string_or_numeric\");\n  return assert$4(t >= -n.shape.length && t < n.shape.length, () => \"Axis = \".concat(t, \" is not in [-\").concat(n.shape.length, \", \").concat(n.shape.length, \")\")), ENGINE.runKernel(Unpack, {\n    value: n\n  }, {\n    axis: t\n  });\n}\n\nvar unstack = op({\n  unstack_\n});\n\nfunction variable(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n  var n = arguments.length > 2 ? arguments[2] : undefined;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  return ENGINE.makeVariable(e, t, n, r);\n}\n\nfunction whereImpl$2(e, t) {\n  var n = [];\n\n  for (var _e586 = 0; _e586 < t.length; _e586++) {\n    t[_e586] && n.push(_e586);\n  }\n\n  var r = buffer(e, \"int32\"),\n      a = buffer([n.length, e.length], \"int32\");\n\n  for (var _t413 = 0; _t413 < n.length; _t413++) {\n    var s = r.indexToLoc(n[_t413]);\n    a.values.set(s, _t413 * e.length);\n  }\n\n  return a.toTensor();\n}\n\nfunction whereAsync_(_x71) {\n  return _whereAsync_.apply(this, arguments);\n}\n\nfunction _whereAsync_() {\n  _whereAsync_ = _asyncToGenerator(function* (e) {\n    var t = convertToTensor(e, \"condition\", \"whereAsync\", \"bool\"),\n        n = yield t.data(),\n        r = whereImpl$2(t.shape, n);\n    return e !== t && t.dispose(), r;\n  });\n  return _whereAsync_.apply(this, arguments);\n}\n\nvar whereAsync = whereAsync_;\n\nfunction booleanMaskAsync_(_x72, _x73, _x74) {\n  return _booleanMaskAsync_.apply(this, arguments);\n}\n\nfunction _booleanMaskAsync_() {\n  _booleanMaskAsync_ = _asyncToGenerator(function* (e, t, n) {\n    var r = convertToTensor(e, \"tensor\", \"boolMask\"),\n        a = convertToTensor(t, \"mask\", \"boolMask\", \"bool\"),\n        s = null == n ? 0 : n,\n        o = a.rank,\n        i = r.shape;\n    assert$4(o > 0, () => \"mask cannot be scalar\"), assertShapesMatch(i.slice(s, s + o), a.shape, \"mask's shape must match the first K dimensions of tensor's shape,\");\n    var l = 1;\n\n    for (var _e1148 = s; _e1148 < s + o; _e1148++) {\n      l *= i[_e1148];\n    }\n\n    var u = i.slice(0, s).concat([l], i.slice(s + o)),\n        c = reshape$3(r, u),\n        p = reshape$3(a, [-1]),\n        d = yield whereAsync(p),\n        h = squeeze(d, [1]),\n        m = gather$1(c, h, s);\n    return e !== r && r.dispose(), t !== a && a.dispose(), h.dispose(), c.dispose(), p.dispose(), d.dispose(), m;\n  });\n  return _booleanMaskAsync_.apply(this, arguments);\n}\n\nvar booleanMaskAsync = booleanMaskAsync_;\n\nfunction norm_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"euclidean\";\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = normImpl(e = convertToTensor(e, \"x\", \"norm\"), t, n);\n  var s = a.shape;\n\n  if (r) {\n    var _t414 = parseAxisParam(n, e.shape);\n\n    s = expandShapeToKeepDim(a.shape, _t414);\n  }\n\n  return reshape$3(a, s);\n}\n\nfunction normImpl(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n  if (0 === e.rank) return abs$2(e);\n  if (1 !== e.rank && null === n) return normImpl(reshape$3(e, [-1]), t, n);\n\n  if (1 === e.rank || \"number\" == typeof n || Array.isArray(n) && 1 === n.length) {\n    if (1 === t) return sum$2(abs$2(e), n);\n    if (Infinity === t) return max$3(abs$2(e), n);\n    if (-Infinity === t) return min$3(abs$2(e), n);\n    if (\"euclidean\" === t || 2 === t) return sqrt$2(sum$2(pow$2(abs$2(e), scalar(2, \"int32\")), n));\n    throw new Error(\"Error in norm: invalid ord value: \".concat(t));\n  }\n\n  if (Array.isArray(n) && 2 === n.length) {\n    if (1 === t) return max$3(sum$2(abs$2(e), n[0]), n[1] - 1);\n    if (Infinity === t) return max$3(sum$2(abs$2(e), n[1]), n[0]);\n    if (-Infinity === t) return min$3(sum$2(abs$2(e), n[1]), n[0]);\n    if (\"fro\" === t || \"euclidean\" === t) return sqrt$2(sum$2(square$2(e), n));\n    throw new Error(\"Error in norm: invalid ord value: \".concat(t));\n  }\n\n  throw new Error(\"Error in norm: invalid axis: \".concat(n));\n}\n\nvar norm = op({\n  norm_\n});\n\nfunction movingAverage_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !0;\n  var s = convertToTensor(e, \"v\", \"movingAverage\"),\n      o = convertToTensor(t, \"x\", \"movingAverage\"),\n      i = convertToTensor(n, \"decay\", \"movingAverage\");\n  assertTypesMatch(s, o), assert$4(arraysEqual(s.shape, o.shape), () => \"Shape mismatch in v and x\");\n  var l = scalar(1),\n      u = sub$2(l, i);\n  var c = mul(sub$2(o, s), u);\n\n  if (a) {\n    assert$4(null != r, () => \"When using zeroDebias: true, step is required.\");\n\n    var _e587 = convertToTensor(r, \"step\", \"movingAverage\");\n\n    c = div$1(c, sub$2(l, pow$2(i, _e587)));\n  }\n\n  return add$2(s, c);\n}\n\nvar movingAverage = op({\n  movingAverage_\n});\n\nfunction scatterND_(e, t, n) {\n  var r = convertToTensor(e, \"indices\", \"scatterND\", \"int32\"),\n      a = convertToTensor(t, \"updates\", \"scatterND\");\n  return validateInput$1(a, r, n), ENGINE.runKernel(ScatterNd, {\n    indices: r,\n    updates: a\n  }, {\n    shape: n\n  });\n}\n\nvar scatterND = op({\n  scatterND_\n});\n\nfunction validateInput(e, t, n, r) {\n  if (\"int32\" !== e.dtype) throw new Error(\"tf.sparseToDense() expects the indices to be int32 type, but the dtype was \".concat(e.dtype, \".\"));\n  if (e.rank > 2) throw new Error(\"sparseIndices should be a scalar, vector, or matrix, but got shape \".concat(e.shape, \".\"));\n  var a = e.rank > 0 ? e.shape[0] : 1,\n      s = e.rank > 1 ? e.shape[1] : 1;\n  if (n.length !== s) throw new Error(\"outputShape has incorrect number of elements:, \".concat(n.length, \", should be: \").concat(s, \".\"));\n  if (0 !== t.rank && (1 !== t.rank || t.size !== a)) throw new Error(\"sparseValues has incorrect shape \".concat(t.shape, \", should be [] or [\").concat(a, \"]\"));\n  if (t.dtype !== r.dtype) throw new Error(\"sparseValues.dtype must match defaultValues.dtype\");\n}\n\nfunction sparseToDense_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var a = convertToTensor(e, \"sparseIndices\", \"sparseToDense\", \"int32\"),\n      s = convertToTensor(t, \"sparseValues\", \"sparseToDense\"),\n      o = convertToTensor(r, \"defaultValue\", \"sparseToDense\", s.dtype);\n  return validateInput(a, s, n, o), ENGINE.runKernel(SparseToDense, {\n    sparseIndices: a,\n    sparseValues: s,\n    defaultValue: o\n  }, {\n    outputShape: n\n  });\n}\n\nvar sparseToDense$2 = op({\n  sparseToDense_\n});\n\nfunction gatherND_(e, t) {\n  var n = convertToTensor(t, \"indices\", \"gatherND\", \"int32\"),\n      r = convertToTensor(e, \"x\", \"gatherND\", \"string_or_numeric\");\n  return ENGINE.runKernel(GatherNd, {\n    params: r,\n    indices: n\n  });\n}\n\nvar gatherND = op({\n  gatherND_\n});\n\nfunction getNoiseShape(e, t) {\n  if (null == t) return e.shape.slice();\n  if (arraysEqual(e.shape, t)) return t;\n\n  if (e.shape.length === t.length) {\n    var n = [];\n\n    for (var r = 0; r < e.shape.length; r++) {\n      n.push(null == t[r] && null != e.shape[r] ? e.shape[r] : t[r]);\n    }\n\n    return n;\n  }\n\n  return t;\n}\n\nfunction dropout_(e, t, n, r) {\n  var a = convertToTensor(e, \"x\", \"dropout\");\n  if (assert$4(\"float32\" === a.dtype, () => \"x has to be a floating point tensor since it's going to be scaled, but got a \".concat(a.dtype, \" tensor instead.\")), assert$4(t >= 0 && t < 1, () => \"rate must be a float in the range [0, 1), but got \".concat(t, \".\")), 0 === t) return e instanceof Tensor ? a.clone() : a;\n  var s = getNoiseShape(a, n),\n      o = 1 - t,\n      i = div$1(floor$2(add$2(randomUniform$1(s, 0, 1, \"float32\", r), o)), o);\n  return mul(a, i);\n}\n\nvar dropout$2 = op({\n  dropout_\n});\n\nfunction enclosingPowerOfTwo(e) {\n  return Math.floor(Math.pow(2, Math.ceil(Math.log(e) / Math.log(2))));\n}\n\nfunction cosineWindow(e, t, n) {\n  var r = 1 - e % 2,\n      a = new Float32Array(e);\n\n  for (var s = 0; s < e; ++s) {\n    var o = 2 * Math.PI * s / (e + r - 1);\n    a[s] = t - n * Math.cos(o);\n  }\n\n  return tensor1d(a, \"float32\");\n}\n\nfunction inTopKAsync_(_x75, _x76) {\n  return _inTopKAsync_.apply(this, arguments);\n}\n\nfunction _inTopKAsync_() {\n  _inTopKAsync_ = _asyncToGenerator(function* (e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n    var r = convertToTensor(e, \"predictions\", \"inTopK\"),\n        a = convertToTensor(t, \"targets\", \"inTopK\");\n    assert$4(r.rank > 1, () => \"inTopK() expects the predictions to be of rank 2 or higher, but got \".concat(r.rank)), assert$4(r.rank - 1 === a.rank, () => \"predictions rank should be 1 larger than targets rank, but got predictions rank \".concat(r.rank, \" and targets rank \").concat(a.rank)), assertShapesMatch(r.shape.slice(0, r.shape.length - 1), a.shape, \"predictions's shape should be align with the targets' shape, except the last dimension.\");\n    var s = r.shape[r.shape.length - 1];\n    assert$4(n > 0 && n <= s, () => \"'k' passed to inTopK() must be > 0 && <= the predictions last dimension (\".concat(s, \"), but got \").concat(n));\n    var o = yield r.data(),\n        i = yield a.data(),\n        [l, u] = [o.length / s, s],\n        c = getTypedArrayFromDType(\"bool\", l);\n\n    for (var _e1149 = 0; _e1149 < l; _e1149++) {\n      var _t775 = _e1149 * u,\n          _r449 = o.subarray(_t775, _t775 + u),\n          _a320 = [];\n\n      for (var _e1150 = 0; _e1150 < _r449.length; _e1150++) {\n        _a320.push({\n          value: _r449[_e1150],\n          index: _e1150\n        });\n      }\n\n      _a320.sort((e, t) => t.value - e.value), c[_e1149] = 0;\n\n      for (var _t776 = 0; _t776 < n; _t776++) {\n        if (_a320[_t776].index === i[_e1149]) {\n          c[_e1149] = 1;\n          break;\n        }\n      }\n    }\n\n    return e !== r && r.dispose(), t !== a && a.dispose(), tensor(c, a.shape, \"bool\");\n  });\n  return _inTopKAsync_.apply(this, arguments);\n}\n\nvar inTopKAsync = inTopKAsync_;\n\nfunction conv2DBackpropFilter_(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : \"NHWC\";\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = e;\n  3 === e.rank && (i = reshape$3(e, [1, e.shape[0], e.shape[1], e.shape[2]]));\n  var l = t;\n  3 === l.rank && (l = reshape$3(t, [1, t.shape[0], t.shape[1], t.shape[2]])), assert$4(4 === i.rank, () => \"Error in conv2dDerFilter: input must be rank 4, but got shape \".concat(i.shape, \".\")), assert$4(4 === l.rank, () => \"Error in conv2dDerFilter: dy must be rank 4, but got shape \".concat(l.shape, \".\")), assert$4(4 === n.length, () => \"Error in conv2dDerFilter: filterShape must be length 4, but got \".concat(n, \".\"));\n  var u = \"NHWC\" === s ? i.shape[3] : i.shape[1],\n      c = \"NHWC\" === s ? l.shape[3] : l.shape[1];\n  return assert$4(u === n[2], () => \"Error in conv2dDerFilter: depth of input \".concat(u, \") must match input depth in filter (\").concat(n[2], \".\")), assert$4(c === n[3], () => \"Error in conv2dDerFilter: depth of dy (\".concat(c, \") must match output depth for filter (\").concat(n[3], \").\")), null != o && assert$4(isInt(a), () => \"Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(a, \".\")), ENGINE.runKernel(Conv2DBackpropFilter, {\n    x: i,\n    dy: l\n  }, {\n    strides: r,\n    pad: a,\n    dataFormat: s,\n    dimRoundingMode: o,\n    filterShape: n\n  });\n}\n\nvar conv2DBackpropFilter$2 = op({\n  conv2DBackpropFilter_\n});\n\nfunction getFusedDyActivation(e, t, n) {\n  if (null == n || \"linear\" === n) return e;\n  if (\"relu\" === n) return mul(e, step$2(t));\n  throw new Error(\"Cannot compute gradient for fused activation \".concat(n, \".\"));\n}\n\nfunction getFusedBiasGradient(e, t) {\n  var n = t;\n  var r = getReductionAxes(e.shape, t.shape);\n  return r.length > 0 && (n = sum$2(n, r)), reshape$3(n, e.shape);\n}\n\nfunction applyActivation$1(e, t, n, r) {\n  if (\"linear\" === t) return e;\n  if (\"relu\" === t) return relu$3(e);\n  if (\"elu\" === t) return elu$4(e);\n  if (\"relu6\" === t) return relu6$2(e);\n  if (\"prelu\" === t) return prelu$3(e, n);\n  if (\"leakyrelu\" === t) return leakyRelu$2(e, r);\n  if (\"sigmoid\" === t) return sigmoid$2(e);\n  throw new Error(\"Unknown fused activation \".concat(t, \".\"));\n}\n\nvar shouldFuse = (e, t) => !(e > 0) || \"linear\" === t;\n\nfunction fusedConv2d_(_ref29) {\n  var {\n    x: e,\n    filter: t,\n    strides: n,\n    pad: r,\n    dataFormat: a = \"NHWC\",\n    dilations: s = [1, 1],\n    dimRoundingMode: o,\n    bias: i,\n    activation: l = \"linear\",\n    preluActivationWeights: u,\n    leakyreluAlpha: c\n  } = _ref29;\n\n  if (!1 === shouldFuse(ENGINE.state.gradientDepth, l = l || \"linear\")) {\n    var _p20 = conv2d$3(e, t, n, r, a, s, o);\n\n    return null != i && (_p20 = add$2(_p20, i)), applyActivation$1(_p20, l, u, c);\n  }\n\n  var p = convertToTensor(e, \"x\", \"conv2d\"),\n      d = convertToTensor(t, \"filter\", \"conv2d\");\n  var h = p,\n      m = !1;\n  3 === p.rank && (m = !0, h = reshape$3(p, [1, p.shape[0], p.shape[1], p.shape[2]])), assert$4(4 === h.rank, () => \"Error in fused conv2d: input must be rank 4, but got rank \".concat(h.rank, \".\")), assert$4(4 === d.rank, () => \"Error in fused conv2d: filter must be rank 4, but got rank \".concat(d.rank, \".\")), null != o && assert$4(isInt(r), () => \"Error in fused conv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\")), assert$4(h.shape[3] === d.shape[2], () => \"Error in conv2d: depth of input (\".concat(h.shape[3], \") must match input depth for filter \").concat(d.shape[2], \".\")), assert$4(eitherStridesOrDilationsAreOne(n, s), () => \"Error in conv2D: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '\").concat(s, \"'\")), assert$4(\"NHWC\" === a, () => \"Error in conv2d: got dataFormat of \".concat(a, \" but only NHWC is currently supported.\"));\n  var f = computeConv2DInfo(h.shape, d.shape, n, s, r, o);\n  var g, $;\n  null != i && (g = convertToTensor(i, \"bias\", \"fused conv2d\"), [g] = makeTypesMatch(g, p), assertAndGetBroadcastShape(f.outShape, g.shape)), null != u && ($ = convertToTensor(u, \"prelu weights\", \"fused conv2d\"));\n\n  var y = (e, t) => {\n    var [a, o, i, u] = t,\n        c = getFusedDyActivation(e, i, l);\n    assert$4(tupleValuesAreOne(s), () => \"Error in gradient of fused conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '\".concat(s, \"'\"));\n    var p = [conv2DBackpropInput$2(o.shape, c, a, n, r), conv2DBackpropFilter$2(o, c, a.shape, n, r)];\n\n    if (null != u) {\n      var _e588 = getFusedBiasGradient(u, c);\n\n      p.push(_e588);\n    }\n\n    return p;\n  },\n      b = {\n    x: h,\n    filter: d,\n    bias: g,\n    preluActivationWeights: $\n  },\n      x = {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s,\n    dimRoundingMode: o,\n    activation: l,\n    leakyreluAlpha: c\n  };\n\n  return null == i ? customGrad((e, t, n) => {\n    var r = ENGINE.runKernel(FusedConv2D, b, x);\n    return n([t, e, r]), m && (r = reshape$3(r, [r.shape[1], r.shape[2], r.shape[3]])), {\n      value: r,\n      gradFunc: y\n    };\n  })(h, d) : customGrad((e, t, n, r) => {\n    var a = ENGINE.runKernel(FusedConv2D, b, x);\n    return r([t, e, a, n]), m && (a = reshape$3(a, [a.shape[1], a.shape[2], a.shape[3]])), {\n      value: a,\n      gradFunc: y\n    };\n  })(h, d, g);\n}\n\nvar conv2d$2 = op({\n  fusedConv2d_\n});\n\nfunction depthwiseConv2dNativeBackpropFilter_(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = e;\n  3 === e.rank && (i = reshape$3(e, [1, e.shape[0], e.shape[1], e.shape[2]]));\n  var l = t;\n  return 3 === l.rank && (l = reshape$3(t, [1, t.shape[0], t.shape[1], t.shape[2]])), ENGINE.runKernel(DepthwiseConv2dNativeBackpropFilter, {\n    x: i,\n    dy: l\n  }, {\n    strides: r,\n    pad: a,\n    dimRoundingMode: o,\n    dilations: s,\n    filterShape: n\n  });\n}\n\nvar depthwiseConv2dNativeBackpropFilter$2 = op({\n  depthwiseConv2dNativeBackpropFilter_\n});\n\nfunction depthwiseConv2dNativeBackpropInput_(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : [1, 1];\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = t,\n      l = !1;\n  3 === t.rank && (l = !0, i = reshape$3(t, [1, t.shape[0], t.shape[1], t.shape[2]]));\n  var u = ENGINE.runKernel(DepthwiseConv2dNativeBackpropInput, {\n    dy: i,\n    filter: n\n  }, {\n    strides: r,\n    pad: a,\n    dimRoundingMode: o,\n    dilations: s,\n    inputShape: e\n  });\n  return l ? reshape$3(u, [u.shape[1], u.shape[2], u.shape[3]]) : u;\n}\n\nvar depthwiseConv2dNativeBackpropInput$2 = op({\n  depthwiseConv2dNativeBackpropInput_\n});\n\nfunction fusedDepthwiseConv2d_(_ref30) {\n  var {\n    x: e,\n    filter: t,\n    strides: n,\n    pad: r,\n    dataFormat: a = \"NHWC\",\n    dilations: s = [1, 1],\n    dimRoundingMode: o,\n    bias: i,\n    activation: l = \"linear\",\n    preluActivationWeights: u,\n    leakyreluAlpha: c\n  } = _ref30;\n\n  if (!1 === shouldFuse(ENGINE.state.gradientDepth, l)) {\n    var _p21 = depthwiseConv2d$3(e, t, n, r, a, s, o);\n\n    return null != i && (_p21 = add$2(_p21, i)), applyActivation$1(_p21, l, u, c);\n  }\n\n  var p = convertToTensor(e, \"x\", \"depthwiseConv2d\"),\n      d = convertToTensor(t, \"filter\", \"depthwiseConv2d\");\n  var h = p,\n      m = !1;\n  3 === p.rank && (m = !0, h = reshape$3(p, [1, p.shape[0], p.shape[1], p.shape[2]])), assert$4(4 === h.rank, () => \"Error in fused depthwiseConv2d: input must be rank 4, but got rank \".concat(h.rank, \".\")), assert$4(4 === d.rank, () => \"Error in fused depthwiseConv2d: filter must be rank 4, but got rank \".concat(d.rank, \".\")), assert$4(h.shape[3] === d.shape[2], () => \"Error in fused depthwiseConv2d: number of input channels (\".concat(h.shape[3], \") must match the inChannels dimension in filter \").concat(d.shape[2], \".\")), null == s && (s = [1, 1]), assert$4(eitherStridesOrDilationsAreOne(n, s), () => \"Error in fused depthwiseConv2d: Either strides or dilations must be 1. Got strides \".concat(n, \" and dilations '\").concat(s, \"'\")), null != o && assert$4(isInt(r), () => \"Error in fused depthwiseConv2d: pad must be an integer when using dimRoundingMode \".concat(o, \" but got pad \").concat(r, \".\"));\n  var f = computeConv2DInfo(h.shape, d.shape, n, s, r, o, !0);\n  var g, $;\n  null != i && (g = convertToTensor(i, \"bias\", \"fused conv2d\"), [g] = makeTypesMatch(g, p), assertAndGetBroadcastShape(f.outShape, g.shape)), null != u && ($ = convertToTensor(u, \"prelu weights\", \"fused depthwiseConv2d\"));\n\n  var y = (e, t) => {\n    assert$4(tupleValuesAreOne(s), () => \"Error in gradient of fused depthwiseConv2d: dilation rates greater than 1 are not yet supported. Got dilations '\".concat(s, \"'\"));\n    var [a, i, u, c] = t,\n        p = getFusedDyActivation(e, u, l),\n        d = depthwiseConv2dNativeBackpropInput$2(i.shape, p, a, n, r, s, o),\n        h = depthwiseConv2dNativeBackpropFilter$2(i, p, a.shape, n, r, s, o);\n    return null != c ? [d, h, getFusedBiasGradient(g, p)] : [d, h];\n  },\n      b = {\n    x: h,\n    filter: d,\n    bias: g,\n    preluActivationWeights: $\n  },\n      x = {\n    strides: n,\n    pad: r,\n    dataFormat: a,\n    dilations: s,\n    dimRoundingMode: o,\n    activation: l,\n    leakyreluAlpha: c\n  };\n\n  return null == i ? customGrad((e, t, n) => {\n    var r = ENGINE.runKernel(FusedDepthwiseConv2D, b, x);\n    return n([t, e, r]), m && (r = reshape$3(r, [r.shape[1], r.shape[2], r.shape[3]])), {\n      value: r,\n      gradFunc: y\n    };\n  })(h, d) : customGrad((e, t, n, r) => {\n    var a = ENGINE.runKernel(FusedDepthwiseConv2D, b, x);\n    return r([t, e, a, n]), m && (a = reshape$3(a, [a.shape[1], a.shape[2], a.shape[3]])), {\n      value: a,\n      gradFunc: y\n    };\n  })(h, d, g);\n}\n\nvar depthwiseConv2d$2 = op({\n  fusedDepthwiseConv2d_\n});\n\nfunction fusedMatMul_(_ref31) {\n  var {\n    a: e,\n    b: t,\n    transposeA: n = !1,\n    transposeB: r = !1,\n    bias: a,\n    activation: s = \"linear\",\n    preluActivationWeights: o,\n    leakyreluAlpha: i\n  } = _ref31;\n\n  if (!1 === shouldFuse(ENGINE.state.gradientDepth, s)) {\n    var _l36 = matMul$1(e, t, n, r);\n\n    return null != a && (_l36 = add$2(_l36, a)), applyActivation$1(_l36, s, o, i);\n  }\n\n  var l = convertToTensor(e, \"a\", \"fused matMul\"),\n      u = convertToTensor(t, \"b\", \"fused matMul\");\n  [l, u] = makeTypesMatch(l, u);\n  var c = n ? l.shape[l.rank - 2] : l.shape[l.rank - 1],\n      p = r ? u.shape[u.rank - 1] : u.shape[u.rank - 2],\n      d = n ? l.shape[l.rank - 1] : l.shape[l.rank - 2],\n      h = r ? u.shape[u.rank - 2] : u.shape[u.rank - 1],\n      m = l.shape.slice(0, -2),\n      f = u.shape.slice(0, -2),\n      g = sizeFromShape(m),\n      $ = sizeFromShape(f);\n  assert$4(l.rank >= 2 && u.rank >= 2 && l.rank === u.rank, () => \"Error in fused matMul: inputs must have the same rank of at least 2, got ranks \".concat(l.rank, \" and \").concat(u.rank, \".\")), assert$4(arraysEqual(m, f), () => \"Error in fused matMul: outer dimensions (\".concat(m, \") and (\").concat(f, \") of Tensors with shapes \").concat(l.shape, \" and \").concat(u.shape, \" must match.\")), assert$4(c === p, () => \"Error in fused matMul: inner shapes (\".concat(c, \") and (\").concat(p, \") of Tensors with shapes \").concat(l.shape, \" and \").concat(u.shape, \" and transposeA=\").concat(n, \" and transposeB=\").concat(r, \" must match.\"));\n  var y = l.shape.slice(0, -2).concat([d, h]),\n      b = reshape$3(l, n ? [g, c, d] : [g, d, c]),\n      x = reshape$3(u, r ? [$, h, p] : [$, p, h]);\n  var v, I;\n  null != a && (v = convertToTensor(a, \"bias\", \"fused matMul\"), [v] = makeTypesMatch(v, l), assertAndGetBroadcastShape(y, v.shape)), null != o && (I = convertToTensor(o, \"prelu weights\", \"fused matMul\"));\n\n  var C = (e, t) => {\n    var [o, i, l, u] = t,\n        c = getFusedDyActivation(reshape$3(e, l.shape), l, s);\n    var p, d;\n    return n || r ? !n && r ? (p = matMul$1(c, i, !1, !1), d = matMul$1(c, o, !0, !1)) : n && !r ? (p = matMul$1(i, c, !1, !0), d = matMul$1(o, c, !1, !1)) : (p = matMul$1(i, c, !0, !0), d = matMul$1(c, o, !0, !0)) : (p = matMul$1(c, i, !1, !0), d = matMul$1(o, c, !0, !1)), null != a ? [p, d, getFusedBiasGradient(u, c)] : [p, d];\n  },\n      S = {\n    a: b,\n    b: x,\n    bias: v,\n    preluActivationWeights: I\n  },\n      k = {\n    transposeA: n,\n    transposeB: r,\n    activation: s,\n    leakyreluAlpha: i\n  };\n\n  return null == a ? customGrad((e, t, n) => {\n    var r = ENGINE.runKernel(_FusedMatMul, S, k);\n    return n([e, t, r]), {\n      value: reshape$3(r, y),\n      gradFunc: C\n    };\n  })(b, x) : customGrad((e, t, n, r) => {\n    var a = ENGINE.runKernel(_FusedMatMul, S, k);\n    return r([e, t, a, n]), {\n      value: reshape$3(a, y),\n      gradFunc: C\n    };\n  })(b, x, v);\n}\n\nvar matMul = op({\n  fusedMatMul_\n});\nvar fused_ops = {\n  __proto__: null,\n  conv2d: conv2d$2,\n  depthwiseConv2d: depthwiseConv2d$2,\n  matMul\n};\n\nfunction hammingWindow_(e) {\n  return cosineWindow(e, .54, .46);\n}\n\nvar hammingWindow = op({\n  hammingWindow_\n});\n\nfunction hannWindow_(e) {\n  return cosineWindow(e, .5, .5);\n}\n\nvar hannWindow = op({\n  hannWindow_\n});\n\nfunction frame_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n  var s = 0;\n  var o = [];\n\n  for (; s + t <= e.size;) {\n    o.push(slice$2(e, s, t)), s += n;\n  }\n\n  if (r) for (; s < e.size;) {\n    var _r199 = s + t - e.size,\n        i = concat$2([slice$2(e, s, t - _r199), fill$2([_r199], a)]);\n\n    o.push(i), s += n;\n  }\n  return 0 === o.length ? tensor2d([], [0, t]) : reshape$3(concat$2(o), [o.length, t]);\n}\n\nvar frame = op({\n  frame_\n});\n\nfunction stft_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : hannWindow;\n  null == r && (r = enclosingPowerOfTwo(t));\n  var s = frame(e, t, n),\n      o = mul(s, a(t));\n  return rfft(o, r);\n}\n\nvar stft = op({\n  stft_\n});\n\nfunction cropAndResize_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"bilinear\";\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n  var o = convertToTensor(e, \"image\", \"cropAndResize\"),\n      i = convertToTensor(t, \"boxes\", \"cropAndResize\", \"float32\"),\n      l = convertToTensor(n, \"boxInd\", \"cropAndResize\", \"int32\"),\n      u = i.shape[0];\n  return assert$4(4 === o.rank, () => \"Error in cropAndResize: image must be rank 4,but got rank \".concat(o.rank, \".\")), assert$4(2 === i.rank && 4 === i.shape[1], () => \"Error in cropAndResize: boxes must be have size [\".concat(u, \",4] but had shape \").concat(i.shape, \".\")), assert$4(1 === l.rank && l.shape[0] === u, () => \"Error in cropAndResize: boxInd must be have size [\".concat(u, \"] but had shape \").concat(i.shape, \".\")), assert$4(2 === r.length, () => \"Error in cropAndResize: cropSize must be of length 2, but got length \".concat(r.length, \".\")), assert$4(r[0] >= 1 && r[1] >= 1, () => \"cropSize must be atleast [1,1], but was \".concat(r)), assert$4(\"bilinear\" === a || \"nearest\" === a, () => \"method must be bilinear or nearest, but was \".concat(a)), ENGINE.runKernel(CropAndResize, {\n    image: o,\n    boxes: i,\n    boxInd: l\n  }, {\n    method: a,\n    extrapolationValue: s,\n    cropSize: r\n  });\n}\n\nvar cropAndResize$2 = op({\n  cropAndResize_\n});\n\nfunction flipLeftRight_(e) {\n  var t = convertToTensor(e, \"image\", \"flipLeftRight\", \"float32\");\n  return assert$4(4 === t.rank, () => \"Error in flipLeftRight: image must be rank 4,but got rank \".concat(t.rank, \".\")), ENGINE.runKernel(FlipLeftRight, {\n    image: t\n  }, {});\n}\n\nvar flipLeftRight = op({\n  flipLeftRight_\n});\n\nfunction rotateWithOffset_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = convertToTensor(e, \"image\", \"rotateWithOffset\", \"float32\");\n  return assert$4(4 === a.rank, () => \"Error in rotateWithOffset: image must be rank 4,but got rank \".concat(a.rank, \".\")), ENGINE.runKernel(RotateWithOffset, {\n    image: a\n  }, {\n    radians: t,\n    fillValue: n,\n    center: r\n  });\n}\n\nvar rotateWithOffset = op({\n  rotateWithOffset_\n});\n\nfunction nonMaxSuppSanityCheck(e, t, n, r, a, s) {\n  null == r && (r = .5), null == a && (a = Number.NEGATIVE_INFINITY), null == s && (s = 0);\n  var o = e.shape[0];\n  return n = Math.min(n, o), assert$4(0 <= r && r <= 1, () => \"iouThreshold must be in [0, 1], but was '\".concat(r, \"'\")), assert$4(2 === e.rank, () => \"boxes must be a 2D tensor, but was of rank '\".concat(e.rank, \"'\")), assert$4(4 === e.shape[1], () => \"boxes must have 4 columns, but 2nd dimension was \".concat(e.shape[1])), assert$4(1 === t.rank, () => \"scores must be a 1D tensor\"), assert$4(t.shape[0] === o, () => \"scores has incompatible shape with boxes. Expected \".concat(o, \", but was \").concat(t.shape[0])), assert$4(0 <= s && s <= 1, () => \"softNmsSigma must be in [0, 1], but was '\".concat(s, \"'\")), {\n    maxOutputSize: n,\n    iouThreshold: r,\n    scoreThreshold: a,\n    softNmsSigma: s\n  };\n}\n\nfunction nonMaxSuppression_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n  var s = convertToTensor(e, \"boxes\", \"nonMaxSuppression\"),\n      o = convertToTensor(t, \"scores\", \"nonMaxSuppression\"),\n      i = nonMaxSuppSanityCheck(s, o, n, r, a);\n  return ENGINE.runKernel(NonMaxSuppressionV3, {\n    boxes: s,\n    scores: o\n  }, {\n    maxOutputSize: n = i.maxOutputSize,\n    iouThreshold: r = i.iouThreshold,\n    scoreThreshold: a = i.scoreThreshold\n  });\n}\n\nvar nonMaxSuppression = op({\n  nonMaxSuppression_\n});\n\nfunction binaryInsert(e, t, n) {\n  var r = binarySearch(e, t, n);\n  e.splice(r < 0 ? -(r + 1) : r, 0, t);\n}\n\nfunction binarySearch(e, t, n) {\n  return binarySearch_(e, t, n || defaultComparator);\n}\n\nfunction defaultComparator(e, t) {\n  return e > t ? 1 : e < t ? -1 : 0;\n}\n\nfunction binarySearch_(e, t, n) {\n  var r = 0,\n      a = e.length,\n      s = 0,\n      o = !1;\n\n  for (; r < a;) {\n    s = r + (a - r >>> 1);\n    var i = n(t, e[s]);\n    i > 0 ? r = s + 1 : (a = s, o = !i);\n  }\n\n  return o ? r : -r - 1;\n}\n\nfunction nonMaxSuppressionV3Impl$2(e, t, n, r, a) {\n  return nonMaxSuppressionImpl_(e, t, n, r, a, 0);\n}\n\nfunction nonMaxSuppressionV4Impl$2(e, t, n, r, a, s) {\n  return nonMaxSuppressionImpl_(e, t, n, r, a, 0, !1, s, !0);\n}\n\nfunction nonMaxSuppressionV5Impl$2(e, t, n, r, a, s) {\n  return nonMaxSuppressionImpl_(e, t, n, r, a, s, !0);\n}\n\nfunction nonMaxSuppressionImpl_(e, t, n, r, a, s) {\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !1;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : !1;\n  var l = arguments.length > 8 && arguments[8] !== undefined ? arguments[8] : !1;\n  var u = [];\n\n  for (var _e589 = 0; _e589 < t.length; _e589++) {\n    t[_e589] > a && u.push({\n      score: t[_e589],\n      boxIndex: _e589,\n      suppressBeginIndex: 0\n    });\n  }\n\n  u.sort(ascendingComparator);\n  var c = s > 0 ? -.5 / s : 0,\n      p = [],\n      d = [];\n\n  for (; p.length < n && u.length > 0;) {\n    var _t415 = u.pop(),\n        {\n      score: _n235,\n      boxIndex: _s97,\n      suppressBeginIndex: _o70\n    } = _t415;\n\n    if (_n235 < a) break;\n\n    var _i44 = !1;\n\n    for (var _n236 = p.length - 1; _n236 >= _o70; --_n236) {\n      var _o71 = intersectionOverUnion(e, _s97, p[_n236]);\n\n      if (_o71 >= r) {\n        _i44 = !0;\n        break;\n      }\n\n      if (_t415.score = _t415.score * suppressWeight(r, c, _o71), _t415.score <= a) break;\n    }\n\n    _t415.suppressBeginIndex = p.length, _i44 || (_t415.score === _n235 ? (p.push(_s97), d.push(_t415.score)) : _t415.score > a && binaryInsert(u, _t415, ascendingComparator));\n  }\n\n  var h = p.length,\n      m = n - h;\n  i && m > 0 && (p.push(...new Array(m).fill(0)), d.push(...new Array(m).fill(0)));\n  var f = {\n    selectedIndices: p\n  };\n  return o && (f.selectedScores = d), l && (f.validOutputs = h), f;\n}\n\nfunction intersectionOverUnion(e, t, n) {\n  var r = e.subarray(4 * t, 4 * t + 4),\n      a = e.subarray(4 * n, 4 * n + 4),\n      s = Math.min(r[0], r[2]),\n      o = Math.min(r[1], r[3]),\n      i = Math.max(r[0], r[2]),\n      l = Math.max(r[1], r[3]),\n      u = Math.min(a[0], a[2]),\n      c = Math.min(a[1], a[3]),\n      p = Math.max(a[0], a[2]),\n      d = Math.max(a[1], a[3]),\n      h = (i - s) * (l - o),\n      m = (p - u) * (d - c);\n  if (h <= 0 || m <= 0) return 0;\n  var f = Math.max(s, u),\n      g = Math.max(o, c),\n      $ = Math.min(i, p),\n      y = Math.min(l, d),\n      b = Math.max($ - f, 0) * Math.max(y - g, 0);\n  return b / (h + m - b);\n}\n\nfunction suppressWeight(e, t, n) {\n  var r = Math.exp(t * n * n);\n  return n <= e ? r : 0;\n}\n\nfunction ascendingComparator(e, t) {\n  return e.score - t.score || e.score === t.score && t.boxIndex - e.boxIndex;\n}\n\nfunction nonMaxSuppressionAsync_(_x77, _x78, _x79) {\n  return _nonMaxSuppressionAsync_.apply(this, arguments);\n}\n\nfunction _nonMaxSuppressionAsync_() {\n  _nonMaxSuppressionAsync_ = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n    var s = convertToTensor(e, \"boxes\", \"nonMaxSuppressionAsync\"),\n        o = convertToTensor(t, \"scores\", \"nonMaxSuppressionAsync\"),\n        i = nonMaxSuppSanityCheck(s, o, n, r, a);\n    n = i.maxOutputSize, r = i.iouThreshold, a = i.scoreThreshold;\n    var l = yield Promise.all([s.data(), o.data()]),\n        u = l[0],\n        c = l[1],\n        {\n      selectedIndices: p\n    } = nonMaxSuppressionV3Impl$2(u, c, n, r, a);\n    return s !== e && s.dispose(), o !== t && o.dispose(), tensor1d(p, \"int32\");\n  });\n  return _nonMaxSuppressionAsync_.apply(this, arguments);\n}\n\nvar nonMaxSuppressionAsync = nonMaxSuppressionAsync_;\n\nfunction nonMaxSuppressionWithScore_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n  var o = convertToTensor(e, \"boxes\", \"nonMaxSuppression\"),\n      i = convertToTensor(t, \"scores\", \"nonMaxSuppression\"),\n      l = nonMaxSuppSanityCheck(o, i, n, r, a, s),\n      u = ENGINE.runKernel(NonMaxSuppressionV5, {\n    boxes: o,\n    scores: i\n  }, {\n    maxOutputSize: n = l.maxOutputSize,\n    iouThreshold: r = l.iouThreshold,\n    scoreThreshold: a = l.scoreThreshold,\n    softNmsSigma: s = l.softNmsSigma\n  });\n  return {\n    selectedIndices: u[0],\n    selectedScores: u[1]\n  };\n}\n\nvar nonMaxSuppressionWithScore = op({\n  nonMaxSuppressionWithScore_\n});\n\nfunction nonMaxSuppressionWithScoreAsync_(_x80, _x81, _x82) {\n  return _nonMaxSuppressionWithScoreAsync_.apply(this, arguments);\n}\n\nfunction _nonMaxSuppressionWithScoreAsync_() {\n  _nonMaxSuppressionWithScoreAsync_ = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n    var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 0;\n    var o = convertToTensor(e, \"boxes\", \"nonMaxSuppressionAsync\"),\n        i = convertToTensor(t, \"scores\", \"nonMaxSuppressionAsync\"),\n        l = nonMaxSuppSanityCheck(o, i, n, r, a, s);\n    n = l.maxOutputSize, r = l.iouThreshold, a = l.scoreThreshold, s = l.softNmsSigma;\n    var u = yield Promise.all([o.data(), i.data()]),\n        c = u[0],\n        p = u[1],\n        {\n      selectedIndices: d,\n      selectedScores: h\n    } = nonMaxSuppressionV5Impl$2(c, p, n, r, a, s);\n    return o !== e && o.dispose(), i !== t && i.dispose(), {\n      selectedIndices: tensor1d(d, \"int32\"),\n      selectedScores: tensor1d(h)\n    };\n  });\n  return _nonMaxSuppressionWithScoreAsync_.apply(this, arguments);\n}\n\nvar nonMaxSuppressionWithScoreAsync = nonMaxSuppressionWithScoreAsync_;\n\nfunction nonMaxSuppressionPadded_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = convertToTensor(e, \"boxes\", \"nonMaxSuppression\"),\n      i = convertToTensor(t, \"scores\", \"nonMaxSuppression\"),\n      l = nonMaxSuppSanityCheck(o, i, n, r, a, null),\n      u = ENGINE.runKernel(NonMaxSuppressionV4, {\n    boxes: o,\n    scores: i\n  }, {\n    maxOutputSize: l.maxOutputSize,\n    iouThreshold: l.iouThreshold,\n    scoreThreshold: l.scoreThreshold,\n    padToMaxOutputSize: s\n  });\n  return {\n    selectedIndices: u[0],\n    validOutputs: u[1]\n  };\n}\n\nvar nonMaxSuppressionPadded = op({\n  nonMaxSuppressionPadded_\n});\n\nfunction nonMaxSuppressionPaddedAsync_(_x83, _x84, _x85) {\n  return _nonMaxSuppressionPaddedAsync_.apply(this, arguments);\n}\n\nfunction _nonMaxSuppressionPaddedAsync_() {\n  _nonMaxSuppressionPaddedAsync_ = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Number.NEGATIVE_INFINITY;\n    var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n    var o = convertToTensor(e, \"boxes\", \"nonMaxSuppressionAsync\"),\n        i = convertToTensor(t, \"scores\", \"nonMaxSuppressionAsync\"),\n        l = nonMaxSuppSanityCheck(o, i, n, r, a, null),\n        u = l.maxOutputSize,\n        c = l.iouThreshold,\n        p = l.scoreThreshold,\n        [d, h] = yield Promise.all([o.data(), i.data()]),\n        {\n      selectedIndices: m,\n      validOutputs: f\n    } = nonMaxSuppressionV4Impl$2(d, h, u, c, p, s);\n    return o !== e && o.dispose(), i !== t && i.dispose(), {\n      selectedIndices: tensor1d(m, \"int32\"),\n      validOutputs: scalar(f, \"int32\")\n    };\n  });\n  return _nonMaxSuppressionPaddedAsync_.apply(this, arguments);\n}\n\nvar nonMaxSuppressionPaddedAsync = nonMaxSuppressionPaddedAsync_;\n\nfunction resizeBilinear_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor(e, \"images\", \"resizeBilinear\");\n  assert$4(3 === a.rank || 4 === a.rank, () => \"Error in resizeBilinear: x must be rank 3 or 4, but got rank \".concat(a.rank, \".\")), assert$4(2 === t.length, () => \"Error in resizeBilinear: new shape must 2D, but got shape \".concat(t, \".\")), assert$4(!1 === r || !1 === n, () => \"Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false.\");\n  var s = a,\n      o = !1;\n  3 === a.rank && (o = !0, s = reshape$3(a, [1, a.shape[0], a.shape[1], a.shape[2]]));\n  var i = ENGINE.runKernel(ResizeBilinear, {\n    images: s\n  }, {\n    alignCorners: n,\n    halfPixelCenters: r,\n    size: t\n  });\n  return o ? reshape$3(i, [i.shape[1], i.shape[2], i.shape[3]]) : i;\n}\n\nvar resizeBilinear$2 = op({\n  resizeBilinear_\n});\n\nfunction resizeNearestNeighbor_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = convertToTensor(e, \"images\", \"resizeNearestNeighbor\");\n  assert$4(3 === a.rank || 4 === a.rank, () => \"Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank \".concat(a.rank, \".\")), assert$4(2 === t.length, () => \"Error in resizeNearestNeighbor: new shape must 2D, but got shape \".concat(t, \".\")), assert$4(\"float32\" === a.dtype || \"int32\" === a.dtype, () => \"`images` must have `int32` or `float32` as dtype\"), assert$4(!1 === r || !1 === n, () => \"Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false.\");\n  var s = a,\n      o = !1;\n  3 === a.rank && (o = !0, s = reshape$3(a, [1, a.shape[0], a.shape[1], a.shape[2]]));\n  var i = ENGINE.runKernel(ResizeNearestNeighbor, {\n    images: s\n  }, {\n    alignCorners: n,\n    halfPixelCenters: r,\n    size: t\n  });\n  return o ? reshape$3(i, [i.shape[1], i.shape[2], i.shape[3]]) : i;\n}\n\nvar resizeNearestNeighbor$2 = op({\n  resizeNearestNeighbor_\n});\n\nfunction threshold_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"binary\";\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : .5;\n  var a = convertToTensor(e, \"image\", \"threshold\"),\n      s = a.shape[0] * a.shape[1];\n  var o,\n      i,\n      l,\n      u,\n      c = mul(tensor1d([r]), 255);\n\n  if (assert$4(3 === a.rank, () => \"Error in threshold: image must be rank 3,but got rank \".concat(a.rank, \".\")), assert$4(3 === a.shape[2] || 1 === a.shape[2], () => \"Error in threshold: image color channel must be equal to 3 or 1but got \".concat(a.shape[2], \".\")), assert$4(\"int32\" === a.dtype || \"float32\" === a.dtype, () => \"Error in dtype: image dtype must be int32 or float32,but got dtype \".concat(a.dtype, \".\")), assert$4(\"otsu\" === t || \"binary\" === t, () => \"Method must be binary or otsu, but was \".concat(t)), 3 === a.shape[2]) {\n    [o, i, l] = split$2(a, [1, 1, 1], -1);\n\n    var _e590 = mul(o, .2989),\n        _t416 = mul(i, .587),\n        _n237 = mul(l, .114);\n\n    u = add$2(add$2(_e590, _t416), _n237);\n  } else u = e;\n\n  \"otsu\" === t && (c = otsu(bincount$2(cast$3(round$2(u), \"int32\"), tensor([]), 256), s));\n  var p = n ? lessEqual$2(u, c) : greater$3(u, c);\n  return cast$3(mul(p, 255), \"int32\");\n}\n\nfunction otsu(e, t) {\n  var n,\n      r,\n      a,\n      s,\n      o,\n      i,\n      l = tensor1d([-1]),\n      u = tensor1d([0]),\n      c = tensor1d([0]);\n\n  for (var _p22 = 0; _p22 < e.size - 1; _p22++) {\n    n = slice$2(e, 0, _p22 + 1), r = slice$2(e, _p22 + 1), o = div$1(sum$2(n), t), i = div$1(sum$2(r), t);\n    var d = sum$2(mul(n, range$4(0, n.size)));\n    a = div$1(d, sum$2(n));\n    var h = fill$2(r.shape, n.size),\n        m = add$2(range$4(0, r.size), h),\n        f = mul(r, m);\n    s = div$1(sum$2(f), sum$2(r));\n    var g = sub$2(a, s),\n        $ = sub$2(a, s),\n        y = mul(o, i);\n    c = mul(mul(y, g), $);\n    var b = greater$3(c, u);\n    u = where(b, c, u), l = where(b, tensor1d([_p22]), l);\n  }\n\n  return l;\n}\n\nvar threshold$1 = op({\n  threshold_\n});\n\nfunction transform_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"nearest\";\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"constant\";\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = convertToTensor(e, \"image\", \"transform\", \"float32\"),\n      i = convertToTensor(t, \"transforms\", \"transform\", \"float32\");\n  return assert$4(4 === o.rank, () => \"Error in transform: image must be rank 4,but got rank \".concat(o.rank, \".\")), assert$4(2 === i.rank && (i.shape[0] === o.shape[0] || 1 === i.shape[0]) && 8 === i.shape[1], () => \"Error in transform: Input transform should be batch x 8 or 1 x 8\"), assert$4(null == s || 2 === s.length, () => \"Error in transform: outputShape must be [height, width] or null, but got \".concat(s, \".\")), ENGINE.runKernel(Transform, {\n    image: o,\n    transforms: i\n  }, {\n    interpolation: n,\n    fillMode: r,\n    fillValue: a,\n    outputShape: s\n  });\n}\n\nvar transform$2 = op({\n  transform_\n});\n\nfunction bandPart_(e, t, n) {\n  assert$4(t % 1 == 0, () => \"bandPart(): numLower must be an integer, got \".concat(t, \".\")), assert$4(n % 1 == 0, () => \"bandPart(): numUpper must be an integer, got \".concat(n, \".\"));\n  var r = convertToTensor(e, \"a\", \"bandPart\");\n  assert$4(r.rank >= 2, () => \"bandPart(): Rank must be at least 2, got \".concat(r.rank, \".\"));\n  var a = r.shape,\n      [s, o] = r.shape.slice(-2);\n  if (!(t <= s)) throw new Error(\"bandPart(): numLower (\".concat(t, \") must not be greater than the number of rows (\").concat(s, \").\"));\n  if (!(n <= o)) throw new Error(\"bandPart(): numUpper (\".concat(n, \") must not be greater than the number of columns (\").concat(o, \").\"));\n  t < 0 && (t = s), n < 0 && (n = o);\n  var i = reshape$3(range$4(0, s, 1, \"int32\"), [-1, 1]),\n      l = range$4(0, o, 1, \"int32\"),\n      u = sub$2(i, l),\n      c = logicalAnd$2(lessEqual$2(u, scalar(+t, \"int32\")), greaterEqual$2(u, scalar(-n, \"int32\"))),\n      p = zeros$2([s, o], r.dtype);\n  return reshape$3(stack(unstack(reshape$3(r, [-1, s, o])).map(e => where(c, e, p))), a);\n}\n\nvar bandPart = op({\n  bandPart_\n});\n\nfunction gramSchmidt_(e) {\n  var t;\n\n  if (Array.isArray(e)) {\n    (function () {\n      t = !1, assert$4(null != e && e.length > 0, () => \"Gram-Schmidt process: input must not be null, undefined, or empty\");\n      var n = e[0].shape[0];\n\n      var _loop33 = function _loop33(_t417) {\n        assert$4(e[_t417].shape[0] === n, () => \"Gram-Schmidt: Non-unique lengths found in the input vectors: (\".concat(e[_t417].shape[0], \" vs. \").concat(n, \")\"));\n      };\n\n      for (var _t417 = 1; _t417 < e.length; ++_t417) {\n        _loop33(_t417);\n      }\n    })();\n  } else t = !0, e = split$2(e, e.shape[0], 0).map(e => squeeze(e, [0]));\n\n  assert$4(e.length <= e[0].shape[0], () => \"Gram-Schmidt: Number of vectors (\".concat(e.length, \") exceeds number of dimensions (\").concat(e[0].shape[0], \").\"));\n  var n = [],\n      r = e;\n\n  var _loop34 = function _loop34(_t418) {\n    n.push(ENGINE.tidy(() => {\n      var e = r[_t418];\n      if (_t418 > 0) for (var _r200 = 0; _r200 < _t418; ++_r200) {\n        var _t419 = mul(sum$2(mul(n[_r200], e)), n[_r200]);\n\n        e = sub$2(e, _t419);\n      }\n      return div$1(e, norm(e, \"euclidean\"));\n    }));\n  };\n\n  for (var _t418 = 0; _t418 < e.length; ++_t418) {\n    _loop34(_t418);\n  }\n\n  return t ? stack(n, 0) : n;\n}\n\nvar gramSchmidt = op({\n  gramSchmidt_\n});\n\nfunction qr_(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  if (assert$4(e.rank >= 2, () => \"qr() requires input tensor to have a rank >= 2, but got rank \".concat(e.rank)), 2 === e.rank) return qr2d(e, t);\n  {\n    var n = e.shape.slice(0, e.shape.length - 2).reduce((e, t) => e * t),\n        r = unstack(reshape$3(e, [n, e.shape[e.shape.length - 2], e.shape[e.shape.length - 1]]), 0),\n        a = [],\n        s = [];\n    return r.forEach(e => {\n      var [n, r] = qr2d(e, t);\n      a.push(n), s.push(r);\n    }), [reshape$3(stack(a, 0), e.shape), reshape$3(stack(s, 0), e.shape)];\n  }\n}\n\nfunction qr2d(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  return ENGINE.tidy(() => {\n    assert$4(2 === e.shape.length, () => \"qr2d() requires a 2D Tensor, but got a \".concat(e.shape.length, \"D Tensor.\"));\n    var n = e.shape[0],\n        r = e.shape[1];\n    var a = eye(n),\n        s = clone(e);\n    var o = tensor2d([[1]], [1, 1]);\n    var i = clone(o);\n    var l = n >= r ? r : n;\n\n    var _loop35 = function _loop35(_e591) {\n      var t = s,\n          l = i,\n          u = a;\n      [i, s, a] = ENGINE.tidy(() => {\n        var t = slice$2(s, [_e591, _e591], [n - _e591, 1]),\n            l = norm(t),\n            u = slice$2(s, [_e591, _e591], [1, 1]),\n            c = where(greater$3(u, 0), tensor2d([[-1]]), tensor2d([[1]])),\n            p = sub$2(u, mul(c, l)),\n            d = div$1(t, p);\n        i = 1 === d.shape[0] ? clone(o) : concat$2([o, slice$2(d, [1, 0], [d.shape[0] - 1, d.shape[1]])], 0);\n        var h = neg$2(div$1(matMul$1(c, p), l)),\n            m = slice$2(s, [_e591, 0], [n - _e591, r]),\n            f = mul(h, i),\n            g = transpose$2(i);\n        if (0 === _e591) s = sub$2(m, matMul$1(f, matMul$1(g, m)));else {\n          var _t420 = sub$2(m, matMul$1(f, matMul$1(g, m)));\n\n          s = concat$2([slice$2(s, [0, 0], [_e591, r]), _t420], 0);\n        }\n        var $ = transpose$2(f),\n            y = slice$2(a, [0, _e591], [n, a.shape[1] - _e591]);\n        if (0 === _e591) a = sub$2(y, matMul$1(matMul$1(y, i), $));else {\n          var _t421 = sub$2(y, matMul$1(matMul$1(y, i), $));\n\n          a = concat$2([slice$2(a, [0, 0], [n, _e591]), _t421], 1);\n        }\n        return [i, s, a];\n      }), dispose([t, l, u]);\n    };\n\n    for (var _e591 = 0; _e591 < l; ++_e591) {\n      _loop35(_e591);\n    }\n\n    return !t && n > r && (a = slice$2(a, [0, 0], [n, r]), s = slice$2(s, [0, 0], [r, r])), [a, s];\n  });\n}\n\nvar qr = op({\n  qr_\n});\nvar Reduction;\n\nfunction computeWeightedLoss_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var r = convertToTensor(e, \"losses\", \"computeWeightedLoss\");\n  var a = null;\n  null != t && (a = convertToTensor(t, \"weights\", \"computeWeightedLoss\"));\n  var s = null == a ? r : mul(r, a);\n  if (n === Reduction.NONE) return s;\n  if (n === Reduction.SUM) return sum$2(s);\n\n  if (n === Reduction.MEAN) {\n    if (null == a) return mean$1(s);\n    {\n      var _e592 = r.size / a.size,\n          _t422 = div$1(sum$2(s), sum$2(a));\n\n      return _e592 > 1 ? div$1(_t422, scalar(_e592)) : _t422;\n    }\n  }\n\n  if (n === Reduction.SUM_BY_NONZERO_WEIGHTS) {\n    if (null == a) return div$1(sum$2(s), scalar(r.size));\n    {\n      var _e593 = mul(a, ones$1(r.shape)),\n          _t423 = cast$3(sum$2(notEqual$2(_e593, scalar(0))), \"float32\");\n\n      return div$1(sum$2(s), _t423);\n    }\n  }\n\n  throw Error(\"Unknown reduction: \".concat(n));\n}\n\n!function (e) {\n  e[e.NONE = 0] = \"NONE\", e[e.MEAN = 1] = \"MEAN\", e[e.SUM = 2] = \"SUM\", e[e.SUM_BY_NONZERO_WEIGHTS = 3] = \"SUM_BY_NONZERO_WEIGHTS\";\n}(Reduction || (Reduction = {}));\nvar computeWeightedLoss$1 = op({\n  computeWeightedLoss_\n});\n\nfunction absoluteDifference_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var a = convertToTensor(e, \"labels\", \"absoluteDifference\"),\n      s = convertToTensor(t, \"predictions\", \"absoluteDifference\");\n  var o = null;\n  null != n && (o = convertToTensor(n, \"weights\", \"absoluteDifference\")), assertShapesMatch(a.shape, s.shape, \"Error in absoluteDifference: \");\n  var i = abs$2(sub$2(a, s));\n  return computeWeightedLoss$1(i, o, r);\n}\n\nvar absoluteDifference = op({\n  absoluteDifference_\n});\n\nfunction cosineDistance_(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor(e, \"labels\", \"cosineDistance\"),\n      o = convertToTensor(t, \"predictions\", \"cosineDistance\");\n  var i = null;\n  null != r && (i = convertToTensor(r, \"weights\", \"cosineDistance\")), assertShapesMatch(s.shape, o.shape, \"Error in cosineDistance: \");\n  var l = scalar(1),\n      u = sub$2(l, sum$2(mul(s, o), n, !0));\n  return computeWeightedLoss$1(u, i, a);\n}\n\nvar cosineDistance = op({\n  cosineDistance_\n});\n\nfunction hingeLoss_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var a = convertToTensor(e, \"labels\", \"hingeLoss\");\n  var s = convertToTensor(t, \"predictions\", \"hingeLoss\");\n  var o = null;\n  null != n && (o = convertToTensor(n, \"weights\", \"hingeLoss\")), assertShapesMatch(a.shape, s.shape, \"Error in hingeLoss: \");\n  var i = scalar(1);\n  a = sub$2(mul(scalar(2), a), i);\n  var l = relu$3(sub$2(i, mul(a, s)));\n  return computeWeightedLoss$1(l, o, r);\n}\n\nvar hingeLoss = op({\n  hingeLoss_\n});\n\nfunction huberLoss_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor(e, \"labels\", \"huberLoss\"),\n      o = convertToTensor(t, \"predictions\", \"huberLoss\");\n  var i = null;\n  null != n && (i = convertToTensor(n, \"weights\", \"huberLoss\")), assertShapesMatch(s.shape, o.shape, \"Error in huberLoss: \");\n  var l = scalar(r),\n      u = abs$2(sub$2(o, s)),\n      c = minimum$3(u, l),\n      p = sub$2(u, c),\n      d = add$2(mul(scalar(.5), square$2(c)), mul(l, p));\n  return computeWeightedLoss$1(d, i, a);\n}\n\nvar huberLoss = op({\n  huberLoss_\n});\n\nfunction logLoss_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1e-7;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor(e, \"labels\", \"logLoss\"),\n      o = convertToTensor(t, \"predictions\", \"logLoss\");\n  var i = null;\n  null != n && (i = convertToTensor(n, \"weights\", \"logLoss\")), assertShapesMatch(s.shape, o.shape, \"Error in logLoss: \");\n  var l = scalar(1),\n      u = scalar(r),\n      c = neg$2(mul(s, log$3(add$2(o, u)))),\n      p = mul(sub$2(l, s), log$3(add$2(sub$2(l, o), u))),\n      d = sub$2(c, p);\n  return computeWeightedLoss$1(d, i, a);\n}\n\nvar logLoss = op({\n  logLoss_\n});\n\nfunction meanSquaredError_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var a = convertToTensor(e, \"labels\", \"meanSquaredError\"),\n      s = convertToTensor(t, \"predictions\", \"meanSquaredError\");\n  var o = null;\n  null != n && (o = convertToTensor(n, \"weights\", \"meanSquaredError\")), assertShapesMatch(a.shape, s.shape, \"Error in meanSquaredError: \");\n  var i = squaredDifference$2(a, s);\n  return computeWeightedLoss$1(i, o, r);\n}\n\nvar meanSquaredError$2 = op({\n  meanSquaredError_\n});\n\nfunction sigmoidCrossEntropyWithLogits_(e, t) {\n  var n = convertToTensor(e, \"labels\", \"sigmoidCrossEntropyWithLogits\"),\n      r = convertToTensor(t, \"logits\", \"sigmoidCrossEntropyWithLogits\");\n  assertShapesMatch(n.shape, r.shape, \"Error in sigmoidCrossEntropyWithLogits: \");\n  var a = relu$3(r),\n      s = mul(r, n),\n      o = log1p$2(exp$2(neg$2(abs$2(r))));\n  return add$2(sub$2(a, s), o);\n}\n\nfunction sigmoidCrossEntropy_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor(e, \"multiClassLabels\", \"sigmoidCrossEntropy\");\n  var o = convertToTensor(t, \"logits\", \"sigmoidCrossEntropy\");\n  var i = null;\n\n  if (null != n && (i = convertToTensor(n, \"weights\", \"sigmoidCrossEntropy\")), assertShapesMatch(s.shape, o.shape, \"Error in sigmoidCrossEntropy: \"), r > 0) {\n    var _e594 = scalar(r),\n        _t424 = scalar(1),\n        _n238 = scalar(.5);\n\n    s = add$2(mul(s, sub$2(_t424, _e594)), mul(_n238, _e594));\n  }\n\n  var l = sigmoidCrossEntropyWithLogits_(s, o);\n  return computeWeightedLoss$1(l, i, a);\n}\n\nvar sigmoidCrossEntropy = op({\n  sigmoidCrossEntropy_\n});\n\nfunction softmaxCrossEntropyWithLogits_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : -1;\n  if (-1 === n && (n = t.rank - 1), n !== t.rank - 1) throw Error(\"Softmax cross entropy along a non-last dimension is not yet supported. Labels / logits was rank \".concat(t.rank, \" and dim was \").concat(n));\n  return customGrad((e, t, r) => {\n    var a = logSumExp(t, [n], !0),\n        s = sub$2(cast$3(t, \"float32\"), a);\n    r([e, s]);\n    var o = neg$2(mul(s, e));\n    return {\n      value: sum$2(o, [n]),\n      gradFunc: (e, t) => {\n        var [r, a] = t,\n            s = expandShapeToKeepDim(e.shape, [n]);\n        return [mul(reshape$3(e, s), sub$2(cast$3(r, \"float32\"), exp$2(a))), mul(reshape$3(e, s), sub$2(exp$2(a), cast$3(r, \"float32\")))];\n      }\n    };\n  })(e, t);\n}\n\nfunction softmaxCrossEntropy_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var s = convertToTensor(e, \"onehotLabels\", \"softmaxCrossEntropy\");\n  var o = convertToTensor(t, \"logits\", \"softmaxCrossEntropy\");\n  var i = null;\n\n  if (null != n && (i = convertToTensor(n, \"weights\", \"softmaxCrossEntropy\")), assertShapesMatch(s.shape, o.shape, \"Error in softmaxCrossEntropy: \"), r > 0) {\n    var _e595 = scalar(r),\n        _t425 = scalar(1),\n        _n239 = scalar(s.shape[1]);\n\n    s = add$2(mul(s, sub$2(_t425, _e595)), div$1(_e595, _n239));\n  }\n\n  var l = softmaxCrossEntropyWithLogits_(s, o);\n  return computeWeightedLoss$1(l, i, a);\n}\n\nvar softmaxCrossEntropy = op({\n  softmaxCrossEntropy_\n});\n\nfunction sparseFillEmptyRows_(e, t, n, r) {\n  var a = convertToTensor(e, \"indices\", \"sparseFillEmptyRows\"),\n      s = convertToTensor(t, \"values\", \"sparseFillEmptyRows\"),\n      o = convertToTensor(n, \"denseShape\", \"sparseFillEmptyRows\"),\n      i = convertToTensor(r, \"defaultValue\", \"sparseFillEmptyRows\", s.dtype);\n  if (2 !== a.rank) throw new Error(\"Indices should be Tensor2D but received shape\\n        \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"Values should be Tensor1D but received shape \".concat(s.shape));\n  if (1 !== o.rank) throw new Error(\"Dense shape should be Tensor1D but received shape \".concat(o.shape));\n  if (0 !== i.rank) throw new Error(\"Default value should be a scalar but received shape \".concat(i.shape));\n  var l = ENGINE.runKernel(SparseFillEmptyRows, {\n    indices: a,\n    values: s,\n    denseShape: o,\n    defaultValue: i\n  });\n  return {\n    outputIndices: l[0],\n    outputValues: l[1],\n    emptyRowIndicator: l[2],\n    reverseIndexMap: l[3]\n  };\n}\n\nvar sparseFillEmptyRows$2 = op({\n  sparseFillEmptyRows_\n});\n\nfunction sparseReshape_(e, t, n) {\n  var r = convertToTensor(e, \"inputIndices\", \"sparseReshape\"),\n      a = convertToTensor(t, \"inputShape\", \"sparseReshape\"),\n      s = convertToTensor(n, \"newShape\", \"sparseReshape\");\n  if (2 !== r.rank) throw new Error(\"Input indices should be Tensor2D but received shape\\n        \".concat(r.shape));\n  if (1 !== a.rank) throw new Error(\"Input shape should be Tensor1D but received shape \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"New shape should be Tensor1D but received shape \".concat(s.shape));\n  var o = ENGINE.runKernel(SparseReshape, {\n    inputIndices: r,\n    inputShape: a,\n    newShape: s\n  });\n  return {\n    outputIndices: o[0],\n    outputShape: o[1]\n  };\n}\n\nvar sparseReshape$2 = op({\n  sparseReshape_\n});\n\nfunction sparseSegmentMean_(e, t, n) {\n  var r = convertToTensor(e, \"data\", \"sparseSegmentMean\"),\n      a = convertToTensor(t, \"indices\", \"sparseSegmentMean\"),\n      s = convertToTensor(n, \"segmentIds\", \"sparseSegmentMean\");\n  if (r.rank < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.rank) throw new Error(\"Indices should be Tensor1D but received shape\\n          \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"Segment ids should be Tensor1D but received shape\\n          \".concat(s.shape));\n  return ENGINE.runKernel(SparseSegmentMean, {\n    data: r,\n    indices: a,\n    segmentIds: s\n  });\n}\n\nvar sparseSegmentMean$2 = op({\n  sparseSegmentMean_\n});\n\nfunction sparseSegmentSum_(e, t, n) {\n  var r = convertToTensor(e, \"data\", \"sparseSegmentSum\"),\n      a = convertToTensor(t, \"indices\", \"sparseSegmentSum\"),\n      s = convertToTensor(n, \"segmentIds\", \"sparseSegmentSum\");\n  if (r.rank < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.rank) throw new Error(\"Indices should be Tensor1D but received shape\\n         \".concat(a.shape));\n  if (1 !== s.rank) throw new Error(\"Segment ids should be Tensor1D but received shape\\n         \".concat(s.shape));\n  return ENGINE.runKernel(SparseSegmentSum, {\n    data: r,\n    indices: a,\n    segmentIds: s\n  });\n}\n\nvar sparseSegmentSum$2 = op({\n  sparseSegmentSum_\n});\n\nfunction stringNGrams_(e, t, n, r, a, s, o, i) {\n  var l = convertToTensor(e, \"data\", \"stringNGrams\", \"string\");\n  if (\"string\" !== l.dtype) throw new Error(\"Data must be of datatype string\");\n  if (1 !== l.shape.length) throw new Error(\"Data must be a vector, saw: \".concat(l.shape));\n  var u = convertToTensor(t, \"dataSplits\", \"stringNGrams\");\n  if (\"int32\" !== u.dtype) throw new Error(\"Data splits must be of datatype int32\");\n  var c = ENGINE.runKernel(StringNGrams, {\n    data: l,\n    dataSplits: u\n  }, {\n    separator: n,\n    nGramWidths: r,\n    leftPad: a,\n    rightPad: s,\n    padWidth: o,\n    preserveShortSequences: i\n  });\n  return {\n    nGrams: c[0],\n    nGramsSplits: c[1]\n  };\n}\n\nvar stringNGrams$2 = op({\n  stringNGrams_\n});\n\nfunction stringSplit_(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n  var r = convertToTensor(e, \"input\", \"stringSplit\", \"string\"),\n      a = convertToTensor(t, \"delimiter\", \"stringSplit\", \"string\");\n  if (1 !== r.rank) throw new Error(\"Input should be Tensor1D but received shape \".concat(r.shape));\n  if (0 !== a.rank) throw new Error(\"Delimiter should be a scalar but received shape \".concat(a.shape));\n  var s = ENGINE.runKernel(StringSplit, {\n    input: r,\n    delimiter: a\n  }, {\n    skipEmpty: n\n  });\n  return {\n    indices: s[0],\n    values: s[1],\n    shape: s[2]\n  };\n}\n\nvar stringSplit$2 = op({\n  stringSplit_\n});\n\nfunction stringToHashBucketFast_(e, t) {\n  var n = convertToTensor(e, \"input\", \"stringToHashBucketFast\", \"string\"),\n      r = {\n    numBuckets: t\n  };\n  if (t <= 0) throw new Error(\"Number of buckets must be at least 1\");\n  return ENGINE.runKernel(StringToHashBucketFast, {\n    input: n\n  }, r);\n}\n\nvar stringToHashBucketFast$2 = op({\n  stringToHashBucketFast_\n}),\n    spectral$1 = {\n  fft: fft$2,\n  ifft: ifft$2,\n  rfft,\n  irfft\n},\n    signal = {\n  hammingWindow,\n  hannWindow,\n  frame,\n  stft\n},\n    image$1 = {\n  flipLeftRight,\n  resizeNearestNeighbor: resizeNearestNeighbor$2,\n  resizeBilinear: resizeBilinear$2,\n  rotateWithOffset,\n  cropAndResize: cropAndResize$2,\n  nonMaxSuppression,\n  nonMaxSuppressionAsync,\n  nonMaxSuppressionWithScore,\n  nonMaxSuppressionWithScoreAsync,\n  nonMaxSuppressionPadded,\n  nonMaxSuppressionPaddedAsync,\n  threshold: threshold$1,\n  transform: transform$2\n},\n    linalg = {\n  bandPart,\n  gramSchmidt,\n  qr\n},\n    losses = {\n  absoluteDifference,\n  computeWeightedLoss: computeWeightedLoss$1,\n  cosineDistance,\n  hingeLoss,\n  huberLoss,\n  logLoss,\n  meanSquaredError: meanSquaredError$2,\n  sigmoidCrossEntropy,\n  softmaxCrossEntropy\n},\n    sparse$1 = {\n  sparseFillEmptyRows: sparseFillEmptyRows$2,\n  sparseReshape: sparseReshape$2,\n  sparseSegmentMean: sparseSegmentMean$2,\n  sparseSegmentSum: sparseSegmentSum$2\n},\n    string$1 = {\n  stringNGrams: stringNGrams$2,\n  stringSplit: stringSplit$2,\n  stringToHashBucketFast: stringToHashBucketFast$2\n};\n\nclass Optimizer extends Serializable {\n  minimize(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 ? arguments[2] : undefined;\n    var {\n      value: r,\n      grads: a\n    } = this.computeGradients(e, n);\n\n    if (null != n) {\n      var _e596 = n.map(e => ({\n        name: e.name,\n        tensor: a[e.name]\n      }));\n\n      this.applyGradients(_e596);\n    } else this.applyGradients(a);\n\n    return dispose(a), t ? r : (r.dispose(), null);\n  }\n\n  get iterations() {\n    return null == this.iterations_ && (this.iterations_ = 0), this.iterations_;\n  }\n\n  incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n\n  computeGradients(e, t) {\n    return variableGrads(e, t);\n  }\n\n  dispose() {\n    null != this.iterations_ && dispose(this.iterations_);\n  }\n\n  saveIterations() {\n    var _this101 = this;\n\n    return _asyncToGenerator(function* () {\n      return null == _this101.iterations_ && (_this101.iterations_ = 0), {\n        name: \"iter\",\n        tensor: scalar(_this101.iterations_, \"int32\")\n      };\n    })();\n  }\n\n  getWeights() {\n    return _asyncToGenerator(function* () {\n      throw new Error(\"getWeights() is not implemented for this optimizer yet.\");\n    })();\n  }\n\n  setWeights(e) {\n    var _this102 = this;\n\n    return _asyncToGenerator(function* () {\n      throw new Error(\"setWeights() is not implemented for this optimizer class \".concat(_this102.getClassName()));\n    })();\n  }\n\n  extractIterations(e) {\n    var _this103 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this103.iterations_ = (yield e[0].tensor.data())[0], e.slice(1);\n    })();\n  }\n\n}\n\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: e => null != e.minimize && null != e.computeGradients && null != e.applyGradients\n});\n\nclass AdadeltaOptimizer extends Optimizer {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    super(), this.learningRate = e, this.rho = t, this.epsilon = n, this.accumulatedGrads = [], this.accumulatedUpdates = [], null == n && (this.epsilon = ENGINE.backend.epsilon());\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE.registeredVariables[t];\n      null == this.accumulatedGrads[n] && (this.accumulatedGrads[n] = {\n        originalName: \"\".concat(t, \"/accum_grad\"),\n        variable: tidy(() => zerosLike$2(r).variable(!1))\n      }), null == this.accumulatedUpdates[n] && (this.accumulatedUpdates[n] = {\n        originalName: \"\".concat(t, \"/accum_var\"),\n        variable: tidy(() => zerosLike$2(r).variable(!1))\n      });\n      var a = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == a) return;\n      var s = this.accumulatedGrads[n].variable,\n          o = this.accumulatedUpdates[n].variable;\n      tidy(() => {\n        var e = add$2(mul(s, this.rho), mul(square$2(a), 1 - this.rho)),\n            t = mul(div$1(sqrt$2(add$2(o, this.epsilon)), sqrt$2(add$2(s, this.epsilon))), a),\n            n = add$2(mul(o, this.rho), mul(square$2(t), 1 - this.rho));\n        s.assign(e), o.assign(n);\n        var i = add$2(mul(t, -this.learningRate), r);\n        r.assign(i);\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    null != this.accumulatedUpdates && (dispose(this.accumulatedGrads.map(e => e.variable)), dispose(this.accumulatedUpdates.map(e => e.variable)));\n  }\n\n  getWeights() {\n    var _this104 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [..._this104.accumulatedGrads, ..._this104.accumulatedUpdates];\n      return [yield _this104.saveIterations()].concat(e.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this105 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = (e = yield _this105.extractIterations(e)).length / 2;\n      _this105.accumulatedGrads = e.slice(0, t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      })), _this105.accumulatedUpdates = e.slice(t, 2 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      rho: this.rho,\n      epsilon: this.epsilon\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.rho, t.epsilon);\n  }\n\n}\n\nAdadeltaOptimizer.className = \"Adadelta\", registerClass(AdadeltaOptimizer);\n\nclass AdagradOptimizer extends Optimizer {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .1;\n    super(), this.learningRate = e, this.initialAccumulatorValue = t, this.accumulatedGrads = [];\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE.registeredVariables[t];\n\n      if (null == this.accumulatedGrads[n]) {\n        var _e597 = !1;\n\n        this.accumulatedGrads[n] = {\n          originalName: \"\".concat(t, \"/accumulator\"),\n          variable: tidy(() => fill$2(r.shape, this.initialAccumulatorValue).variable(_e597))\n        };\n      }\n\n      var a = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == a) return;\n      var s = this.accumulatedGrads[n].variable;\n      tidy(() => {\n        var e = add$2(s, square$2(a));\n        s.assign(e);\n        var t = add$2(mul(div$1(a, sqrt$2(add$2(e, ENGINE.backend.epsilon()))), -this.learningRate), r);\n        r.assign(t);\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    null != this.accumulatedGrads && dispose(this.accumulatedGrads.map(e => e.variable));\n  }\n\n  getWeights() {\n    var _this106 = this;\n\n    return _asyncToGenerator(function* () {\n      return [yield _this106.saveIterations()].concat(_this106.accumulatedGrads.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this107 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this107.extractIterations(e), _this107.accumulatedGrads = e.map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      initialAccumulatorValue: this.initialAccumulatorValue\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.initialAccumulatorValue);\n  }\n\n}\n\nAdagradOptimizer.className = \"Adagrad\", registerClass(AdagradOptimizer);\n\nclass AdamOptimizer extends Optimizer {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    super(), this.learningRate = e, this.beta1 = t, this.beta2 = n, this.epsilon = r, this.accumulatedFirstMoment = [], this.accumulatedSecondMoment = [], tidy(() => {\n      this.accBeta1 = scalar(t).variable(), this.accBeta2 = scalar(n).variable();\n    }), null == r && (this.epsilon = ENGINE.backend.epsilon());\n  }\n\n  applyGradients(e) {\n    var t = Array.isArray(e) ? e.map(e => e.name) : Object.keys(e);\n    tidy(() => {\n      var n = sub$2(1, this.accBeta1),\n          r = sub$2(1, this.accBeta2);\n      t.forEach((t, a) => {\n        var s = ENGINE.registeredVariables[t];\n        null == this.accumulatedFirstMoment[a] && (this.accumulatedFirstMoment[a] = {\n          originalName: \"\".concat(t, \"/m\"),\n          variable: tidy(() => zerosLike$2(s).variable(!1))\n        }), null == this.accumulatedSecondMoment[a] && (this.accumulatedSecondMoment[a] = {\n          originalName: \"\".concat(t, \"/v\"),\n          variable: tidy(() => zerosLike$2(s).variable(!1))\n        });\n        var o = Array.isArray(e) ? e[a].tensor : e[t];\n        if (null == o) return;\n        var i = this.accumulatedFirstMoment[a].variable,\n            l = this.accumulatedSecondMoment[a].variable,\n            u = add$2(mul(i, this.beta1), mul(o, 1 - this.beta1)),\n            c = add$2(mul(l, this.beta2), mul(square$2(o), 1 - this.beta2)),\n            p = div$1(u, n),\n            d = div$1(c, r);\n        i.assign(u), l.assign(c);\n        var h = add$2(mul(div$1(p, add$2(sqrt$2(d), this.epsilon)), -this.learningRate), s);\n        s.assign(h);\n      }), this.accBeta1.assign(mul(this.accBeta1, this.beta1)), this.accBeta2.assign(mul(this.accBeta2, this.beta2));\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    this.accBeta1.dispose(), this.accBeta2.dispose(), null != this.accumulatedFirstMoment && dispose(this.accumulatedFirstMoment.map(e => e.variable)), null != this.accumulatedSecondMoment && dispose(this.accumulatedSecondMoment.map(e => e.variable));\n  }\n\n  getWeights() {\n    var _this108 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [..._this108.accumulatedFirstMoment, ..._this108.accumulatedSecondMoment];\n      return [yield _this108.saveIterations()].concat(e.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this109 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this109.extractIterations(e), tidy(() => {\n        _this109.accBeta1.assign(pow$2(_this109.beta1, _this109.iterations_ + 1)), _this109.accBeta2.assign(pow$2(_this109.beta2, _this109.iterations_ + 1));\n      });\n      var t = e.length / 2;\n      _this109.accumulatedFirstMoment = e.slice(0, t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      })), _this109.accumulatedSecondMoment = e.slice(t, 2 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      beta1: this.beta1,\n      beta2: this.beta2,\n      epsilon: this.epsilon\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.beta1, t.beta2, t.epsilon);\n  }\n\n}\n\nAdamOptimizer.className = \"Adam\", registerClass(AdamOptimizer);\n\nclass AdamaxOptimizer extends Optimizer {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n    super(), this.learningRate = e, this.beta1 = t, this.beta2 = n, this.epsilon = r, this.decay = a, this.accumulatedFirstMoment = [], this.accumulatedWeightedInfNorm = [], tidy(() => {\n      this.iteration = scalar(0).variable(), this.accBeta1 = scalar(t).variable();\n    }), null == r && (this.epsilon = ENGINE.backend.epsilon());\n  }\n\n  applyGradients(e) {\n    var t = Array.isArray(e) ? e.map(e => e.name) : Object.keys(e);\n    tidy(() => {\n      var n = sub$2(1, this.accBeta1),\n          r = div$1(-this.learningRate, add$2(mul(this.iteration, this.decay), 1));\n      t.forEach((t, a) => {\n        var s = ENGINE.registeredVariables[t];\n        null == this.accumulatedFirstMoment[a] && (this.accumulatedFirstMoment[a] = {\n          originalName: \"\".concat(t, \"/m\"),\n          variable: zerosLike$2(s).variable(!1)\n        }), null == this.accumulatedWeightedInfNorm[a] && (this.accumulatedWeightedInfNorm[a] = {\n          originalName: \"\".concat(t, \"/v\"),\n          variable: zerosLike$2(s).variable(!1)\n        });\n        var o = Array.isArray(e) ? e[a].tensor : e[t];\n        if (null == o) return;\n        var i = this.accumulatedFirstMoment[a].variable,\n            l = this.accumulatedWeightedInfNorm[a].variable,\n            u = add$2(mul(i, this.beta1), mul(o, 1 - this.beta1)),\n            c = mul(l, this.beta2),\n            p = abs$2(o),\n            d = maximum$3(c, p);\n        i.assign(u), l.assign(d);\n        var h = add$2(mul(div$1(r, n), div$1(u, add$2(d, this.epsilon))), s);\n        s.assign(h);\n      }), this.iteration.assign(add$2(this.iteration, 1)), this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    this.accBeta1.dispose(), this.iteration.dispose(), null != this.accumulatedFirstMoment && dispose(this.accumulatedFirstMoment.map(e => e.variable)), null != this.accumulatedWeightedInfNorm && dispose(this.accumulatedWeightedInfNorm.map(e => e.variable));\n  }\n\n  getWeights() {\n    return _asyncToGenerator(function* () {\n      throw new Error(\"getWeights() is not implemented for Adamax yet.\");\n    })();\n  }\n\n  setWeights(e) {\n    return _asyncToGenerator(function* () {\n      throw new Error(\"setWeights() is not implemented for Adamax yet.\");\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      beta1: this.beta1,\n      beta2: this.beta2,\n      epsilon: this.epsilon,\n      decay: this.decay\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.beta1, t.beta2, t.epsilon, t.decay);\n  }\n\n}\n\nAdamaxOptimizer.className = \"Adamax\", registerClass(AdamaxOptimizer);\n\nclass SGDOptimizer extends Optimizer {\n  constructor(e) {\n    super(), this.learningRate = e, this.setLearningRate(e);\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == r) return;\n      var a = ENGINE.registeredVariables[t];\n      tidy(() => {\n        var e = add$2(mul(this.c, r), a);\n        a.assign(e);\n      });\n    }), this.incrementIterations();\n  }\n\n  setLearningRate(e) {\n    this.learningRate = e, null != this.c && this.c.dispose(), this.c = keep(scalar(-e));\n  }\n\n  dispose() {\n    this.c.dispose();\n  }\n\n  getWeights() {\n    var _this110 = this;\n\n    return _asyncToGenerator(function* () {\n      return [yield _this110.saveIterations()];\n    })();\n  }\n\n  setWeights(e) {\n    var _this111 = this;\n\n    return _asyncToGenerator(function* () {\n      if (0 !== (e = yield _this111.extractIterations(e)).length) throw new Error(\"SGD optimizer does not have settable weights.\");\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate);\n  }\n\n}\n\nSGDOptimizer.className = \"SGD\", registerClass(SGDOptimizer);\n\nclass MomentumOptimizer extends SGDOptimizer {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    super(e), this.learningRate = e, this.momentum = t, this.useNesterov = n, this.accumulations = [], this.m = scalar(this.momentum);\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE.registeredVariables[t];\n\n      if (null == this.accumulations[n]) {\n        var _e598 = !1;\n\n        this.accumulations[n] = {\n          originalName: \"\".concat(t, \"/momentum\"),\n          variable: tidy(() => zerosLike$2(r).variable(_e598))\n        };\n      }\n\n      var a = this.accumulations[n].variable,\n          s = Array.isArray(e) ? e[n].tensor : e[t];\n      null != s && tidy(() => {\n        var e;\n        var t = add$2(mul(this.m, a), s);\n        e = add$2(mul(this.c, this.useNesterov ? add$2(s, mul(t, this.m)) : t), r), a.assign(t), r.assign(e);\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    this.m.dispose(), null != this.accumulations && dispose(this.accumulations.map(e => e.variable));\n  }\n\n  setMomentum(e) {\n    this.momentum = e;\n  }\n\n  getWeights() {\n    var _this112 = this;\n\n    return _asyncToGenerator(function* () {\n      return [yield _this112.saveIterations()].concat(_this112.accumulations.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this113 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this113.extractIterations(e), _this113.accumulations = e.map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(!1)\n      }));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      momentum: this.momentum,\n      useNesterov: this.useNesterov\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.momentum, t.useNesterov);\n  }\n\n}\n\nMomentumOptimizer.className = \"Momentum\", registerClass(MomentumOptimizer);\n\nclass RMSPropOptimizer extends Optimizer {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    if (super(), this.learningRate = e, this.decay = t, this.momentum = n, this.epsilon = r, this.accumulatedMeanSquares = [], this.accumulatedMoments = [], this.accumulatedMeanGrads = [], this.centered = a, null == r && (this.epsilon = ENGINE.backend.epsilon()), null == e) throw new Error(\"learningRate for RMSPropOptimizer must be defined.\");\n  }\n\n  applyGradients(e) {\n    (Array.isArray(e) ? e.map(e => e.name) : Object.keys(e)).forEach((t, n) => {\n      var r = ENGINE.registeredVariables[t],\n          a = !1;\n      null == this.accumulatedMeanSquares[n] && (this.accumulatedMeanSquares[n] = {\n        originalName: \"\".concat(t, \"/rms\"),\n        variable: tidy(() => zerosLike$2(r).variable(a))\n      }), null == this.accumulatedMoments[n] && (this.accumulatedMoments[n] = {\n        originalName: \"\".concat(t, \"/momentum\"),\n        variable: tidy(() => zerosLike$2(r).variable(a))\n      }), null == this.accumulatedMeanGrads[n] && this.centered && (this.accumulatedMeanGrads[n] = {\n        originalName: \"\".concat(t, \"/mg\"),\n        variable: tidy(() => zerosLike$2(r).variable(a))\n      });\n      var s = Array.isArray(e) ? e[n].tensor : e[t];\n      if (null == s) return;\n      var o = this.accumulatedMeanSquares[n].variable,\n          i = this.accumulatedMoments[n].variable;\n      tidy(() => {\n        var e = add$2(mul(o, this.decay), mul(square$2(s), 1 - this.decay));\n\n        if (this.centered) {\n          var _t426 = this.accumulatedMeanGrads[n].variable,\n              _a130 = add$2(mul(_t426, this.decay), mul(s, 1 - this.decay)),\n              l = div$1(mul(s, this.learningRate), sqrt$2(sub$2(e, add$2(square$2(_a130), this.epsilon)))),\n              u = add$2(mul(i, this.momentum), l);\n\n          o.assign(e), _t426.assign(_a130), i.assign(u);\n          var c = sub$2(r, u);\n          r.assign(c);\n        } else {\n          var _e599 = add$2(mul(o, this.decay), mul(square$2(s), 1 - this.decay)),\n              _t427 = add$2(mul(i, this.momentum), div$1(mul(s, this.learningRate), sqrt$2(add$2(_e599, this.epsilon))));\n\n          o.assign(_e599), i.assign(_t427);\n\n          var _n240 = sub$2(r, _t427);\n\n          r.assign(_n240);\n        }\n      });\n    }), this.incrementIterations();\n  }\n\n  dispose() {\n    null != this.accumulatedMeanSquares && dispose(this.accumulatedMeanSquares.map(e => e.variable)), null != this.accumulatedMeanGrads && this.centered && dispose(this.accumulatedMeanGrads.map(e => e.variable)), null != this.accumulatedMoments && dispose(this.accumulatedMoments.map(e => e.variable));\n  }\n\n  getWeights() {\n    var _this114 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [..._this114.accumulatedMeanSquares, ..._this114.accumulatedMoments];\n      return _this114.centered && e.push(..._this114.accumulatedMeanGrads), [yield _this114.saveIterations()].concat(e.map(e => ({\n        name: e.originalName,\n        tensor: e.variable\n      })));\n    })();\n  }\n\n  setWeights(e) {\n    var _this115 = this;\n\n    return _asyncToGenerator(function* () {\n      e = yield _this115.extractIterations(e);\n      var t = _this115.centered ? e.length / 3 : e.length / 2,\n          n = !1;\n      _this115.accumulatedMeanSquares = e.slice(0, t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(n)\n      })), _this115.accumulatedMoments = e.slice(t, 2 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(n)\n      })), _this115.centered && (_this115.accumulatedMeanGrads = e.slice(2 * t, 3 * t).map(e => ({\n        originalName: e.name,\n        variable: e.tensor.variable(n)\n      })));\n    })();\n  }\n\n  getConfig() {\n    return {\n      learningRate: this.learningRate,\n      decay: this.decay,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      centered: this.centered\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e(t.learningRate, t.decay, t.momentum, t.epsilon, t.centered);\n  }\n\n}\n\nRMSPropOptimizer.className = \"RMSProp\", registerClass(RMSPropOptimizer);\n\nclass OptimizerConstructors {\n  static sgd(e) {\n    return new SGDOptimizer(e);\n  }\n\n  static momentum(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    return new MomentumOptimizer(e, t, n);\n  }\n\n  static rmsprop(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    return new RMSPropOptimizer(e, t, n, r, a);\n  }\n\n  static adam() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .001;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : .999;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    return new AdamOptimizer(e, t, n, r);\n  }\n\n  static adadelta() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .001;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .95;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    return new AdadeltaOptimizer(e, t, n);\n  }\n\n  static adamax() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .002;\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : .999;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0;\n    return new AdamaxOptimizer(e, t, n, r, a);\n  }\n\n  static adagrad(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .1;\n    return new AdagradOptimizer(e, t);\n  }\n\n}\n\nvar train = {\n  sgd: OptimizerConstructors.sgd,\n  momentum: OptimizerConstructors.momentum,\n  adadelta: OptimizerConstructors.adadelta,\n  adagrad: OptimizerConstructors.adagrad,\n  rmsprop: OptimizerConstructors.rmsprop,\n  adamax: OptimizerConstructors.adamax,\n  adam: OptimizerConstructors.adam\n},\n    delayCallback = \"undefined\" != typeof requestAnimationFrame ? requestAnimationFrame : \"undefined\" != typeof setImmediate ? setImmediate : e => e();\n\nfunction nextFrame() {\n  return new Promise(e => delayCallback(() => e()));\n}\n\nfunction assertParamsConsistent(e, t) {\n  var n = e[0].length;\n  e.forEach((e, t) => {\n    assert$4(e.length === n, () => \"Error in concat\".concat(n, \"D: rank of tensors[\").concat(t, \"] must be the same as the rank of the rest (\").concat(n, \")\"));\n  }), assert$4(t >= 0 && t < n, () => \"Error in concat\".concat(n, \"D: axis must be between 0 and \").concat(n - 1, \".\"));\n  var r = e[0];\n  e.forEach((e, a) => {\n    for (var s = 0; s < n; s++) {\n      assert$4(s === t || e[s] === r[s], () => \"Error in concat\".concat(n, \"D: Shape of tensors[\").concat(a, \"] (\").concat(e, \") does not match the shape of the rest (\").concat(r, \") along the non-concatenated axis \").concat(a, \".\"));\n    }\n  });\n}\n\nfunction computeOutShape$1(e, t) {\n  var n = e[0].slice();\n\n  for (var r = 1; r < e.length; r++) {\n    n[t] += e[r][t];\n  }\n\n  return n;\n}\n\nvar PARALLELIZE_THRESHOLD = 30;\n\nfunction computeOptimalWindowSize(e) {\n  return e <= PARALLELIZE_THRESHOLD ? e : nearestDivisor(e, Math.floor(Math.sqrt(e)));\n}\n\nfunction getImageCenter(e, t, n) {\n  return [n * (\"number\" == typeof e ? e : e[0]), t * (\"number\" == typeof e ? e : e[1])];\n}\n\nfunction getReshaped(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = [];\n  if (r) a = a.concat(t.slice(0)), a.push(e[0] / n), a = a.concat(e.slice(1));else {\n    a = a.concat(e[0]);\n    var _n241 = t.length;\n\n    for (var _r201 = 0; _r201 < _n241; ++_r201) {\n      a = a.concat([e[_r201 + 1] / t[_r201], t[_r201]]);\n    }\n\n    a = a.concat(e.slice(_n241 + 1));\n  }\n  return a;\n}\n\nfunction getPermuted(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n  var r = [];\n\n  if (n) {\n    r.push(t);\n\n    for (var _n242 = t + 1; _n242 < e; ++_n242) {\n      _n242 <= 2 * t ? (r.push(_n242), r.push(_n242 - (t + 1))) : r.push(_n242);\n    }\n  } else {\n    var _n243 = [],\n        a = [];\n\n    for (var _r202 = 1; _r202 < e; ++_r202) {\n      _r202 >= 2 * t + 1 || _r202 % 2 == 1 ? a.push(_r202) : _n243.push(_r202);\n    }\n\n    r.push(..._n243), r.push(0), r.push(...a);\n  }\n\n  return r;\n}\n\nfunction getReshapedPermuted(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = [];\n  a.push(r ? e[0] / n : e[0] * n);\n\n  for (var _n244 = 1; _n244 < e.length; ++_n244) {\n    a.push(_n244 <= t.length ? r ? t[_n244 - 1] * e[_n244] : e[_n244] / t[_n244 - 1] : e[_n244]);\n  }\n\n  return a;\n}\n\nfunction getSliceBeginCoords(e, t) {\n  var n = [0];\n\n  for (var r = 0; r < t; ++r) {\n    n.push(e[r][0]);\n  }\n\n  return n;\n}\n\nfunction getSliceSize(e, t, n) {\n  var r = e.slice(0, 1);\n\n  for (var a = 0; a < n; ++a) {\n    r.push(e[a + 1] - t[a][0] - t[a][1]);\n  }\n\n  return r;\n}\n\nvar SELU_SCALEALPHA = 1.7580993408473768,\n    SELU_SCALE = 1.0507009873554805,\n    ERF_P = .3275911,\n    ERF_A1 = .254829592,\n    ERF_A2 = -.284496736,\n    ERF_A3 = 1.421413741,\n    ERF_A4 = -1.453152027,\n    ERF_A5 = 1.061405429;\n\nfunction warn() {\n  env().getBool(\"IS_TEST\") || console.warn(...arguments);\n}\n\nfunction log$2() {\n  env().getBool(\"IS_TEST\") || console.log(...arguments);\n}\n\nfunction mergeRealAndImagArrays(e, t) {\n  if (e.length !== t.length) throw new Error(\"Cannot merge real and imag arrays of different lengths. real:\".concat(e.length, \", imag: \").concat(t.length, \".\"));\n  var n = new Float32Array(2 * e.length);\n\n  for (var r = 0; r < n.length; r += 2) {\n    n[r] = e[r / 2], n[r + 1] = t[r / 2];\n  }\n\n  return n;\n}\n\nfunction splitRealAndImagArrays(e) {\n  var t = new Float32Array(e.length / 2),\n      n = new Float32Array(e.length / 2);\n\n  for (var r = 0; r < e.length; r += 2) {\n    t[r / 2] = e[r], n[r / 2] = e[r + 1];\n  }\n\n  return {\n    real: t,\n    imag: n\n  };\n}\n\nfunction complexWithEvenIndex(e) {\n  var t = Math.ceil(e.length / 4),\n      n = new Float32Array(t),\n      r = new Float32Array(t);\n\n  for (var _t428 = 0; _t428 < e.length; _t428 += 4) {\n    n[Math.floor(_t428 / 4)] = e[_t428], r[Math.floor(_t428 / 4)] = e[_t428 + 1];\n  }\n\n  return {\n    real: n,\n    imag: r\n  };\n}\n\nfunction complexWithOddIndex(e) {\n  var t = Math.floor(e.length / 4),\n      n = new Float32Array(t),\n      r = new Float32Array(t);\n\n  for (var _t429 = 2; _t429 < e.length; _t429 += 4) {\n    n[Math.floor(_t429 / 4)] = e[_t429], r[Math.floor(_t429 / 4)] = e[_t429 + 1];\n  }\n\n  return {\n    real: n,\n    imag: r\n  };\n}\n\nfunction getComplexWithIndex(e, t) {\n  return {\n    real: e[2 * t],\n    imag: e[2 * t + 1]\n  };\n}\n\nfunction assignToTypedArray(e, t, n, r) {\n  e[2 * r] = t, e[2 * r + 1] = n;\n}\n\nfunction exponents(e, t) {\n  var n = new Float32Array(e / 2),\n      r = new Float32Array(e / 2);\n\n  for (var a = 0; a < Math.ceil(e / 2); a++) {\n    var s = (t ? 2 : -2) * Math.PI * (a / e);\n    n[a] = Math.cos(s), r[a] = Math.sin(s);\n  }\n\n  return {\n    real: n,\n    imag: r\n  };\n}\n\nfunction exponent(e, t, n) {\n  var r = (n ? 2 : -2) * Math.PI * (e / t);\n  return {\n    real: Math.cos(r),\n    imag: Math.sin(r)\n  };\n}\n\nvar ARROW = \"->\",\n    ARROW_REGEX = /->/g,\n    COMMA = \",\",\n    ELLIPSIS = \"...\";\n\nfunction decodeEinsumEquation(e, t) {\n  var n = ((e = e.replace(/\\s/g, \"\")).length - e.replace(ARROW_REGEX, \"\").length) / ARROW.length;\n  if (n < 1) throw new Error(\"Equations without an arrow are not supported.\");\n  if (n > 1) throw new Error(\"Equation must contain exactly one arrow (\\\"\".concat(ARROW, \"\\\").\"));\n  var [r, a] = e.split(ARROW);\n  assert$4(-1 === r.indexOf(ELLIPSIS), () => \"The ellipsis notation (\\\"\".concat(ELLIPSIS, \"\\\") is not supported yet.\"));\n  var s = r.split(COMMA),\n      o = s.length;\n  if (t !== o) throw new Error(\"Expected \".concat(o, \" input tensors, received \").concat(t));\n  if (o > 2) throw new Error(\"Support for more than 2 input tensors is not implemented yet.\");\n  var i = [];\n\n  var _loop36 = function _loop36(_e600) {\n    var t = a[_e600];\n    if (!s.some(e => -1 !== e.indexOf(t))) throw new Error(\"Output subscripts contain the label \".concat(t, \" not present in the input subscripts.\"));\n    -1 === i.indexOf(t) && i.push(t);\n  };\n\n  for (var _e600 = 0; _e600 < a.length; ++_e600) {\n    _loop36(_e600);\n  }\n\n  for (var _e601 = 0; _e601 < r.length; ++_e601) {\n    var _t430 = r[_e601];\n    -1 === i.indexOf(_t430) && _t430 !== COMMA && i.push(_t430);\n  }\n\n  var l = new Array(s.length);\n\n  for (var _e602 = 0; _e602 < o; ++_e602) {\n    if (new Set(s[_e602].split(\"\")).size !== s[_e602].length) throw new Error(\"Found duplicate axes in input component \".concat(s[_e602], \". Support for duplicate axes in input is not implemented yet.\"));\n    l[_e602] = [];\n\n    for (var _t431 = 0; _t431 < s[_e602].length; ++_t431) {\n      l[_e602].push(i.indexOf(s[_e602][_t431]));\n    }\n  }\n\n  var u = i.length,\n      c = [];\n\n  for (var _e603 = a.length; _e603 < u; ++_e603) {\n    c.push(_e603);\n  }\n\n  return {\n    allDims: i,\n    summedDims: c,\n    idDims: l\n  };\n}\n\nfunction getEinsumPermutation(e, t) {\n  var n = new Array(e);\n  n.fill(-1);\n\n  for (var _e604 = 0; _e604 < t.length; ++_e604) {\n    n[t[_e604]] = _e604;\n  }\n\n  var r = [];\n\n  for (var _t432 = 0; _t432 < e; ++_t432) {\n    -1 === n[_t432] && r.push(_t432);\n  }\n\n  return n = n.filter(e => -1 !== e), {\n    permutationIndices: n,\n    expandDims: r\n  };\n}\n\nfunction checkEinsumDimSizes(e, t, n) {\n  var r = new Array(e);\n\n  var _loop37 = function _loop37(_e605) {\n    var a = n[_e605].shape;\n\n    var _loop38 = function _loop38(_n245) {\n      void 0 === r[t[_e605][_n245]] ? r[t[_e605][_n245]] = a[_n245] : assert$4(r[t[_e605][_n245]] === a[_n245], () => \"Expected dimension \".concat(r[t[_e605][_n245]], \" at axis \").concat(_n245, \" of input shaped \").concat(JSON.stringify(a), \", but got dimension \").concat(a[_n245]));\n    };\n\n    for (var _n245 = 0; _n245 < t[_e605].length; ++_n245) {\n      _loop38(_n245);\n    }\n  };\n\n  for (var _e605 = 0; _e605 < n.length; ++_e605) {\n    _loop37(_e605);\n  }\n}\n\nfunction getEinsumComputePath(e, t) {\n  var n = e,\n      r = [];\n  var a = 0;\n  0 === e.length && n.push(-1), a = e.length + 1;\n\n  for (var _e606 = 0; _e606 < a; ++_e606) {\n    r.push([]);\n  }\n\n  var s = [];\n\n  for (var _e607 = 0; _e607 < n.length; ++_e607) {\n    var _a131 = findTermsWithDim(t, n[_e607]);\n\n    for (var _t433 of _a131) {\n      -1 === s.indexOf(_t433) && (r[_e607].push(_t433), s.push(_t433));\n    }\n  }\n\n  return {\n    path: n,\n    steps: r\n  };\n}\n\nfunction isIdentityPermutation(e) {\n  return e.every((e, t) => e === t);\n}\n\nfunction findTermsWithDim(e, t) {\n  var n = [];\n\n  for (var r = 0; r < e.length; ++r) {\n    0 !== e[r].length && -1 === e[r].indexOf(t) && -1 !== t || n.push(r);\n  }\n\n  return n;\n}\n\nfunction prepareSplitSize(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = [];\n  if (\"number\" == typeof t) assert$4(e.shape[n] % t == 0, () => \"Number of splits must evenly divide the axis.\"), r = new Array(t).fill(e.shape[n] / t);else {\n    assert$4(t.reduce((e, t) => (-1 === t && (e += 1), e), 0) <= 1, () => \"There should be only one negative value in split array.\");\n    var a = t.indexOf(-1);\n\n    if (-1 !== a) {\n      var _r203 = t.reduce((e, t) => t > 0 ? e + t : e);\n\n      t[a] = e.shape[n] - _r203;\n    }\n\n    assert$4(e.shape[n] === t.reduce((e, t) => e + t), () => \"The sum of sizes must match the size of the axis dimension.\"), r = t;\n  }\n  return r;\n}\n\nfunction segOpComputeOptimalWindowSize(e, t) {\n  var n,\n      r = !1;\n\n  for (e <= PARALLELIZE_THRESHOLD ? (n = e, r = !0) : n = nearestDivisor(e, Math.floor(Math.sqrt(e))); !r;) {\n    n > t || n === e ? r = !0 : n = nearestDivisor(e, n + 1);\n  }\n\n  return n;\n}\n\nfunction computeOutShape(e, t, n) {\n  var r = [],\n      a = e.length;\n\n  for (var s = 0; s < a; s++) {\n    r.push(s !== t ? e[s] : n);\n  }\n\n  return r;\n}\n\nfunction collectGatherOpShapeInfo(e, t, n, r) {\n  var a = t.shape.length,\n      s = e.shape.length;\n  if (0 !== r && (r < -a || r > a)) throw new Error(\"Expect batchDims in the range of [-\".concat(a, \", \").concat(a, \"], but got \").concat(r));\n  if (r < 0 && (r += a), r > s) throw new Error(\"batchDims (\".concat(r, \") must be less than rank(x) (\\n    \").concat(s, \").\"));\n  if (n < r) throw new Error(\"batchDims (\".concat(r, \") must be less than or equal to axis (\").concat(n, \").\"));\n\n  for (var _n246 = 0; _n246 < r; ++_n246) {\n    if (e.shape[_n246] !== t.shape[_n246]) throw new Error(\"x.shape[\".concat(_n246, \"]: \").concat(e.shape[_n246], \" should be equal to indices.shape[\").concat(_n246, \"]: \").concat(t.shape[_n246], \".\"));\n  }\n\n  var o = e.shape[n],\n      i = [];\n  var l = 1,\n      u = 1,\n      c = 1;\n\n  for (var _t434 = 0; _t434 < r; ++_t434) {\n    i.push(e.shape[_t434]), l *= e.shape[_t434];\n  }\n\n  for (var _t435 = r; _t435 < n; _t435++) {\n    i.push(e.shape[_t435]), u *= e.shape[_t435];\n  }\n\n  for (var _e608 = r; _e608 < a; _e608++) {\n    i.push(t.shape[_e608]);\n  }\n\n  for (var _t436 = n + 1; _t436 < s; _t436++) {\n    i.push(e.shape[_t436]), c *= e.shape[_t436];\n  }\n\n  return {\n    batchSize: l,\n    sliceSize: c,\n    outerSize: u,\n    dimSize: o,\n    outputShape: i\n  };\n}\n\nvar segment_util = {\n  __proto__: null,\n  segOpComputeOptimalWindowSize,\n  computeOutShape,\n  collectGatherOpShapeInfo\n};\n\nfunction fromUint8ToStringArray(e) {\n  try {\n    return e.map(e => decodeString(e));\n  } catch (e) {\n    throw new Error(\"Failed to decode encoded string bytes into utf-8, error: \".concat(e));\n  }\n}\n\nfunction fromStringArrayToUint8(e) {\n  return e.map(e => encodeString(e));\n}\n\nvar backend_util = {\n  __proto__: null,\n  slice_util,\n  segment_util,\n  fromUint8ToStringArray,\n  fromStringArrayToUint8,\n  upcastType,\n  axesAreInnerMostDims,\n  combineLocations,\n  computeOutAndReduceShapes,\n  expandShapeToKeepDim,\n  assertAxesAreInnerMostDims,\n  getAxesPermutation,\n  getUndoAxesPermutation,\n  getInnerMostAxes,\n  getBroadcastDims: getBroadcastDims$1,\n  getReductionAxes,\n  assertAndGetBroadcastShape,\n  assertParamsConsistent,\n  computeOutShape: computeOutShape$1,\n  computeDilation2DInfo,\n  computePool2DInfo,\n  computePool3DInfo,\n  computeConv2DInfo,\n  computeConv3DInfo,\n  computeDefaultPad,\n  tupleValuesAreOne,\n  eitherStridesOrDilationsAreOne,\n  convertConv2DDataFormat,\n  getFusedDyActivation,\n  getFusedBiasGradient,\n  applyActivation: applyActivation$1,\n  shouldFuse,\n  PARALLELIZE_THRESHOLD,\n  computeOptimalWindowSize,\n  getImageCenter,\n  getReshaped,\n  getPermuted,\n  getReshapedPermuted,\n  getSliceBeginCoords,\n  getSliceSize,\n  prepareAndValidate,\n  validateUpdateShape,\n  validateInput: validateInput$1,\n  calculateShapes,\n  SELU_SCALEALPHA,\n  SELU_SCALE,\n  ERF_P,\n  ERF_A1,\n  ERF_A2,\n  ERF_A3,\n  ERF_A4,\n  ERF_A5,\n  warn,\n  log: log$2,\n  mergeRealAndImagArrays,\n  splitRealAndImagArrays,\n  complexWithEvenIndex,\n  complexWithOddIndex,\n  getComplexWithIndex,\n  assignToTypedArray,\n  exponents,\n  exponent,\n  decodeEinsumEquation,\n  getEinsumPermutation,\n  checkEinsumDimSizes,\n  getEinsumComputePath,\n  isIdentityPermutation,\n  prepareSplitSize\n},\n    kernel_impls = {\n  __proto__: null,\n  nonMaxSuppressionV3Impl: nonMaxSuppressionV3Impl$2,\n  nonMaxSuppressionV4Impl: nonMaxSuppressionV4Impl$2,\n  nonMaxSuppressionV5Impl: nonMaxSuppressionV5Impl$2,\n  whereImpl: whereImpl$2\n};\nvar absGradConfig = {\n  kernelName: Abs,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(e, step$2(cast$3(n, \"float32\"), -1))\n    };\n  }\n},\n    acosGradConfig = {\n  kernelName: Acos,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = square$2(cast$3(n, \"float32\")),\n            r = sqrt$2(sub$2(scalar(1), t));\n        return neg$2(div$1(e, r));\n      }\n    };\n  }\n},\n    acoshGradConfig = {\n  kernelName: Acosh,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = sqrt$2(sub$2(square$2(cast$3(n, \"float32\")), 1));\n        return div$1(e, t);\n      }\n    };\n  }\n},\n    addGradConfig = {\n  kernelName: Add$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a132 = assertAndGetBroadcastShape(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = e;\n        var r = getReductionAxes(n.shape, _a132);\n        return r.length > 0 && (t = sum$2(t, r)), reshape$3(t, n.shape);\n      },\n      b: () => {\n        var t = e;\n        var n = getReductionAxes(r.shape, _a132);\n        return n.length > 0 && (t = sum$2(t, n)), reshape$3(t, r.shape);\n      }\n    };\n  }\n},\n    addNGradConfig = {\n  kernelName: AddN,\n  saveAllInputs: !0,\n  gradFunc: (e, t) => {\n    var n = {};\n    return t.forEach((t, r) => {\n      n[r] = () => e.clone();\n    }), n;\n  }\n},\n    argMaxGradConfig = {\n  kernelName: ArgMax,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => zerosLike$2(n)\n    };\n  }\n},\n    argMinGradConfig = {\n  kernelName: ArgMin,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => zerosLike$2(n)\n    };\n  }\n},\n    asinGradConfig = {\n  kernelName: Asin,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, sqrt$2(sub$2(scalar(1), square$2(cast$3(n, \"float32\")))))\n    };\n  }\n},\n    asinhGradConfig = {\n  kernelName: Asinh,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = sqrt$2(add$2(scalar(1), square$2(cast$3(n, \"float32\"))));\n        return div$1(e, t);\n      }\n    };\n  }\n},\n    atan2GradConfig = {\n  kernelName: Atan2,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a133 = assertAndGetBroadcastShape(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = add$2(square$2(n), square$2(r));\n        var s = mul(e, div$1(r, t));\n        var o = getReductionAxes(n.shape, _a133);\n        return o.length > 0 && (s = sum$2(s, o)), reshape$3(s, n.shape);\n      },\n      b: () => {\n        var t = add$2(square$2(n), square$2(r));\n        var s = neg$2(mul(e, div$1(n, t)));\n        var o = getReductionAxes(r.shape, _a133);\n        return o.length > 0 && (s = sum$2(s, o)), reshape$3(s, r.shape);\n      }\n    };\n  }\n},\n    atanGradConfig = {\n  kernelName: Atan,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, add$2(square$2(cast$3(n, \"float32\")), 1))\n    };\n  }\n},\n    atanhGradConfig = {\n  kernelName: Atanh,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, sub$2(scalar(1), square$2(cast$3(n, \"float32\"))))\n    };\n  }\n};\n\nfunction avgPool3dGrad_(e, t, n, r, a, s) {\n  var o = convertToTensor(e, \"dy\", \"avgPool3dGrad\"),\n      i = convertToTensor(t, \"input\", \"avgPool3dGrad\");\n  var l = o,\n      u = i,\n      c = !1;\n  4 === i.rank && (c = !0, l = reshape$3(o, [1, o.shape[0], o.shape[1], o.shape[2], o.shape[3]]), u = reshape$3(i, [1, i.shape[0], i.shape[1], i.shape[2], i.shape[3]])), assert$4(5 === l.rank, () => \"Error in avgPool3dGrad: dy must be rank 5 but got rank \".concat(l.rank, \".\")), assert$4(5 === u.rank, () => \"Error in avgPool3dGrad: input must be rank 5 but got rank \".concat(u.rank, \".\")), null != s && assert$4(isInt(a), () => \"Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode \".concat(s, \" but got pad \").concat(a, \".\"));\n  var p = ENGINE.runKernel(AvgPool3DGrad, {\n    dy: l,\n    input: u\n  }, {\n    filterSize: n,\n    strides: r,\n    pad: a,\n    dimRoundingMode: s\n  });\n  return c ? reshape$3(p, [p.shape[1], p.shape[2], p.shape[3], p.shape[4]]) : p;\n}\n\nvar avgPool3dGrad = op({\n  avgPool3dGrad_\n}),\n    avgPool3DGradConfig$1 = {\n  kernelName: AvgPool3D,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o,\n      dimRoundingMode: i\n    } = n;\n    return {\n      x: () => avgPool3dGrad(e, r, a, s, o, i)\n    };\n  }\n};\n\nfunction avgPoolGrad_(e, t, n, r, a) {\n  var s = convertToTensor(e, \"dy\", \"avgPoolGrad\"),\n      o = convertToTensor(t, \"input\", \"avgPoolGrad\");\n  assert$4(o.rank === s.rank, () => \"Rank of input (\".concat(o.rank, \") does not match rank of dy (\").concat(s.rank, \")\"));\n  var i = o,\n      l = s,\n      u = !1;\n  3 === o.rank && (u = !0, i = reshape$3(o, [1, o.shape[0], o.shape[1], o.shape[2]]), l = reshape$3(s, [1, s.shape[0], s.shape[1], s.shape[2]])), assert$4(4 === l.rank, () => \"Error in avgPoolGrad: dy must be rank 4 but got rank \".concat(l.rank, \".\")), assert$4(4 === i.rank, () => \"Error in avgPoolGrad: input must be rank 4 but got rank \".concat(i.rank, \".\"));\n  var c = ENGINE.runKernel(AvgPoolGrad, {\n    dy: l,\n    input: i\n  }, {\n    filterSize: n,\n    strides: r,\n    pad: a\n  });\n  return u ? reshape$3(c, [c.shape[1], c.shape[2], c.shape[3]]) : c;\n}\n\nvar avgPoolGrad$2 = op({\n  avgPoolGrad_\n}),\n    avgPoolGradConfig$2 = {\n  kernelName: AvgPool,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o\n    } = n;\n    return {\n      x: () => avgPoolGrad$2(e, r, a, s, o)\n    };\n  }\n},\n    batchMatMulGradConfig = {\n  kernelName: BatchMatMul,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t, n) => {\n    var [r, _a134] = t,\n        {\n      transposeA: s,\n      transposeB: o\n    } = n;\n    return s || o ? !s && o ? {\n      a: () => matMul$1(e, _a134, !1, !1),\n      b: () => matMul$1(e, r, !0, !1)\n    } : s && !o ? {\n      a: () => matMul$1(_a134, e, !1, !0),\n      b: () => matMul$1(r, e, !1, !1)\n    } : {\n      a: () => matMul$1(_a134, e, !0, !0),\n      b: () => matMul$1(e, r, !0, !0)\n    } : {\n      a: () => matMul$1(e, _a134, !1, !0),\n      b: () => matMul$1(r, e, !0, !1)\n    };\n  }\n},\n    batchToSpaceNDGradConfig = {\n  kernelName: BatchToSpaceND,\n  gradFunc: (e, t, n) => {\n    var {\n      blockShape: r,\n      crops: a\n    } = n;\n    return {\n      x: () => spaceToBatchND$2(e, r, a)\n    };\n  }\n},\n    broadcastToGradConfig = {\n  kernelName: BroadcastTo,\n  gradFunc: (e, t, n) => {\n    var r = n.inputShape,\n        a = n.shape,\n        s = Array.from(a);\n\n    for (var _e609 = r.length - 1; _e609 >= 0; _e609--) {\n      if (r[_e609] === a[_e609]) s[_e609] = 1;else if (1 !== r[_e609]) throw new Error(\"broadcastTo(): [\".concat(r, \"] cannot be broadcast to [\").concat(a, \"].\"));\n    }\n\n    var o = [];\n\n    for (var _e610 = 0; _e610 < s.length; _e610++) {\n      s[_e610] > 1 && o.push(_e610);\n    }\n\n    return {\n      x: () => sum$2(e, o, !0)\n    };\n  }\n},\n    castGradConfig = {\n  kernelName: Cast,\n  gradFunc: e => ({\n    x: () => e.clone()\n  })\n},\n    ceilGradConfig = {\n  kernelName: Ceil,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    clipByValueGradConfig = {\n  kernelName: ClipByValue,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      clipValueMin: a,\n      clipValueMax: s\n    } = n;\n    return {\n      x: () => where(logicalAnd$2(greaterEqual$2(r, a), lessEqual$2(r, s)), e, zerosLike$2(e))\n    };\n  }\n},\n    complexAbsGradConfig = {\n  kernelName: ComplexAbs,\n  inputsToSave: [\"x\"],\n  gradFunc: absGradConfig.gradFunc\n},\n    concatGradConfig = {\n  kernelName: Concat,\n  saveAllInputs: !0,\n  gradFunc: (e, t, n) => {\n    var r = t.map(e => e.shape),\n        {\n      axis: a\n    } = n,\n        s = parseAxisParam(a, t[0].shape)[0],\n        o = r.map(e => e[s]);\n    return split$2(e, o, s).map(e => () => e);\n  }\n},\n    conv2DGradConfig = {\n  kernelName: Conv2D$1,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      dilations: s,\n      strides: o,\n      pad: i,\n      dataFormat: l\n    } = n;\n    return assert$4(tupleValuesAreOne(s), () => \"Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '\".concat(s, \"'\")), {\n      x: () => conv2DBackpropInput$2(r.shape, e, a, o, i, l),\n      filter: () => conv2DBackpropFilter$2(r, e, a.shape, o, i, l)\n    };\n  }\n},\n    conv2DBackpropInputGradConfig = {\n  kernelName: Conv2DBackpropInput,\n  inputsToSave: [\"dy\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      strides: s,\n      pad: o,\n      dataFormat: i,\n      dimRoundingMode: l\n    } = n;\n    return {\n      dy: () => conv2d$3(e, a, s, o, i, 1, l),\n      filter: () => conv2DBackpropFilter$2(e, r, a.shape, s, o, i, l)\n    };\n  }\n};\n\nfunction conv3DBackpropFilter_(e, t, n, r, a) {\n  var s = e;\n  4 === e.rank && (s = reshape$3(e, [1, e.shape[0], e.shape[1], e.shape[2], e.shape[3]]));\n  var o = t;\n  return 4 === o.rank && (o = reshape$3(t, [1, t.shape[0], t.shape[1], t.shape[2], t.shape[3]])), assert$4(5 === s.rank, () => \"Error in conv3dDerFilter: input must be rank 5, but got shape \".concat(s.shape, \".\")), assert$4(5 === o.rank, () => \"Error in conv3dDerFilter: dy must be rank 5, but got shape \".concat(o.shape, \".\")), assert$4(5 === n.length, () => \"Error in conv3dDerFilter: filterShape must be length 5, but got \".concat(n, \".\")), assert$4(s.shape[4] === n[3], () => \"Error in conv3dDerFilter: depth of input \".concat(s.shape[4], \") must match input depth in filter (\").concat(n[3], \".\")), assert$4(o.shape[4] === n[4], () => \"Error in conv3dDerFilter: depth of dy (\".concat(o.shape[4], \") must match output depth for filter (\").concat(n[4], \").\")), ENGINE.runKernel(Conv3DBackpropFilterV2, {\n    x: s,\n    dy: o\n  }, {\n    strides: r,\n    pad: a,\n    filterShape: n\n  });\n}\n\nvar conv3DBackpropFilter = op({\n  conv3DBackpropFilter_\n}),\n    conv3DGradConfig = {\n  kernelName: Conv3D$1,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var {\n      dilations: r,\n      strides: a,\n      pad: s\n    } = n;\n    assert$4(tupleValuesAreOne(r), () => \"Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '\".concat(r, \"'\"));\n    var [o, i] = t;\n    return {\n      x: () => conv3DBackpropInput$1(o.shape, e, i, a, s),\n      filter: () => conv3DBackpropFilter(o, e, i.shape, a, s)\n    };\n  }\n},\n    cosGradConfig = {\n  kernelName: Cos,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(neg$2(sin$2(cast$3(n, \"float32\"))), e)\n    };\n  }\n},\n    coshGradConfig = {\n  kernelName: Cosh,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(sinh$2(cast$3(n, \"float32\")), e)\n    };\n  }\n},\n    cumsumGradConfig = {\n  kernelName: Cumsum,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      axis: a,\n      exclusive: s,\n      reverse: o\n    } = n;\n    return {\n      x: () => {\n        var t = getAxesPermutation([a], r.rank);\n        var n = cumsum$2(e, a, s, !o);\n        return null != t && (n = transpose$2(n, t)), n;\n      }\n    };\n  }\n},\n    depthwiseConv2dNativeGradConfig = {\n  kernelName: DepthwiseConv2dNative,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var {\n      dilations: r,\n      strides: a,\n      pad: s,\n      dimRoundingMode: o\n    } = n,\n        i = null == r ? [1, 1] : r;\n    assert$4(tupleValuesAreOne(i), () => \"Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '\".concat(i, \"'\"));\n    var [l, u] = t;\n    return assert$4(4 === l.rank, () => \"Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank \".concat(l.rank, \".\")), assert$4(4 === u.rank, () => \"Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank \".concat(u.rank, \".\")), assert$4(l.shape[3] === u.shape[2], () => \"Error in gradient of depthwiseConv2d: number of input channels (\".concat(l.shape[3], \") must match the inChannels dimension in filter \").concat(u.shape[2], \".\")), assert$4(eitherStridesOrDilationsAreOne(a, i), () => \"Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides \".concat(a, \" and dilations '\").concat(i, \"'.\")), null != o && assert$4(isInt(s), () => \"Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(s, \".\")), {\n      x: () => depthwiseConv2dNativeBackpropInput$2(l.shape, e, u, a, s, r, o),\n      filter: () => depthwiseConv2dNativeBackpropFilter$2(l, e, u.shape, a, s, r, o)\n    };\n  }\n},\n    dilation2dGradConfig = {\n  kernelName: Dilation2D,\n  inputsToSave: [\"x\", \"filter\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        s = {\n      x: r,\n      filter: a,\n      dy: e\n    },\n        o = {\n      x: r,\n      filter: a,\n      dy: e\n    };\n    return {\n      x: () => ENGINE.runKernel(Dilation2DBackpropInput, s, n),\n      filter: () => ENGINE.runKernel(Dilation2DBackpropFilter, o, n)\n    };\n  }\n},\n    eluGradConfig$2 = {\n  kernelName: Elu$1,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t,\n        r = {\n      dy: e,\n      y: n\n    };\n    return {\n      x: () => ENGINE.runKernel(EluGrad, r)\n    };\n  }\n},\n    erfGradConfig = {\n  kernelName: Erf,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t,\n        r = mul(exp$2(neg$2(square$2(n))), 2 / Math.sqrt(Math.PI));\n    return {\n      x: () => mul(e, r)\n    };\n  }\n},\n    expGradConfig = {\n  kernelName: Exp,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(e, n)\n    };\n  }\n},\n    expandDimsGradConfig = {\n  kernelName: ExpandDims,\n  inputsToSave: [\"input\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      input: () => reshape$3(e, n.shape)\n    };\n  }\n},\n    expm1GradConfig = {\n  kernelName: Expm1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(e, exp$2(n))\n    };\n  }\n},\n    floorGradConfig = {\n  kernelName: Floor,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    floorDivGradConfig = {\n  kernelName: FloorDiv,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a135 = assertAndGetBroadcastShape(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = div$1(e, cast$3(r, \"float32\")),\n            s = getReductionAxes(n.shape, _a135);\n        return s.length > 0 ? reshape$3(sum$2(t, s), n.shape) : t;\n      },\n      b: () => {\n        var t = mul(e, cast$3(n, \"float32\"));\n        var s = getReductionAxes(r.shape, _a135);\n        s.length > 0 && (t = reshape$3(sum$2(t, s), r.shape));\n        var o = square$2(r);\n        return neg$2(div$1(t, cast$3(o, \"float32\")));\n      }\n    };\n  }\n},\n    fusedBatchNormGradConfig = {\n  kernelName: FusedBatchNorm,\n  inputsToSave: [\"x\", \"mean\", \"variance\", \"scale\"],\n  gradFunc: (e, t, n) => {\n    var {\n      varianceEpsilon: r\n    } = n,\n        [a, s, o, i] = t,\n        l = null == i ? scalar(1) : i,\n        u = getReductionAxes(s.shape, a.shape),\n        c = [];\n\n    if (1 === s.rank) {\n      for (var _e611 = 0; _e611 < a.shape.length - 1; ++_e611) {\n        c.push(a.shape[_e611]);\n      }\n\n      c.push(1);\n    }\n\n    var p = sub$2(a, s),\n        d = mul(e, l),\n        h = rsqrt$2(add$2(o, scalar(r))),\n        m = mul(mul(mul(h, h), h), scalar(-.5));\n    return {\n      x: () => reshape$3(mul(mul(e, 1 === s.rank ? tile$3(reshape$3(h, [1, 1, 1, s.shape[0]]), c) : h), l), a.shape),\n      mean: () => {\n        var e = mul(mul(h, scalar(-1)), d);\n        return 1 === s.rank && (e = sum$2(e, u)), reshape$3(e, s.shape);\n      },\n      variance: () => {\n        var e = mul(mul(m, p), d);\n        return 1 === s.rank && (e = sum$2(e, u)), reshape$3(e, s.shape);\n      },\n      scale: () => {\n        var t = mul(p, h);\n        var n = mul(e, t);\n        return 1 === s.rank && (n = sum$2(n, u)), reshape$3(n, s.shape);\n      },\n      offset: () => {\n        var t = e;\n        return 1 === s.rank && (t = sum$2(t, u)), reshape$3(t, s.shape);\n      }\n    };\n  }\n},\n    gatherGradConfig = {\n  kernelName: GatherV2,\n  inputsToSave: [\"x\", \"indices\"],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      axis: s\n    } = n,\n        o = parseAxisParam(s, r.shape)[0];\n    return {\n      x: () => {\n        var t = r.shape,\n            n = a.size,\n            i = t.slice(0, o),\n            l = i.length,\n            u = t.slice(s, t.length).slice(1),\n            c = u.length,\n            p = arrayRange(0, l),\n            d = arrayRange(l + 1, l + 1 + c),\n            h = arrayConcat([i, [n], u]),\n            m = reshape$3(e, h),\n            f = reshape$3(a, [n]),\n            g = arrayConcat([[l], p, d]),\n            $ = transpose$2(m, g);\n        var y = unsortedSegmentSum$2($, f, r.shape[o]);\n        var b = getUndoAxesPermutation(g);\n        return y = transpose$2(y, b), y;\n      },\n      indices: () => a\n    };\n  }\n};\n\nfunction arrayRange(e, t) {\n  var n = [];\n\n  for (var r = e; r < t; ++r) {\n    n.push(r);\n  }\n\n  return n;\n}\n\nfunction arrayConcat(e) {\n  var t = [];\n\n  for (var n = 0; n < e.length; ++n) {\n    for (var r = 0; r < e[n].length; ++r) {\n      t.push(e[n][r]);\n    }\n  }\n\n  return t;\n}\n\nvar greaterEqualGradConfig = {\n  kernelName: GreaterEqual,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t;\n    return {\n      a: () => zerosLike$2(n),\n      b: () => zerosLike$2(r)\n    };\n  }\n},\n    identityGradConfig = {\n  kernelName: Identity$1,\n  gradFunc: e => ({\n    x: () => cast$3(e, \"float32\")\n  })\n},\n    isFiniteGradConfig = {\n  kernelName: IsFinite,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    isInfGradConfig = {\n  kernelName: IsInf,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    isNanGradConfig = {\n  kernelName: IsNan,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    leakyReluGradConfig = {\n  kernelName: LeakyRelu,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      alpha: a\n    } = n,\n        s = greater$3(r, 0);\n    return {\n      x: () => where(s, e, mul(e, a))\n    };\n  }\n},\n    log1pGradConfig = {\n  kernelName: Log1p,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, add$2(n, 1))\n    };\n  }\n},\n    logGradConfig = {\n  kernelName: Log,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, cast$3(n, \"float32\"))\n    };\n  }\n},\n    logSoftmaxGradConfig = {\n  kernelName: LogSoftmax$1,\n  inputsToSave: [],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      axis: a\n    } = n;\n    return {\n      logits: () => {\n        var t = exp$2(r);\n        return sub$2(e, mul(sum$2(e, a, !0), t));\n      }\n    };\n  }\n};\n\nfunction localResponseNormalizationBackprop_(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 5;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : .5;\n  return ENGINE.runKernel(LRNGrad, {\n    x: e,\n    y: t,\n    dy: n\n  }, {\n    depthRadius: r,\n    bias: a,\n    alpha: s,\n    beta: o\n  });\n}\n\nvar localResponseNormalizationBackprop = op({\n  localResponseNormalizationBackprop_\n}),\n    lrnGradConfig = {\n  kernelName: LRN,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      depthRadius: s,\n      bias: o,\n      alpha: i,\n      beta: l\n    } = n;\n    return {\n      x: () => localResponseNormalizationBackprop(r, a, e, s, o, i, l)\n    };\n  }\n};\n\nfunction gradForMinAndMax(e, t, n, r) {\n  return t.rank < n.rank && (t = reshape$3(t, expandShapeToKeepDim(t.shape, r))), e.rank < n.rank && (e = reshape$3(e, expandShapeToKeepDim(e.shape, r))), {\n    x: () => mul(e, cast$3(equal$2(n, t), e.dtype))\n  };\n}\n\nvar maxGradConfig = {\n  kernelName: Max,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      reductionIndices: a\n    } = r,\n        s = t[0],\n        o = gradForMinAndMax(e, t[1], s, parseAxisParam(a, s.shape));\n    return {\n      x: () => o.x()\n    };\n  }\n},\n    maximumGradConfig = {\n  kernelName: Maximum$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t;\n    return {\n      a: () => mul(e, cast$3(greaterEqual$2(n, r), \"float32\")),\n      b: () => mul(e, cast$3(less$3(n, r), \"float32\"))\n    };\n  }\n};\n\nfunction maxPool3dGrad_(e, t, n, r, a, s, o) {\n  var i = convertToTensor(e, \"dy\", \"maxPool3dGrad\"),\n      l = convertToTensor(t, \"input\", \"maxPool3dGrad\"),\n      u = convertToTensor(n, \"output\", \"maxPool3dGrad\");\n  var c = i,\n      p = l,\n      d = u,\n      h = !1;\n  4 === l.rank && (h = !0, c = reshape$3(i, [1, i.shape[0], i.shape[1], i.shape[2], i.shape[3]]), p = reshape$3(l, [1, l.shape[0], l.shape[1], l.shape[2], l.shape[3]]), d = reshape$3(u, [1, u.shape[0], u.shape[1], u.shape[2], u.shape[3]])), assert$4(5 === c.rank, () => \"Error in maxPool3dGrad: dy must be rank 5 but got rank \".concat(c.rank, \".\")), assert$4(5 === p.rank, () => \"Error in maxPool3dGrad: input must be rank 5 but got rank \".concat(p.rank, \".\")), assert$4(5 === d.rank, () => \"Error in maxPool3dGrad: output must be rank 5 but got rank \".concat(d.rank, \".\")), null != o && assert$4(isInt(s), () => \"Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(s, \".\"));\n  var m = ENGINE.runKernel(MaxPool3DGrad, {\n    dy: c,\n    input: p,\n    output: d\n  }, {\n    filterSize: r,\n    strides: a,\n    pad: s,\n    dimRoundingMode: o\n  });\n  return h ? reshape$3(m, [m.shape[1], m.shape[2], m.shape[3], m.shape[4]]) : m;\n}\n\nvar maxPool3dGrad = op({\n  maxPool3dGrad_\n}),\n    maxPool3DGradConfig$1 = {\n  kernelName: MaxPool3D,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      filterSize: s,\n      strides: o,\n      pad: i,\n      dimRoundingMode: l\n    } = n;\n    return {\n      x: () => maxPool3dGrad(e, r, a, s, o, i, l)\n    };\n  }\n};\n\nfunction maxPoolGrad_(e, t, n, r, a, s, o) {\n  var i = convertToTensor(e, \"dy\", \"maxPoolGrad\"),\n      l = convertToTensor(t, \"input\", \"maxPoolGrad\"),\n      u = convertToTensor(n, \"output\", \"maxPoolGrad\");\n  return assert$4(l.rank === i.rank, () => \"Rank of input (\".concat(l.rank, \") does not match rank of dy (\").concat(i.rank, \")\")), assert$4(4 === i.rank, () => \"Error in maxPoolGrad: dy must be rank 4 but got rank \".concat(i.rank, \".\")), assert$4(4 === l.rank, () => \"Error in maxPoolGrad: input must be rank 4 but got rank \".concat(l.rank, \".\")), null != o && assert$4(isInt(s), () => \"Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode \".concat(o, \" but got pad \").concat(s, \".\")), ENGINE.runKernel(MaxPoolGrad, {\n    dy: i,\n    input: l,\n    output: u\n  }, {\n    filterSize: r,\n    strides: a,\n    pad: s,\n    dimRoundingMode: o\n  });\n}\n\nvar maxPoolGrad$2 = op({\n  maxPoolGrad_\n}),\n    maxPoolGradConfig$2 = {\n  kernelName: MaxPool,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r, a] = t,\n        {\n      filterSize: s,\n      strides: o,\n      pad: i\n    } = n;\n    return {\n      x: () => maxPoolGrad$2(e, r, a, s, o, i)\n    };\n  }\n},\n    meanGradConfig = {\n  kernelName: Mean,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      axis: a\n    } = n,\n        s = parseAxisParam(a, r.shape),\n        o = sizeFromShape(computeOutAndReduceShapes(r.shape, s)[1]);\n    return {\n      x: () => {\n        var t = r.shape.slice();\n        s.forEach(e => {\n          t[e] = 1;\n        });\n        var n = reshape$3(e, t);\n        return div$1(mul(n, ones$1(r.shape, \"float32\")), o);\n      }\n    };\n  }\n},\n    minGradConfig = {\n  kernelName: Min,\n  inputsToSave: [\"x\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      axis: a\n    } = r,\n        [s, o] = t,\n        i = gradForMinAndMax(e, o, s, parseAxisParam(a, s.shape));\n    return {\n      x: () => i.x()\n    };\n  }\n},\n    minimumGradConfig = {\n  kernelName: Minimum$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t;\n    return {\n      a: () => mul(e, cast$3(lessEqual$2(n, r), \"float32\")),\n      b: () => mul(e, cast$3(greater$3(n, r), \"float32\"))\n    };\n  }\n},\n    mirrorPadGradConfig = {\n  kernelName: MirrorPad,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var r = t[0],\n        {\n      paddings: a\n    } = n,\n        s = a.map(e => e[0]);\n    return {\n      x: () => slice$2(e, s, r.shape)\n    };\n  }\n},\n    modGradConfig = {\n  kernelName: Mod,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a136 = assertAndGetBroadcastShape(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = getReductionAxes(n.shape, _a136);\n        return t.length > 0 ? reshape$3(sum$2(e, t), n.shape) : e;\n      },\n      b: () => {\n        var t = mul(e, neg$2(floor$2(div$1(n, r)))),\n            s = getReductionAxes(r.shape, _a136);\n        return s.length > 0 ? reshape$3(sum$2(t, s), r.shape) : t;\n      }\n    };\n  }\n},\n    multiplyGradConfig = {\n  kernelName: Multiply$1,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a137 = assertAndGetBroadcastShape(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = mul(e, cast$3(r, \"float32\")),\n            s = getReductionAxes(n.shape, _a137);\n        return s.length > 0 ? reshape$3(sum$2(t, s), n.shape) : t;\n      },\n      b: () => {\n        var t = mul(e, cast$3(n, \"float32\")),\n            s = getReductionAxes(r.shape, _a137);\n        return s.length > 0 ? reshape$3(sum$2(t, s), r.shape) : t;\n      }\n    };\n  }\n},\n    negGradConfig = {\n  kernelName: Neg,\n  gradFunc: e => ({\n    x: () => neg$2(e)\n  })\n},\n    oneHotGradConfig = {\n  kernelName: OneHot,\n  inputsToSave: [\"indices\"],\n  gradFunc: (e, t) => {\n    var n = t[0];\n    return {\n      indices: () => zeros$2(n.shape, \"float32\")\n    };\n  }\n},\n    onesLikeGradConfig = {\n  kernelName: OnesLike,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    packGradConfig = {\n  kernelName: Pack,\n  saveAllInputs: !0,\n  gradFunc: (e, t, n) => {\n    var {\n      axis: r\n    } = n;\n    return unstack(e, r).map(e => () => e);\n  }\n},\n    padV2GradConfig = {\n  kernelName: PadV2,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var r = t[0],\n        {\n      paddings: a\n    } = n,\n        s = a.map(e => e[0]);\n    return {\n      x: () => slice$2(e, s, r.shape)\n    };\n  }\n},\n    powGradConfig = {\n  kernelName: Pow,\n  inputsToSave: [\"a\", \"b\"],\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n, r, a] = t,\n        s = n,\n        o = r,\n        i = assertAndGetBroadcastShape(s.shape, o.shape);\n    return {\n      a: () => {\n        var t = cast$3(o, \"float32\");\n        var n = mul(e, mul(t, pow$2(s, sub$2(t, scalar(1)))));\n        var r = getReductionAxes(s.shape, i);\n        return r.length > 0 && (n = sum$2(n, r)), reshape$3(n, s.shape);\n      },\n      b: () => {\n        var t = greater$3(s, 0),\n            n = where(t, log$3(s), zerosLike$2(s));\n        var r = mul(e, mul(a, n));\n        var l = getReductionAxes(o.shape, i);\n        return l.length > 0 && (r = sum$2(r, l)), reshape$3(r, o.shape);\n      }\n    };\n  }\n},\n    preluGradConfig = {\n  kernelName: Prelu,\n  inputsToSave: [\"x\", \"alpha\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        a = greater$3(n, 0);\n    return {\n      x: () => where(a, e, mul(e, r)),\n      alpha: () => {\n        var t = where(a, zerosLike$2(e), mul(e, n));\n        var s = getReductionAxes(r.shape, e.shape);\n        return s.length > 0 && (t = sum$2(t, s)), reshape$3(t, r.shape);\n      }\n    };\n  }\n},\n    divGradConfig = {\n  kernelName: RealDiv,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a138 = assertAndGetBroadcastShape(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = div$1(e, cast$3(r, \"float32\")),\n            s = getReductionAxes(n.shape, _a138);\n        return s.length > 0 ? reshape$3(sum$2(t, s), n.shape) : t;\n      },\n      b: () => {\n        var t = mul(e, cast$3(n, \"float32\"));\n        var s = getReductionAxes(r.shape, _a138);\n        s.length > 0 && (t = reshape$3(sum$2(t, s), r.shape));\n        var o = square$2(r);\n        return neg$2(div$1(t, cast$3(o, \"float32\")));\n      }\n    };\n  }\n},\n    reciprocalGradConfig = {\n  kernelName: Reciprocal,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, neg$2(square$2(n)))\n    };\n  }\n},\n    relu6GradConfig = {\n  kernelName: Relu6$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t,\n        r = mul(lessEqual$2(n, 6), step$2(n));\n    return {\n      x: () => mul(e, cast$3(r, \"float32\"))\n    };\n  }\n},\n    reluGradConfig = {\n  kernelName: Relu$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(e, cast$3(step$2(n), \"float32\"))\n    };\n  }\n},\n    reshapeGradConfig = {\n  kernelName: Reshape$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => reshape$3(e, n.shape)\n    };\n  }\n},\n    resizeBilinearGradConfig$2 = {\n  kernelName: ResizeBilinear,\n  inputsToSave: [\"images\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        a = {\n      dy: e,\n      images: r\n    };\n    return {\n      images: () => ENGINE.runKernel(ResizeBilinearGrad, a, n)\n    };\n  }\n},\n    resizeNearestNeighborGradConfig$2 = {\n  kernelName: ResizeNearestNeighbor,\n  inputsToSave: [\"images\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        a = {\n      dy: e,\n      images: r\n    };\n    return {\n      images: () => ENGINE.runKernel(ResizeNearestNeighborGrad, a, n)\n    };\n  }\n},\n    reverseGradConfig = {\n  kernelName: Reverse,\n  gradFunc: (e, t, n) => {\n    var {\n      dims: r\n    } = n,\n        a = parseAxisParam(r, e.shape);\n    return {\n      x: () => reverse$2(e, a)\n    };\n  }\n},\n    roundGradConfig = {\n  kernelName: Round,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    rsqrtGradConfig = {\n  kernelName: Rsqrt,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => neg$2(div$1(e, mul(pow$2(n, 1.5), 2)))\n    };\n  }\n},\n    selectGradConfig = {\n  kernelName: Select,\n  inputsToSave: [\"condition\"],\n  gradFunc: (_e612, t) => {\n    var [n] = t;\n    return {\n      condition: () => cast$3(zerosLike$2(n), \"float32\"),\n      t: () => mul(_e612, cast$3(n, _e612.dtype)),\n      e: () => mul(_e612, cast$3(logicalNot$2(n), _e612.dtype))\n    };\n  }\n},\n    seluGradConfig = {\n  kernelName: Selu$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => {\n        var t = greater$3(n, scalar(0)),\n            r = scalar(SELU_SCALEALPHA),\n            a = scalar(SELU_SCALE),\n            s = mul(e, a),\n            o = mul(mul(e, r), exp$2(cast$3(n, \"float32\")));\n        return where(t, s, o);\n      }\n    };\n  }\n},\n    sigmoidGradConfig = {\n  kernelName: Sigmoid$1,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(e, mul(n, sub$2(scalar(1), n)))\n    };\n  }\n},\n    signGradConfig = {\n  kernelName: Sign,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    sinGradConfig = {\n  kernelName: Sin,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(cos$2(cast$3(n, \"float32\")), e)\n    };\n  }\n},\n    sinhGradConfig = {\n  kernelName: Sinh,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(cosh$2(cast$3(n, \"float32\")), e)\n    };\n  }\n},\n    sliceGradConfig = {\n  kernelName: Slice,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      begin: a,\n      size: s\n    } = n,\n        o = r.shape,\n        [i, l] = parseSliceParams(r, a, s),\n        u = [];\n\n    for (var _t437 = 0; _t437 < e.rank; _t437++) {\n      u.push([i[_t437], o[_t437] - i[_t437] - l[_t437]]);\n    }\n\n    return {\n      x: () => pad(e, u)\n    };\n  }\n},\n    softmaxGradConfig = {\n  kernelName: Softmax$2,\n  outputsToSave: [!0],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      dim: a\n    } = n,\n        s = mul(e, r);\n    return {\n      logits: () => sub$2(s, mul(sum$2(s, [a], !0), r))\n    };\n  }\n},\n    softplusGradConfig = {\n  kernelName: Softplus$1,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(e, sigmoid$2(n))\n    };\n  }\n},\n    spaceToBatchNDGradConfig = {\n  kernelName: SpaceToBatchND,\n  gradFunc: (e, t, n) => {\n    var {\n      blockShape: r,\n      paddings: a\n    } = n;\n    return {\n      x: () => batchToSpaceND$2(e, r, a)\n    };\n  }\n},\n    splitVGradConfig = {\n  kernelName: SplitV,\n  gradFunc: (e, t, n) => {\n    var {\n      axis: r\n    } = n;\n    return {\n      x: () => concat$2(e, r)\n    };\n  }\n},\n    sqrtGradConfig = {\n  kernelName: Sqrt,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, mul(sqrt$2(cast$3(n, \"float32\")), 2))\n    };\n  }\n},\n    squareGradConfig = {\n  kernelName: Square,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(e, mul(cast$3(n, \"float32\"), 2))\n    };\n  }\n},\n    squaredDifferenceGradConfig = {\n  kernelName: SquaredDifference,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a139 = scalar(2);\n\n    return {\n      a: () => mul(e, mul(_a139, sub$2(n, r))),\n      b: () => mul(e, mul(_a139, sub$2(r, n)))\n    };\n  }\n},\n    stepGradConfig = {\n  kernelName: Step,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    subGradConfig = {\n  kernelName: Sub,\n  inputsToSave: [\"a\", \"b\"],\n  gradFunc: (e, t) => {\n    var [n, r] = t,\n        _a140 = assertAndGetBroadcastShape(n.shape, r.shape);\n\n    return {\n      a: () => {\n        var t = e;\n        var r = getReductionAxes(n.shape, _a140);\n        return r.length > 0 && (t = sum$2(t, r)), reshape$3(t, n.shape);\n      },\n      b: () => {\n        var t = e;\n        var n = getReductionAxes(r.shape, _a140);\n        return n.length > 0 && (t = sum$2(t, n)), reshape$3(neg$2(t), r.shape);\n      }\n    };\n  }\n},\n    sumGradConfig = {\n  kernelName: Sum,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        a = r.shape.slice(),\n        {\n      axis: s\n    } = n;\n    parseAxisParam(s, r.shape).forEach(e => {\n      a[e] = 1;\n    });\n    var o = reshape$3(e, a),\n        i = mul(o, ones$1(r.shape, \"float32\"));\n    return {\n      x: () => i\n    };\n  }\n},\n    tanGradConfig = {\n  kernelName: Tan,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => div$1(e, square$2(cos$2(n)))\n    };\n  }\n},\n    tanhGradConfig = {\n  kernelName: Tanh$1,\n  outputsToSave: [!0],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => mul(sub$2(scalar(1), square$2(n)), e)\n    };\n  }\n},\n    tileGradConfig = {\n  kernelName: Tile,\n  inputsToSave: [\"x\"],\n  gradFunc: (e, t, n) => {\n    var [r] = t,\n        {\n      reps: a\n    } = n;\n    return {\n      x: () => {\n        var t = zerosLike$2(r);\n        if (1 === r.rank) for (var _n247 = 0; _n247 < a[0]; ++_n247) {\n          t = add$2(t, slice$2(e, [_n247 * r.shape[0]], [r.shape[0]]));\n        } else if (2 === r.rank) for (var _n248 = 0; _n248 < a[0]; ++_n248) {\n          for (var s = 0; s < a[1]; ++s) {\n            t = add$2(t, slice$2(e, [_n248 * r.shape[0], s * r.shape[1]], [r.shape[0], r.shape[1]]));\n          }\n        } else if (3 === r.rank) for (var _n249 = 0; _n249 < a[0]; ++_n249) {\n          for (var _s98 = 0; _s98 < a[1]; ++_s98) {\n            for (var o = 0; o < a[2]; ++o) {\n              t = add$2(t, slice$2(e, [_n249 * r.shape[0], _s98 * r.shape[1], o * r.shape[2]], [r.shape[0], r.shape[1], r.shape[2]]));\n            }\n          }\n        } else {\n          if (4 !== r.rank) throw new Error(\"Gradient for tile operation is not implemented for rank-\".concat(r.rank, \" tensors yet.\"));\n\n          for (var _n250 = 0; _n250 < a[0]; ++_n250) {\n            for (var _s99 = 0; _s99 < a[1]; ++_s99) {\n              for (var _o72 = 0; _o72 < a[2]; ++_o72) {\n                for (var i = 0; i < a[3]; ++i) {\n                  t = add$2(t, slice$2(e, [_n250 * r.shape[0], _s99 * r.shape[1], _o72 * r.shape[2], i * r.shape[3]], [r.shape[0], r.shape[1], r.shape[2], r.shape[3]]));\n                }\n              }\n            }\n          }\n        }\n        return t;\n      }\n    };\n  }\n},\n    transposeGradConfig = {\n  kernelName: Transpose,\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      perm: a\n    } = r,\n        s = getUndoAxesPermutation(a);\n    return {\n      x: () => transpose$2(e, s)\n    };\n  }\n},\n    unpackGradConfig = {\n  kernelName: Unpack,\n  gradFunc: (e, t, n) => {\n    var r = n,\n        {\n      axis: a\n    } = r;\n    return {\n      value: () => stack(e, a)\n    };\n  }\n},\n    unsortedSegmentSumGradConfig = {\n  kernelName: UnsortedSegmentSum,\n  inputsToSave: [\"segmentIds\"],\n  gradFunc: (e, t) => {\n    var [n] = t;\n    return {\n      x: () => gatherDropNegatives(e, n)\n    };\n  }\n};\n\nfunction gatherDropNegatives(e, t) {\n  var n = maximum$3(t, zerosLike$2(t)),\n      r = gather$1(e, n);\n  var a = greaterEqual$2(t, scalar(0, \"int32\"));\n  var s = r.rank - a.rank;\n\n  for (var _e613 = 0; _e613 < s; ++_e613) {\n    a = expandDims$3(a, _e613 + 1);\n  }\n\n  a = logicalAnd$2(a, ones$1(r.shape, \"bool\"));\n  var o = zerosLike$2(r);\n  return where(a, r, o);\n}\n\nvar zerosLikeGradConfig = {\n  kernelName: ZerosLike,\n  gradFunc: e => ({\n    x: () => zerosLike$2(e)\n  })\n},\n    gradConfigs = [absGradConfig, acosGradConfig, acoshGradConfig, addGradConfig, addNGradConfig, argMaxGradConfig, argMinGradConfig, asinGradConfig, asinhGradConfig, atan2GradConfig, atanGradConfig, atanhGradConfig, avgPool3DGradConfig$1, avgPoolGradConfig$2, batchMatMulGradConfig, batchToSpaceNDGradConfig, broadcastToGradConfig, castGradConfig, ceilGradConfig, clipByValueGradConfig, complexAbsGradConfig, concatGradConfig, conv2DBackpropInputGradConfig, conv2DGradConfig, conv3DGradConfig, cosGradConfig, coshGradConfig, cumsumGradConfig, depthwiseConv2dNativeGradConfig, dilation2dGradConfig, divGradConfig, eluGradConfig$2, erfGradConfig, expGradConfig, expandDimsGradConfig, expm1GradConfig, floorDivGradConfig, floorGradConfig, fusedBatchNormGradConfig, gatherGradConfig, greaterEqualGradConfig, identityGradConfig, isFiniteGradConfig, isInfGradConfig, isNanGradConfig, leakyReluGradConfig, log1pGradConfig, logGradConfig, logSoftmaxGradConfig, lrnGradConfig, maxGradConfig, maxGradConfig, maximumGradConfig, maxPool3DGradConfig$1, maxPoolGradConfig$2, meanGradConfig, minGradConfig, minimumGradConfig, mirrorPadGradConfig, modGradConfig, multiplyGradConfig, negGradConfig, oneHotGradConfig, onesLikeGradConfig, packGradConfig, padV2GradConfig, padV2GradConfig, powGradConfig, preluGradConfig, reciprocalGradConfig, relu6GradConfig, reluGradConfig, reshapeGradConfig, resizeBilinearGradConfig$2, resizeNearestNeighborGradConfig$2, reverseGradConfig, roundGradConfig, rsqrtGradConfig, selectGradConfig, seluGradConfig, sigmoidGradConfig, signGradConfig, sinGradConfig, sinhGradConfig, sliceGradConfig, softmaxGradConfig, softplusGradConfig, spaceToBatchNDGradConfig, spaceToBatchNDGradConfig, splitVGradConfig, splitVGradConfig, sqrtGradConfig, squaredDifferenceGradConfig, squareGradConfig, stepGradConfig, subGradConfig, sumGradConfig, tanGradConfig, tanhGradConfig, tileGradConfig, transposeGradConfig, unpackGradConfig, unsortedSegmentSumGradConfig, zerosLikeGradConfig];\n\nfor (var _e614 of gradConfigs) {\n  registerGradient(_e614);\n}\n\nvar _epsilon;\n\nfunction epsilon$1() {\n  return null == _epsilon && (_epsilon = backend().epsilon()), _epsilon;\n}\n\nfunction imageDataFormat() {\n  return \"channelsLast\";\n}\n\ngetGlobalTensorClass().prototype.abs = function () {\n  return this.throwIfDisposed(), abs$2(this);\n}, getGlobalTensorClass().prototype.acos = function () {\n  return this.throwIfDisposed(), acos$2(this);\n}, getGlobalTensorClass().prototype.acosh = function () {\n  return this.throwIfDisposed(), acosh$2(this);\n}, getGlobalTensorClass().prototype.add = function (e) {\n  return this.throwIfDisposed(), add$2(this, e);\n}, getGlobalTensorClass().prototype.all = function (e, t) {\n  return this.throwIfDisposed(), all$2(this, e, t);\n}, getGlobalTensorClass().prototype.any = function (e, t) {\n  return this.throwIfDisposed(), any$2(this, e, t);\n}, getGlobalTensorClass().prototype.argMax = function (e) {\n  return this.throwIfDisposed(), argMax$2(this, e);\n}, getGlobalTensorClass().prototype.argMin = function (e) {\n  return this.throwIfDisposed(), argMin$2(this, e);\n}, getGlobalTensorClass().prototype.asScalar = function () {\n  return this.throwIfDisposed(), assert$4(1 === this.size, () => \"The array must have only 1 element.\"), reshape$3(this, []);\n}, getGlobalTensorClass().prototype.asType = function (e) {\n  return this.throwIfDisposed(), cast$3(this, e);\n}, getGlobalTensorClass().prototype.as1D = function () {\n  return this.throwIfDisposed(), reshape$3(this, [this.size]);\n}, getGlobalTensorClass().prototype.as2D = function (e, t) {\n  return this.throwIfDisposed(), reshape$3(this, [e, t]);\n}, getGlobalTensorClass().prototype.as3D = function (e, t, n) {\n  return this.throwIfDisposed(), reshape$3(this, [e, t, n]);\n}, getGlobalTensorClass().prototype.as4D = function (e, t, n, r) {\n  return this.throwIfDisposed(), reshape$3(this, [e, t, n, r]);\n}, getGlobalTensorClass().prototype.as5D = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), reshape$3(this, [e, t, n, r, a]);\n}, getGlobalTensorClass().prototype.asin = function () {\n  return this.throwIfDisposed(), asin$2(this);\n}, getGlobalTensorClass().prototype.asinh = function () {\n  return this.throwIfDisposed(), asinh$2(this);\n}, getGlobalTensorClass().prototype.atan = function () {\n  return this.throwIfDisposed(), atan$2(this);\n}, getGlobalTensorClass().prototype.atan2 = function (e) {\n  return this.throwIfDisposed(), atan2$2(this, e);\n}, getGlobalTensorClass().prototype.atanh = function () {\n  return this.throwIfDisposed(), atanh$2(this);\n}, getGlobalTensorClass().prototype.avgPool = function (e, t, n, r) {\n  return this.throwIfDisposed(), avgPool$2(this, e, t, n, r);\n}, getGlobalTensorClass().prototype.batchToSpaceND = function (e, t) {\n  return this.throwIfDisposed(), batchToSpaceND$2(this, e, t);\n}, getGlobalTensorClass().prototype.batchNorm = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), batchNorm$2(this, e, t, n, r, a);\n}, getGlobalTensorClass().prototype.broadcastTo = function (e) {\n  return this.throwIfDisposed(), broadcastTo(this, e);\n}, getGlobalTensorClass().prototype.cast = function (e) {\n  return this.throwIfDisposed(), cast$3(this, e);\n}, getGlobalTensorClass().prototype.ceil = function () {\n  return this.throwIfDisposed(), ceil$2(this);\n}, getGlobalTensorClass().prototype.clipByValue = function (e, t) {\n  return this.throwIfDisposed(), clipByValue$1(this, e, t);\n}, getGlobalTensorClass().prototype.concat = function (e, t) {\n  return this.throwIfDisposed(), e instanceof Tensor && (e = [e]), concat$2([this, ...e], t);\n}, getGlobalTensorClass().prototype.conv1d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), conv1d$1(this, e, t, n, r, a, s);\n}, getGlobalTensorClass().prototype.conv2dTranspose = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), conv2dTranspose$1(this, e, t, n, r, a);\n}, getGlobalTensorClass().prototype.conv2d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), conv2d$3(this, e, t, n, r, a, s);\n}, getGlobalTensorClass().prototype.cos = function () {\n  return this.throwIfDisposed(), cos$2(this);\n}, getGlobalTensorClass().prototype.cosh = function () {\n  return this.throwIfDisposed(), cosh$2(this);\n}, getGlobalTensorClass().prototype.cumsum = function (e, t, n) {\n  return this.throwIfDisposed(), cumsum$2(this, e, t, n);\n}, getGlobalTensorClass().prototype.depthToSpace = function (e, t) {\n  return this.throwIfDisposed(), depthToSpace$2(this, e, t);\n}, getGlobalTensorClass().prototype.depthwiseConv2d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), depthwiseConv2d$3(this, e, t, n, r, a, s);\n}, getGlobalTensorClass().prototype.dilation2d = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), dilation2d(this, e, t, n, r, a);\n}, getGlobalTensorClass().prototype.divNoNan = function (e) {\n  return this.throwIfDisposed(), divNoNan(this, e);\n}, getGlobalTensorClass().prototype.div = function (e) {\n  return this.throwIfDisposed(), div$1(this, e);\n}, getGlobalTensorClass().prototype.dot = function (e) {\n  return this.throwIfDisposed(), dot$2(this, e);\n}, getGlobalTensorClass().prototype.elu = function () {\n  return this.throwIfDisposed(), elu$4(this);\n}, getGlobalTensorClass().prototype.equal = function (e) {\n  return this.throwIfDisposed(), equal$2(this, e);\n}, getGlobalTensorClass().prototype.erf = function () {\n  return this.throwIfDisposed(), erf$2(this);\n}, getGlobalTensorClass().prototype.exp = function () {\n  return this.throwIfDisposed(), exp$2(this);\n}, getGlobalTensorClass().prototype.expandDims = function (e) {\n  return this.throwIfDisposed(), expandDims$3(this, e);\n}, getGlobalTensorClass().prototype.expm1 = function () {\n  return this.throwIfDisposed(), expm1$2(this);\n}, getGlobalTensorClass().prototype.fft = function () {\n  return this.throwIfDisposed(), fft$2(this);\n}, getGlobalTensorClass().prototype.flatten = function () {\n  return this.throwIfDisposed(), reshape$3(this, [this.size]);\n}, getGlobalTensorClass().prototype.floor = function () {\n  return this.throwIfDisposed(), floor$2(this);\n}, getGlobalTensorClass().prototype.floorDiv = function (e) {\n  return this.throwIfDisposed(), floorDiv$2(this, e);\n}, getGlobalTensorClass().prototype.gather = function (e, t) {\n  return this.throwIfDisposed(), gather$1(this, e, t);\n}, getGlobalTensorClass().prototype.greaterEqual = function (e) {\n  return this.throwIfDisposed(), greaterEqual$2(this, e);\n}, getGlobalTensorClass().prototype.greater = function (e) {\n  return this.throwIfDisposed(), greater$3(this, e);\n}, getGlobalTensorClass().prototype.ifft = function () {\n  return this.throwIfDisposed(), ifft$2(this);\n}, getGlobalTensorClass().prototype.irfft = function () {\n  return this.throwIfDisposed(), irfft(this);\n}, getGlobalTensorClass().prototype.isFinite = function () {\n  return this.throwIfDisposed(), isFinite$3(this);\n}, getGlobalTensorClass().prototype.isInf = function () {\n  return this.throwIfDisposed(), isInf$2(this);\n}, getGlobalTensorClass().prototype.isNaN = function () {\n  return this.throwIfDisposed(), isNaN$3(this);\n}, getGlobalTensorClass().prototype.leakyRelu = function (e) {\n  return this.throwIfDisposed(), leakyRelu$2(this, e);\n}, getGlobalTensorClass().prototype.lessEqual = function (e) {\n  return this.throwIfDisposed(), lessEqual$2(this, e);\n}, getGlobalTensorClass().prototype.less = function (e) {\n  return this.throwIfDisposed(), less$3(this, e);\n}, getGlobalTensorClass().prototype.localResponseNormalization = function (e, t, n, r) {\n  return this.throwIfDisposed(), localResponseNormalization(this, e, t, n, r);\n}, getGlobalTensorClass().prototype.logSigmoid = function () {\n  return this.throwIfDisposed(), logSigmoid(this);\n}, getGlobalTensorClass().prototype.logSoftmax = function (e) {\n  return this.throwIfDisposed(), logSoftmax(this, e);\n}, getGlobalTensorClass().prototype.logSumExp = function (e, t) {\n  return this.throwIfDisposed(), logSumExp(this, e, t);\n}, getGlobalTensorClass().prototype.log = function () {\n  return this.throwIfDisposed(), log$3(this);\n}, getGlobalTensorClass().prototype.log1p = function () {\n  return this.throwIfDisposed(), log1p$2(this);\n}, getGlobalTensorClass().prototype.logicalAnd = function (e) {\n  return this.throwIfDisposed(), logicalAnd$2(this, e);\n}, getGlobalTensorClass().prototype.logicalNot = function () {\n  return this.throwIfDisposed(), logicalNot$2(this);\n}, getGlobalTensorClass().prototype.logicalOr = function (e) {\n  return this.throwIfDisposed(), logicalOr$2(this, e);\n}, getGlobalTensorClass().prototype.logicalXor = function (e) {\n  return this.throwIfDisposed(), logicalXor(this, e);\n}, getGlobalTensorClass().prototype.matMul = function (e, t, n) {\n  return this.throwIfDisposed(), matMul$1(this, e, t, n);\n}, getGlobalTensorClass().prototype.maxPool = function (e, t, n, r) {\n  return this.throwIfDisposed(), maxPool$2(this, e, t, n, r);\n}, getGlobalTensorClass().prototype.max = function (e, t) {\n  return this.throwIfDisposed(), max$3(this, e, t);\n}, getGlobalTensorClass().prototype.maximum = function (e) {\n  return this.throwIfDisposed(), maximum$3(this, e);\n}, getGlobalTensorClass().prototype.mean = function (e, t) {\n  return this.throwIfDisposed(), mean$1(this, e, t);\n}, getGlobalTensorClass().prototype.min = function (e, t) {\n  return this.throwIfDisposed(), min$3(this, e, t);\n}, getGlobalTensorClass().prototype.minimum = function (e) {\n  return this.throwIfDisposed(), minimum$3(this, e);\n}, getGlobalTensorClass().prototype.mirrorPad = function (e, t) {\n  return this.throwIfDisposed(), mirrorPad$1(this, e, t);\n}, getGlobalTensorClass().prototype.mod = function (e) {\n  return this.throwIfDisposed(), mod$2(this, e);\n}, getGlobalTensorClass().prototype.mul = function (e) {\n  return this.throwIfDisposed(), mul(this, e);\n}, getGlobalTensorClass().prototype.neg = function () {\n  return this.throwIfDisposed(), neg$2(this);\n}, getGlobalTensorClass().prototype.norm = function (e, t, n) {\n  return this.throwIfDisposed(), norm(this, e, t, n);\n}, getGlobalTensorClass().prototype.notEqual = function (e) {\n  return this.throwIfDisposed(), notEqual$2(this, e);\n}, getGlobalTensorClass().prototype.oneHot = function (e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  return this.throwIfDisposed(), oneHot$2(this, e, t, n);\n}, getGlobalTensorClass().prototype.onesLike = function () {\n  return this.throwIfDisposed(), onesLike$2(this);\n}, getGlobalTensorClass().prototype.pad = function (e, t) {\n  return this.throwIfDisposed(), pad(this, e, t);\n}, getGlobalTensorClass().prototype.pool = function (e, t, n, r, a) {\n  return this.throwIfDisposed(), pool$1(this, e, t, n, r, a);\n}, getGlobalTensorClass().prototype.pow = function (e) {\n  return this.throwIfDisposed(), pow$2(this, e);\n}, getGlobalTensorClass().prototype.prelu = function (e) {\n  return this.throwIfDisposed(), prelu$3(this, e);\n}, getGlobalTensorClass().prototype.prod = function (e, t) {\n  return this.throwIfDisposed(), prod$2(this, e, t);\n}, getGlobalTensorClass().prototype.reciprocal = function () {\n  return this.throwIfDisposed(), reciprocal$2(this);\n}, getGlobalTensorClass().prototype.relu = function () {\n  return this.throwIfDisposed(), relu$3(this);\n}, getGlobalTensorClass().prototype.relu6 = function () {\n  return this.throwIfDisposed(), relu6$2(this);\n}, getGlobalTensorClass().prototype.reshapeAs = function (e) {\n  return this.throwIfDisposed(), reshape$3(this, e.shape);\n}, getGlobalTensorClass().prototype.reshape = function (e) {\n  return this.throwIfDisposed(), reshape$3(this, e);\n}, getGlobalTensorClass().prototype.resizeBilinear = function (e, t, n) {\n  return this.throwIfDisposed(), resizeBilinear$2(this, e, t, n);\n}, getGlobalTensorClass().prototype.resizeNearestNeighbor = function (e, t, n) {\n  return this.throwIfDisposed(), resizeNearestNeighbor$2(this, e, t, n);\n}, getGlobalTensorClass().prototype.reverse = function (e) {\n  return this.throwIfDisposed(), reverse$2(this, e);\n}, getGlobalTensorClass().prototype.rfft = function () {\n  return this.throwIfDisposed(), rfft(this);\n}, getGlobalTensorClass().prototype.round = function () {\n  return this.throwIfDisposed(), round$2(this);\n}, getGlobalTensorClass().prototype.rsqrt = function () {\n  return this.throwIfDisposed(), rsqrt$2(this);\n}, getGlobalTensorClass().prototype.selu = function () {\n  return this.throwIfDisposed(), selu$2(this);\n}, getGlobalTensorClass().prototype.separableConv2d = function (e, t, n, r, a, s) {\n  return this.throwIfDisposed(), separableConv2d$1(this, e, t, n, r, a, s);\n}, getGlobalTensorClass().prototype.sigmoid = function () {\n  return this.throwIfDisposed(), sigmoid$2(this);\n}, getGlobalTensorClass().prototype.sign = function () {\n  return this.throwIfDisposed(), sign$2(this);\n}, getGlobalTensorClass().prototype.sin = function () {\n  return this.throwIfDisposed(), sin$2(this);\n}, getGlobalTensorClass().prototype.sinh = function () {\n  return this.throwIfDisposed(), sinh$2(this);\n}, getGlobalTensorClass().prototype.slice = function (e, t) {\n  return this.throwIfDisposed(), slice$2(this, e, t);\n}, getGlobalTensorClass().prototype.softmax = function (e) {\n  return this.throwIfDisposed(), softmax$3(this, e);\n}, getGlobalTensorClass().prototype.softplus = function () {\n  return this.throwIfDisposed(), softplus$2(this);\n}, getGlobalTensorClass().prototype.spaceToBatchND = function (e, t) {\n  return this.throwIfDisposed(), spaceToBatchND$2(this, e, t);\n}, getGlobalTensorClass().prototype.split = function (e, t) {\n  return this.throwIfDisposed(), split$2(this, e, t);\n}, getGlobalTensorClass().prototype.sqrt = function () {\n  return this.throwIfDisposed(), sqrt$2(this);\n}, getGlobalTensorClass().prototype.square = function () {\n  return this.throwIfDisposed(), square$2(this);\n}, getGlobalTensorClass().prototype.squaredDifference = function (e) {\n  return this.throwIfDisposed(), squaredDifference$2(this, e);\n}, getGlobalTensorClass().prototype.squeeze = function (e) {\n  return this.throwIfDisposed(), squeeze(this, e);\n}, getGlobalTensorClass().prototype.stack = function (e, t) {\n  this.throwIfDisposed();\n  var n = e instanceof Tensor ? [this, e] : [this, ...e];\n  return stack(n, t);\n}, getGlobalTensorClass().prototype.step = function (e) {\n  return this.throwIfDisposed(), step$2(this, e);\n}, getGlobalTensorClass().prototype.stridedSlice = function (e, t, n, r, a, s, o, i) {\n  return this.throwIfDisposed(), stridedSlice$2(this, e, t, n, r, a, s, o, i);\n}, getGlobalTensorClass().prototype.sub = function (e) {\n  return this.throwIfDisposed(), sub$2(this, e);\n}, getGlobalTensorClass().prototype.sum = function (e, t) {\n  return this.throwIfDisposed(), sum$2(this, e, t);\n}, getGlobalTensorClass().prototype.tan = function () {\n  return this.throwIfDisposed(), tan$2(this);\n}, getGlobalTensorClass().prototype.tanh = function () {\n  return this.throwIfDisposed(), tanh$2(this);\n}, getGlobalTensorClass().prototype.tile = function (e) {\n  return this.throwIfDisposed(), tile$3(this, e);\n}, getGlobalTensorClass().prototype.toBool = function () {\n  return this.throwIfDisposed(), cast$3(this, \"bool\");\n}, getGlobalTensorClass().prototype.toFloat = function () {\n  return this.throwIfDisposed(), cast$3(this, \"float32\");\n}, getGlobalTensorClass().prototype.toInt = function () {\n  return this.throwIfDisposed(), cast$3(this, \"int32\");\n}, getGlobalTensorClass().prototype.topk = function (e, t) {\n  return this.throwIfDisposed(), topk(this, e, t);\n}, getGlobalTensorClass().prototype.transpose = function (e) {\n  return this.throwIfDisposed(), transpose$2(this, e);\n}, getGlobalTensorClass().prototype.unique = function (e) {\n  return this.throwIfDisposed(), unique$3(this, e);\n}, getGlobalTensorClass().prototype.unsortedSegmentSum = function (e, t) {\n  return this.throwIfDisposed(), unsortedSegmentSum$2(this, e, t);\n}, getGlobalTensorClass().prototype.unstack = function (e) {\n  return this.throwIfDisposed(), unstack(this, e);\n}, getGlobalTensorClass().prototype.where = function (e, t) {\n  return this.throwIfDisposed(), where(e, this, t);\n}, getGlobalTensorClass().prototype.zerosLike = function () {\n  return this.throwIfDisposed(), zerosLike$2(this);\n};\n\nclass AttributeError extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, AttributeError.prototype);\n  }\n\n}\n\nclass RuntimeError extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, RuntimeError.prototype);\n  }\n\n}\n\nclass ValueError extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, ValueError.prototype);\n  }\n\n}\n\nclass NotImplementedError extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, NotImplementedError.prototype);\n  }\n\n}\n\nclass AssertionError extends Error {\n  constructor(e) {\n    super(e), Object.setPrototypeOf(this, AssertionError.prototype);\n  }\n\n}\n\nfunction pyListRepeat(e, t) {\n  if (Array.isArray(e)) {\n    var n = [];\n\n    for (var r = 0; r < t; r++) {\n      n = n.concat(e);\n    }\n\n    return n;\n  }\n\n  {\n    var _n251 = new Array(t);\n\n    return _n251.fill(e), _n251;\n  }\n}\n\nfunction assert$3(e, t) {\n  if (!e) throw new AssertionError(t);\n}\n\nfunction count(e, t) {\n  var n = 0;\n\n  for (var r of e) {\n    r === t && n++;\n  }\n\n  return n;\n}\n\nfunction singletonOrArray(e) {\n  return 1 === e.length ? e[0] : e;\n}\n\nfunction toList(e) {\n  return Array.isArray(e) ? e : [e];\n}\n\nfunction toSnakeCase(e) {\n  var t = e.replace(/(.)([A-Z][a-z0-9]+)/g, \"$1_$2\").replace(/([a-z])([A-Z])/g, \"$1_$2\").toLowerCase();\n  return \"_\" !== t[0] ? t : \"private\" + t;\n}\n\nfunction toCamelCase(e) {\n  return e.length <= 1 || -1 === e.indexOf(\"_\") ? e : e.replace(/[_]+(\\w|$)/g, (e, t) => t.toUpperCase());\n}\n\nvar _GLOBAL_CUSTOM_OBJECTS = {};\n\nfunction serializeKerasObject(e) {\n  if (null == e) return null;\n  var t = {};\n  return t.className = e.getClassName(), t.config = e.getConfig(), t;\n}\n\nfunction convertNDArrayScalarsInConfig(e) {\n  if (null != e && \"object\" == typeof e) if (Array.isArray(e)) e.forEach(e => convertNDArrayScalarsInConfig(e));else {\n    var t = Object.keys(e);\n\n    for (var n of t) {\n      var _t438 = e[n];\n      null != _t438 && \"object\" == typeof _t438 && (Array.isArray(_t438) || \"ndarray\" !== _t438.type || \"number\" != typeof _t438.value ? convertNDArrayScalarsInConfig(_t438) : e[n] = _t438.value);\n    }\n  }\n}\n\nfunction deserializeKerasObject(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"object\";\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n\n  if (\"string\" == typeof e) {\n    var _a141 = e;\n    var s;\n    if (_a141 in n) s = n[_a141];else if (_a141 in _GLOBAL_CUSTOM_OBJECTS) s = _GLOBAL_CUSTOM_OBJECTS[_a141];else if (s = t[_a141], null == s) throw new ValueError(\"Unknown \".concat(r, \": \").concat(e, \". This may be due to one of the following reasons:\\n1. The \").concat(r, \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \").concat(r, \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\"));\n    return s;\n  }\n\n  {\n    var _s100 = e;\n    if (null == _s100.className || null == _s100.config) throw new ValueError(\"\".concat(r, \": Improper config format: \").concat(JSON.stringify(_s100), \".\\n'className' and 'config' must set.\"));\n    var o = _s100.className;\n    var i, l;\n    if (o in n ? [i, l] = n[o] : o in _GLOBAL_CUSTOM_OBJECTS ? [i, l] = _GLOBAL_CUSTOM_OBJECTS.className : o in t && ([i, l] = t[o]), null == i) throw new ValueError(\"Unknown \".concat(r, \": \").concat(o, \". This may be due to one of the following reasons:\\n1. The \").concat(r, \" is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\\n2. The custom \").concat(r, \" is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().\"));\n\n    if (null != l) {\n      var _e615 = {};\n\n      for (var _t440 of Object.keys(_GLOBAL_CUSTOM_OBJECTS)) {\n        _e615[_t440] = _GLOBAL_CUSTOM_OBJECTS[_t440];\n      }\n\n      for (var _t441 of Object.keys(n)) {\n        _e615[_t441] = n[_t441];\n      }\n\n      _s100.config.customObjects = _e615;\n\n      var _t439 = Object.assign({}, _GLOBAL_CUSTOM_OBJECTS);\n\n      for (var _e616 of Object.keys(n)) {\n        _GLOBAL_CUSTOM_OBJECTS[_e616] = n[_e616];\n      }\n\n      convertNDArrayScalarsInConfig(_s100.config);\n\n      var _r204 = l(i, _s100.config, n, a);\n\n      return _GLOBAL_CUSTOM_OBJECTS = Object.assign({}, _t439), _r204;\n    }\n\n    {\n      var _e617 = Object.assign({}, _GLOBAL_CUSTOM_OBJECTS);\n\n      for (var _e618 of Object.keys(n)) {\n        _GLOBAL_CUSTOM_OBJECTS[_e618] = n[_e618];\n      }\n\n      var _t442 = new i(_s100.config);\n\n      return _GLOBAL_CUSTOM_OBJECTS = Object.assign({}, _e617), _t442;\n    }\n  }\n}\n\nfunction numberCompare(e, t) {\n  return e < t ? -1 : e > t ? 1 : 0;\n}\n\nfunction reverseNumberCompare(e, t) {\n  return -1 * numberCompare(e, t);\n}\n\nfunction unique$2(e) {\n  if (null == e) return e;\n  var t = [];\n\n  for (var n of e) {\n    -1 === t.indexOf(n) && t.push(n);\n  }\n\n  return t;\n}\n\nfunction isObjectEmpty(e) {\n  if (null == e) throw new ValueError(\"Invalid value in obj: \".concat(JSON.stringify(e)));\n\n  for (var t in e) {\n    if (e.hasOwnProperty(t)) return !1;\n  }\n\n  return !0;\n}\n\nfunction checkStringTypeUnionValue(e, t, n) {\n  if (null != n && e.indexOf(n) < 0) throw new ValueError(\"\".concat(n, \" is not a valid \").concat(t, \".  Valid values are \").concat(e, \" or null/undefined.\"));\n}\n\nfunction checkArrayTypeAndLength(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : Infinity;\n  return assert$3(n >= 0), assert$3(r >= n), Array.isArray(e) && e.length >= n && e.length <= r && e.every(e => typeof e === t);\n}\n\nfunction assertPositiveInteger(e, t) {\n  Array.isArray(e) ? (assert$4(e.length > 0, () => \"\".concat(t, \" is unexpectedly an empty array.\")), e.forEach((e, n) => assertPositiveInteger(e, \"element \".concat(n + 1, \" of \").concat(t)))) : assert$4(Number.isInteger(e) && e > 0, () => \"Expected \".concat(t, \" to be a positive integer, but got \").concat(formatAsFriendlyString(e), \".\"));\n}\n\nfunction formatAsFriendlyString(e) {\n  return null === e ? \"null\" : Array.isArray(e) ? \"[\" + e.map(e => formatAsFriendlyString(e)).join(\",\") + \"]\" : \"string\" == typeof e ? \"\\\"\".concat(e, \"\\\"\") : \"\".concat(e);\n}\n\nfunction debounce(e, t) {\n  var n,\n      r = now();\n  return function () {\n    var s = now();\n    return s - r < t || (r = s, n = e(...arguments)), n;\n  };\n}\n\nfunction mapActivationToFusedKernel(e) {\n  return \"relu\" === e ? \"relu\" : \"linear\" === e ? \"linear\" : \"elu\" === e ? \"elu\" : null;\n}\n\nfunction calcL2Norms(e, t) {\n  return tidy(() => sqrt$2(sum$2(mul(e, e), t, !0)));\n}\n\nclass Constraint extends Serializable {\n  getConfig() {\n    return {};\n  }\n\n}\n\nclass MaxNorm extends Constraint {\n  constructor(e) {\n    super(), this.defaultMaxValue = 2, this.defaultAxis = 0, this.maxValue = null != e.maxValue ? e.maxValue : this.defaultMaxValue, this.axis = null != e.axis ? e.axis : this.defaultAxis;\n  }\n\n  apply(e) {\n    return tidy(() => {\n      var t = calcL2Norms(e, this.axis),\n          n = clipByValue$1(t, 0, this.maxValue);\n      return mul(e, div$1(n, add$2(epsilon$1(), t)));\n    });\n  }\n\n  getConfig() {\n    return {\n      maxValue: this.maxValue,\n      axis: this.axis\n    };\n  }\n\n}\n\nMaxNorm.className = \"MaxNorm\", registerClass(MaxNorm);\n\nclass UnitNorm extends Constraint {\n  constructor(e) {\n    super(), this.defaultAxis = 0, this.axis = null != e.axis ? e.axis : this.defaultAxis;\n  }\n\n  apply(e) {\n    return tidy(() => div$1(e, add$2(epsilon$1(), calcL2Norms(e, this.axis))));\n  }\n\n  getConfig() {\n    return {\n      axis: this.axis\n    };\n  }\n\n}\n\nUnitNorm.className = \"UnitNorm\", registerClass(UnitNorm);\n\nclass NonNeg extends Constraint {\n  apply(e) {\n    return relu$3(e);\n  }\n\n}\n\nNonNeg.className = \"NonNeg\", registerClass(NonNeg);\n\nclass MinMaxNorm extends Constraint {\n  constructor(e) {\n    super(), this.defaultMinValue = 0, this.defaultMaxValue = 1, this.defaultRate = 1, this.defaultAxis = 0, this.minValue = null != e.minValue ? e.minValue : this.defaultMinValue, this.maxValue = null != e.maxValue ? e.maxValue : this.defaultMaxValue, this.rate = null != e.rate ? e.rate : this.defaultRate, this.axis = null != e.axis ? e.axis : this.defaultAxis;\n  }\n\n  apply(e) {\n    return tidy(() => {\n      var t = calcL2Norms(e, this.axis),\n          n = add$2(mul(this.rate, clipByValue$1(t, this.minValue, this.maxValue)), mul(1 - this.rate, t));\n      return mul(e, div$1(n, add$2(epsilon$1(), t)));\n    });\n  }\n\n  getConfig() {\n    return {\n      minValue: this.minValue,\n      maxValue: this.maxValue,\n      rate: this.rate,\n      axis: this.axis\n    };\n  }\n\n}\n\nMinMaxNorm.className = \"MinMaxNorm\", registerClass(MinMaxNorm);\nvar CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP = {\n  maxNorm: \"MaxNorm\",\n  minMaxNorm: \"MinMaxNorm\",\n  nonNeg: \"NonNeg\",\n  unitNorm: \"UnitNorm\"\n};\n\nfunction serializeConstraint(e) {\n  return serializeKerasObject(e);\n}\n\nfunction deserializeConstraint(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject(e, SerializationMap.getMap().classNameMap, t, \"constraint\");\n}\n\nfunction getConstraint(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeConstraint({\n    className: e in CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP ? CONSTRAINT_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e,\n    config: {}\n  }) : e instanceof Constraint ? e : deserializeConstraint(e);\n}\n\nfunction maxNorm(e) {\n  return new MaxNorm(e);\n}\n\nfunction unitNorm(e) {\n  return new UnitNorm(e);\n}\n\nfunction nonNeg() {\n  return new NonNeg();\n}\n\nfunction minMaxNorm(e) {\n  return new MinMaxNorm(e);\n}\n\nvar exports_constraints = {\n  __proto__: null,\n  maxNorm,\n  unitNorm,\n  nonNeg,\n  minMaxNorm\n};\nvar VALID_DATA_FORMAT_VALUES = [\"channelsFirst\", \"channelsLast\"],\n    VALID_INTERPOLATION_FORMAT_VALUES = [\"nearest\", \"bilinear\"],\n    VALID_PADDING_MODE_VALUES = [\"valid\", \"same\", \"causal\"],\n    VALID_POOL_MODE_VALUES = [\"max\", \"avg\"],\n    VALID_BIDIRECTIONAL_MERGE_MODES = [\"sum\", \"mul\", \"concat\", \"ave\"],\n    nameMap = new Map();\n\nfunction checkDataFormat(e) {\n  checkStringTypeUnionValue(VALID_DATA_FORMAT_VALUES, \"DataFormat\", e);\n}\n\nfunction checkInterpolationFormat(e) {\n  checkStringTypeUnionValue(VALID_INTERPOLATION_FORMAT_VALUES, \"InterpolationFormat\", e);\n}\n\nfunction checkPaddingMode(e) {\n  checkStringTypeUnionValue(VALID_PADDING_MODE_VALUES, \"PaddingMode\", e);\n}\n\nfunction checkPoolMode(e) {\n  checkStringTypeUnionValue(VALID_POOL_MODE_VALUES, \"PoolMode\", e);\n}\n\nvar _nameScopeStack = [],\n    _nameScopeDivider = \"/\";\n\nfunction nameScope(e, t) {\n  _nameScopeStack.push(e);\n\n  try {\n    var _e619 = t();\n\n    return _nameScopeStack.pop(), _e619;\n  } catch (e) {\n    throw _nameScopeStack.pop(), e;\n  }\n}\n\nfunction currentNameScopePrefix() {\n  return 0 === _nameScopeStack.length ? \"\" : _nameScopeStack.join(_nameScopeDivider) + _nameScopeDivider;\n}\n\nfunction getScopedTensorName(e) {\n  if (!isValidTensorName(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");\n  return currentNameScopePrefix() + e;\n}\n\nfunction getUniqueTensorName(e) {\n  if (!isValidTensorName(e)) throw new Error(\"Not a valid tensor name: '\" + e + \"'\");\n  nameMap.has(e) || nameMap.set(e, 0);\n  var t = nameMap.get(e);\n\n  if (nameMap.set(e, nameMap.get(e) + 1), t > 0) {\n    var n = \"\".concat(e, \"_\").concat(t);\n    return nameMap.set(n, 1), n;\n  }\n\n  return e;\n}\n\nvar tensorNameRegex = new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\\._\\/]*$/);\n\nfunction isValidTensorName(e) {\n  return !!e.match(tensorNameRegex);\n}\n\nfunction isInteger(e) {\n  return e === parseInt(e.toString(), 10);\n}\n\nfunction arrayProd(e, t, n) {\n  null == t && (t = 0), null == n && (n = e.length);\n  var r = 1;\n\n  for (var a = t; a < n; ++a) {\n    r *= e[a];\n  }\n\n  return r;\n}\n\nfunction min$2(e) {\n  if (0 === e.length) return Number.NaN;\n  var t = Number.POSITIVE_INFINITY;\n\n  for (var n = 0; n < e.length; n++) {\n    var r = e[n];\n    r < t && (t = r);\n  }\n\n  return t;\n}\n\nfunction max$2(e) {\n  if (0 === e.length) return Number.NaN;\n  var t = Number.NEGATIVE_INFINITY;\n\n  for (var n = 0; n < e.length; n++) {\n    var r = e[n];\n    r > t && (t = r);\n  }\n\n  return t;\n}\n\nfunction range$3(e, t) {\n  if (t < e) throw new ValueError(\"end (\".concat(t, \") < begin (\").concat(e, \") is forbidden.\"));\n  var n = [];\n\n  for (var r = e; r < t; ++r) {\n    n.push(r);\n  }\n\n  return n;\n}\n\nfunction cast$2(e, t) {\n  return cast$3(e, t);\n}\n\nfunction expandDims$2(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n = e.shape.slice();\n  return t < 0 && (t = n.length + t + 1), n.splice(t, 0, 1), reshape$3(e, n);\n}\n\nfunction repeat$1(e, t) {\n  return tidy(() => {\n    if (2 !== e.shape.length) throw new ValueError(\"repeat() expects a rank-2 tensor, but received a rank-\".concat(e.shape.length, \" tensor.\"));\n    return tile$2(expandDims$2(e, 1), [1, t, 1]);\n  });\n}\n\nfunction flatten$2(e) {\n  var t = [arrayProd(e.shape)];\n  return reshape$3(e, t);\n}\n\nfunction batchFlatten(e) {\n  if (e.rank <= 1) throw new ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \".concat(e.rank, \".\"));\n  var t = [e.shape[0], arrayProd(e.shape, 1)];\n  return reshape$3(e, t);\n}\n\nfunction sliceAlongFirstAxis(e, t, n) {\n  return tidy(() => {\n    switch (e.rank) {\n      case 1:\n        return slice1d(e, t, n);\n\n      case 2:\n        return slice2d(e, [t, 0], [n, e.shape[1]]);\n\n      case 3:\n        return slice3d(e, [t, 0, 0], [n, e.shape[1], e.shape[2]]);\n\n      case 4:\n        return slice4d(e, [t, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3]]);\n\n      case 5:\n        return slice$2(e, [t, 0, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3], e.shape[4]]);\n\n      case 6:\n        return slice$2(e, [t, 0, 0, 0, 0, 0], [n, e.shape[1], e.shape[2], e.shape[3], e.shape[4], e.shape[5]]);\n\n      default:\n        throw new ValueError(\"sliceAlongFirstAxis() received an unsupported tensor rank: \".concat(e.rank));\n    }\n  });\n}\n\nfunction sliceAlongLastAxis(e, t, n) {\n  return tidy(() => {\n    switch (e.rank) {\n      case 1:\n        return slice1d(e, t, n);\n\n      case 2:\n        return slice2d(e, [0, t], [e.shape[0], n]);\n\n      case 3:\n        return slice3d(e, [0, 0, t], [e.shape[0], e.shape[1], n]);\n\n      case 4:\n        return slice4d(e, [0, 0, 0, t], [e.shape[0], e.shape[1], e.shape[2], n]);\n\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \".concat(e.rank));\n    }\n  });\n}\n\nfunction sliceAlongAxis(e, t, n, r) {\n  return tidy(() => {\n    switch (e.rank) {\n      case 1:\n        return slice1d(e, t, n);\n\n      case 2:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis(e, t, n);\n\n          case 2:\n            return sliceAlongLastAxis(e, t, n);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \".concat(r));\n        }\n\n      case 3:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis(e, t, n);\n\n          case 2:\n            return slice3d(e, [0, t, 0], [e.shape[0], n, e.shape[2]]);\n\n          case 3:\n            return sliceAlongLastAxis(e, t, n);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \".concat(r));\n        }\n\n      case 4:\n        switch (r) {\n          case 1:\n            return sliceAlongFirstAxis(e, t, n);\n\n          case 2:\n            return slice4d(e, [0, t, 0, 0], [e.shape[0], n, e.shape[2], e.shape[3]]);\n\n          case 3:\n            return slice4d(e, [0, 0, t, 0], [e.shape[0], e.shape[1], n, e.shape[3]]);\n\n          case 4:\n            return sliceAlongLastAxis(e, t, n);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \".concat(r));\n        }\n\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \".concat(e.rank));\n    }\n  });\n}\n\nfunction concatenate$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var n;\n  return t < 0 && (n = e[0].rank, t = 0 !== n ? n : 0), t === e[0].rank && (t = -1), concat$2(e, t);\n}\n\nfunction concatAlongFirstAxis(e, t) {\n  switch (e.rank) {\n    case 1:\n      return concat1d([e, t]);\n\n    case 2:\n      return concat2d([e, t], 0);\n\n    case 3:\n      return concat3d([e, t], 0);\n\n    case 4:\n      return concat4d([e, t], 0);\n\n    default:\n      throw new ValueError(\"concatAlongFirstAxis() received an unsupported tensor rank: \".concat(e.rank));\n  }\n}\n\nfunction tile$2(e, t) {\n  if (Array.isArray(t) || (t = [t]), e.rank !== t.length) throw new ValueError(\"The length of input n (\".concat(t.length, \") does not match the number of dimensions in input x (\").concat(e.rank, \")\"));\n  return tile$3(e, t);\n}\n\nfunction randomNormal$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0;\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  return randomNormal$2(e, t, n, r, a);\n}\n\nfunction dot$1(e, t, n, r) {\n  if (e.rank < 2 || t.rank < 2) throw new NotImplementedError(\"dot requires both inputs to be rank >= 2 but got x shape = \".concat(e.shape, \" and y shape = \").concat(t.shape));\n  if (t.rank >= 3 && e.shape.slice(-1)[0] !== t.shape.slice(-2)[0]) throw new NotImplementedError(\"If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = \".concat(e.shape, \" and  y shape = \").concat(t.shape));\n  if (2 === e.rank && 2 === t.rank) return matMul({\n    a: e,\n    b: t,\n    transposeA: !1,\n    transposeB: !1,\n    bias: r ? reshapeBias(e.rank, r, imageDataFormat()) : null,\n    activation: n\n  });\n  {\n    var a = e.shape.slice(),\n        s = a.pop();\n    e = reshape$3(e, [-1, s]);\n    var o = t.shape.slice(),\n        i = o.pop(),\n        l = o.pop(),\n        u = [...o, i],\n        c = Array.from({\n      length: t.rank\n    }, (e, n) => 0 === n ? t.rank - 2 : n <= t.rank - 2 ? n - 1 : n);\n    t = reshape$3(transpose$2(t, c), [l, -1]);\n    var _p23 = [...a, ...u];\n    return reshape$3(matMul({\n      a: e,\n      b: t,\n      transposeA: !1,\n      transposeB: !1,\n      bias: r ? reshapeBias(e.rank, r, imageDataFormat()) : null,\n      activation: n\n    }), _p23);\n  }\n}\n\nfunction gather(e, t, n) {\n  return tidy(() => (t = Array.isArray(t) ? tensor1d(t, \"int32\") : cast$3(t, \"int32\"), gather$1(e, t, n)));\n}\n\nfunction square$1(e) {\n  return mul(e, e);\n}\n\nfunction reshapeBias(e, t, n) {\n  var r = t.shape;\n  if (1 !== t.rank && t.rank !== e) throw new ValueError(\"Unexpected bias dimensions: \".concat(t.rank, \"; expected it to be 1 or \").concat(e));\n\n  if (5 === e) {\n    if (\"channelsFirst\" === n) return reshape$3(t, 1 === r.length ? [1, r[0], 1, 1, 1] : [1, r[3], r[0], r[1], r[2]]);\n    if (\"channelsLast\" === n) return reshape$3(t, 1 === r.length ? [1, 1, 1, 1, r[0]] : [1].concat(r));\n  } else if (4 === e) {\n    if (\"channelsFirst\" === n) return reshape$3(t, 1 === r.length ? [1, r[0], 1, 1] : [1, r[2], r[0], r[1]]);\n    if (\"channelsLast\" === n) return reshape$3(t, 1 === r.length ? [1, 1, 1, r[0]] : [1].concat(r));\n  } else if (3 === e) {\n    if (\"channelsFirst\" === n) return reshape$3(t, 1 === r.length ? [1, r[0], 1] : [1, r[1], r[0]]);\n    if (\"channelsLast\" === n) return reshape$3(t, 1 === r.length ? [1, 1, r[0]] : [1].concat(r));\n  } else if (e < 3) return t;\n\n  throw new ValueError(\"Unsupported input rank by biasAdd: \".concat(t.rank));\n}\n\nfunction biasAdd(e, t, n) {\n  return tidy(() => (null == n && (n = imageDataFormat()), checkDataFormat(n), add$2(e, reshapeBias(e.rank, t, n))));\n}\n\nfunction elu$3(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  if (1 !== t) throw new NotImplementedError(\"Support for alpha values other than 1 (\".concat(t, \") is not implemented yet.\"));\n  return elu$4(e);\n}\n\nfunction softsign(e) {\n  return tidy(() => div$1(e, add$2(abs$2(e), 1)));\n}\n\nfunction dropout$1(e, t, n, r) {\n  return tidy(() => dropout$2(e, t, n, r));\n}\n\nfunction hardSigmoid(e) {\n  return tidy(() => {\n    var t = add$2(.5, mul(.2, e));\n    return clipByValue$1(t, 0, 1);\n  });\n}\n\nfunction inTrainPhase(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return n ? e() : t();\n}\n\nvar VALID_FAN_MODE_VALUES = [\"fanIn\", \"fanOut\", \"fanAvg\"],\n    VALID_DISTRIBUTION_VALUES = [\"normal\", \"uniform\", \"truncatedNormal\"];\n\nfunction checkFanMode(e) {\n  checkStringTypeUnionValue(VALID_FAN_MODE_VALUES, \"FanMode\", e);\n}\n\nfunction checkDistribution(e) {\n  checkStringTypeUnionValue(VALID_DISTRIBUTION_VALUES, \"Distribution\", e);\n}\n\nclass Initializer extends Serializable {\n  fromConfigUsesCustomObjects() {\n    return !1;\n  }\n\n  getConfig() {\n    return {};\n  }\n\n}\n\nclass Zeros extends Initializer {\n  apply(e, t) {\n    return zeros$2(e, t);\n  }\n\n}\n\nZeros.className = \"Zeros\", registerClass(Zeros);\n\nclass Ones extends Initializer {\n  apply(e, t) {\n    return ones$1(e, t);\n  }\n\n}\n\nOnes.className = \"Ones\", registerClass(Ones);\n\nclass Constant extends Initializer {\n  constructor(e) {\n    if (super(), \"object\" != typeof e) throw new ValueError(\"Expected argument of type ConstantConfig but got \".concat(e));\n    if (void 0 === e.value) throw new ValueError(\"config must have value set but got \".concat(e));\n    this.value = e.value;\n  }\n\n  apply(e, t) {\n    return tidy(() => mul(scalar(this.value), ones$1(e, t)));\n  }\n\n  getConfig() {\n    return {\n      value: this.value\n    };\n  }\n\n}\n\nConstant.className = \"Constant\", registerClass(Constant);\n\nclass RandomUniform extends Initializer {\n  constructor(e) {\n    super(), this.DEFAULT_MINVAL = -.05, this.DEFAULT_MAXVAL = .05, this.minval = e.minval || this.DEFAULT_MINVAL, this.maxval = e.maxval || this.DEFAULT_MAXVAL, this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    return randomUniform$1(e, this.minval, this.maxval, t);\n  }\n\n  getConfig() {\n    return {\n      minval: this.minval,\n      maxval: this.maxval,\n      seed: this.seed\n    };\n  }\n\n}\n\nRandomUniform.className = \"RandomUniform\", registerClass(RandomUniform);\n\nclass RandomNormal extends Initializer {\n  constructor(e) {\n    super(), this.DEFAULT_MEAN = 0, this.DEFAULT_STDDEV = .05, this.mean = e.mean || this.DEFAULT_MEAN, this.stddev = e.stddev || this.DEFAULT_STDDEV, this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(\"randomNormal does not support dType \".concat(t, \".\"));\n    return randomNormal$1(e, this.mean, this.stddev, t, this.seed);\n  }\n\n  getConfig() {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }\n\n}\n\nRandomNormal.className = \"RandomNormal\", registerClass(RandomNormal);\n\nclass TruncatedNormal extends Initializer {\n  constructor(e) {\n    super(), this.DEFAULT_MEAN = 0, this.DEFAULT_STDDEV = .05, this.mean = e.mean || this.DEFAULT_MEAN, this.stddev = e.stddev || this.DEFAULT_STDDEV, this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(\"truncatedNormal does not support dType \".concat(t, \".\"));\n    return truncatedNormal$1(e, this.mean, this.stddev, t, this.seed);\n  }\n\n  getConfig() {\n    return {\n      mean: this.mean,\n      stddev: this.stddev,\n      seed: this.seed\n    };\n  }\n\n}\n\nTruncatedNormal.className = \"TruncatedNormal\", registerClass(TruncatedNormal);\n\nclass Identity extends Initializer {\n  constructor(e) {\n    super(), this.gain = null != e.gain ? e.gain : 1;\n  }\n\n  apply(e, t) {\n    return tidy(() => {\n      if (2 !== e.length || e[0] !== e[1]) throw new ValueError(\"Identity matrix initializer can only be used for 2D square matrices.\");\n      return mul(this.gain, eye(e[0]));\n    });\n  }\n\n  getConfig() {\n    return {\n      gain: this.gain\n    };\n  }\n\n}\n\nfunction computeFans(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"channelsLast\";\n  var n, r;\n  if (checkDataFormat(t), 2 === e.length) n = e[0], r = e[1];else if (-1 !== [3, 4, 5].indexOf(e.length)) {\n    if (\"channelsFirst\" === t) {\n      var _t443 = arrayProd(e, 2);\n\n      n = e[1] * _t443, r = e[0] * _t443;\n    } else if (\"channelsLast\" === t) {\n      var _t444 = arrayProd(e, 0, e.length - 2);\n\n      n = e[e.length - 2] * _t444, r = e[e.length - 1] * _t444;\n    }\n  } else {\n    var _t445 = arrayProd(e);\n\n    n = Math.sqrt(_t445), r = Math.sqrt(_t445);\n  }\n  return [n, r];\n}\n\nIdentity.className = \"Identity\", registerClass(Identity);\n\nclass VarianceScaling extends Initializer {\n  constructor(e) {\n    if (super(), e.scale < 0) throw new ValueError(\"scale must be a positive float. Got: \".concat(e.scale));\n    this.scale = null == e.scale ? 1 : e.scale, this.mode = null == e.mode ? \"fanIn\" : e.mode, checkFanMode(this.mode), this.distribution = null == e.distribution ? \"normal\" : e.distribution, checkDistribution(this.distribution), this.seed = e.seed;\n  }\n\n  apply(e, t) {\n    var n = computeFans(e),\n        r = n[0],\n        a = n[1];\n    var s = this.scale;\n\n    if (s /= \"fanIn\" === this.mode ? Math.max(1, r) : \"fanOut\" === this.mode ? Math.max(1, a) : Math.max(1, (r + a) / 2), \"normal\" === this.distribution) {\n      var _n252 = Math.sqrt(s);\n\n      if (\"float32\" !== (t = t || \"float32\") && \"int32\" !== t) throw new NotImplementedError(\"\".concat(this.getClassName(), \" does not support dType \").concat(t, \".\"));\n      return truncatedNormal$1(e, 0, _n252, t, this.seed);\n    }\n\n    {\n      var _n253 = Math.sqrt(3 * s);\n\n      return randomUniform$1(e, -_n253, _n253, t);\n    }\n  }\n\n  getConfig() {\n    return {\n      scale: this.scale,\n      mode: this.mode,\n      distribution: this.distribution,\n      seed: this.seed\n    };\n  }\n\n}\n\nVarianceScaling.className = \"VarianceScaling\", registerClass(VarianceScaling);\n\nclass GlorotUniform extends VarianceScaling {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanAvg\",\n      distribution: \"uniform\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling.className;\n  }\n\n}\n\nGlorotUniform.className = \"GlorotUniform\", registerClass(GlorotUniform);\n\nclass GlorotNormal extends VarianceScaling {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanAvg\",\n      distribution: \"normal\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling.className;\n  }\n\n}\n\nGlorotNormal.className = \"GlorotNormal\", registerClass(GlorotNormal);\n\nclass HeNormal extends VarianceScaling {\n  constructor(e) {\n    super({\n      scale: 2,\n      mode: \"fanIn\",\n      distribution: \"normal\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling.className;\n  }\n\n}\n\nHeNormal.className = \"HeNormal\", registerClass(HeNormal);\n\nclass HeUniform extends VarianceScaling {\n  constructor(e) {\n    super({\n      scale: 2,\n      mode: \"fanIn\",\n      distribution: \"uniform\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling.className;\n  }\n\n}\n\nHeUniform.className = \"HeUniform\", registerClass(HeUniform);\n\nclass LeCunNormal extends VarianceScaling {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanIn\",\n      distribution: \"normal\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling.className;\n  }\n\n}\n\nLeCunNormal.className = \"LeCunNormal\", registerClass(LeCunNormal);\n\nclass LeCunUniform extends VarianceScaling {\n  constructor(e) {\n    super({\n      scale: 1,\n      mode: \"fanIn\",\n      distribution: \"uniform\",\n      seed: null == e ? null : e.seed\n    });\n  }\n\n  getClassName() {\n    return VarianceScaling.className;\n  }\n\n}\n\nLeCunUniform.className = \"LeCunNormal\", registerClass(LeCunUniform);\n\nclass Orthogonal extends Initializer {\n  constructor(e) {\n    if (super(), this.DEFAULT_GAIN = 1, this.gain = null == e.gain ? this.DEFAULT_GAIN : e.gain, this.seed = e.seed, null != this.seed) throw new NotImplementedError(\"Random seed is not implemented for Orthogonal Initializer yet.\");\n  }\n\n  apply(e, t) {\n    return tidy(() => {\n      if (e.length < 2) throw new NotImplementedError(\"Shape must be at least 2D.\");\n      e[0] * e[1] > 2e3 && console.warn(\"Orthogonal initializer is being called on a matrix with more than 2000 (\".concat(e[0] * e[1], \") elements: Slowness may result.\"));\n      var t = randomNormal$1(e[0] > e[1] ? [e[1], e[0]] : e, 0, 1, \"float32\");\n      var n = linalg.gramSchmidt(t);\n      return e[0] > e[1] && (n = transpose$2(n)), mul(this.gain, n);\n    });\n  }\n\n  getConfig() {\n    return {\n      gain: this.gain,\n      seed: this.seed\n    };\n  }\n\n}\n\nOrthogonal.className = \"Orthogonal\", registerClass(Orthogonal);\nvar INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = {\n  constant: \"Constant\",\n  glorotNormal: \"GlorotNormal\",\n  glorotUniform: \"GlorotUniform\",\n  heNormal: \"HeNormal\",\n  heUniform: \"HeUniform\",\n  identity: \"Identity\",\n  leCunNormal: \"LeCunNormal\",\n  leCunUniform: \"LeCunUniform\",\n  ones: \"Ones\",\n  orthogonal: \"Orthogonal\",\n  randomNormal: \"RandomNormal\",\n  randomUniform: \"RandomUniform\",\n  truncatedNormal: \"TruncatedNormal\",\n  varianceScaling: \"VarianceScaling\",\n  zeros: \"Zeros\"\n};\n\nfunction deserializeInitializer(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject(e, SerializationMap.getMap().classNameMap, t, \"initializer\");\n}\n\nfunction serializeInitializer(e) {\n  return serializeKerasObject(e);\n}\n\nfunction getInitializer(e) {\n  if (\"string\" == typeof e) {\n    var t = e in INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? INITIALIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e;\n    if (\"GlorotNormal\" === t) return new GlorotNormal();\n    if (\"GlorotUniform\" === t) return new GlorotUniform();\n    if (\"HeNormal\" === t) return new HeNormal();\n    if (\"HeUniform\" === t) return new HeUniform();\n    if (\"LeCunNormal\" === t) return new LeCunNormal();\n    if (\"LeCunUniform\" === t) return new LeCunUniform();\n    {\n      var _e620 = {};\n      return _e620.className = t, _e620.config = {}, deserializeInitializer(_e620);\n    }\n  }\n\n  return e instanceof Initializer ? e : deserializeInitializer(e);\n}\n\nfunction zeros$1() {\n  return new Zeros();\n}\n\nfunction ones() {\n  return new Ones();\n}\n\nfunction constant(e) {\n  return new Constant(e);\n}\n\nfunction randomUniform(e) {\n  return new RandomUniform(e);\n}\n\nfunction randomNormal(e) {\n  return new RandomNormal(e);\n}\n\nfunction truncatedNormal(e) {\n  return new TruncatedNormal(e);\n}\n\nfunction identity$2(e) {\n  return new Identity(e);\n}\n\nfunction varianceScaling(e) {\n  return new VarianceScaling(e);\n}\n\nfunction glorotUniform(e) {\n  return new GlorotUniform(e);\n}\n\nfunction glorotNormal(e) {\n  return new GlorotNormal(e);\n}\n\nfunction heNormal(e) {\n  return new HeNormal(e);\n}\n\nfunction heUniform(e) {\n  return new HeUniform(e);\n}\n\nfunction leCunNormal(e) {\n  return new LeCunNormal(e);\n}\n\nfunction leCunUniform(e) {\n  return new LeCunUniform(e);\n}\n\nfunction orthogonal(e) {\n  return new Orthogonal(e);\n}\n\nvar exports_initializers = {\n  __proto__: null,\n  zeros: zeros$1,\n  ones,\n  constant,\n  randomUniform,\n  randomNormal,\n  truncatedNormal,\n  identity: identity$2,\n  varianceScaling,\n  glorotUniform,\n  glorotNormal,\n  heNormal,\n  heUniform,\n  leCunNormal,\n  leCunUniform,\n  orthogonal\n};\nvar _nextUniqueTensorId = 0;\n\nfunction getNextUniqueTensorId() {\n  return _nextUniqueTensorId++;\n}\n\nvar _uidPrefixes = {};\n\nfunction getUid() {\n  var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : \"\";\n  return e in _uidPrefixes || (_uidPrefixes[e] = 0), _uidPrefixes[e] += 1, e + _uidPrefixes[e].toString();\n}\n\nfunction isArrayOfShapes(e) {\n  return Array.isArray(e) && Array.isArray(e[0]);\n}\n\nfunction normalizeShapeList(e) {\n  return 0 === e.length ? [] : Array.isArray(e[0]) ? e : [e];\n}\n\nfunction getExactlyOneTensor(e) {\n  var t;\n\n  if (Array.isArray(e)) {\n    if (1 !== e.length) throw new ValueError(\"Expected Tensor length to be 1; got \".concat(e.length));\n    t = e[0];\n  } else t = e;\n\n  return t;\n}\n\nfunction getExactlyOneShape(e) {\n  if (Array.isArray(e) && Array.isArray(e[0])) {\n    if (1 === e.length) return (e = e)[0];\n    throw new ValueError(\"Expected exactly 1 Shape; got \".concat(e.length));\n  }\n\n  return e;\n}\n\nfunction countParamsInWeights(e) {\n  var t = 0;\n\n  for (var n of e) {\n    t += 0 === n.shape.length ? 1 : n.shape.reduce((e, t) => e * t);\n  }\n\n  return t;\n}\n\nvar DEFAULT_VARIABLE_NAME_PREFIX = \"Variable\";\n\nclass LayerVariable {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"float32\";\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : DEFAULT_VARIABLE_NAME_PREFIX;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : null;\n    this.dtype = null == t ? \"float32\" : t, this.shape = e.shape, this.id = getNextUniqueTensorId(), this.originalName = getScopedTensorName(n = null == n ? DEFAULT_VARIABLE_NAME_PREFIX : n), this.name = getUniqueTensorName(this.originalName), this.trainable_ = r, this.constraint = a, this.val = variable(e, this.trainable_, this.name, this.dtype);\n  }\n\n  read() {\n    return this.assertNotDisposed(), this.val;\n  }\n\n  write(e) {\n    return this.assertNotDisposed(), checkShapesMatch(this.val, e), this.val.id !== e.id && (this.val.assign(e), null != this.constraint && this.val.assign(this.constraint.apply(this.val))), this;\n  }\n\n  dispose() {\n    this.assertNotDisposed(), this.val.dispose();\n  }\n\n  assertNotDisposed() {\n    if (this.val.isDisposed) throw new Error(\"LayersVariable \".concat(this.name, \" is already disposed.\"));\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(e) {\n    this.trainable_ = e, this.val.trainable = e;\n  }\n\n}\n\nfunction checkShapesMatch(e, t) {\n  if (e.shape.toString() !== t.shape.toString()) throw new Error(\"Shape mismatch: \" + JSON.stringify(e.shape) + \" vs. \" + JSON.stringify(t.shape));\n}\n\nfunction batchGetValue(e) {\n  return e.map(e => e.read());\n}\n\nfunction batchSetValue(e) {\n  e.forEach(e => {\n    e[0].write(e[1]);\n  });\n}\n\nclass InputSpec {\n  constructor(e) {\n    this.dtype = e.dtype, this.shape = e.shape, this.ndim = null != e.shape ? e.shape.length : e.ndim, this.maxNDim = e.maxNDim, this.minNDim = e.minNDim, this.axes = e.axes || {};\n  }\n\n}\n\nclass SymbolicTensor {\n  constructor(e, t, n, r, a, s, o) {\n    this.dtype = e, this.shape = t, this.sourceLayer = n, this.inputs = r, this.callArgs = a, this.outputTensorIndex = o, this.id = getNextUniqueTensorId(), null != s && (this.originalName = getScopedTensorName(s), this.name = getUniqueTensorName(this.originalName)), this.rank = t.length;\n  }\n\n}\n\nvar _nextNodeID = 0;\n\nclass Node {\n  constructor(e, t) {\n    this.callArgs = t, this.id = _nextNodeID++, this.outboundLayer = e.outboundLayer, this.inboundLayers = e.inboundLayers, this.nodeIndices = e.nodeIndices, this.tensorIndices = e.tensorIndices, this.inputTensors = e.inputTensors, this.outputTensors = e.outputTensors, this.inputMasks = e.inputMasks, this.outputMasks = e.outputMasks, this.inputShapes = e.inputShapes, this.outputShapes = e.outputShapes;\n\n    for (var _t446 of e.inboundLayers) {\n      null != _t446 && _t446.outboundNodes.push(this);\n    }\n\n    e.outboundLayer.inboundNodes.push(this);\n  }\n\n  getConfig() {\n    var e = [];\n\n    for (var t of this.inboundLayers) {\n      e.push(null != t ? t.name : null);\n    }\n\n    return {\n      outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,\n      inboundLayers: e,\n      nodeIndices: this.nodeIndices,\n      tensorIndices: this.tensorIndices\n    };\n  }\n\n}\n\nvar _nextLayerID = 0;\n\nclass Layer extends Serializable {\n  constructor() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    super(), this._callHook = null, this._addedWeightNames = [], this._stateful = !1, this.id = _nextLayerID++, this.activityRegularizer = null, this.inputSpec = null, this.supportsMasking = !1, this._trainableWeights = [], this._nonTrainableWeights = [], this._losses = [], this._updates = [], this._built = !1, this.inboundNodes = [], this.outboundNodes = [];\n    var t = e.name;\n\n    if (!t) {\n      var _e621 = this.getClassName();\n\n      t = toSnakeCase(_e621) + \"_\" + getUid(_e621);\n    }\n\n    if (this.name = t, this.trainable_ = null == e.trainable || e.trainable, null != e.inputShape || null != e.batchInputShape) {\n      var _t447;\n\n      if (null != e.batchInputShape) _t447 = e.batchInputShape;else if (null != e.inputShape) {\n        var _n254 = null;\n        null != e.batchSize && (_n254 = e.batchSize), _t447 = [_n254].concat(e.inputShape);\n      }\n      this.batchInputShape = _t447;\n      var n = e.dtype;\n      null == n && (n = e.inputDType), null == n && (n = \"float32\"), this.dtype = n;\n    }\n\n    this.initialWeights = null != e.weights ? e.weights : null, this._refCount = null, this.fastWeightInitDuringBuild = !1;\n  }\n\n  static nodeKey(e, t) {\n    return e.name + \"_ib-\" + t.toString();\n  }\n\n  getNodeAtIndex(e, t) {\n    if (0 === this.inboundNodes.length) throw new RuntimeError(\"The layer has never been called and thus has no defined \".concat(t, \".\"));\n    if (this.inboundNodes.length <= e) throw new ValueError(\"Asked to get \".concat(t, \" at node \").concat(e, \", but the layer has only \").concat(this.inboundNodes.length, \" inbound nodes.\"));\n    return this.inboundNodes[e];\n  }\n\n  getInputAt(e) {\n    return singletonOrArray(this.getNodeAtIndex(e, \"input\").inputTensors);\n  }\n\n  getOutputAt(e) {\n    return singletonOrArray(this.getNodeAtIndex(e, \"output\").outputTensors);\n  }\n\n  get input() {\n    if (this.inboundNodes.length > 1) throw new AttributeError(\"Layer \".concat(this.name, \" has multiple inbound nodes, hence the notion of \\\"layer input\\\" is ill-defined. Use `getInputAt(nodeIndex)` instead.\"));\n    if (0 === this.inboundNodes.length) throw new AttributeError(\"Layer \".concat(this.name, \" is not connected, no input to return.\"));\n    return singletonOrArray(this.getNodeAtIndex(0, \"input\").inputTensors);\n  }\n\n  get output() {\n    if (0 === this.inboundNodes.length) throw new AttributeError(\"Layer \".concat(this.name, \" has no inbound nodes.\"));\n    if (this.inboundNodes.length > 1) throw new AttributeError(\"Layer \".concat(this.name, \" has multiple inbound nodes, hence the notion of \\\"layer output\\\" is ill-defined. Use `getOutputAt(nodeIndex)` instead.\"));\n    return singletonOrArray(this.getNodeAtIndex(0, \"output\").outputTensors);\n  }\n\n  get losses() {\n    return this._losses;\n  }\n\n  calculateLosses() {\n    return this.losses.map(e => e());\n  }\n\n  get updates() {\n    return this._updates;\n  }\n\n  get built() {\n    return this._built;\n  }\n\n  set built(e) {\n    this._built = e;\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(e) {\n    this._trainableWeights.forEach(t => t.trainable = e), this.trainable_ = e;\n  }\n\n  get trainableWeights() {\n    return this.trainable_ ? this._trainableWeights.filter(e => e.trainable) : [];\n  }\n\n  set trainableWeights(e) {\n    this._trainableWeights = e;\n  }\n\n  get nonTrainableWeights() {\n    return this.trainable ? this._trainableWeights.filter(e => !e.trainable).concat(this._nonTrainableWeights) : this._trainableWeights.concat(this._nonTrainableWeights);\n  }\n\n  set nonTrainableWeights(e) {\n    this._nonTrainableWeights = e;\n  }\n\n  get weights() {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  get stateful() {\n    return this._stateful;\n  }\n\n  resetStates() {\n    if (!this.stateful) throw new Error(\"Cannot call the resetStates() method of a non-stateful Layer object.\");\n  }\n\n  assertInputCompatibility(e) {\n    if (e = toList(e), null == this.inputSpec || 0 === this.inputSpec.length) return;\n    var t = toList(this.inputSpec);\n    if (e.length !== t.length) throw new ValueError(\"Layer \".concat(this.name, \" expects \").concat(t.length, \" inputs, but it received \").concat(e.length, \" input tensors. Input received: \").concat(e));\n\n    for (var n = 0; n < e.length; n++) {\n      var r = e[n],\n          a = t[n];\n      if (null == a) continue;\n      var s = r.rank;\n      if (null != a.ndim && s !== a.ndim) throw new ValueError(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected ndim=\").concat(a.ndim, \", found ndim=\").concat(s));\n      if (null != a.maxNDim && s > a.maxNDim) throw new ValueError(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected max_ndim=\").concat(a.maxNDim, \", found ndim=\").concat(s));\n      if (null != a.minNDim && s < a.minNDim) throw new ValueError(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected min_ndim=\").concat(a.minNDim, \", found ndim=\").concat(s, \".\"));\n      if (null != a.dtype && r.dtype !== a.dtype) throw new ValueError(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \" : expected dtype=\").concat(a.dtype, \", found dtype=\").concat(r.dtype, \".\"));\n\n      if (a.axes) {\n        var _e622 = r.shape;\n\n        for (var _t448 in a.axes) {\n          var _r205 = Number(_t448),\n              _s101 = a.axes[_t448],\n              o = _r205 >= 0 ? _e622[_r205] : _e622[_e622.length + _r205];\n\n          if (null != _s101 && -1 === [_s101, null].indexOf(o)) throw new ValueError(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected axis \").concat(_r205, \" of input shape to have value \").concat(_s101, \" but got shape \").concat(_e622, \".\"));\n        }\n      }\n\n      if (null != a.shape) for (var _e623 = 0; _e623 < a.shape.length; ++_e623) {\n        var _t449 = a.shape[_e623],\n            _s102 = r.shape[_e623];\n        if (null != _t449 && null != _s102 && _t449 !== _s102) throw new ValueError(\"Input \".concat(n, \" is incompatible with layer \").concat(this.name, \": expected shape=\").concat(a.shape, \", found shape=\").concat(r.shape, \".\"));\n      }\n    }\n  }\n\n  call(e, t) {\n    return e;\n  }\n\n  invokeCallHook(e, t) {\n    null != this._callHook && this._callHook(e, t);\n  }\n\n  setCallHook(e) {\n    this._callHook = e;\n  }\n\n  clearCallHook() {\n    this._callHook = null;\n  }\n\n  apply(e, t) {\n    t = t || {}, this.assertNotDisposed();\n    var n = toList(e);\n    var r = !0;\n\n    for (var _e624 of n) {\n      if (!(_e624 instanceof SymbolicTensor)) {\n        r = !1;\n        break;\n      }\n    }\n\n    var a = !0;\n\n    for (var _e625 of n) {\n      if (_e625 instanceof SymbolicTensor) {\n        a = !1;\n        break;\n      }\n    }\n\n    if (r === a) throw new ValueError(\"Arguments to apply() must be all SymbolicTensors or all Tensors\");\n    return nameScope(this.name, () => {\n      if (!this.built) {\n        this.assertInputCompatibility(e);\n        var _t450 = [];\n\n        for (var _n255 of toList(e)) {\n          _t450.push(_n255.shape);\n        }\n\n        this.build(singletonOrArray(_t450)), this.built = !0, this.initialWeights && this.setWeights(this.initialWeights), null === this._refCount && a && (this._refCount = 1);\n      }\n\n      if (this.assertInputCompatibility(e), a) {\n        var _r206 = this.call(e, t);\n\n        var _a142 = toList(_r206),\n            s = [];\n\n        for (var _e626 of _a142) {\n          -1 !== n.indexOf(_e626) && (_e626 = _e626.clone()), s.push(_e626);\n        }\n\n        if (_r206 = singletonOrArray(s), null != this.activityRegularizer) throw new NotImplementedError(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");\n        return _r206;\n      }\n\n      {\n        var _n256 = collectInputShape(e),\n            _r207 = this.computeOutputShape(_n256);\n\n        var _a143;\n\n        var _s103 = guessOutputDType(e);\n\n        if (this.warnOnIncompatibleInputShape(Array.isArray(e) ? _n256[0] : _n256), _a143 = null != _r207 && _r207.length > 0 && Array.isArray(_r207[0]) ? _r207.map((n, r) => new SymbolicTensor(_s103, n, this, toList(e), t, this.name, r)) : new SymbolicTensor(_s103, _r207, this, toList(e), t, this.name), this.addInboundNode(e, _a143, null, null, _n256, _r207, t), this._refCount++, null != this.activityRegularizer) throw new NotImplementedError(\"Layer invocation in the presence of activity regularizer(s) is not supported yet.\");\n        return _a143;\n      }\n    });\n  }\n\n  warnOnIncompatibleInputShape(e) {\n    if (null != this.batchInputShape) if (e.length !== this.batchInputShape.length) console.warn(\"The rank of the input tensor provided (shape: \".concat(JSON.stringify(e), \") does not match that of the batchInputShape (\").concat(JSON.stringify(this.batchInputShape), \") of the layer \").concat(this.name));else {\n      var t = !1;\n      this.batchInputShape.forEach((n, r) => {\n        null != n && null != e[r] && e[r] !== n && (t = !0);\n      }), t && console.warn(\"The shape of the input tensor (\".concat(JSON.stringify(e), \") does not match the expectation of layer \").concat(this.name, \": \").concat(JSON.stringify(this.batchInputShape)));\n    }\n  }\n\n  get outputShape() {\n    if (null == this.inboundNodes || 0 === this.inboundNodes.length) throw new AttributeError(\"The layer \".concat(this.name, \" has never been called and thus has no defined output shape.\"));\n    var e = [];\n\n    for (var t of this.inboundNodes) {\n      var n = JSON.stringify(t.outputShapes);\n      -1 === e.indexOf(n) && e.push(n);\n    }\n\n    if (1 === e.length) {\n      var _e627 = this.inboundNodes[0].outputShapes;\n      return Array.isArray(_e627) && Array.isArray(_e627[0]) && 1 === _e627.length ? _e627[0] : _e627;\n    }\n\n    throw new AttributeError(\"The layer \".concat(this.name, \" has multiple inbound nodes with different output shapes. Hence the notion of \\\"output shape\\\" is ill-defined for the layer.\"));\n  }\n\n  countParams() {\n    if (!this.built) throw new RuntimeError(\"You tried to call countParams() on \".concat(this.name, \", but the layer is not built yet. Build it first by calling build(batchInputShape).\"));\n    return countParamsInWeights(this.weights);\n  }\n\n  build(e) {\n    this.built = !0;\n  }\n\n  getWeights() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : !1;\n    return batchGetValue(e ? this.trainableWeights : this.weights);\n  }\n\n  setWeights(e) {\n    tidy(() => {\n      var t = this.weights;\n      if (t.length !== e.length) throw new ValueError(\"You called setWeights(weights) on layer \\\"\".concat(this.name, \"\\\" with a weight list of length \").concat(e.length, \", but the layer was expecting \").concat(t.length, \" weights. Provided weights: \").concat(e, \"...\"));\n      if (0 === t.length) return;\n      var n = [],\n          r = batchGetValue(t);\n\n      for (var a = 0; a < r.length; ++a) {\n        var s = r[a],\n            o = t[a],\n            i = e[a];\n        if (!arraysEqual(s.shape, i.shape)) throw new ValueError(\"Layer weight shape \".concat(s.shape, \" not compatible with provided weight shape \").concat(i.shape));\n        n.push([o, i]);\n      }\n\n      batchSetValue(n);\n    });\n  }\n\n  addWeight(e, t, n, r, a, s, o) {\n    if (-1 !== this._addedWeightNames.indexOf(e)) throw new ValueError(\"Duplicate weight name \".concat(e, \" for layer \").concat(this.name));\n    this._addedWeightNames.push(e), null == n && (n = \"float32\"), this.fastWeightInitDuringBuild && (r = getInitializer(\"zeros\"));\n    var i = r.apply(t, n),\n        l = new LayerVariable(i, n, e, s, o);\n    return i.dispose(), null != a && this.addLoss(() => a.apply(l.read())), null == s && (s = !0), s ? this._trainableWeights.push(l) : this._nonTrainableWeights.push(l), l;\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    this.fastWeightInitDuringBuild = e;\n  }\n\n  addLoss(e) {\n    null == e || Array.isArray(e) && 0 === e.length || (e = toList(e), null != this._losses && this.losses.push(...e));\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  computeMask(e, t) {\n    if (!this.supportsMasking) {\n      if (null != t) {\n        if (!Array.isArray(t)) throw new TypeError(\"Layer \".concat(this.name, \" does not support masking, but was passed an inputMask.\"));\n        t.forEach(e => {\n          if (null != e) throw new TypeError(\"Layer \".concat(this.name, \" does not support masking, but was passed an inputMask.\"));\n        });\n      }\n\n      return null;\n    }\n\n    return t;\n  }\n\n  addInboundNode(e, t, n, r, a, s) {\n    var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : null;\n    var i = toList(e);\n    t = toList(t), n = toList(n), r = toList(r), a = normalizeShapeList(a), s = normalizeShapeList(s);\n    var l = [],\n        u = [],\n        c = [];\n\n    for (var _e628 of i) {\n      l.push(_e628.sourceLayer), u.push(_e628.nodeIndex), c.push(_e628.tensorIndex);\n    }\n\n    new Node({\n      outboundLayer: this,\n      inboundLayers: l,\n      nodeIndices: u,\n      tensorIndices: c,\n      inputTensors: i,\n      outputTensors: t,\n      inputMasks: n,\n      outputMasks: r,\n      inputShapes: a,\n      outputShapes: s\n    }, o);\n\n    for (var _e629 = 0; _e629 < t.length; _e629++) {\n      t[_e629].sourceLayer = this, t[_e629].nodeIndex = this.inboundNodes.length - 1, t[_e629].tensorIndex = _e629;\n    }\n  }\n\n  getConfig() {\n    var e = {\n      name: this.name,\n      trainable: this.trainable\n    };\n    return null != this.batchInputShape && (e.batchInputShape = this.batchInputShape), null != this.dtype && (e.dtype = this.dtype), e;\n  }\n\n  disposeWeights() {\n    return this.weights.forEach(e => e.dispose()), this.weights.length;\n  }\n\n  assertNotDisposed() {\n    if (0 === this._refCount) throw new Error(\"Layer '\".concat(this.name, \"' is already disposed.\"));\n  }\n\n  dispose() {\n    if (!this.built) throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been built yet.\"));\n    if (null === this._refCount) throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been used yet.\"));\n    this.assertNotDisposed();\n    var e = 0;\n    return 0 == --this._refCount && (e = this.disposeWeights()), {\n      refCountAfterDispose: this._refCount,\n      numDisposedVariables: e\n    };\n  }\n\n}\n\nfunction collectInputShape(e) {\n  e = toList(e);\n  var t = [];\n\n  for (var n of e) {\n    t.push(n.shape);\n  }\n\n  return singletonOrArray(t);\n}\n\nfunction guessOutputDType(e) {\n  return \"float32\";\n}\n\nfunction getSourceInputs(e, t, n) {\n  if ((null == t || null != n && n > 0) && (t = e.sourceLayer, n = e.nodeIndex), 0 === t.inboundNodes.length) return [e];\n  {\n    var _e630 = t.inboundNodes[n];\n    if (0 === _e630.inboundLayers.length) return _e630.inputTensors;\n    {\n      var _t451 = [];\n\n      for (var _n257 = 0; _n257 < _e630.inboundLayers.length; _n257++) {\n        var r = getSourceInputs(_e630.inputTensors[_n257], _e630.inboundLayers[_n257], _e630.nodeIndices[_n257]);\n\n        for (var _e631 of r) {\n          -1 === _t451.indexOf(_e631) && _t451.push(_e631);\n        }\n      }\n\n      return _t451;\n    }\n  }\n}\n\nclass InputLayer extends Layer {\n  constructor(e) {\n    if (super({\n      dtype: e.dtype,\n      name: null != e.name ? e.name : getUid(\"input\").toString()\n    }), null == e.batchSize && (e.batchSize = null), null == e.sparse && (e.sparse = !1), this.trainable = !1, this.built = !0, this.sparse = e.sparse, null != e.inputShape && null != e.batchInputShape) throw new ValueError(\"Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.\");\n    var t = e.batchInputShape;\n\n    if (null == t) {\n      if (null == e.inputShape) throw new ValueError(\"An InputLayer should be passed either a `batchInputShape` or an `inputShape`.\");\n      t = [e.batchSize].concat(e.inputShape);\n    } else if (null != e.batchSize) throw new ValueError(\"Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.\");\n\n    var n = e.dtype || \"float32\";\n    this.batchInputShape = t, this.dtype = n, this.inputSpec = [{\n      shape: t\n    }];\n    var r = new SymbolicTensor(this.dtype, this.batchInputShape, this, [], {}, this.name);\n    r.nodeIndex = 0, r.tensorIndex = 0, new Node({\n      outboundLayer: this,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: [r],\n      outputTensors: [r],\n      inputMasks: [null],\n      outputMasks: [null],\n      inputShapes: [t],\n      outputShapes: [t]\n    });\n  }\n\n  apply(e, t) {\n    throw new ValueError(\"Cannot pass any input to an InputLayer's apply() method. InputLayer name: \".concat(this.name));\n  }\n\n  dispose() {\n    return {\n      refCountAfterDispose: this._refCount,\n      numDisposedVariables: 0\n    };\n  }\n\n  getConfig() {\n    return {\n      batchInputShape: this.batchInputShape,\n      dtype: this.dtype,\n      sparse: this.sparse,\n      name: this.name\n    };\n  }\n\n}\n\nfunction Input(e) {\n  if (null == e.batchShape && null == e.shape) throw new Error(\"Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.\");\n  if (null != e.batchShape && null != e.shape) throw new ValueError(\"Please provide either a `shape` or `batchShape` argument to Input, but not both.\");\n  var t = e.batchShape;\n  null != e.shape && null == t && (t = [null].concat(e.shape));\n  var n = e.dtype;\n  return null == n && (n = \"float32\"), new InputLayer({\n    batchInputShape: t,\n    name: e.name,\n    dtype: n,\n    sparse: e.sparse\n  }).inboundNodes[0].outputTensors[0];\n}\n\nfunction resolveScalarsInLogs(_x86) {\n  return _resolveScalarsInLogs.apply(this, arguments);\n}\n\nfunction _resolveScalarsInLogs() {\n  _resolveScalarsInLogs = _asyncToGenerator(function* (e) {\n    if (null == e) return;\n    var t = [],\n        n = [],\n        r = [];\n\n    for (var a in e) {\n      var s = e[a];\n\n      if (\"number\" != typeof s) {\n        var _e1151 = s;\n        t.push(_e1151.data()), n.push(a), r.push(_e1151);\n      }\n    }\n\n    if (t.length > 0) {\n      var _a321 = yield Promise.all(t);\n\n      for (var _t777 = 0; _t777 < _a321.length; ++_t777) {\n        e[n[_t777]] = _a321[_t777][0];\n      }\n\n      dispose(r);\n    }\n  });\n  return _resolveScalarsInLogs.apply(this, arguments);\n}\n\nfunction disposeTensorsInLogs(e) {\n  if (null != e) for (var t in e) {\n    var n = e[t];\n    \"number\" != typeof n && n.dispose();\n  }\n}\n\nvar ModelLoggingVerbosity;\nInputLayer.className = \"InputLayer\", registerClass(InputLayer), function (e) {\n  e[e.SILENT = 0] = \"SILENT\", e[e.VERBOSE = 1] = \"VERBOSE\";\n}(ModelLoggingVerbosity || (ModelLoggingVerbosity = {}));\nvar DEFAULT_YIELD_EVERY_MS = 125;\n\nclass BaseCallback {\n  constructor() {\n    this.validationData = null;\n  }\n\n  setParams(e) {\n    this.params = e;\n  }\n\n  onEpochBegin(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onEpochEnd(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onBatchBegin(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onBatchEnd(e, t) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onTrainBegin(e) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  onTrainEnd(e) {\n    return _asyncToGenerator(function* () {})();\n  }\n\n  setModel(e) {}\n\n}\n\nclass CallbackList {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 10;\n    null == e && (e = []), this.callbacks = e, this.queueLength = t;\n  }\n\n  append(e) {\n    this.callbacks.push(e);\n  }\n\n  setParams(e) {\n    for (var t of this.callbacks) {\n      t.setParams(e);\n    }\n  }\n\n  setModel(e) {\n    for (var t of this.callbacks) {\n      t.setModel(e);\n    }\n  }\n\n  onEpochBegin(e, t) {\n    var _this116 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this116.callbacks) {\n        yield n.onEpochBegin(e, t);\n      }\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this117 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this117.callbacks) {\n        yield n.onEpochEnd(e, t);\n      }\n    })();\n  }\n\n  onBatchBegin(e, t) {\n    var _this118 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this118.callbacks) {\n        yield n.onBatchBegin(e, t);\n      }\n    })();\n  }\n\n  onBatchEnd(e, t) {\n    var _this119 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n\n      for (var n of _this119.callbacks) {\n        yield n.onBatchEnd(e, t);\n      }\n    })();\n  }\n\n  onTrainBegin(e) {\n    var _this120 = this;\n\n    return _asyncToGenerator(function* () {\n      null == e && (e = {});\n\n      for (var t of _this120.callbacks) {\n        yield t.onTrainBegin(e);\n      }\n    })();\n  }\n\n  onTrainEnd(e) {\n    var _this121 = this;\n\n    return _asyncToGenerator(function* () {\n      null == e && (e = {});\n\n      for (var t of _this121.callbacks) {\n        yield t.onTrainEnd(e);\n      }\n    })();\n  }\n\n}\n\nclass BaseLogger extends BaseCallback {\n  constructor() {\n    super();\n  }\n\n  onEpochBegin(e) {\n    var _this122 = this;\n\n    return _asyncToGenerator(function* () {\n      _this122.seen = 0, _this122.totals = {};\n    })();\n  }\n\n  onBatchEnd(e, t) {\n    var _this123 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {});\n      var n = null == t.size ? 0 : t.size;\n      _this123.seen += n;\n\n      var _loop39 = function _loop39(_e632) {\n        var r = t[_e632];\n        if (\"number\" == typeof r) _this123.totals.hasOwnProperty(_e632) || (_this123.totals[_e632] = 0), _this123.totals[_e632] = _this123.totals[_e632] + r * n;else {\n          var _t452;\n\n          _e632 in _this123.totals ? _t452 = _this123.totals[_e632] : _this123.totals[_e632] = 0;\n          var a = tidy(() => add$2(_this123.totals[_e632], mul(r, n)));\n          _this123.totals[_e632] = a, null != _t452 && _t452.dispose();\n        }\n      };\n\n      for (var _e632 in t) {\n        _loop39(_e632);\n      }\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this124 = this;\n\n    return _asyncToGenerator(function* () {\n      if (null != t) {\n        var _loop40 = function _loop40(_e633) {\n          null != _this124.totals[_e633] && (\"number\" == typeof _this124.totals[_e633] ? t[_e633] = _this124.totals[_e633] / _this124.seen : tidy(() => {\n            var n = mul(div$1(1, _this124.seen), _this124.totals[_e633]);\n            t[_e633] = n, _this124.totals[_e633].dispose(), keep(t[_e633]);\n          }));\n        };\n\n        for (var _e633 of _this124.params.metrics) {\n          _loop40(_e633);\n        }\n      }\n    })();\n  }\n\n}\n\nclass History extends BaseCallback {\n  onTrainBegin(e) {\n    var _this125 = this;\n\n    return _asyncToGenerator(function* () {\n      _this125.epoch = [], _this125.history = {};\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this126 = this;\n\n    return _asyncToGenerator(function* () {\n      null == t && (t = {}), _this126.epoch.push(e);\n\n      for (var _e634 in t) {\n        null == _this126.history[_e634] && (_this126.history[_e634] = []), _this126.history[_e634].push(t[_e634]);\n      }\n    })();\n  }\n\n  syncData() {\n    var _this127 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [],\n          t = [],\n          n = [];\n\n      for (var _r208 in _this127.history) {\n        var a = _this127.history[_r208];\n\n        for (var s = 0; s < a.length; ++s) {\n          \"number\" != typeof a[s] && (e.push(a[s].data()), t.push(_r208), n.push(s));\n        }\n      }\n\n      var r = yield Promise.all(e);\n\n      for (var _e635 = 0; _e635 < r.length; ++_e635) {\n        _this127.history[t[_e635]][n[_e635]].dispose(), _this127.history[t[_e635]][n[_e635]] = r[_e635][0];\n      }\n    })();\n  }\n\n}\n\nclass CustomCallback extends BaseCallback {\n  constructor(e, t) {\n    if (super(), this.currentEpoch = 0, this.yieldEvery = t || \"auto\", \"auto\" === this.yieldEvery && (this.yieldEvery = DEFAULT_YIELD_EVERY_MS), \"never\" === this.yieldEvery && null != e.onYield) throw new Error(\"yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback\");\n    isNumber(this.yieldEvery) && (this.maybeWait = debounce(this.maybeWait.bind(this), this.yieldEvery)), this.trainBegin = e.onTrainBegin, this.trainEnd = e.onTrainEnd, this.epochBegin = e.onEpochBegin, this.epochEnd = e.onEpochEnd, this.batchBegin = e.onBatchBegin, this.batchEnd = e.onBatchEnd, this.yield = e.onYield;\n  }\n\n  maybeWait(e, t, n) {\n    var _this128 = this;\n\n    return _asyncToGenerator(function* () {\n      var r = [];\n      null != _this128.yield && (yield resolveScalarsInLogs(n), r.push(_this128.yield(e, t, n))), r.push(nextFrame()), yield Promise.all(r);\n    })();\n  }\n\n  onEpochBegin(e, t) {\n    var _this129 = this;\n\n    return _asyncToGenerator(function* () {\n      _this129.currentEpoch = e, null != _this129.epochBegin && (yield resolveScalarsInLogs(t), yield _this129.epochBegin(e, t));\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this130 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = [];\n      null != _this130.epochEnd && (yield resolveScalarsInLogs(t), n.push(_this130.epochEnd(e, t))), \"epoch\" === _this130.yieldEvery && n.push(nextFrame()), yield Promise.all(n);\n    })();\n  }\n\n  onBatchBegin(e, t) {\n    var _this131 = this;\n\n    return _asyncToGenerator(function* () {\n      null != _this131.batchBegin && (yield resolveScalarsInLogs(t), yield _this131.batchBegin(e, t));\n    })();\n  }\n\n  onBatchEnd(e, t) {\n    var _this132 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = [];\n      null != _this132.batchEnd && (yield resolveScalarsInLogs(t), n.push(_this132.batchEnd(e, t))), \"batch\" === _this132.yieldEvery ? n.push(nextFrame()) : isNumber(_this132.yieldEvery) && n.push(_this132.maybeWait(_this132.currentEpoch, e, t)), yield Promise.all(n);\n    })();\n  }\n\n  onTrainBegin(e) {\n    var _this133 = this;\n\n    return _asyncToGenerator(function* () {\n      null != _this133.trainBegin && (yield resolveScalarsInLogs(e), yield _this133.trainBegin(e));\n    })();\n  }\n\n  onTrainEnd(e) {\n    var _this134 = this;\n\n    return _asyncToGenerator(function* () {\n      null != _this134.trainEnd && (yield resolveScalarsInLogs(e), yield _this134.trainEnd(e));\n    })();\n  }\n\n}\n\nfunction standardizeCallbacks(e, t) {\n  return null == e && (e = {}), e instanceof BaseCallback ? [e] : Array.isArray(e) && e[0] instanceof BaseCallback ? e : toList(e).map(e => new CustomCallback(e, t));\n}\n\nclass CallbackConstructorRegistry {\n  constructor() {}\n\n  static registerCallbackConstructor(e, t) {\n    assert$4(e >= 0 && Number.isInteger(e), () => \"Verbosity level is expected to be an integer >= 0, but got \".concat(e)), CallbackConstructorRegistry.checkForDuplicate(t), null == CallbackConstructorRegistry.constructors[e] && (CallbackConstructorRegistry.constructors[e] = []), CallbackConstructorRegistry.constructors[e].push(t);\n  }\n\n  static checkForDuplicate(e) {\n    for (var t in CallbackConstructorRegistry.constructors) {\n      CallbackConstructorRegistry.constructors[+t].forEach(t => {\n        if (t === e) throw new ValueError(\"Duplicate callback constructor.\");\n      });\n    }\n  }\n\n  static clear() {\n    CallbackConstructorRegistry.constructors = {};\n  }\n\n  static createCallbacks(e) {\n    var t = [];\n\n    for (var n in CallbackConstructorRegistry.constructors) {\n      var r = +n;\n      e >= r && t.push(...CallbackConstructorRegistry.constructors[r]);\n    }\n\n    return t.map(e => new e());\n  }\n\n}\n\nfunction configureCallbacks(e, t, n, r, a, s, o, i, l) {\n  var u = new History(),\n      c = [new BaseLogger(), ...CallbackConstructorRegistry.createCallbacks(t)];\n  null != e && c.push(...e), c.push(u);\n  var p = new CallbackList(c);\n  return p.setParams({\n    epochs: n,\n    initialEpoch: r,\n    samples: a,\n    steps: s,\n    batchSize: o,\n    verbose: t,\n    doValidation: i,\n    metrics: l\n  }), {\n    callbackList: p,\n    history: u\n  };\n}\n\nfunction deserialize(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return deserializeKerasObject(e, SerializationMap.getMap().classNameMap, t, \"layer\", n);\n}\n\nfunction l2Normalize(e, t) {\n  return tidy(() => {\n    \"float32\" !== e.dtype && (e = cast$3(e, \"float32\"));\n    var n = sum$2(square$1(e), t, !0),\n        r = fill$2(n.shape, epsilon$1()),\n        a = sqrt$2(maximum$3(n, r));\n    return div$1(e, a);\n  });\n}\n\nfunction meanSquaredError$1(e, t) {\n  return tidy(() => mean$1(square$1(sub$2(t, e)), -1));\n}\n\nfunction meanAbsoluteError$1(e, t) {\n  return tidy(() => mean$1(abs$2(sub$2(t, e)), -1));\n}\n\nfunction meanAbsolutePercentageError$1(e, t) {\n  return tidy(() => {\n    var n = sub$2(e, t),\n        r = clipByValue$1(abs$2(e), epsilon$1(), Number.MAX_VALUE),\n        a = abs$2(div$1(n, r));\n    return mul(100, mean$1(a, -1));\n  });\n}\n\nfunction meanSquaredLogarithmicError(e, t) {\n  return tidy(() => {\n    var n = clipByValue$1(t, epsilon$1(), Number.MAX_VALUE),\n        r = log$3(add$2(1, n)),\n        a = clipByValue$1(e, epsilon$1(), Number.MAX_VALUE),\n        s = log$3(add$2(1, a));\n    return mean$1(square$1(sub$2(r, s)), -1);\n  });\n}\n\nfunction squaredHinge(e, t) {\n  return tidy(() => {\n    var n = maximum$3(0, sub$2(1, mul(e, t)));\n    return mean$1(square$1(n), -1);\n  });\n}\n\nfunction hinge(e, t) {\n  return tidy(() => {\n    var n = maximum$3(0, sub$2(1, mul(e, t)));\n    return mean$1(n, -1);\n  });\n}\n\nfunction categoricalHinge(e, t) {\n  return tidy(() => {\n    var n = sum$2(mul(e, t), -1),\n        r = max$3(mul(sub$2(1, e), t), -1);\n    return maximum$3(0, add$2(1, sub$2(r, n)));\n  });\n}\n\nfunction logcosh(e, t) {\n  return tidy(() => {\n    var n = Math.log(2),\n        r = sub$2(t, e),\n        a = sub$2(add$2(r, softplus$2(mul(-2, r))), n);\n    return mean$1(a, -1);\n  });\n}\n\nfunction categoricalCrossentropy$2(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return tidy(() => {\n    if (n) t = softmax$3(t);else {\n      var _e636 = sum$2(t, t.shape.length - 1, !0);\n\n      t = div$1(t, _e636);\n    }\n    return t = clipByValue$1(t, epsilon$1(), 1 - epsilon$1()), neg$2(sum$2(mul(cast$3(e, \"float32\"), log$3(t)), t.shape.length - 1));\n  });\n}\n\nfunction sparseCategoricalCrossentropy$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  return tidy(() => {\n    var r = cast$3(floor$2(flatten$2(e)), \"int32\"),\n        a = (t = clipByValue$1(t, epsilon$1(), 1 - epsilon$1())).shape;\n    return categoricalCrossentropy$2(reshape$3(oneHot$2(r, a[a.length - 1]), a), t, n);\n  });\n}\n\nfunction sigmoidCrossEntropyWithLogits(e, t) {\n  if (!arraysEqual(e.shape, t.shape)) throw new ValueError(\"logits and labels must have the same shape, but got shapes \".concat(JSON.stringify(e.shape), \" and \").concat(JSON.stringify(t.shape)));\n  return tidy(() => {\n    var n = relu$3(t),\n        r = neg$2(abs$2(t));\n    return add$2(sub$2(n, mul(t, e)), log1p$2(exp$2(r)));\n  });\n}\n\nfunction binaryCrossentropy$2(e, t) {\n  return tidy(() => {\n    var n;\n    return n = clipByValue$1(t, epsilon$1(), 1 - epsilon$1()), n = log$3(div$1(n, sub$2(1, n))), mean$1(sigmoidCrossEntropyWithLogits(e, n), -1);\n  });\n}\n\nfunction kullbackLeiblerDivergence(e, t) {\n  return tidy(() => {\n    var n = clipByValue$1(e, epsilon$1(), 1),\n        r = clipByValue$1(t, epsilon$1(), 1);\n    return sum$2(mul(e, log$3(div$1(n, r))), -1);\n  });\n}\n\nfunction poisson(e, t) {\n  return tidy(() => {\n    var n = log$3(add$2(epsilon$1(), t));\n    return mean$1(sub$2(t, mul(e, n)), -1);\n  });\n}\n\nfunction cosineProximity$1(e, t) {\n  return tidy(() => {\n    var n = l2Normalize(e, -1),\n        r = l2Normalize(t, -1),\n        a = mul(n, r);\n    return neg$2(sum$2(a, -1));\n  });\n}\n\nCallbackConstructorRegistry.constructors = {};\nvar lossesMap = {\n  meanSquaredError: meanSquaredError$1,\n  meanAbsoluteError: meanAbsoluteError$1,\n  meanAbsolutePercentageError: meanAbsolutePercentageError$1,\n  meanSquaredLogarithmicError,\n  squaredHinge,\n  hinge,\n  categoricalHinge,\n  logcosh,\n  categoricalCrossentropy: categoricalCrossentropy$2,\n  sparseCategoricalCrossentropy: sparseCategoricalCrossentropy$1,\n  binaryCrossentropy: binaryCrossentropy$2,\n  kullbackLeiblerDivergence,\n  poisson,\n  cosineProximity: cosineProximity$1\n};\n\nfunction get$1(e) {\n  if (\"string\" == typeof e) {\n    if (e in lossesMap) return lossesMap[e];\n    var t = \"Unknown loss \".concat(e);\n    throw e.toLowerCase().includes(\"softmaxcrossentropy\") && (t = \"Unknown loss \".concat(e, \". Use \\\"categoricalCrossentropy\\\" as the string name for tf.losses.softmaxCrossEntropy\")), new ValueError(t);\n  }\n\n  return e;\n}\n\nfunction binaryAccuracy$1(e, t) {\n  return tidy(() => {\n    var n = mul(.5, onesLike$2(t)),\n        r = cast$2(greater$3(t, n), e.dtype);\n    return mean$1(equal$2(e, r), -1);\n  });\n}\n\nfunction categoricalAccuracy$1(e, t) {\n  return tidy(() => cast$2(equal$2(argMax$2(e, -1), argMax$2(t, -1)), \"float32\"));\n}\n\nfunction truePositives(e, t) {\n  return tidy(() => cast$3(sum$2(logicalAnd$2(equal$2(e, 1), equal$2(t, 1))), \"float32\"));\n}\n\nfunction falseNegatives(e, t) {\n  return tidy(() => cast$3(sum$2(logicalAnd$2(equal$2(e, 1), equal$2(t, 0))), \"float32\"));\n}\n\nfunction falsePositives(e, t) {\n  return tidy(() => cast$3(sum$2(logicalAnd$2(equal$2(e, 0), equal$2(t, 1))), \"float32\"));\n}\n\nfunction precision$1(e, t) {\n  return tidy(() => {\n    var n = truePositives(e, t),\n        r = falsePositives(e, t),\n        a = add$2(n, r);\n    return cast$3(where(greater$3(a, 0), div$1(n, a), 0), \"float32\");\n  });\n}\n\nfunction recall$1(e, t) {\n  return tidy(() => {\n    var n = truePositives(e, t),\n        r = falseNegatives(e, t),\n        a = add$2(n, r);\n    return cast$3(where(greater$3(a, 0), div$1(n, a), 0), \"float32\");\n  });\n}\n\nfunction binaryCrossentropy$1(e, t) {\n  return binaryCrossentropy$2(e, t);\n}\n\nfunction sparseCategoricalAccuracy$1(e, t) {\n  return e.rank === t.rank && (e = squeeze(e, [e.rank - 1])), (t = argMax$2(t, -1)).dtype !== e.dtype && (t = cast$3(t, e.dtype)), cast$3(equal$2(e, t), \"float32\");\n}\n\nvar mse$1 = meanSquaredError$1,\n    MSE$1 = meanSquaredError$1,\n    mae = meanAbsoluteError$1,\n    MAE = meanAbsoluteError$1,\n    mape$1 = meanAbsolutePercentageError$1,\n    MAPE$1 = meanAbsolutePercentageError$1,\n    categoricalCrossentropy$1 = categoricalCrossentropy$2,\n    cosine = cosineProximity$1,\n    sparseCategoricalCrossentropy = sparseCategoricalCrossentropy$1,\n    metricsMap = {\n  binaryAccuracy: binaryAccuracy$1,\n  categoricalAccuracy: categoricalAccuracy$1,\n  precision: precision$1,\n  categoricalCrossentropy: categoricalCrossentropy$1,\n  sparseCategoricalCrossentropy,\n  mse: mse$1,\n  MSE: MSE$1,\n  mae,\n  MAE,\n  mape: mape$1,\n  MAPE: MAPE$1,\n  cosine\n};\n\nfunction get(e) {\n  if (\"string\" == typeof e && e in metricsMap) return metricsMap[e];\n  if (\"string\" != typeof e && null != e) return e;\n  throw new ValueError(\"Unknown metric \".concat(e));\n}\n\nfunction getLossOrMetricName(e) {\n  if (assert$3(null !== e, \"Unknown LossOrMetricFn \".concat(e)), \"string\" == typeof e) return e;\n  {\n    var t;\n\n    for (var n of Object.keys(lossesMap)) {\n      if (lossesMap[n] === e) {\n        t = n;\n        break;\n      }\n    }\n\n    if (void 0 !== t) return t;\n\n    for (var _n258 of Object.keys(metricsMap)) {\n      if (metricsMap[_n258] === e) {\n        t = _n258;\n        break;\n      }\n    }\n\n    return void 0 !== t ? t : e.name;\n  }\n}\n\nfunction getOptimizer(e) {\n  var t = {\n    Adagrad: () => train.adagrad(.01),\n    Adadelta: () => train.adadelta(1, .95, epsilon$1()),\n    Adam: () => train.adam(.001, .9, .999, epsilon$1()),\n    Adamax: () => train.adamax(.002, .9, .999, epsilon$1(), 0),\n    RMSProp: () => train.rmsprop(.001, .9, 0, epsilon$1()),\n    SGD: () => train.sgd(.01)\n  };\n  if (t.adagrad = t.Adagrad, t.adadelta = t.Adadelta, t.adam = t.Adam, t.adamax = t.Adamax, t.rmsprop = t.RMSProp, t.sgd = t.SGD, e in t) return t[e]();\n  throw new ValueError(\"Unknown Optimizer \".concat(e));\n}\n\nvar MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH = 1048576;\n\nfunction checkUserDefinedMetadata(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  if (null == e || \"object\" != typeof e || Object.getPrototypeOf(e) !== Object.prototype || !plainObjectCheck(e)) throw new Error(\"User-defined metadata is expected to be a JSON object, but is not.\");\n\n  if (n) {\n    var _n259 = JSON.stringify(e);\n\n    _n259.length > MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH && console.warn(\"User-defined metadata of model \\\"\".concat(t, \"\\\" is too large in size (length=\").concat(_n259.length, \" when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= \").concat(MAX_USER_DEFINED_METADATA_SERIALIZED_LENGTH, \".\"));\n  }\n}\n\nfunction plainObjectCheck(e) {\n  if (null === e) return !0;\n\n  if (\"object\" == typeof e) {\n    if (Object.getPrototypeOf(e) === Object.prototype) {\n      var t = Object.keys(e);\n\n      for (var n of t) {\n        if (\"string\" != typeof n) return !1;\n        if (!plainObjectCheck(e[n])) return !1;\n      }\n\n      return !0;\n    }\n\n    if (Array.isArray(e)) {\n      for (var _t453 of e) {\n        if (!plainObjectCheck(_t453)) return !1;\n      }\n\n      return !0;\n    }\n\n    return !1;\n  }\n\n  {\n    var _t454 = typeof e;\n\n    return \"string\" === _t454 || \"number\" === _t454 || \"boolean\" === _t454;\n  }\n}\n\nfunction printSummary(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : console.log;\n  var a = isModelSequentialLike(e),\n      s = [\"Layer (type)\", \"Output shape\", \"Param #\"];\n  var o;\n\n  if (a ? (t = t || 65, n = n || [.45, .85, 1]) : (t = t || 98, n = n || [.33, .55, .67, 1]), n[n.length - 1] <= 1 && (n = n.map(e => Math.floor(t * e))), !a) {\n    s.push(\"Receives inputs\"), o = [];\n\n    for (var _t455 in e.nodesByDepth) {\n      o.push(...e.nodesByDepth[_t455]);\n    }\n  }\n\n  r(\"_\".repeat(t)), printRow(s, n, r), r(\"=\".repeat(t));\n  var i = e.layers;\n\n  for (var _e637 = 0; _e637 < i.length; ++_e637) {\n    a ? printLayerSummary(i[_e637], n, r) : printLayerSummaryWithConnections(i[_e637], n, o, r), r((_e637 === i.length - 1 ? \"=\" : \"_\").repeat(t));\n  }\n\n  e.checkTrainableWeightsConsistency();\n  var l = countTrainableParams(e),\n      u = countParamsInWeights(e.nonTrainableWeights);\n  r(\"Total params: \".concat(l + u)), r(\"Trainable params: \".concat(l)), r(\"Non-trainable params: \".concat(u)), r(\"_\".repeat(t));\n}\n\nfunction countTrainableParams(e) {\n  var t;\n  return t = countParamsInWeights(null != e.collectedTrainableWeights ? e.collectedTrainableWeights : e.trainableWeights), t;\n}\n\nfunction isModelSequentialLike(e) {\n  var t = !0;\n  var n = [],\n      r = [];\n\n  for (var _t456 in e.nodesByDepth) {\n    n.push(e.nodesByDepth[_t456]);\n  }\n\n  for (var _e638 of n) {\n    if (_e638.length > 1 || 1 === _e638.length && _e638[0].inboundLayers.length > 1) {\n      t = !1;\n      break;\n    }\n\n    r.push(..._e638);\n  }\n\n  if (t) for (var _n260 of e.layers) {\n    var _e639 = !1;\n\n    for (var a of _n260.inboundNodes) {\n      if (-1 !== r.indexOf(a)) {\n        if (_e639) {\n          t = !1;\n          break;\n        }\n\n        _e639 = !0;\n      }\n    }\n\n    if (!t) break;\n  }\n  return t;\n}\n\nfunction printRow(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : console.log;\n  var r = \"\";\n\n  for (var _n261 = 0; _n261 < e.length; ++_n261) {\n    _n261 > 0 && (r = r.slice(0, r.length - 1) + \" \"), r += e[_n261], r = r.slice(0, t[_n261]), r += \" \".repeat(t[_n261] - r.length);\n  }\n\n  n(r);\n}\n\nfunction printLayerSummary(e, t, n) {\n  var r;\n\n  try {\n    r = JSON.stringify(e.outputShape);\n  } catch (e) {\n    r = \"multiple\";\n  }\n\n  printRow([\"\".concat(e.name, \" (\").concat(e.getClassName(), \")\"), r, e.countParams().toString()], t, n);\n}\n\nfunction printLayerSummaryWithConnections(e, t, n, r) {\n  var a;\n\n  try {\n    a = JSON.stringify(e.outputShape);\n  } catch (e) {\n    a = \"multiple\";\n  }\n\n  var s = [];\n\n  for (var _t457 of e.inboundNodes) {\n    if (!(null != n && n.length > 0 && -1 === n.indexOf(_t457))) for (var _e640 = 0; _e640 < _t457.inboundLayers.length; ++_e640) {\n      s.push(\"\".concat(_t457.inboundLayers[_e640].name, \"[\").concat(_t457.nodeIndices[_e640], \"][\").concat(_t457.tensorIndices[_e640], \"]\"));\n    }\n  }\n\n  var o = e.name,\n      i = e.getClassName(),\n      l = 0 === s.length ? \"\" : s[0];\n  printRow([\"\".concat(o, \" (\").concat(i, \")\"), a, e.countParams().toString(), l], t, r);\n\n  for (var _e641 = 1; _e641 < s.length; ++_e641) {\n    printRow([\"\", \"\", \"\", s[_e641]], t, r);\n  }\n}\n\nfunction isArrayItemInputOrOutputName(e, t, n) {\n  return (\"inboundNodes\" === e || \"outputLayers\" === e || \"inputLayers\" === e) && 0 === t && \"string\" == typeof n;\n}\n\nfunction convertPythonicToTs(e, t) {\n  if (null === e) return null;\n  if (\"string\" == typeof e) return toCamelCase(e);\n  if (\"number\" == typeof e || \"boolean\" == typeof e) return e;\n\n  if (e instanceof Array) {\n    var n = [],\n        r = e.length;\n\n    for (var a = 0; a < r; ++a) {\n      var _r209 = e[a];\n      isArrayItemInputOrOutputName(t, a, _r209) ? n.push(_r209) : n.push(convertPythonicToTs(_r209, t));\n    }\n\n    return n;\n  }\n\n  {\n    var _t458 = {};\n\n    for (var _n262 of Object.keys(e)) {\n      var _r210 = e[_n262];\n      if (\"name\" === _n262 && \"string\" == typeof _r210) _t458[_n262] = _r210;else {\n        var _e642 = toCamelCase(_n262);\n\n        _t458[_e642] = convertPythonicToTs(_r210, _e642);\n      }\n    }\n\n    return _t458;\n  }\n}\n\nfunction convertTsToPythonic(e, t) {\n  if (null == e) return null;\n  if (\"string\" == typeof e) return toSnakeCase(e);\n  if (\"number\" == typeof e || \"boolean\" == typeof e) return e;\n\n  if (e instanceof Array) {\n    var n = [],\n        r = e.length;\n\n    for (var a = 0; a < r; ++a) {\n      var _r211 = e[a];\n      isArrayItemInputOrOutputName(t, a, _r211) ? n.push(_r211) : n.push(convertTsToPythonic(_r211, t));\n    }\n\n    return n;\n  }\n\n  {\n    var _t459 = {};\n\n    for (var _n263 of Object.keys(e)) {\n      var _r212 = e[_n263];\n      _t459[toSnakeCase(_n263)] = \"name\" !== _n263 && \"className\" !== _n263 || \"string\" != typeof _r212 ? convertTsToPythonic(_r212, _n263) : _r212;\n    }\n\n    return _t459;\n  }\n}\n\nvar version$6 = \"3.8.0\";\n\nfunction assertFeedCompatibility(e, t) {\n  if (null == e.dtype || e.dtype === t.dtype) return t;\n\n  try {\n    return cast$3(t, e.dtype);\n  } catch (n) {\n    throw new ValueError(\"The dtype of the feed (\".concat(t.dtype, \") can not be cast to the dtype of the key '\").concat(e.name, \"' (\").concat(e.dtype, \").\"));\n  }\n}\n\nclass FeedDict {\n  constructor(e) {\n    if (this.id2Value = {}, this.id2Mask = {}, this.name2Id = {}, e instanceof FeedDict) for (var t in e.id2Value) {\n      this.id2Value[t] = e.id2Value[t], t in e.id2Mask && (this.id2Mask[t] = e.id2Mask[t]);\n    } else {\n      if (null == e) return;\n\n      for (var _t460 of e) {\n        this.add(_t460.key, _t460.value);\n      }\n    }\n  }\n\n  add(e, t, n) {\n    if (null != this.id2Value[e.id]) throw new ValueError(\"Duplicate key: name=\".concat(e.name, \", id=\").concat(e.id));\n    return this.id2Value[e.id] = assertFeedCompatibility(e, t), this.name2Id[e.name] = e.id, null != n && (this.id2Mask[e.id] = n), this;\n  }\n\n  addFeed(e) {\n    this.add(e.key, e.value);\n  }\n\n  hasKey(e) {\n    return null != this.id2Value[e.id];\n  }\n\n  names() {\n    return Object.keys(this.name2Id);\n  }\n\n  getValue(e) {\n    if (e instanceof SymbolicTensor) {\n      if (null == this.id2Value[e.id]) throw new ValueError(\"Nonexistent key: \".concat(e.name));\n      return this.id2Value[e.id];\n    }\n\n    {\n      var t = this.name2Id[e];\n      if (null == t) throw new ValueError(\"Feed dict has no SymbolicTensor name: \".concat(e));\n      return this.id2Value[t];\n    }\n  }\n\n  getMask(e) {\n    if (e instanceof SymbolicTensor) {\n      if (null == this.id2Value[e.id]) throw new ValueError(\"Nonexistent key: \".concat(e.name));\n      return this.id2Mask[e.id];\n    }\n\n    {\n      var t = this.name2Id[e];\n      if (null == t) throw new ValueError(\"Feed dict has no SymbolicTensor name: \".concat(e));\n      return this.id2Mask[t];\n    }\n  }\n\n  disposeMasks() {\n    null != this.id2Mask && dispose(this.id2Mask);\n  }\n\n}\n\nvar cachedSorted = {},\n    cachedRecipientCounts = {};\n\nfunction execute(e, t, n, r) {\n  var a = null != n && n.training,\n      s = Array.isArray(e),\n      o = s ? e : [e],\n      i = o.map(e => e.name),\n      l = [],\n      u = t.names();\n\n  for (var _e643 of i) {\n    -1 !== u.indexOf(_e643) ? l.push(t.getValue(_e643)) : l.push(null);\n  }\n\n  null != r && (r.maxNumTensors = -Infinity, r.minNumTensors = Infinity);\n  var c = i.join(\",\") + \"|\" + t.names().join(\",\");\n  var p, d;\n\n  if (null == cachedSorted[c]) {\n    var _e644 = getTopologicalSortAndRecipientCounts(o, t);\n\n    p = _e644.sorted, d = _e644.recipientCounts, cachedSorted[c] = p, cachedRecipientCounts[c] = d;\n  }\n\n  p = cachedSorted[c], d = {}, a || Object.assign(d, cachedRecipientCounts[c]);\n  var h = new FeedDict(t);\n\n  for (var _e645 = 0; _e645 < p.length; ++_e645) {\n    if (null != r) {\n      var _e646 = memory().numTensors;\n      _e646 > r.maxNumTensors && (r.maxNumTensors = _e646), _e646 < r.minNumTensors && (r.minNumTensors = _e646);\n    }\n\n    var _s104 = p[_e645],\n        _o73 = _s104.sourceLayer;\n    if (_o73 instanceof InputLayer) continue;\n    var _u31 = [],\n        _c21 = [],\n        m = [];\n    var f = !1;\n\n    for (var _e647 of _s104.inputs) {\n      var _n264 = h.getValue(_e647),\n          _r213 = h.getMask(_e647);\n\n      _u31.push(_n264), _c21.push(_r213), null != _r213 && (f = !0), a || (d[_e647.name]--, 0 !== d[_e647.name] || t.hasKey(_e647) || -1 !== i.indexOf(_e647.name) || _n264.isDisposed || !0 === _e647.sourceLayer.stateful || m.push(_n264));\n    }\n\n    f && ((n = n || {}).mask = _c21[0]);\n    var g = toList(_o73.apply(_u31, n));\n    var $ = null;\n    _o73.supportsMasking && ($ = _o73.computeMask(_u31, _c21));\n    var y = getNodeOutputs(_s104),\n        b = Array.isArray(y) ? y : [y];\n\n    for (var _e648 = 0; _e648 < b.length; ++_e648) {\n      h.hasKey(b[_e648]) || h.add(b[_e648], g[_e648], Array.isArray($) ? $[0] : $);\n\n      var _t461 = i.indexOf(b[_e648].name);\n\n      -1 !== _t461 && (l[_t461] = g[_e648]);\n    }\n\n    a || dispose(m);\n  }\n\n  return h.disposeMasks(), s ? l : l[0];\n}\n\nfunction getTopologicalSortAndRecipientCounts(e, t) {\n  assert$4(null != e && e.length > 0, () => \"Expected at least one fetch, got none\");\n  var n = [],\n      r = {};\n\n  if (1 === e.length) {\n    var a = getTopologicalSortAndRecipientCountsForOneFetch(e[0], t);\n    n = a.sorted, r = a.recipientMap;\n  } else {\n    var _a144 = new Set();\n\n    for (var s of e) {\n      var {\n        sorted: _e649,\n        recipientMap: o\n      } = getTopologicalSortAndRecipientCountsForOneFetch(s, t);\n\n      for (var _t462 of _e649) {\n        _a144.has(_t462.name) || (n.push(_t462), _a144.add(_t462.name));\n      }\n\n      var _loop41 = function _loop41(_e650) {\n        null == r[_e650] && (r[_e650] = new Set()), o[_e650].forEach(t => r[_e650].add(t));\n      };\n\n      for (var _e650 in o) {\n        _loop41(_e650);\n      }\n    }\n  }\n\n  return {\n    sorted: n,\n    recipientCounts: recipientMap2Counts(r)\n  };\n}\n\nfunction recipientMap2Counts(e) {\n  var t = {};\n\n  for (var n in e) {\n    t[n] = e[n].size;\n  }\n\n  return t;\n}\n\nfunction getTopologicalSortAndRecipientCountsForOneFetch(e, t) {\n  var n = new Set(),\n      r = [],\n      a = {};\n\n  for (var _e651 of t.names()) {\n    n.add(_e651);\n  }\n\n  var s = [],\n      o = [];\n\n  for (s.push(e); s.length > 0;) {\n    var _e652 = s[s.length - 1];\n\n    if (n.has(_e652.name)) {\n      s.pop();\n      continue;\n    }\n\n    var _t463 = o[o.length - 1] === s.length - 1;\n\n    if (0 === _e652.inputs.length || _t463) s.pop(), r.push(_e652), n.add(_e652.name), _t463 && o.pop();else {\n      o.push(s.length - 1);\n\n      for (var _t464 of _e652.inputs) {\n        null == a[_t464.name] && (a[_t464.name] = new Set()), a[_t464.name].add(_e652.name), n.has(_t464.name) || s.push(_t464);\n      }\n    }\n  }\n\n  return {\n    sorted: r,\n    recipientMap: a\n  };\n}\n\nfunction getNodeOutputs(e) {\n  var t;\n  if (1 === e.sourceLayer.inboundNodes.length) t = e.sourceLayer.output;else {\n    var n = null;\n\n    for (var _t465 = 0; _t465 < e.sourceLayer.inboundNodes.length; ++_t465) {\n      for (var r of e.sourceLayer.inboundNodes[_t465].outputTensors) {\n        if (r.id === e.id) {\n          n = _t465;\n          break;\n        }\n      }\n    }\n\n    t = e.sourceLayer.getOutputAt(n);\n  }\n  return t;\n}\n\nclass Container extends Layer {\n  constructor(e) {\n    if (super({}), this.containerNodes = new Set(), this.name = e.name, null == this.name) {\n      var _e653 = this.getClassName().toLowerCase();\n\n      this.name = getUid(_e653);\n    }\n\n    if (this.supportsMasking = !1, this.trainable_ = !0, this.inputs = Array.isArray(e.inputs) ? e.inputs.slice() : [e.inputs], this.outputs = Array.isArray(e.outputs) ? e.outputs.slice() : [e.outputs], unique$2(this.inputs).length !== this.inputs.length) throw new ValueError(\"The list of inputs passed to the model is redundant. All inputs should only appear once. Found: \".concat(this.inputs.map(e => e.name)));\n    unique$2(this.outputs).length !== this.outputs.length && console.warn(\"The list of outputs passed to the model is redundant. All outputs should only appear once. Found: \".concat(this.outputs.map(e => e.name))), this.inputLayers = [], this.inputLayersNodeIndices = [], this.inputLayersTensorIndices = [], this.outputLayers = [], this.outputLayersNodeIndices = [], this.outputLayersTensorIndices = [], this.layers = [], this.internalContainerRefs = [];\n\n    for (var _e654 of this.outputs) {\n      var _t466 = _e654.nodeIndex,\n          _n265 = _e654.tensorIndex;\n      this.outputLayers.push(_e654.sourceLayer), this.outputLayersNodeIndices.push(_t466), this.outputLayersTensorIndices.push(_n265);\n    }\n\n    for (var _e655 of this.inputs) {\n      var _t467 = _e655.sourceLayer,\n          _n266 = _e655.nodeIndex,\n          _r214 = _e655.tensorIndex;\n      assert$3(0 === _n266, \"input layer has >1 nodes\"), assert$3(0 === _r214, \"input layer has >1 tensors\"), this.inputLayers.push(_t467), this.inputLayersNodeIndices.push(_n266), this.inputLayersTensorIndices.push(_r214);\n    }\n\n    this.inputNames = [], this.outputNames = [], this.feedInputShapes = [], this.feedInputNames = [], this.feedOutputNames = [];\n\n    for (var _t468 = 0; _t468 < this.inputLayers.length; _t468++) {\n      var _n267 = this.inputLayers[_t468];\n      if (!(_n267 instanceof InputLayer)) throw new TypeError(\"Input layers to a LayersModel must be InputLayer objects. Received inputs: \".concat(e.inputs, \". Input \").concat(_t468, \" (0-based) originates from layer type \").concat(_n267.getClassName(), \".\"));\n      this.inputNames.push(_n267.name), this.feedInputShapes.push(_n267.batchInputShape), this.feedInputNames.push(_n267.name);\n    }\n\n    for (var _e656 of this.outputLayers) {\n      this.outputNames.push(_e656.name);\n    }\n\n    this.internalInputShapes = this.inputs.map(e => e.shape), this.internalOutputShapes = this.outputs.map(e => e.shape);\n\n    var t = {},\n        n = {},\n        r = {},\n        a = {},\n        s = {},\n        o = [],\n        i = (e, t, n, r, a, l) => {\n      null != r && null != a && null != l || (r = e.sourceLayer, a = e.nodeIndex, l = e.tensorIndex);\n      var u = r.inboundNodes[a];\n      if (-1 !== n.indexOf(u)) throw new RuntimeError(\"The tensor \".concat(e.name, \" at layer \\\"\").concat(r.name, \"\\\" is part of a cycle.\"));\n      if (-1 !== t.indexOf(u)) return;\n      this.containerNodes.add(Container.nodeKey(r, a)), r.id in s || (s[r.id] = Object.keys(s).length), -1 === n.indexOf(u) && n.push(u);\n      var c = u.inboundLayers.length;\n\n      for (var _e657 = 0; _e657 < c; _e657++) {\n        i(u.inputTensors[_e657], t, n, u.inboundLayers[_e657], u.nodeIndices[_e657], u.tensorIndices[_e657]);\n      }\n\n      for (t.push(u); n.indexOf(u) >= 0;) {\n        n.splice(n.indexOf(u), 1);\n      }\n\n      o.push(u);\n    },\n        l = [],\n        u = [];\n\n    for (var _e658 of this.outputs) {\n      i(_e658, l, u);\n    }\n\n    var c = o.slice().reverse();\n\n    for (var _e659 of c) {\n      n[_e659.id] = _e659, _e659.id in t || (t[_e659.id] = 0);\n      var _s105 = t[_e659.id];\n      _s105 = Math.max(_s105, null == r[_e659.outboundLayer.id] ? 0 : r[_e659.outboundLayer.id]), r[_e659.outboundLayer.id] = _s105, a[_e659.outboundLayer.id] = _e659.outboundLayer, t[_e659.id] = _s105;\n\n      for (var _r215 = 0; _r215 < _e659.inboundLayers.length; _r215++) {\n        var _a145 = _e659.inboundLayers[_r215].inboundNodes[_e659.nodeIndices[_r215]];\n        t[_a145.id] = Math.max(_s105 + 1, null == t[_a145.id] ? 0 : t[_a145.id]), n[_a145.id] = _a145;\n      }\n    }\n\n    var p = {};\n\n    for (var _e660 in t) {\n      var _r216 = t[_e660];\n      _r216 in p || (p[_r216] = []), p[_r216].push(n[_e660]);\n    }\n\n    var d = {};\n\n    for (var _e661 in r) {\n      var _t469 = r[_e661];\n      _t469 in d || (d[_t469] = []), d[_t469].push(a[_e661]);\n    }\n\n    var h = Object.keys(d).map(e => parseInt(e, 10)).sort(reverseNumberCompare);\n    this.layers = [];\n\n    for (var _e662 of h) {\n      var _t470 = d[_e662];\n\n      _t470.sort((e, t) => {\n        var n = s[e.id],\n            r = s[t.id];\n        return n < r ? -1 : n > r ? 1 : 0;\n      });\n\n      for (var _e663 of _t470) {\n        _e663 instanceof Container && this.internalContainerRefs.push(_e663), this.layers.push(_e663);\n      }\n    }\n\n    this.layersByDepth = d, h = Object.keys(p).map(e => parseInt(e, 10)).sort(reverseNumberCompare);\n    var m = this.inputs.slice(),\n        f = [];\n\n    for (var _e664 of h) {\n      for (var _t471 of p[_e664]) {\n        var _e665 = _t471.outboundLayer;\n\n        if (null != _e665) {\n          for (var _n268 of _t471.inputTensors) {\n            if (-1 === m.indexOf(_n268)) throw new RuntimeError(\"Graph disconnected: cannot obtain value for tensor \".concat(_n268, \" at layer \\\"\").concat(_e665.name, \"\\\". The following previous layers were accessed without issue: \").concat(f));\n          }\n\n          for (var _e666 of _t471.outputTensors) {\n            m.push(_e666);\n          }\n\n          f.push(_e665.name);\n        }\n      }\n    }\n\n    this.nodesByDepth = p;\n    var g = this.layers.map(e => e.name);\n\n    var _loop42 = function _loop42(_e667) {\n      var t = g.filter(t => t === _e667).length;\n      if (1 !== t) throw new RuntimeError(\"The name \\\"\".concat(_e667, \"\\\" is used \").concat(t, \" times in the model. All layer names should be unique. Layer names: \") + JSON.stringify(g));\n    };\n\n    for (var _e667 of g) {\n      _loop42(_e667);\n    }\n\n    this.outboundNodes = [], this.inboundNodes = [], new Node({\n      outboundLayer: this,\n      inboundLayers: [],\n      nodeIndices: [],\n      tensorIndices: [],\n      inputTensors: this.inputs,\n      outputTensors: this.outputs,\n      inputMasks: this.inputs.map(e => null),\n      outputMasks: this.outputs.map(e => null),\n      inputShapes: this.inputs.map(e => e.shape),\n      outputShapes: this.outputs.map(e => e.shape)\n    }), this.built = !0, this._refCount = 1;\n  }\n\n  assertNotDisposed() {\n    if (0 === this._refCount) throw new Error(\"Container '\".concat(this.name, \"' is already disposed.\"));\n  }\n\n  dispose() {\n    this.assertNotDisposed();\n    var e = {\n      refCountAfterDispose: null,\n      numDisposedVariables: 0\n    };\n\n    if (0 == --this._refCount) {\n      for (var t of this.layers) {\n        e.numDisposedVariables += t.dispose().numDisposedVariables;\n      }\n\n      for (var _t472 of this.internalContainerRefs) {\n        e.numDisposedVariables += _t472.dispose().numDisposedVariables;\n      }\n    }\n\n    return e.refCountAfterDispose = this._refCount, e;\n  }\n\n  get trainable() {\n    return this.trainable_;\n  }\n\n  set trainable(e) {\n    this.layers.forEach(t => {\n      t._trainableWeights.forEach(t => t.trainable = e);\n    }), this.trainable_ = e;\n  }\n\n  get trainableWeights() {\n    if (this._trainableWeights.length > 0) throw new ValueError(\"Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.\");\n    if (!this.trainable) return [];\n    var e = [];\n\n    for (var t of this.layers) {\n      e = e.concat(t.trainableWeights);\n    }\n\n    return e;\n  }\n\n  get nonTrainableWeights() {\n    var e = [];\n\n    for (var t of this.layers) {\n      e.push(...t.nonTrainableWeights);\n    }\n\n    if (!this.trainable) {\n      var _t473 = [];\n\n      for (var _e668 of this.layers) {\n        _t473.push(..._e668.trainableWeights);\n      }\n\n      return _t473.concat(e);\n    }\n\n    return e;\n  }\n\n  get weights() {\n    return this.trainableWeights.concat(this.nonTrainableWeights);\n  }\n\n  loadWeights(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = {};\n    var r = 0;\n\n    for (var _e669 of this.layers) {\n      for (var _t474 of _e669.weights) {\n        if (null != n[_t474.originalName]) throw new ValueError(\"Duplicate weight name: \".concat(_t474.originalName));\n        n[_t474.originalName] = _t474, r++;\n      }\n    }\n\n    var a = [];\n\n    for (var _r217 in e) {\n      var s = _r217;\n\n      if (null == n[_r217]) {\n        var _e670 = _r217.split(\"/\");\n\n        s = _e670.slice(0, -2).concat([_e670[_e670.length - 1]]).join(\"/\");\n      }\n\n      if (null != n[s]) a.push([n[s], e[_r217]]);else if (t) throw new ValueError(\"Provided weight data has no target variable: \".concat(_r217));\n      delete n[s];\n    }\n\n    if (t) {\n      var _e671 = [];\n\n      for (var _t475 in n) {\n        _e671.push(_t475);\n      }\n\n      if (_e671.length > 0) throw new ValueError(\"\".concat(_e671.length, \" of \").concat(r, \" weights are not set: \").concat(_e671));\n    }\n\n    batchSetValue(a);\n  }\n\n  updatedConfig() {\n    var e = this.getConfig(),\n        t = {};\n    return t.className = this.getClassName(), t.config = e, t.kerasVersion = \"tfjs-layers \".concat(version$6), t.backend = \"TensorFlow.js\", t;\n  }\n\n  toJSON(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = convertTsToPythonic(this.updatedConfig());\n    return t ? JSON.stringify(n) : n;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      e = toList(e);\n      var n = new FeedDict();\n\n      for (var _t476 = 0; _t476 < this.inputs.length; ++_t476) {\n        n.add(this.inputs[_t476], e[_t476]);\n      }\n\n      return execute(this.outputs, n, t);\n    });\n  }\n\n  computeMask(e, t) {\n    return tidy(() => {\n      var n;\n      return e = toList(e), n = null == t ? pyListRepeat(null, e.length) : toList(t), this.runInternalGraph(e, n)[1];\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = normalizeShapeList(e);\n    if (t.length !== this.inputLayers.length) throw new ValueError(\"Invalid inputShape argument \".concat(e, \": model has \").concat(this.inputLayers.length, \" tensor inputs.\"));\n    var n = {};\n\n    for (var _e672 = 0; _e672 < t.length; _e672++) {\n      n[this.inputLayers[_e672].name + \"_0_0\"] = t[_e672];\n    }\n\n    var r = Object.keys(this.nodesByDepth).map(e => parseInt(e, 10)).sort(reverseNumberCompare);\n    if (r.length > 1) for (var _e673 of r) {\n      var _t477 = this.nodesByDepth[_e673];\n\n      for (var _e674 of _t477) {\n        var _t478 = _e674.outboundLayer;\n        if (-1 !== this.inputLayers.map(e => e.id).indexOf(_t478.id)) continue;\n        var _r218 = [];\n\n        for (var _t479 = 0; _t479 < _e674.inboundLayers.length; _t479++) {\n          _r218.push(n[\"\".concat(_e674.inboundLayers[_t479].name, \"_\").concat(_e674.nodeIndices[_t479], \"_\").concat(_e674.tensorIndices[_t479])]);\n        }\n\n        var _a146 = normalizeShapeList(_t478.computeOutputShape(singletonOrArray(_r218))),\n            _s106 = _t478.inboundNodes.indexOf(_e674);\n\n        for (var _e675 = 0; _e675 < _a146.length; _e675++) {\n          n[\"\".concat(_t478.name, \"_\").concat(_s106, \"_\").concat(_e675)] = _a146[_e675];\n        }\n      }\n    }\n    var a = [],\n        s = [];\n\n    for (var _e676 = 0; _e676 < this.outputLayers.length; _e676++) {\n      s.push(\"\".concat(this.outputLayers[_e676].name, \"_\").concat(this.outputLayersNodeIndices[_e676], \"_\").concat(this.outputLayersTensorIndices[_e676]));\n    }\n\n    for (var _e677 = 0; _e677 < s.length; _e677++) {\n      var _t480 = s[_e677];\n      assert$3(_t480 in n), a.push(n[_t480]);\n    }\n\n    return singletonOrArray(a);\n  }\n\n  runInternalGraph(e, t) {\n    null == t && (t = pyListRepeat(null, e.length));\n    var n = {};\n\n    for (var _r219 = 0; _r219 < this.inputs.length; ++_r219) {\n      n[this.inputs[_r219].id] = [e[_r219], t[_r219]];\n    }\n\n    var r = Object.keys(this.nodesByDepth).map(e => parseInt(e, 10)).sort(reverseNumberCompare);\n\n    for (var _e678 of r) {\n      var _t481 = this.nodesByDepth[_e678];\n\n      for (var _e679 of _t481) {\n        var _t482 = _e679.outboundLayer,\n            _r220 = _e679.inputTensors,\n            _a147 = _e679.outputTensors,\n            _s107 = new Array();\n\n        for (var _e680 of _r220) {\n          _e680.id in n && _s107.push(n[_e680.id]);\n        }\n\n        if (_s107.length === _r220.length) {\n          var _r221 = void 0,\n              _o74 = void 0,\n              i = void 0,\n              l = void 0,\n              u = {};\n\n          if (null != _e679.callArgs && (u = _e679.callArgs), 1 === _s107.length) {\n            var [_e681, _n269] = _s107[0];\n            null == u.mask && (u.mask = _n269), i = toList(_t482.call(_e681, u)), l = toList(_t482.computeMask(_e681, _n269)), _r221 = [_e681], _o74 = [_n269];\n          } else _r221 = _s107.map(e => e[0]), _o74 = _s107.map(e => e[1]), null == u.mask && (u.mask = _o74), i = toList(_t482.call(_r221, u)), l = toList(_t482.computeMask(_r221, _o74));\n\n          if (_t482.activityRegularizer) throw new NotImplementedError(\"LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.\");\n\n          for (var _e682 = 0; _e682 < _a147.length; ++_e682) {\n            n[_a147[_e682].id] = [i[_e682], l[_e682]];\n          }\n        }\n      }\n    }\n\n    var a = [],\n        s = [],\n        o = [];\n\n    for (var _e683 of this.outputs) {\n      assert$3(_e683.id in n, \"Could not compute output \".concat(_e683.name, \" : \").concat(_e683.id));\n      var [_t483, _r222] = n[_e683.id];\n      o.push(_t483.shape), a.push(_t483), s.push(_r222);\n    }\n\n    return [a, s, o];\n  }\n\n  buildNodeConversionMap(e) {\n    var t = {};\n    var n;\n\n    for (var _e684 of this.layers) {\n      n = _e684 instanceof Container ? 1 : 0;\n\n      for (var r = 0; r < _e684.inboundNodes.length; r++) {\n        var a = Container.nodeKey(_e684, r);\n        this.containerNodes.has(a) && (t[a] = n, n += 1);\n      }\n    }\n\n    return t;\n  }\n\n  getLayer(e, t) {\n    if (null != t) {\n      if (this.layers.length <= t) throw new ValueError(\"Was asked to retrieve layer at index \".concat(t, \", but model only has \").concat(this.layers.length, \" layer(s).\"));\n      return this.layers[t];\n    }\n\n    if (null == e) throw new ValueError(\"Provide either a layer name or layer index\");\n\n    for (var _t484 of this.layers) {\n      if (_t484.name === e) return _t484;\n    }\n\n    throw new ValueError(\"No such layer: \".concat(e));\n  }\n\n  calculateLosses() {\n    return tidy(() => {\n      var e = [];\n\n      for (var t of this.layers) {\n        for (var n = 0; n < t.inboundNodes.length; ++n) {\n          var r = Container.nodeKey(t, n);\n          this.containerNodes.has(r) && e.push(...t.calculateLosses());\n        }\n      }\n\n      return e;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      name: this.name\n    },\n        t = this.buildNodeConversionMap(this.layers),\n        n = [];\n\n    for (var _e685 of this.layers) {\n      var _r223 = _e685.getClassName(),\n          _a148 = _e685.getConfig(),\n          s = [];\n\n      for (var _n270 = 0; _n270 < _e685.inboundNodes.length; _n270++) {\n        var _r224 = _e685.inboundNodes[_n270],\n            _a149 = Container.nodeKey(_e685, _n270);\n\n        var _o75 = {};\n\n        if (this.containerNodes.has(_a149)) {\n          if (_r224.callArgs) try {\n            JSON.stringify(_r224.callArgs), _o75 = _r224.callArgs;\n          } catch (t) {\n            console.warn(\"Layer \".concat(_e685.name, \" was passed non-serializable keyword arguments: \").concat(_r224.callArgs, \". They will not be included in the serialized model (and thus will be missing at deserialization time).\")), _o75 = {};\n          }\n\n          if (_r224.inboundLayers.length > 0) {\n            var _e686 = [];\n\n            for (var _n271 = 0; _n271 < _r224.inboundLayers.length; _n271++) {\n              var _a150 = _r224.inboundLayers[_n271],\n                  _s108 = _r224.tensorIndices[_n271];\n              var i = t[Container.nodeKey(_a150, _r224.nodeIndices[_n271])];\n              null == i && (i = 0), _e686.push([_a150.name, i, _s108, _o75]);\n            }\n\n            s.push(_e686);\n          }\n        }\n      }\n\n      var o = {};\n      o.name = _e685.name, o.className = _r223, o.config = _a148, o.inboundNodes = s, n.push(o);\n    }\n\n    e.layers = n;\n    var r = [];\n\n    for (var _e687 = 0; _e687 < this.inputLayers.length; _e687++) {\n      var _n272 = this.inputLayers[_e687],\n          _a151 = Container.nodeKey(_n272, this.inputLayersNodeIndices[_e687]);\n\n      if (!this.containerNodes.has(_a151)) continue;\n      var _s109 = t[_a151];\n      null == _s109 && (_s109 = 0), r.push([_n272.name, _s109, this.inputLayersTensorIndices[_e687]]);\n    }\n\n    e.inputLayers = r;\n    var a = [];\n\n    for (var _e688 = 0; _e688 < this.outputLayers.length; _e688++) {\n      var _n273 = this.outputLayers[_e688],\n          _r225 = Container.nodeKey(_n273, this.outputLayersNodeIndices[_e688]);\n\n      if (!this.containerNodes.has(_r225)) continue;\n      var _s110 = t[_r225];\n      null == _s110 && (_s110 = 0), a.push([_n273.name, _s110, this.outputLayersTensorIndices[_e688]]);\n    }\n\n    return e.outputLayers = a, e;\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = {},\n        s = {};\n\n    function o(e, t) {\n      e.name in s ? s[e.name].push(t) : s[e.name] = [t];\n    }\n\n    function i(e, t) {\n      var n = [];\n      var r;\n\n      for (var _s111 of t) {\n        var _i45 = _s111[0],\n            _l37 = _s111[1],\n            _u32 = _s111[2];\n        if (r = null == _s111[3] ? {} : _s111[3], !(_i45 in a)) return void o(e, t);\n        var _c22 = a[_i45];\n        if (_c22.inboundNodes.length <= _l37) return void o(e, t);\n        n.push(_c22.inboundNodes[_l37].outputTensors[_u32]);\n      }\n\n      n.length > 0 && e.apply(singletonOrArray(n), r);\n    }\n\n    function l(e) {\n      var n = e.name,\n          s = deserialize(e, null != t.customObjects ? t.customObjects : {});\n      s.setFastWeightInitDuringBuild(r), a[n] = s, e.inboundNodes.forEach(e => {\n        if (!(e instanceof Array)) throw new ValueError(\"Corrupted configuration, expected array for nodeData: \".concat(e));\n        o(s, e);\n      });\n    }\n\n    var u = t.name,\n        c = t.layers;\n\n    for (var _e689 of c) {\n      l(_e689);\n    }\n\n    for (; !isObjectEmpty(s);) {\n      for (var _e690 of c) {\n        var _t485 = a[_e690.name];\n\n        if (_t485.name in s) {\n          var _e691 = s[_t485.name];\n          delete s[_t485.name];\n\n          for (var _n274 of _e691) {\n            i(_t485, _n274);\n          }\n        }\n      }\n    }\n\n    var p = [],\n        d = [],\n        h = t.inputLayers;\n\n    for (var _e692 of h) {\n      var _t486 = _e692[0],\n          _n275 = _e692[1],\n          _r226 = _e692[2];\n      assert$3(_t486 in a), p.push(a[_t486].inboundNodes[_n275].outputTensors[_r226]);\n    }\n\n    var m = t.outputLayers;\n\n    for (var _e693 of m) {\n      var _t487 = _e693[0],\n          _n276 = _e693[1],\n          _r227 = _e693[2];\n      assert$3(_t487 in a), d.push(a[_t487].inboundNodes[_n276].outputTensors[_r227]);\n    }\n\n    return new e({\n      inputs: p,\n      outputs: d,\n      name: u\n    });\n  }\n\n  get stateful() {\n    if (this._stateful) throw new ValueError(\"Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.\");\n\n    for (var _e694 of this.layers) {\n      if (_e694.stateful) return !0;\n    }\n\n    return !1;\n  }\n\n  resetStates() {\n    tidy(() => {\n      this.layers.forEach(e => {\n        e.stateful && e.resetStates();\n      });\n    });\n  }\n\n}\n\nfunction standardizeSampleOrClassWeights(e, t, n) {\n  var r = t.length;\n  if (null == e || Array.isArray(e) && 0 === e.length) return t.map(e => null);\n  if (1 === r) return Array.isArray(e) && 1 === e.length ? e : \"object\" == typeof e && t[0] in e ? [e[t[0]]] : [e];\n\n  if (Array.isArray(e)) {\n    if (e.length !== r) throw new Error(\"Provided \".concat(n, \" is an array of \").concat(e.length, \" element(s), but the model has \").concat(r, \" outputs. Make sure a set of weights is provided for each model output.\"));\n    return e;\n  }\n\n  if (\"object\" == typeof e && Object.keys(e).length > 0 && \"object\" == typeof e[Object.keys(e)[0]]) {\n    var _n277 = [];\n    return t.forEach(t => {\n      _n277.push(t in e ? e[t] : null);\n    }), _n277;\n  }\n\n  throw new Error(\"The model has multiple (\".concat(r, \") outputs, so \").concat(n, \" must be either an array with \").concat(r, \" elements or an object with \").concat(t, \" keys. Provided \").concat(n, \" not understood: \").concat(JSON.stringify(e)));\n}\n\nfunction standardizeClassWeights(e, t) {\n  return standardizeSampleOrClassWeights(e, t, \"classWeight\");\n}\n\nfunction standardizeWeights(_x87, _x88, _x89, _x90) {\n  return _standardizeWeights.apply(this, arguments);\n}\n\nfunction _standardizeWeights() {\n  _standardizeWeights = _asyncToGenerator(function* (e, t, n, r) {\n    if (null != t || null != r) throw new Error(\"Support sampleWeight is not implemented yet\");\n\n    if (null != n) {\n      var _t778 = tidy(() => {\n        if (1 === e.shape.length) return clone(e);\n\n        if (2 === e.shape.length) {\n          if (e.shape[1] > 1) return argMax$2(e, 1);\n          if (1 === e.shape[1]) return reshape$3(e, [e.shape[0]]);\n          throw new Error(\"Encountered unexpected last-dimension size (\".concat(e.shape[1], \") during handling of class weights. The size is expected to be >= 1.\"));\n        }\n\n        throw new Error(\"Unexpected rank of target (y) tensor (\".concat(e.rank, \") during handling of class weights. The rank is expected to be 1 or 2.\"));\n      }),\n          _r450 = Array.from(yield _t778.data());\n\n      dispose(_t778);\n      var a = [];\n      return _r450.forEach(e => {\n        if (null == n[e]) throw new Error(\"classWeight must contain all classes in the training data. The class \".concat(e, \" exists in the data but not in classWeight\"));\n        a.push(n[e]);\n      }), tensor1d(a, \"float32\");\n    }\n\n    return null;\n  });\n  return _standardizeWeights.apply(this, arguments);\n}\n\nfunction computeWeightedLoss(e, t) {\n  return mul(e, t);\n}\n\nvar DEFAULT_VALIDATION_BATCH_SIZE = 32;\n\nfunction standardizeDataIteratorOutput(e, t) {\n  var n, r;\n  n = t.xs, r = t.ys, assert$4(null != n && null != r, () => \"A Dataset iterator for fitDataset() is expected to generate objects of the form `{xs: xVal, ys: yVal}`, where the two values may be `tf.Tensor`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates \".concat(t));\n  var a = flattenTensorOrArrayOrMap(\"input\", e.inputNames, n),\n      s = flattenTensorOrArrayOrMap(\"output\", e.outputNames, r),\n      o = a[0].shape[0];\n  assert$4(a.length === e.inputs.length, () => \"LayersModel has \".concat(e.inputs.length, \" inputs, but the dataset provides \").concat(a.length, \" inputs.  (Expected input keys: \").concat(JSON.stringify(e.inputNames), \")\")), assert$4(s.length === e.outputs.length, () => \"LayersModel has \".concat(e.outputs.length, \" outputs, but the dataset provides \").concat(s.length, \" outputs.  (Expected output keys: \").concat(JSON.stringify(e.outputNames), \")\"));\n\n  var _loop43 = function _loop43(_t488) {\n    assert$4(a[_t488].shape[0] === o, () => \"Batch size mismatch: input \".concat(e.inputNames[_t488], \" has \").concat(a[_t488].shape[0], \"; expected  \").concat(o, \" based on input \").concat(e.inputNames[0], \".\"));\n  };\n\n  for (var _t488 = 0; _t488 < a.length; _t488++) {\n    _loop43(_t488);\n  }\n\n  var _loop44 = function _loop44(_t489) {\n    assert$4(s[_t489].shape[0] === o, () => \"Batch size mismatch: output \".concat(e.outputNames[_t489], \" has \").concat(s[_t489].shape[0], \"; expected  \").concat(o, \" based on input \").concat(e.inputNames[0], \".\"));\n  };\n\n  for (var _t489 = 0; _t489 < s.length; _t489++) {\n    _loop44(_t489);\n  }\n\n  return {\n    xs: a,\n    ys: s\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(e, t, n) {\n  if (n instanceof Tensor) return [n];\n  if (Array.isArray(n)) return assert$4(n.length === t.length, () => \"Received an array of \".concat(n.length, \" Tensors, but expected \").concat(t.length, \" to match the \").concat(e, \" keys \").concat(t, \".\")), n;\n  {\n    var r = [];\n\n    for (var a of t) {\n      if (null == n[a]) throw new ValueError(\"The feature data generated by the dataset lacks the required \".concat(e, \" key '\").concat(a, \"'.\"));\n      r.push(n[a]);\n    }\n\n    return r;\n  }\n}\n\nfunction standardizeTensorValidationData(e) {\n  if (3 === e.length) throw new NotImplementedError(\"Validation with sample weights is not implemented yet.\");\n  return {\n    xs: e[0],\n    ys: e[1]\n  };\n}\n\nfunction fitDataset(_x91, _x92, _x93) {\n  return _fitDataset.apply(this, arguments);\n}\n\nfunction _fitDataset() {\n  _fitDataset = _asyncToGenerator(function* (e, t, n) {\n    var r = null != n.batchesPerEpoch;\n    if (assert$4(null != e.optimizer, () => \"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).\"), assert$4(null != n, () => \"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.\"), assert$4(null != n.epochs && n.epochs > 0 && Number.isInteger(n.epochs), () => \"For fitDataset(), config.epochs is expected to be a positive integer, but got \".concat(n.epochs)), assert$4(!r || n.batchesPerEpoch > 0 && Number.isInteger(n.batchesPerEpoch), () => \"For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got \".concat(n.batchesPerEpoch)), assert$4(null == n.validationSplit, () => \"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.\"), e.isTraining) throw new Error(\"Cannot start training because another fit() call is ongoing.\");\n    e.isTraining = !0;\n\n    try {\n      var a = null != n.validationData;\n      var s, o;\n      if (a) if (isDatasetObject(n.validationData)) assert$4(null == n.validationBatches || n.validationBatches > 0 && Number.isInteger(n.validationBatches), () => \"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got \".concat(n.validationBatches));else {\n        var _e1152 = standardizeTensorValidationData(n.validationData);\n\n        s = _e1152.xs, o = _e1152.ys;\n      }\n      var i = e.makeTrainFunction(),\n          l = e.getDedupedMetricsNames();\n      var u;\n      u = a ? l.slice().concat(l.map(e => \"val_\" + e)) : l.slice();\n\n      var c = standardizeCallbacks(n.callbacks, n.yieldEvery),\n          _p43 = null == n.verbose ? 1 : n.verbose,\n          {\n        callbackList: d,\n        history: h\n      } = configureCallbacks(c, _p43, n.epochs, null, null, getStepsPerEpoch(t, n), null, a, u);\n\n      d.setModel(e), e.history = h, yield d.onTrainBegin(), e.stopTraining_ = !1;\n      var m = null == n.initialEpoch ? 0 : n.initialEpoch,\n          f = yield t.iterator();\n\n      for (; m < n.epochs;) {\n        var _u66 = {};\n        yield d.onEpochBegin(m);\n        var _c44 = 0,\n            _p44 = 0;\n\n        for (r || (f = yield t.iterator()); !r || _c44 < n.batchesPerEpoch;) {\n          var _t779 = yield f.next();\n\n          if (r && _t779.done) {\n            console.warn(\"You provided `batchesPerEpoch` as \".concat(n.batchesPerEpoch, \", but your dataset iterator ran out of data after \").concat(_c44, \" batches; interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, \") + n.batchesPerEpoch * n.epochs + \" batches). You may need to use the repeat() function when building your dataset.\");\n            break;\n          }\n\n          if (null != _t779.value) {\n            var {\n              xs: _r451,\n              ys: _a322\n            } = standardizeDataIteratorOutput(e, _t779.value),\n                _s226 = {};\n            _s226.batch = _p44, _s226.size = _r451[0].shape[0], yield d.onBatchBegin(_p44, _s226);\n            var _o161 = [];\n\n            if (null != n.classWeight) {\n              var _t780 = standardizeClassWeights(n.classWeight, e.outputNames);\n\n              for (var _e1153 = 0; _e1153 < _t780.length; ++_e1153) {\n                _o161.push(yield standardizeWeights(_a322[_e1153], null, _t780[_e1153]));\n              }\n            }\n\n            var _u67 = _r451.concat(_a322).concat(_o161),\n                _h26 = i(_u67);\n\n            dispose(_u67);\n\n            for (var _e1154 = 0; _e1154 < l.length; ++_e1154) {\n              var _t781 = _h26[_e1154];\n              _s226[l[_e1154]] = _t781, keep(_t781);\n            }\n\n            yield d.onBatchEnd(_p44, _s226), disposeTensorsInLogs(_s226), _p44++, _c44++;\n          }\n\n          if (r ? _c44 >= n.batchesPerEpoch : _t779.done) {\n            if (a) {\n              var _t782 = void 0;\n\n              _t782 = isDatasetObject(n.validationData) ? toList(yield e.evaluateDataset(n.validationData, {\n                batches: n.validationBatches\n              })) : toList(e.evaluate(s, o, {\n                batchSize: null == n.validationBatchSize ? DEFAULT_VALIDATION_BATCH_SIZE : n.validationBatchSize,\n                verbose: 0\n              }));\n\n              for (var _n456 = 0; _n456 < e.metricsNames.length; ++_n456) {\n                _u66[\"val_\".concat(e.metricsNames[_n456])] = _t782[_n456];\n              }\n            }\n\n            break;\n          }\n\n          if (e.stopTraining_) break;\n        }\n\n        if (yield d.onEpochEnd(m, _u66), m++, e.stopTraining_) break;\n      }\n\n      return yield d.onTrainEnd(), yield e.history.syncData(), e.history;\n    } finally {\n      e.isTraining = !1;\n    }\n  });\n  return _fitDataset.apply(this, arguments);\n}\n\nfunction getStepsPerEpoch(e, t) {\n  var n = null;\n  return null != t.batchesPerEpoch ? n = t.batchesPerEpoch : Number.isFinite(e.size) && (n = e.size), n;\n}\n\nfunction isDatasetObject(e) {\n  return \"function\" == typeof e.iterator;\n}\n\nfunction isLazyIteratorObject(e) {\n  return \"function\" == typeof e.next;\n}\n\nfunction evaluateDataset(_x94, _x95, _x96) {\n  return _evaluateDataset.apply(this, arguments);\n}\n\nfunction _evaluateDataset() {\n  _evaluateDataset = _asyncToGenerator(function* (e, t, n) {\n    var r = null != (n = n || {}).batches,\n        a = e.testFunction;\n    var s = [];\n    if (n.verbose > 0) throw new NotImplementedError(\"Verbose mode is not implemented yet.\");\n    assert$4(!r || n.batches > 0 && Number.isInteger(n.batches), () => \"Test loop expects `batches` to be a positive integer, but received \".concat(JSON.stringify(n.batches)));\n    var o = isLazyIteratorObject(t) ? t : yield t.iterator();\n    var i = 0,\n        l = 0;\n\n    var _loop66 = function* _loop66() {\n      var t = yield o.next();\n\n      if (s = tidy(() => {\n        if (t.value) {\n          (function () {\n            var {\n              xs: n,\n              ys: r\n            } = standardizeDataIteratorOutput(e, t.value),\n                o = n.concat(r),\n                u = tidy(() => a(o));\n            if (dispose(o), 0 === l) for (var _e1156 = 0; _e1156 < u.length; ++_e1156) {\n              s.push(scalar(0));\n            }\n            var c = o[0].shape[0];\n\n            var _loop67 = function _loop67(_e1157) {\n              var t = u[_e1157],\n                  n = s[_e1157];\n              s[_e1157] = tidy(() => add$2(s[_e1157], mul(c, t))), l > 0 && dispose(n);\n            };\n\n            for (var _e1157 = 0; _e1157 < u.length; ++_e1157) {\n              _loop67(_e1157);\n            }\n\n            dispose(u), i += c, ++l;\n          })();\n        }\n\n        return s;\n      }), t.done) {\n        r && console.warn(\"Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least `batches` batches (in this case, \".concat(n.batches, \" batches). You may need to use the repeat() function when building your dataset.\"));\n        return \"break\";\n      }\n    };\n\n    for (; !r || l < n.batches;) {\n      var _ret8 = yield* _loop66();\n\n      if (_ret8 === \"break\") break;\n    }\n\n    for (var _e1155 = 0; _e1155 < s.length; ++_e1155) {\n      var _t783 = s[_e1155];\n      s[_e1155] = div$1(s[_e1155], i), dispose(_t783);\n    }\n\n    return singletonOrArray(s);\n  });\n  return _evaluateDataset.apply(this, arguments);\n}\n\nfunction checkBatchSize(e) {\n  assert$4(e > 0 && Number.isInteger(e), () => \"batchSize is required to be a positive integer, but got \".concat(e));\n}\n\nfunction sliceArrays(e, t, n) {\n  return null == e ? [null] : Array.isArray(e) ? e.map(e => sliceAlongFirstAxis(e, t, n - t)) : sliceAlongFirstAxis(e, t, n - t);\n}\n\nfunction sliceArraysByIndices(e, t) {\n  return tidy(() => null == e ? null : Array.isArray(e) ? e.map(e => sliceArraysByIndices(e, t)) : gather(e, \"int32\" === t.dtype ? t : cast$3(t, \"int32\")));\n}\n\nfunction makeBatches(e, t) {\n  var n = [];\n  var r = 0,\n      a = null;\n\n  for (; r < e;) {\n    a = r + t, a >= e && (a = e), n.push([r, a]), r = a;\n  }\n\n  return n;\n}\n\nfunction fitLoop(_x97, _x98, _x99, _x100, _x101, _x102, _x103, _x104, _x105, _x106, _x107, _x108, _x109, _x110, _x111) {\n  return _fitLoop.apply(this, arguments);\n}\n\nfunction _fitLoop() {\n  _fitLoop = _asyncToGenerator(function* (e, t, n, r, a, s, o, i, l, u, c, p, d, h, m) {\n    null == a && (a = 32), null == s && (s = 1), null == c && (c = !0), null == d && (d = 0);\n    var f = !1;\n    if (null != l && null != u && (f = !0), null != m && (f = !0, null == h)) throw new ValueError(\"Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.\");\n    var g = e.checkNumSamples(n, a, h, \"steps_per_epoch\");\n    var $;\n    null != g && ($ = range$3(0, g)), null == o && (o = 1);\n    var {\n      callbackList: y,\n      history: b\n    } = configureCallbacks(i, o, s, d, g, h, a, f, p);\n    y.setModel(e), e.history = b, yield y.onTrainBegin(), e.stopTraining_ = !1;\n\n    var _loop68 = function* _loop68(_o162) {\n      yield y.onEpochBegin(_o162);\n      var s = {};\n      if (null != h) throw new NotImplementedError(\"stepsPerEpoch mode is not implemented yet.\");\n      {\n        yield* function* () {\n          if (\"batch\" === c) throw new NotImplementedError(\"batch shuffling is not implemneted yet\");\n          c && shuffle($);\n          var o = tensor1d($),\n              i = makeBatches(g, a);\n\n          var _loop69 = function* _loop69(_c45) {\n            var p = {};\n            if (yield y.onBatchBegin(_c45, p), tidy(() => {\n              var d = i[_c45][0],\n                  h = i[_c45][1],\n                  m = sliceAlongFirstAxis(o, d, h - d);\n              p.batch = _c45, p.size = h - d;\n              var g = sliceArraysByIndices(n, m),\n                  $ = t(g);\n\n              for (var _e1158 = 0; _e1158 < r.length; ++_e1158) {\n                var _t784 = $[_e1158];\n                p[r[_e1158]] = _t784, keep(_t784);\n              }\n\n              if (_c45 === i.length - 1 && f) {\n                var _t785 = e.testLoop(l, u, a);\n\n                for (var _e1159 = 0; _e1159 < r.length; ++_e1159) {\n                  var _n457 = r[_e1159],\n                      _a323 = _t785[_e1159];\n                  keep(_a323), s[\"val_\" + _n457] = _a323;\n                }\n              }\n            }), yield y.onBatchEnd(_c45, p), disposeTensorsInLogs(p), e.stopTraining_) return \"break\";\n          };\n\n          for (var _c45 = 0; _c45 < i.length; ++_c45) {\n            var _ret10 = yield* _loop69(_c45);\n\n            if (_ret10 === \"break\") break;\n          }\n\n          o.dispose();\n        }();\n      }\n      if (yield y.onEpochEnd(_o162, s), e.stopTraining_) return \"break\";\n    };\n\n    for (var _o162 = d; _o162 < s; ++_o162) {\n      var _ret9 = yield* _loop68(_o162);\n\n      if (_ret9 === \"break\") break;\n    }\n\n    return yield y.onTrainEnd(), yield e.history.syncData(), e.history;\n  });\n  return _fitLoop.apply(this, arguments);\n}\n\nfunction fitTensors(_x112, _x113, _x114) {\n  return _fitTensors.apply(this, arguments);\n}\n\nfunction _fitTensors() {\n  _fitTensors = _asyncToGenerator(function* (e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : {};\n    if (e.isTraining) throw new Error(\"Cannot start training because another fit() call is ongoing.\");\n    var a, s, o, i, l, u, c;\n    e.isTraining = !0;\n\n    try {\n      var _p45 = null == r.batchSize ? 32 : r.batchSize;\n\n      checkBatchSize(_p45);\n      var d = !1,\n          h = yield e.standardizeUserData(t, n, r.sampleWeight, r.classWeight, d, _p45);\n      a = h[0], s = h[1], c = h[2];\n      var m,\n          f = !1;\n\n      if (null != r.validationData && r.validationData.length > 0) {\n        if (f = !0, 2 !== r.validationData.length) throw 3 === r.validationData.length ? new NotImplementedError(\"validationData including sample weights is not supported yet.\") : new ValueError(\"When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; \".concat(r.validationData, \" is invalid.\"));\n        o = r.validationData[0], i = r.validationData[1];\n\n        var _t786 = !0,\n            _n458 = yield e.standardizeUserData(o, i, null, null, _t786, _p45);\n\n        l = _n458[0], u = _n458[1], m = l.concat(u);\n      } else if (null != r.validationSplit && r.validationSplit > 0 && r.validationSplit < 1) {\n        f = !0;\n\n        var _e1160 = Math.floor(a[0].shape[0] * (1 - r.validationSplit)),\n            _t787 = a[0].shape[0];\n\n        l = sliceArrays(a, _e1160, _t787), a = sliceArrays(a, 0, _e1160), u = sliceArrays(s, _e1160, _t787), s = sliceArrays(s, 0, _e1160), m = l.concat(u);\n      } else null != r.validationSteps && (f = !0);\n\n      var g = a.concat(s).concat(c);\n      e.checkTrainableWeightsConsistency();\n      var $ = e.makeTrainFunction(),\n          y = e.getDedupedMetricsNames();\n      var b, x;\n      f ? (e.makeTestFunction(), b = e.testFunction, x = y.slice().concat(y.map(e => \"val_\" + e))) : (b = null, m = [], x = y.slice());\n      var v = standardizeCallbacks(r.callbacks, r.yieldEvery);\n      return yield fitLoop(e, $, g, y, _p45, r.epochs, r.verbose, v, b, m, r.shuffle, x, r.initialEpoch, null, null);\n    } finally {\n      e.isTraining = !1, disposeNewTensors(a, t), disposeNewTensors(s, n), disposeNewTensors(l, o), disposeNewTensors(u, i), null != c && dispose(c);\n    }\n  });\n  return _fitTensors.apply(this, arguments);\n}\n\nfunction ensureTensorsRank2OrHigher(e) {\n  var t = [];\n  e instanceof Tensor && (e = [e]);\n\n  for (var n = 0; n < e.length; ++n) {\n    var r = e[n];\n    if (1 === r.rank) t.push(expandDims$2(r, 1));else {\n      if (0 === r.rank) throw new Error(\"Expected tensor to be at least 1D, but received a 0D tensor (scalar).\");\n      t.push(r);\n    }\n  }\n\n  return t;\n}\n\nfunction disposeNewTensors(e, t) {\n  if (null == e) return;\n  var n = [];\n  if (t instanceof Tensor) n.push(t.id);else if (Array.isArray(t)) t.forEach(e => n.push(e.id));else if (null != t) for (var _e695 in t) {\n    n.push(t[_e695].id);\n  }\n  var r = [];\n  if (e instanceof Tensor) -1 === n.indexOf(e.id) && r.push(e);else if (Array.isArray(e)) e.forEach(e => {\n    -1 === n.indexOf(e.id) && r.push(e);\n  });else if (null != e) for (var _t490 in e) {\n    var a = e[_t490];\n    -1 === n.indexOf(a.id) && r.push(a);\n  }\n  r.forEach(e => {\n    e.isDisposed || e.dispose();\n  });\n}\n\nfunction isDataTensor(e) {\n  return e instanceof Tensor;\n}\n\nfunction isDataArray(e) {\n  return Array.isArray(e);\n}\n\nfunction isDataDict(e) {\n  return !isDataTensor(e) && !isDataArray(e);\n}\n\nfunction standardizeInputData(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"\";\n\n  if (null == t || 0 === t.length) {\n    if (null != e) {\n      var _t491 = !1;\n\n      if (isDataArray(e) && e.length > 0) _t491 = !0;else if (isDataDict(e)) {\n        for (var _n278 in e) {\n          if (e.hasOwnProperty(_n278)) {\n            _t491 = !0;\n            break;\n          }\n        }\n      } else _t491 = !0;\n      if (_t491) throw new ValueError(\"Error when checking model \".concat(a, \" expected no data, but got \").concat(e));\n    }\n\n    return [];\n  }\n\n  if (null == e) return t.map(e => null);\n  var s;\n\n  if (isDataDict(e)) {\n    e = e, s = [];\n\n    for (var _n279 of t) {\n      if (null == e[_n279]) throw new ValueError(\"No data provided for \\\"\".concat(_n279, \"\\\". Need data for each key in: \").concat(t));\n      s.push(e[_n279]);\n    }\n  } else if (isDataArray(e)) {\n    if ((e = e).length !== t.length) throw new ValueError(\"Error when checking model \".concat(a, \": the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see \").concat(t.length, \" Tensor(s), but instead got the following list of Tensor(s): \").concat(e));\n    s = e;\n  } else {\n    if (e = e, t.length > 1) throw new ValueError(\"The model \".concat(a, \" expects \").concat(t.length, \" Tensor(s), but only received one Tensor. Found: Tensor with shape \").concat(e.shape));\n    s = [e];\n  }\n\n  if (s = ensureTensorsRank2OrHigher(s), null != n) for (var _e696 = 0; _e696 < t.length; ++_e696) {\n    if (null == n[_e696]) continue;\n    var o = s[_e696];\n    if (o.shape.length !== n[_e696].length) throw new ValueError(\"Error when checking \".concat(a, \": expected \").concat(t[_e696], \" to have \").concat(n[_e696].length, \" dimension(s). but got array with shape \").concat(o.shape));\n\n    for (var _s112 = 0; _s112 < n[_e696].length; ++_s112) {\n      if (0 === _s112 && !r) continue;\n      var i = o.shape[_s112],\n          l = n[_e696][_s112];\n      if (null != l && l >= 0 && i !== l) throw new ValueError(\"Error when checking \".concat(a, \": expected \").concat(t[_e696], \" to have shape [\").concat(n[_e696], \"], but got array with shape [\").concat(o.shape, \"].\"));\n    }\n  }\n  return s;\n}\n\nfunction checkArrayLengths(e, t, n) {\n  var r = unique$2(e.map(e => e.shape[0]));\n  r.sort();\n  var a = unique$2(t.map(e => e.shape[0]));\n  if (a.sort(), r.length > 1) throw new ValueError(\"All input Tensors (x) should have the same number of samples. Got array shapes: \".concat(JSON.stringify(e.map(e => e.shape))));\n  if (a.length > 1) throw new ValueError(\"All target Tensors (y) should have the same number of samples. Got array shapes: \".concat(JSON.stringify(t.map(e => e.shape))));\n  if (r.length > 0 && a.length > 0 && !arraysEqual(r, a)) throw new ValueError(\"Input Tensors should have the same number of samples as target Tensors. Found \".concat(r[0], \" input sample(s) and \").concat(a[0], \" target sample(s).\"));\n}\n\nfunction checkLossAndTargetCompatibility(e, t, n) {\n  var r = [meanSquaredError$1, binaryCrossentropy$2, categoricalCrossentropy$2];\n\n  for (var a = 0; a < e.length; ++a) {\n    var s = e[a],\n        o = t[a],\n        i = n[a];\n\n    if (null != o) {\n      if (o === categoricalCrossentropy$2 && 1 === s.shape[s.shape.length - 1]) throw new ValueError(\"You are passing a target array of shape \".concat(s.shape, \" while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].\"));\n\n      if (-1 !== r.indexOf(o)) {\n        var _e697 = s.shape.slice(1),\n            _t492 = i.slice(1);\n\n        for (var _n280 = 0; _n280 < _e697.length; ++_n280) {\n          var _r228 = _e697[_n280],\n              _a152 = _t492[_n280];\n          if (null != _a152 && _r228 !== _a152) throw new ValueError(\"A target Tensor with shape \".concat(s.shape, \" was passed for an output of shape \").concat(i, \", while using a loss function that expects targets to have the same shape as the output.\"));\n        }\n      }\n    }\n  }\n}\n\nfunction checkInputData(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !0;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"\";\n  var s;\n\n  if (Array.isArray(e)) {\n    if (e.length !== t.length) throw new ValueError(\"Error when checking model \".concat(a, \": the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see \").concat(t.length, \" Tensor(s), but instead got \").concat(e.length, \" Tensors(s).\"));\n    s = e;\n  } else {\n    if (t.length > 1) throw new ValueError(\"The model expects \".concat(t.length, \" \").concat(a, \" Tensors, but only received one Tensor. Found: array with shape \").concat(JSON.stringify(e.shape), \".\"));\n    s = [e];\n  }\n\n  if (null != n) for (var _e698 = 0; _e698 < t.length; ++_e698) {\n    if (null == n[_e698]) continue;\n    var o = s[_e698];\n    if (o.shape.length !== n[_e698].length) throw new ValueError(\"Error when checking \".concat(a, \": expected \").concat(t[_e698], \" to have \").concat(n[_e698].length, \" dimension(s), but got array with shape \").concat(JSON.stringify(o.shape)));\n\n    for (var _s113 = 0; _s113 < n[_e698].length; ++_s113) {\n      if (0 === _s113 && !r) continue;\n      var i = o.shape[_s113],\n          l = n[_e698][_s113];\n      if (null != l && l !== i) throw new ValueError(\"Error when checking \".concat(a, \": expected \").concat(t[_e698], \" to have shape \").concat(JSON.stringify(n[_e698]), \" but got array with shape \").concat(JSON.stringify(o.shape), \".\"));\n    }\n  }\n}\n\nfunction collectMetrics(e, t) {\n  if (null == e || Array.isArray(e) && 0 === e.length) return t.map(e => []);\n  var n;\n  if (\"string\" == typeof e || \"function\" == typeof e) n = [e];else {\n    if (!Array.isArray(e) && \"object\" != typeof e) throw new TypeError(\"Type of metrics argument not understood. Expected an string,function, Array, or Object, found: \".concat(e));\n    n = e;\n  }\n  if (Array.isArray(n)) return t.map(e => n);\n  {\n    var _e699 = [];\n\n    for (var r of t) {\n      var _t493 = n.hasOwnProperty(r) ? n[r] : [];\n\n      Array.isArray(_t493) || (_t493 = [_t493]), _e699.push(_t493);\n    }\n\n    return _e699;\n  }\n}\n\nvar LAYERS_MODEL_FORMAT_NAME = \"layers-model\";\n\nclass LayersModel extends Container {\n  constructor(e) {\n    super(e), this.isTraining = !1;\n  }\n\n  summary(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : console.log;\n    if (!this.built) throw new ValueError(\"This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).\");\n    printSummary(this, e, t, n);\n  }\n\n  compile(e) {\n    var _this135 = this;\n\n    if (null == e.loss && (e.loss = []), this.loss = e.loss, \"string\" == typeof e.optimizer) this.optimizer_ = getOptimizer(e.optimizer), this.isOptimizerOwned = !0;else {\n      if (!(e.optimizer instanceof Optimizer)) throw new ValueError(\"User-defined optimizer must be an instance of tf.Optimizer.\");\n      this.optimizer_ = e.optimizer, this.isOptimizerOwned = !1;\n    }\n    var t = [];\n    if (Array.isArray(e.loss) || \"string\" == typeof e.loss || \"function\" == typeof e.loss) {\n      if (Array.isArray(e.loss)) {\n        if (e.loss.length !== this.outputs.length) throw new ValueError(\"When passing an Array as loss, it should have one entry per model output. The model has \".concat(this.outputs.length, \" output(s), but you passed loss=\").concat(e.loss, \".\"));\n        t = e.loss.map(e => get$1(e));\n      } else {\n        var _n281 = get$1(e.loss);\n\n        this.outputs.forEach(e => {\n          t.push(_n281);\n        });\n      }\n    } else {\n      e.loss = e.loss;\n\n      for (var _t494 in e.loss) {\n        if (-1 === this.outputNames.indexOf(_t494)) throw new ValueError(\"Unknown entry in loss dictionary: \\\"\".concat(_t494, \"\\\". Only expected the following keys: \").concat(this.outputNames));\n      }\n\n      for (var _n282 of this.outputNames) {\n        null == e.loss[_n282] && console.warn(\"Output \\\"\".concat(_n282, \"\\\" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to \").concat(_n282, \" during training\")), t.push(get$1(e.loss[_n282]));\n      }\n    }\n    this.lossFunctions = t, this.feedOutputNames = [], this.feedOutputShapes = [], this.feedLossFns = [];\n\n    for (var _e700 = 0; _e700 < this.outputs.length; ++_e700) {\n      var _t495 = this.internalOutputShapes[_e700];\n      this.feedOutputNames.push(this.outputNames[_e700]), this.feedOutputShapes.push(_t495), this.feedLossFns.push(this.lossFunctions[_e700]);\n    }\n\n    var n = [];\n    this.metrics = e.metrics, this.metricsNames = [\"loss\"], this.metricsTensors = [], nameScope(\"loss\", () => {\n      for (var _e701 = 0; _e701 < this.outputs.length; ++_e701) {\n        if (-1 !== n.indexOf(_e701)) continue;\n        var _t496 = this.lossFunctions[_e701];\n        this.outputs.length > 1 && (this.metricsTensors.push([_t496, _e701]), this.metricsNames.push(this.outputNames[_e701] + \"_loss\"));\n      }\n    });\n\n    var r = collectMetrics(e.metrics, this.outputNames),\n        a = (e, t, n) => {\n      this.outputNames.length > 1 && (t = this.outputNames[e] + \"_\" + t), this.metricsNames.push(t), this.metricsTensors.push([n, e]);\n    };\n\n    nameScope(\"metric\", () => {\n      var _loop45 = function _loop45(_e702) {\n        -1 === n.indexOf(_e702) && (t => {\n          var n, r, s;\n\n          for (var o of t) {\n            if (\"string\" == typeof o && -1 !== [\"accuracy\", \"acc\", \"crossentropy\", \"ce\"].indexOf(o)) {\n              var _t498 = _this135.internalOutputShapes[_e702];\n\n              var _a153 = void 0;\n\n              1 === _t498[_t498.length - 1] || _this135.lossFunctions[_e702] === binaryCrossentropy$2 ? -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? r = binaryAccuracy$1 : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (r = binaryCrossentropy$1) : _this135.lossFunctions[_e702] === sparseCategoricalCrossentropy$1 ? -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? r = sparseCategoricalAccuracy$1 : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (r = sparseCategoricalCrossentropy) : -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? r = categoricalAccuracy$1 : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (r = categoricalCrossentropy$1), -1 !== [\"accuracy\", \"acc\"].indexOf(o) ? _a153 = \"acc\" : -1 !== [\"crossentropy\", \"ce\"].indexOf(o) && (_a153 = \"ce\"), s = r, n = \"\" + _a153;\n            } else {\n              var _e703 = get(o);\n\n              s = _e703, n = \"\" + getLossOrMetricName(o);\n            }\n\n            var _t497 = void 0;\n\n            nameScope(n, () => {\n              _t497 = s;\n            }), a(_e702, n, _t497);\n          }\n        })(r[_e702]);\n      };\n\n      for (var _e702 = 0; _e702 < this.outputs.length; ++_e702) {\n        _loop45(_e702);\n      }\n    }), this.collectedTrainableWeights = this.trainableWeights;\n  }\n\n  checkTrainableWeightsConsistency() {\n    null != this.collectedTrainableWeights && this.trainableWeights.length !== this.collectedTrainableWeights.length && console.warn(\"Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?\");\n  }\n\n  evaluate(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = null == n.batchSize ? 32 : n.batchSize;\n    checkBatchSize(r);\n    var a = this.standardizeUserDataXY(e, t, !0, r);\n\n    try {\n      var s = a[0].concat(a[1]);\n      return this.makeTestFunction(), singletonOrArray(this.testLoop(this.testFunction, s, r, n.verbose, n.steps));\n    } finally {\n      disposeNewTensors(a[0], e), disposeNewTensors(a[1], t);\n    }\n  }\n\n  evaluateDataset(e, t) {\n    var _this136 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this136.makeTestFunction(), evaluateDataset(_this136, e, t);\n    })();\n  }\n\n  checkNumSamples(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"steps\";\n    var a;\n\n    if (null != n) {\n      if (a = null, null != t) throw new ValueError(\"If \".concat(r, \" is set, batchSize must be null or undefined.Got batchSize = \").concat(t));\n    } else {\n      if (null == e) throw new ValueError(\"Either the input data should have a defined shape, or \".concat(r, \" shoud be specified.\"));\n      a = Array.isArray(e) ? e[0].shape[0] : e.shape[0];\n    }\n\n    return a;\n  }\n\n  execute(e, t) {\n    if (Array.isArray(t) && 0 === t.length) throw new ValueError(\"`outputs` is an empty Array, which is not allowed.\");\n    var n = Array.isArray(t),\n        r = this.retrieveSymbolicTensors(n ? t : [t]),\n        a = new FeedDict();\n\n    if (e instanceof Tensor && (e = [e]), Array.isArray(e)) {\n      if (e.length !== this.inputs.length) throw new ValueError(\"The number of inputs provided (\".concat(e.length, \") does not match the number of inputs of this model (\").concat(this.inputs.length, \").\"));\n\n      for (var _t499 = 0; _t499 < this.inputs.length; ++_t499) {\n        a.add(this.inputs[_t499], e[_t499]);\n      }\n    } else for (var _t500 of this.inputs) {\n      var _n283 = e[_t500.name];\n      if (null == _n283) throw new ValueError(\"No value is provided for the model's input \".concat(_t500.name));\n      a.add(_t500, _n283);\n    }\n\n    var s = execute(r, a);\n    return n ? s : s[0];\n  }\n\n  retrieveSymbolicTensors(e) {\n    var t = pyListRepeat(null, e.length);\n    var n = e.length;\n\n    for (var r of this.layers) {\n      var a = Array.isArray(r.output) ? r.output : [r.output],\n          s = a.map(e => e.name);\n\n      for (var _r229 = 0; _r229 < e.length; ++_r229) {\n        var o = s.indexOf(e[_r229]);\n        if (-1 !== o && (t[_r229] = a[o], n--), 0 === n) break;\n      }\n\n      if (0 === n) break;\n    }\n\n    if (n > 0) {\n      var _n284 = [];\n      throw t.forEach((t, r) => {\n        null == t && _n284.push(e[r]);\n      }), new ValueError(\"Cannot find SymbolicTensors for output name(s): \".concat(JSON.stringify(_n284)));\n    }\n\n    return t;\n  }\n\n  predictLoop(e) {\n    var _this137 = this;\n\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 32;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    return tidy(() => {\n      var r = this.checkNumSamples(e);\n      if (n) throw new NotImplementedError(\"Verbose predictLoop() is not implemented yet.\");\n      var a = makeBatches(r, t),\n          s = this.outputs.map(e => []);\n\n      var _loop46 = function _loop46(_t501) {\n        tidy(() => {\n          var n = sliceArrays(e, a[_t501][0], a[_t501][1]),\n              r = [];\n          if (Array.isArray(n)) for (var _e704 = 0; _e704 < n.length; ++_e704) {\n            r.push({\n              key: _this137.inputs[_e704],\n              value: n[_e704]\n            });\n          } else r.push({\n            key: _this137.inputs[0],\n            value: n\n          });\n          var s = new FeedDict(r);\n          return execute(_this137.outputs, s);\n        }).forEach((e, t) => s[t].push(e));\n      };\n\n      for (var _t501 = 0; _t501 < a.length; ++_t501) {\n        _loop46(_t501);\n      }\n\n      return singletonOrArray(s.map(e => concat$2(e, 0)));\n    });\n  }\n\n  predict(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    var n = ensureTensorsRank2OrHigher(e);\n    checkInputData(n, this.inputNames, this.feedInputShapes, !1);\n\n    try {\n      var r = null == t.batchSize ? 32 : t.batchSize;\n      return checkBatchSize(r), this.predictLoop(n, r);\n    } finally {\n      disposeNewTensors(n, e);\n    }\n  }\n\n  predictOnBatch(e) {\n    checkInputData(e, this.inputNames, this.feedInputShapes, !0);\n    var t = (Array.isArray(e) ? e[0] : e).shape[0];\n    return this.predictLoop(e, t);\n  }\n\n  standardizeUserDataXY(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n    var r = arguments.length > 3 ? arguments[3] : undefined;\n    if (null == this.optimizer_) throw new RuntimeError(\"You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).\");\n    var a = [];\n\n    for (var _e705 = 0; _e705 < this.feedOutputShapes.length; ++_e705) {\n      var _t502 = this.feedOutputShapes[_e705];\n      a.push(this.feedLossFns[_e705] === sparseCategoricalCrossentropy$1 ? _t502.slice(0, _t502.length - 1).concat([1]) : _t502);\n    }\n\n    if (checkArrayLengths(e = standardizeInputData(e, this.feedInputNames, this.feedInputShapes, !1, \"input\"), t = standardizeInputData(t, this.feedOutputNames, a, !1, \"target\")), checkLossAndTargetCompatibility(t, this.feedLossFns, this.feedOutputShapes), this.stateful && null != r && r > 0 && e[0].shape[0] % r != 0) throw new ValueError(\"In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size \".concat(r, \". Found: \").concat(e[0].shape[0], \" sample(s).\"));\n    return [e, t];\n  }\n\n  standardizeUserData(e, t, n, r) {\n    var _arguments4 = arguments,\n        _this138 = this;\n\n    return _asyncToGenerator(function* () {\n      var a = _arguments4.length > 4 && _arguments4[4] !== undefined ? _arguments4[4] : !0;\n      var s = _arguments4.length > 5 ? _arguments4[5] : undefined;\n\n      var [o, i] = _this138.standardizeUserDataXY(e, t, a, s);\n\n      if (null != n) throw new Error(\"sample weight is not supported yet.\");\n      var l = null;\n\n      if (null != r) {\n        var _e706 = standardizeClassWeights(r, _this138.outputNames);\n\n        l = [];\n\n        for (var _t503 = 0; _t503 < _e706.length; ++_t503) {\n          l.push(yield standardizeWeights(i[_t503], null, _e706[_t503]));\n        }\n      }\n\n      return [o, i, l];\n    })();\n  }\n\n  testLoop(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n    var a = arguments.length > 4 ? arguments[4] : undefined;\n    return tidy(() => {\n      var s = this.checkNumSamples(t, n, a, \"steps\"),\n          o = [];\n      if (r > 0) throw new NotImplementedError(\"Verbose mode is not implemented yet.\");\n      if (null != a) throw new NotImplementedError(\"steps mode in testLoop() is not implemented yet\");\n      {\n        var _r230 = makeBatches(s, n),\n            _a154 = tensor1d(range$3(0, s));\n\n        for (var _n285 = 0; _n285 < _r230.length; ++_n285) {\n          var _s114 = _r230[_n285][0],\n              i = _r230[_n285][1],\n              l = sliceAlongFirstAxis(_a154, _s114, i - _s114),\n              u = sliceArraysByIndices(t, l),\n              c = e(u);\n          if (0 === _n285) for (var _e707 = 0; _e707 < c.length; ++_e707) {\n            o.push(scalar(0));\n          }\n\n          for (var _e708 = 0; _e708 < c.length; ++_e708) {\n            o[_e708] = add$2(o[_e708], mul(i - _s114, c[_e708]));\n          }\n        }\n\n        for (var _e709 = 0; _e709 < o.length; ++_e709) {\n          o[_e709] = div$1(o[_e709], s);\n        }\n      }\n      return o;\n    });\n  }\n\n  getDedupedMetricsNames() {\n    var e = this.metricsNames,\n        t = [];\n\n    for (var n = 0; n < e.length; ++n) {\n      var r = e[n];\n      var a = r;\n      count(e, r) > 1 && (a += \"_\".concat(count(e.slice(0, n), r))), t.push(a);\n    }\n\n    return t;\n  }\n\n  makeTrainFunction() {\n    return e => {\n      var t = [],\n          n = e.slice(0, this.inputs.length),\n          r = e.slice(this.inputs.length, this.inputs.length + this.outputs.length),\n          a = e.slice(this.inputs.length + this.outputs.length, this.inputs.length + 2 * this.outputs.length),\n          s = [],\n          o = this.collectedTrainableWeights.map(e => e.read());\n      return [this.optimizer_.minimize(() => {\n        var e = [];\n\n        for (var _t504 = 0; _t504 < this.inputs.length; ++_t504) {\n          e.push({\n            key: this.inputs[_t504],\n            value: n[_t504]\n          });\n        }\n\n        var o = new FeedDict(e),\n            i = execute(this.outputs, o, {\n          training: !0\n        });\n        var l;\n\n        for (var _e710 = 0; _e710 < this.lossFunctions.length; ++_e710) {\n          var _n286 = (0, this.lossFunctions[_e710])(r[_e710], i[_e710]);\n\n          null != a[_e710] && (_n286 = computeWeightedLoss(_n286, a[_e710]));\n\n          var _s115 = mean$1(_n286);\n\n          t.push(_s115), l = 0 === _e710 ? _n286 : add$2(l, _n286);\n        }\n\n        for (var _e711 = 0; _e711 < this.metricsTensors.length; ++_e711) {\n          var _n287 = void 0;\n\n          if (this.outputs.length > 1 && _e711 < this.outputs.length) _n287 = t[_e711];else {\n            var _t505 = this.metricsTensors[_e711][1];\n            _n287 = mean$1((0, this.metricsTensors[_e711][0])(r[_t505], i[_t505]));\n          }\n          keep(_n287), s.push(_n287);\n        }\n\n        return l = mean$1(l), this.calculateLosses().forEach(e => {\n          l = add$2(l, e);\n        }), l;\n      }, !0, o)].concat(s);\n    };\n  }\n\n  makeTestFunction() {\n    this.testFunction = e => tidy(() => {\n      var t = [];\n      var n;\n      var r = e.slice(0, this.inputs.length),\n          a = e.slice(this.inputs.length, this.inputs.length + this.outputs.length),\n          s = [];\n\n      for (var _e712 = 0; _e712 < this.inputs.length; ++_e712) {\n        s.push({\n          key: this.inputs[_e712],\n          value: r[_e712]\n        });\n      }\n\n      var o = new FeedDict(s),\n          i = execute(this.outputs, o);\n\n      for (var _e713 = 0; _e713 < this.lossFunctions.length; ++_e713) {\n        var _r231 = mean$1((0, this.lossFunctions[_e713])(a[_e713], i[_e713]));\n\n        n = 0 === _e713 ? _r231 : add$2(n, _r231), t.push(n);\n      }\n\n      for (var _e714 = 0; _e714 < this.metricsTensors.length; ++_e714) {\n        var _n288 = this.metricsTensors[_e714][1],\n            _r232 = mean$1((0, this.metricsTensors[_e714][0])(a[_n288], i[_n288]));\n\n        t.push(_r232);\n      }\n\n      return t;\n    });\n  }\n\n  fit(e, t) {\n    var _arguments5 = arguments,\n        _this139 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = _arguments5.length > 2 && _arguments5[2] !== undefined ? _arguments5[2] : {};\n      return fitTensors(_this139, e, t, n);\n    })();\n  }\n\n  fitDataset(e, t) {\n    var _this140 = this;\n\n    return _asyncToGenerator(function* () {\n      return fitDataset(_this140, e, t);\n    })();\n  }\n\n  trainOnBatch(e, t) {\n    var _this141 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = yield _this141.standardizeUserData(e, t),\n          r = n[0],\n          a = n[1],\n          s = _this141.makeTrainFunction()(r.concat(a)),\n          o = [];\n\n      for (var _e715 of s) {\n        var _t506 = yield _e715.data();\n\n        o.push(_t506[0]);\n      }\n\n      return dispose(s), singletonOrArray(o);\n    })();\n  }\n\n  getNamedWeights(e) {\n    var t = [],\n        n = null != e && e.trainableOnly,\n        r = n ? this.trainableWeights : this.weights,\n        a = this.getWeights(n);\n\n    for (var _e716 = 0; _e716 < r.length; ++_e716) {\n      n && !r[_e716].trainable || t.push({\n        name: r[_e716].originalName,\n        tensor: a[_e716]\n      });\n    }\n\n    return t;\n  }\n\n  set stopTraining(e) {\n    this.stopTraining_ = e;\n  }\n\n  get stopTraining() {\n    return this.stopTraining_;\n  }\n\n  get optimizer() {\n    return this.optimizer_;\n  }\n\n  set optimizer(e) {\n    this.optimizer_ !== e && (this.optimizer_ = e, this.isOptimizerOwned = !1);\n  }\n\n  dispose() {\n    var e = super.dispose();\n\n    if (0 === e.refCountAfterDispose && null != this.optimizer && this.isOptimizerOwned) {\n      var t = memory().numTensors;\n      this.optimizer_.dispose(), e.numDisposedVariables += t - memory().numTensors;\n    }\n\n    return e;\n  }\n\n  getLossIdentifiers() {\n    var e;\n    if (\"string\" == typeof this.loss) e = toSnakeCase(this.loss);else if (Array.isArray(this.loss)) {\n      for (var _e717 of this.loss) {\n        if (\"string\" != typeof _e717) throw new Error(\"Serialization of non-string loss is not supported.\");\n      }\n\n      e = this.loss.map(e => toSnakeCase(e));\n    } else {\n      var t = Object.keys(this.loss);\n      e = {};\n      var n = this.loss;\n\n      for (var r of t) {\n        if (\"string\" != typeof n[r]) throw new Error(\"Serialization of non-string loss is not supported.\");\n        e[r] = toSnakeCase(n[r]);\n      }\n    }\n    return e;\n  }\n\n  getMetricIdentifiers() {\n    if (\"string\" == typeof this.metrics || \"function\" == typeof this.metrics) return [toSnakeCase(getLossOrMetricName(this.metrics))];\n    if (Array.isArray(this.metrics)) return this.metrics.map(e => toSnakeCase(getLossOrMetricName(e)));\n    {\n      var _e718 = {};\n\n      for (var t in this.metrics) {\n        _e718[t] = toSnakeCase(getLossOrMetricName(this.metrics[t]));\n      }\n\n      return _e718;\n    }\n  }\n\n  getTrainingConfig() {\n    return {\n      loss: this.getLossIdentifiers(),\n      metrics: this.getMetricIdentifiers(),\n      optimizer_config: {\n        class_name: this.optimizer.getClassName(),\n        config: this.optimizer.getConfig()\n      }\n    };\n  }\n\n  loadTrainingConfig(e) {\n    if (null != e.weighted_metrics) throw new Error(\"Loading weight_metrics is not supported yet.\");\n    if (null != e.loss_weights) throw new Error(\"Loading loss_weights is not supported yet.\");\n    if (null != e.sample_weight_mode) throw new Error(\"Loading sample_weight_mode is not supported yet.\");\n    var t = deserialize(convertPythonicToTs(e.optimizer_config));\n    var n, r;\n    if (\"string\" == typeof e.loss) n = toCamelCase(e.loss);else if (Array.isArray(e.loss)) n = e.loss.map(e => toCamelCase(e));else if (null != e.loss) {\n      n = {};\n\n      for (var _t507 in e.loss) {\n        n[_t507] = toCamelCase(e.loss[_t507]);\n      }\n    }\n    if (Array.isArray(e.metrics)) r = e.metrics.map(e => toCamelCase(e));else if (null != e.metrics) {\n      r = {};\n\n      for (var _t508 in e.metrics) {\n        r[_t508] = toCamelCase(e.metrics[_t508]);\n      }\n    }\n    this.compile({\n      loss: n,\n      metrics: r,\n      optimizer: t\n    });\n  }\n\n  save(e, t) {\n    var _this142 = this;\n\n    return _asyncToGenerator(function* () {\n      if (\"string\" == typeof e) {\n        var _t509 = getSaveHandlers(e);\n\n        if (0 === _t509.length) throw new ValueError(\"Cannot find any save handlers for URL '\".concat(e, \"'\"));\n        if (_t509.length > 1) throw new ValueError(\"Found more than one (\".concat(_t509.length, \") save handlers for URL '\").concat(e, \"'\"));\n        e = _t509[0];\n      }\n\n      if (null == e.save) throw new ValueError(\"LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.\");\n      var n = yield encodeWeights(_this142.getNamedWeights(t)),\n          r = {\n        modelTopology: _this142.toJSON(null, !1),\n        format: LAYERS_MODEL_FORMAT_NAME,\n        generatedBy: \"TensorFlow.js tfjs-layers v\".concat(version$6),\n        convertedBy: null\n      };\n\n      if (null != t && t.includeOptimizer && null != _this142.optimizer) {\n        r.trainingConfig = _this142.getTrainingConfig();\n        var _e719 = \"optimizer\",\n            {\n          data: _t510,\n          specs: a\n        } = yield encodeWeights(yield _this142.optimizer.getWeights(), _e719);\n        n.specs.push(...a), n.data = concatenateArrayBuffers([n.data, _t510]);\n      }\n\n      return null != _this142.userDefinedMetadata && (checkUserDefinedMetadata(_this142.userDefinedMetadata, _this142.name, !0), r.userDefinedMetadata = _this142.userDefinedMetadata), r.weightData = n.data, r.weightSpecs = n.specs, e.save(r);\n    })();\n  }\n\n  setUserDefinedMetadata(e) {\n    checkUserDefinedMetadata(e, this.name), this.userDefinedMetadata = e;\n  }\n\n  getUserDefinedMetadata() {\n    return this.userDefinedMetadata;\n  }\n\n}\n\nLayersModel.className = \"Model\", registerClass(LayersModel);\n\nclass Functional extends LayersModel {}\n\nfunction modelFromJSON(_x115, _x116) {\n  return _modelFromJSON.apply(this, arguments);\n}\n\nfunction _modelFromJSON() {\n  _modelFromJSON = _asyncToGenerator(function* (e, t) {\n    \"modelTopology\" in e || (e = {\n      modelTopology: e\n    });\n    var n = (e = e).modelTopology;\n    null != n.model_config && (n = n.model_config);\n    var r = deserialize(convertPythonicToTs(n), t);\n\n    if (null != e.weightsManifest) {\n      var _t788 = yield loadWeights(e.weightsManifest, e.pathPrefix, r.weights.map(e => e.originalName)),\n          _n459 = {};\n\n      for (var _e1161 of r.weights) {\n        _n459[_e1161.originalName] = _t788[_e1161.originalName];\n      }\n\n      r.loadWeights(_n459), dispose(_t788);\n    }\n\n    return r;\n  });\n  return _modelFromJSON.apply(this, arguments);\n}\n\nfunction loadLayersModelInternal(_x117, _x118) {\n  return _loadLayersModelInternal.apply(this, arguments);\n}\n\nfunction _loadLayersModelInternal() {\n  _loadLayersModelInternal = _asyncToGenerator(function* (e, t) {\n    if (null == t && (t = {}), \"string\" == typeof e) {\n      var n = getLoadHandlers(e, t);\n      if (0 === n.length) n.push(browserHTTPRequest(e, t));else if (n.length > 1) throw new ValueError(\"Found more than one (\".concat(n.length, \") load handlers for URL '\").concat(e, \"'\"));\n      e = n[0];\n    }\n\n    return loadLayersModelFromIOHandler(e, void 0, t);\n  });\n  return _loadLayersModelInternal.apply(this, arguments);\n}\n\nfunction loadLayersModelFromIOHandler(_x119, _x120, _x121) {\n  return _loadLayersModelFromIOHandler.apply(this, arguments);\n}\n\nfunction _loadLayersModelFromIOHandler() {\n  _loadLayersModelFromIOHandler = _asyncToGenerator(function* (e, t, n) {\n    if (null == n && (n = {}), null == e.load) throw new ValueError(\"Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.\");\n    var r = yield e.load();\n    var a = r.modelTopology;\n    null != a.model_config && (a = a.model_config);\n    var s = null == n.strict || n.strict,\n        o = null != r.weightData && null != r.weightSpecs && s,\n        i = deserialize(convertPythonicToTs(a), t, o),\n        l = r.trainingConfig;\n\n    if (null != l && i.loadTrainingConfig(l), null != r.userDefinedMetadata && i.setUserDefinedMetadata(r.userDefinedMetadata), null != r.weightData) {\n      if (null == r.weightSpecs) throw new ValueError(\"LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.\");\n      var {\n        modelWeights: _e1162,\n        optimizerWeights: _t789\n      } = decodeModelAndOptimizerWeights(r.weightData, r.weightSpecs);\n      i.loadWeights(_e1162, s), null != i.optimizer && _t789.length > 0 && (yield i.optimizer.setWeights(_t789)), dispose(_e1162), dispose(_t789.map(e => e.tensor));\n    }\n\n    return i;\n  });\n  return _loadLayersModelFromIOHandler.apply(this, arguments);\n}\n\nfunction decodeModelAndOptimizerWeights(e, t) {\n  var n = decodeWeights(e, t),\n      r = {},\n      a = [];\n  return t.forEach(e => {\n    \"optimizer\" === e.group ? a.push({\n      name: e.name,\n      tensor: n[e.name]\n    }) : r[e.name] = n[e.name];\n  }), {\n    modelWeights: r,\n    optimizerWeights: a\n  };\n}\n\nFunctional.className = \"Functional\", registerClass(Functional);\n\nclass Sequential extends LayersModel {\n  constructor(e) {\n    if (super({\n      inputs: [],\n      outputs: []\n    }), e = e || {}, this.trainable = !0, this.built = !1, this.name = null != e.name ? e.name : getUid(\"sequential_\"), null != e.layers) for (var t of e.layers) {\n      this.add(t);\n    }\n  }\n\n  checkShape(e) {\n    if (e.inboundNodes[0].outputTensors[0].shape.some(e => e < 0)) throw new ValueError(\"Negative dimension size caused by adding layer \".concat(e.name, \" with input shape [\").concat(e.inboundNodes[0].inputTensors[0].shape, \"]\"));\n  }\n\n  add(e) {\n    var t = e instanceof Sequential || e instanceof LayersModel;\n    var n;\n\n    if (t) {\n      if (n = e, 1 !== n.outputs.length) throw new ValueError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n      if (1 !== n.inputs.length) throw new ValueError(\"All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.\");\n    }\n\n    if (0 === this.outputs.length) {\n      if (0 === e.inboundNodes.length) {\n        if (null == e.batchInputShape) throw new ValueError(\"The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.\");\n\n        var _t511 = Input({\n          batchShape: e.batchInputShape,\n          dtype: e.dtype,\n          name: e.name + \"_input\"\n        });\n\n        e.apply(_t511);\n      }\n\n      if (t) this.outputs = n.outputs, this.inputs = n.inputs;else {\n        if (1 !== e.inboundNodes.length) throw new ValueError(\"A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer \".concat(e.name, \" which has \").concat(e.inboundNodes.length, \" pre-existing inbound connections.\"));\n        if (1 !== e.inboundNodes[0].outputTensors.length) throw new ValueError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n        this.checkShape(e), this.outputs = [e.inboundNodes[0].outputTensors[0]], this.inputs = getSourceInputs(this.outputs[0]);\n      }\n      this.inboundNodes = [], new Node({\n        outboundLayer: this,\n        inboundLayers: [],\n        nodeIndices: [],\n        tensorIndices: [],\n        inputTensors: this.inputs,\n        outputTensors: this.outputs,\n        inputMasks: pyListRepeat(null, this.inputs.length),\n        outputMasks: [null],\n        inputShapes: this.inputs.map(e => e.shape),\n        outputShapes: this.outputs[0].shape\n      });\n    } else {\n      var _t512 = e.apply(this.outputs[0]);\n\n      if (Array.isArray(_t512)) throw new TypeError(\"All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.\");\n      this.checkShape(e), this.outputs = [_t512], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n\n    this.layers.push(e), this.built = !1;\n  }\n\n  pop() {\n    if (0 === this.layers.length) throw new TypeError(\"There are no layers in the model.\");\n    if (this.layers.pop(), 0 === this.layers.length) this.outputs = [], this.inboundNodes = [], this.outboundNodes = [];else {\n      var _e720 = this.layers.length - 1;\n\n      this.layers[_e720].outboundNodes = [], this.outputs = [this.layers[_e720].output], this.inboundNodes[0].outputTensors = this.outputs, this.inboundNodes[0].outputShapes = [this.outputs[0].shape];\n    }\n  }\n\n  call(e, t) {\n    return null == this.model && this.build(), this.model.call(e, t);\n  }\n\n  build(e) {\n    if (getExactlyOneShape(e), 0 === this.inputs.length || 0 === this.outputs.length) throw new TypeError(\"Sequential model cannot be built: model is empty. Add some layers first.\");\n    this.model = new LayersModel({\n      inputs: this.inputs,\n      outputs: this.outputs[0],\n      name: this.name + \"_model\"\n    }), this.model.trainable = this.trainable, this.supportsMasking = this.model.supportsMasking, this.inputLayers = this.model.inputLayers, this.inputLayersNodeIndices = this.model.inputLayersNodeIndices, this.inputLayersTensorIndices = this.model.inputLayersTensorIndices, this.outputLayers = this.model.outputLayers, this.outputLayersNodeIndices = this.model.outputLayersNodeIndices, this.outputLayersTensorIndices = this.model.outputLayersTensorIndices, this.nodesByDepth = this.model.nodesByDepth, this.containerNodes = this.model.containerNodes, this.outputNames = this.model.outputNames, this.inputNames = this.model.inputNames, this.built = !0;\n  }\n\n  countParams() {\n    return this.built || this.build(), super.countParams();\n  }\n\n  summary(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : console.log;\n    this.built || this.build(), super.summary(e, t, n);\n  }\n\n  setWeights(e) {\n    null == this.model && this.build(), this.model.setWeights(e);\n  }\n\n  evaluate(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    if (!this.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n    return this.model.evaluate(e, t, n);\n  }\n\n  evaluateDataset(e, t) {\n    var _this143 = this;\n\n    return _asyncToGenerator(function* () {\n      if (!_this143.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n      return _this143.model.evaluateDataset(e, t);\n    })();\n  }\n\n  predict(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    return null == this.model && this.build(), this.model.predict(e, t);\n  }\n\n  predictOnBatch(e) {\n    return null == this.model && this.build(), this.model.predictOnBatch(e);\n  }\n\n  compile(e) {\n    this.build(), this.model.compile(e), this.optimizer_ = this.model.optimizer, this.isOptimizerOwned = this.model.isOptimizerOwned, this.loss = this.model.loss, this.metrics = this.model.metrics, this.metricsTensors = this.model.metricsTensors, this.metricsNames = this.model.metricsNames;\n  }\n\n  get optimizer() {\n    return null == this.model ? void 0 : this.model.optimizer;\n  }\n\n  set optimizer(e) {\n    this.model.optimizer = e;\n  }\n\n  fit(e, t) {\n    var _arguments6 = arguments,\n        _this144 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = _arguments6.length > 2 && _arguments6[2] !== undefined ? _arguments6[2] : {};\n      if (!_this144.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n      return _this144.model.fit(e, t, n);\n    })();\n  }\n\n  fitDataset(e, t) {\n    var _this145 = this;\n\n    return _asyncToGenerator(function* () {\n      if (!_this145.built) throw new RuntimeError(\"The model needs to be compiled before being used.\");\n      return _this145.model.fitDataset(e, t);\n    })();\n  }\n\n  trainOnBatch(e, t) {\n    var _this146 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this146.model.trainOnBatch(e, t);\n    })();\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a,\n        s = {};\n\n    if (t instanceof Array) {\n      if (null == t[0].className || \"Merge\" === t[0].className) throw new ValueError(\"Legacy serialization format not supported yet.\");\n      a = t;\n    } else assert$4(null != t.layers, () => \"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.\"), a = t.layers, delete t.layers, s = t;\n\n    var o = new e(s);\n    if (!(o instanceof Sequential)) throw new NotImplementedError(\"Sequential.fromConfig called on non-Sequential input: \".concat(o));\n\n    for (var _e721 of a) {\n      var _t513 = deserialize(_e721, void 0, r);\n\n      r && _t513.setFastWeightInitDuringBuild(!0), o.add(_t513);\n    }\n\n    return o;\n  }\n\n  set stopTraining(e) {\n    if (null == this.model) throw new ValueError(\"Cannot set the stopTraining property of a sequential model before it is compiled.\");\n    this.model.stopTraining = e;\n  }\n\n  get stopTraining() {\n    if (null == this.model) throw new ValueError(\"Cannot get the stopTraining property of a sequential model before it is compiled.\");\n    return this.model.stopTraining;\n  }\n\n  getConfig() {\n    var e = [];\n\n    for (var t of this.layers) {\n      var n = {};\n      n.className = t.getClassName(), n.config = t.getConfig(), e.push(n);\n    }\n\n    return {\n      name: this.name,\n      layers: e\n    };\n  }\n\n}\n\nfunction model(e) {\n  return new LayersModel(e);\n}\n\nfunction sequential(e) {\n  return new Sequential(e);\n}\n\nfunction loadLayersModel(e, t) {\n  return null == t && (t = {}), loadLayersModelInternal(e, t);\n}\n\nfunction input(e) {\n  return Input(e);\n}\n\nfunction registerCallbackConstructor(e, t) {\n  CallbackConstructorRegistry.registerCallbackConstructor(e, t);\n}\n\nSequential.className = \"Sequential\", registerClass(Sequential);\n\nclass Activation$1 extends Serializable {\n  getConfig() {\n    return {};\n  }\n\n}\n\nclass Elu extends Activation$1 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    return elu$3(e, t);\n  }\n\n}\n\nElu.className = \"elu\", registerClass(Elu);\n\nclass Selu extends Activation$1 {\n  apply(e) {\n    return selu$2(e);\n  }\n\n}\n\nSelu.className = \"selu\", registerClass(Selu);\n\nclass Relu extends Activation$1 {\n  apply(e) {\n    return relu$3(e);\n  }\n\n}\n\nRelu.className = \"relu\", registerClass(Relu);\n\nclass Relu6 extends Activation$1 {\n  apply(e) {\n    return tidy(() => minimum$3(6, relu$3(e)));\n  }\n\n}\n\nRelu6.className = \"relu6\", registerClass(Relu6);\n\nclass Linear extends Activation$1 {\n  apply(e) {\n    return e;\n  }\n\n}\n\nLinear.className = \"linear\", registerClass(Linear);\n\nclass Sigmoid extends Activation$1 {\n  apply(e) {\n    return sigmoid$2(e);\n  }\n\n}\n\nSigmoid.className = \"sigmoid\", registerClass(Sigmoid);\n\nclass HardSigmoid extends Activation$1 {\n  apply(e) {\n    return hardSigmoid(e);\n  }\n\n}\n\nHardSigmoid.className = \"hardSigmoid\", registerClass(HardSigmoid);\n\nclass Softplus extends Activation$1 {\n  apply(e) {\n    return softplus$2(e);\n  }\n\n}\n\nSoftplus.className = \"softplus\", registerClass(Softplus);\n\nclass Softsign extends Activation$1 {\n  apply(e) {\n    return softsign(e);\n  }\n\n}\n\nSoftsign.className = \"softsign\", registerClass(Softsign);\n\nclass Tanh extends Activation$1 {\n  apply(e) {\n    return tanh$2(e);\n  }\n\n}\n\nTanh.className = \"tanh\", registerClass(Tanh);\n\nclass Softmax$1 extends Activation$1 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n    return softmax$3(e, t);\n  }\n\n}\n\nSoftmax$1.className = \"softmax\", registerClass(Softmax$1);\n\nclass LogSoftmax extends Activation$1 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n    return logSoftmax(e, t);\n  }\n\n}\n\nLogSoftmax.className = \"logSoftmax\", registerClass(LogSoftmax);\n\nclass Swish extends Activation$1 {\n  apply(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    return tidy(() => mul(sigmoid$2(mul(e, t)), e));\n  }\n\n}\n\nSwish.className = \"swish\", registerClass(Swish);\n\nclass Mish extends Activation$1 {\n  apply(e) {\n    return tidy(() => mul(e, tanh$2(softplus$2(e))));\n  }\n\n}\n\nfunction serializeActivation(e) {\n  return e.getClassName();\n}\n\nfunction deserializeActivation(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject(e, SerializationMap.getMap().classNameMap, t, \"activation\");\n}\n\nfunction getActivation(e) {\n  if (null == e) return deserializeActivation({\n    className: \"linear\",\n    config: {}\n  });\n\n  if (\"string\" == typeof e) {\n    var t = {};\n    return t.className = e, t.config = {}, deserializeActivation(t);\n  }\n\n  return e instanceof Activation$1 ? e : deserializeActivation(e);\n}\n\nfunction assertObjectArgs(e) {\n  if (null != e && \"object\" != typeof e) throw new Error(\"Argument to L1L2 regularizer's constructor is expected to be an object, but received: \".concat(e));\n}\n\nMish.className = \"mish\", registerClass(Mish);\n\nclass Regularizer extends Serializable {}\n\nclass L1L2 extends Regularizer {\n  constructor(e) {\n    super(), assertObjectArgs(e), this.l1 = null == e || null == e.l1 ? .01 : e.l1, this.l2 = null == e || null == e.l2 ? .01 : e.l2, this.hasL1 = 0 !== this.l1, this.hasL2 = 0 !== this.l2;\n  }\n\n  apply(e) {\n    return tidy(() => {\n      var t = zeros$2([1]);\n      return this.hasL1 && (t = add$2(t, sum$2(mul(this.l1, abs$2(e))))), this.hasL2 && (t = add$2(t, sum$2(mul(this.l2, square$1(e))))), reshape$3(t, []);\n    });\n  }\n\n  getConfig() {\n    return {\n      l1: this.l1,\n      l2: this.l2\n    };\n  }\n\n  static fromConfig(e, t) {\n    return new e({\n      l1: t.l1,\n      l2: t.l2\n    });\n  }\n\n}\n\nfunction l1$1(e) {\n  return assertObjectArgs(e), new L1L2({\n    l1: null != e ? e.l1 : null,\n    l2: 0\n  });\n}\n\nfunction l2$1(e) {\n  return assertObjectArgs(e), new L1L2({\n    l2: null != e ? e.l2 : null,\n    l1: 0\n  });\n}\n\nL1L2.className = \"L1L2\", registerClass(L1L2);\nvar REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP = {\n  l1l2: \"L1L2\"\n};\n\nfunction serializeRegularizer(e) {\n  return serializeKerasObject(e);\n}\n\nfunction deserializeRegularizer(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject(e, SerializationMap.getMap().classNameMap, t, \"regularizer\");\n}\n\nfunction getRegularizer(e) {\n  return null == e ? null : \"string\" == typeof e ? deserializeRegularizer({\n    className: e in REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP ? REGULARIZER_IDENTIFIER_REGISTRY_SYMBOL_MAP[e] : e,\n    config: {}\n  }) : e instanceof Regularizer ? e : deserializeRegularizer(e);\n}\n\nclass ReLU extends Layer {\n  constructor(e) {\n    super(null == e ? {} : e), this.supportsMasking = !0, null != e && (this.maxValue = e.maxValue);\n  }\n\n  call(e, t) {\n    e = getExactlyOneTensor(e);\n    var n = relu$3(e);\n    return null != this.maxValue && (n = clipByValue$1(n, 0, this.maxValue)), n;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      maxValue: this.maxValue\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nReLU.className = \"ReLU\", registerClass(ReLU);\n\nclass LeakyReLU extends Layer {\n  constructor(e) {\n    super(null == e ? {} : e), this.DEFAULT_ALPHA = .3, null == e && (e = {}), this.alpha = null == e.alpha ? this.DEFAULT_ALPHA : e.alpha;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor(e);\n    return leakyRelu$2(n, this.alpha);\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      alpha: this.alpha\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nLeakyReLU.className = \"LeakyReLU\", registerClass(LeakyReLU);\n\nclass PReLU extends Layer {\n  constructor(e) {\n    if (super(null == e ? {} : e), this.DEFAULT_ALPHA_INITIALIZER = \"zeros\", null == e && (e = {}), this.supportsMasking = !0, this.alphaInitializer = getInitializer(e.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER), this.alphaRegularizer = getRegularizer(e.alphaRegularizer), this.alphaConstraint = getConstraint(e.alphaConstraint), null == e.sharedAxes) this.sharedAxes = null;else if (Array.isArray(e.sharedAxes)) this.sharedAxes = e.sharedAxes;else {\n      if (\"number\" != typeof e.sharedAxes) throw new ValueError(\"Expected sharedAxes to be a number or an array of numbers, but got \".concat(e.sharedAxes));\n      this.sharedAxes = [e.sharedAxes];\n    }\n  }\n\n  build(e) {\n    var t = (e = getExactlyOneShape(e)).slice(1);\n    if (null != this.sharedAxes) for (var _e722 of this.sharedAxes) {\n      t[_e722 - 1] = 1;\n    }\n    this.alpha = this.addWeight(\"alpha\", t, \"float32\", this.alphaInitializer, this.alphaRegularizer, !0, this.alphaConstraint);\n    var n = {};\n    if (null != this.sharedAxes) for (var _t514 = 1; _t514 < e.length; ++_t514) {\n      n[_t514] = e[_t514];\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: e.length,\n      axes: n\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return e = getExactlyOneTensor(e), prelu$3(e, this.alpha.read());\n  }\n\n  getConfig() {\n    var e = {\n      alphaInitializer: serializeInitializer(this.alphaInitializer),\n      alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n      alphaConstraint: serializeConstraint(this.alphaConstraint),\n      sharedAxes: this.sharedAxes\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nPReLU.className = \"PReLU\", registerClass(PReLU);\n\nclass ELU$3 extends Layer {\n  constructor(e) {\n    if (super(null == e ? {} : e), this.DEFAULT_ALPHA = 1, null == e && (e = {}), null != e.alpha && e.alpha !== this.DEFAULT_ALPHA) throw new NotImplementedError(\"Non-default alpha value (\".concat(e.alpha, \") is not supported by the ELU layer yet.\"));\n    this.alpha = null == e.alpha ? this.DEFAULT_ALPHA : e.alpha;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor(e);\n    return elu$4(n);\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      alpha: this.alpha\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nELU$3.className = \"ELU\", registerClass(ELU$3);\n\nclass ThresholdedReLU extends Layer {\n  constructor(e) {\n    super(null == e ? {} : e), this.DEFAULT_THETA = 1, null == e && (e = {}), this.theta = null == e.theta ? this.DEFAULT_THETA : e.theta;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor(e);\n    return mul(n, cast$3(greater$3(n, this.theta), \"float32\"));\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      theta: this.theta\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nThresholdedReLU.className = \"ThresholdedReLU\", registerClass(ThresholdedReLU);\n\nclass Softmax extends Layer {\n  constructor(e) {\n    super(null == e ? {} : e), this.DEFAULT_AXIS = 1, null == e && (e = {}), this.softmax = new Softmax$1().apply, this.axis = null == e.axis ? this.DEFAULT_AXIS : e.axis;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor(e);\n    return this.softmax(n, this.axis);\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction normalizeArray(e, t, n) {\n  if (\"number\" == typeof e) return pyListRepeat(e, t);\n  if (e.length !== t) throw new ValueError(\"The \".concat(n, \" argument must be an integer or tuple of \").concat(t, \" integers. Received: \").concat(e.length, \" elements.\"));\n\n  for (var r = 0; r < t; ++r) {\n    var a = e[r];\n    if (!isInteger(a)) throw new ValueError(\"The \".concat(n, \" argument must be an integer or tuple of \").concat(t, \" integers. Received: \").concat(JSON.stringify(e), \" including a non-integer number \").concat(a));\n  }\n\n  return e;\n}\n\nfunction convOutputLength(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1;\n  if (null == e) return e;\n  var s;\n  return s = \"same\" === n ? e : e - (t + (t - 1) * (a - 1)) + 1, Math.floor((s + r - 1) / r);\n}\n\nfunction deconvLength(e, t, n, r) {\n  if (null == e) return null;\n  if (\"valid\" === r) e = e * t + max$2([n - t, 0]);else {\n    if (\"same\" !== r) throw new ValueError(\"Unsupport padding mode: \".concat(r, \".\"));\n    e *= t;\n  }\n  return e;\n}\n\nfunction preprocessConv2DInput(e, t) {\n  return tidy(() => (checkDataFormat(t), \"channelsFirst\" === t ? transpose$2(e, [0, 2, 3, 1]) : e));\n}\n\nfunction preprocessConv3DInput(e, t) {\n  return tidy(() => (checkDataFormat(t), \"channelsFirst\" === t ? transpose$2(e, [0, 2, 3, 4, 1]) : e));\n}\n\nfunction conv1dWithBias(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 1;\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"valid\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : 1;\n  return tidy(() => {\n    if (null == s && (s = imageDataFormat()), checkDataFormat(s), 3 !== e.shape.length) throw new ValueError(\"The input of a conv1dWithBias operation should be 3, but is \".concat(e.shape.length, \" instead.\"));\n    if (3 !== t.shape.length) throw new ValueError(\"The kernel for a conv1dWithBias operation should be 3, but is \".concat(t.shape.length, \" instead\"));\n    if (null != n && 1 !== n.shape.length) throw new ValueError(\"The bias for a conv1dWithBias operation should be 1, but is \".concat(t.shape.length, \" instead\"));\n    if (\"channelsFirst\" === s && (e = transpose$2(e, [0, 2, 1])), \"causal\" === a) throw new NotImplementedError(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");\n    var i = conv1d$1(e, t, r, \"same\" === a ? \"same\" : \"valid\", \"NWC\", o);\n    return null != n && (i = biasAdd(i, n)), i;\n  });\n}\n\nfunction conv2dWithBiasActivation(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : [1, 1];\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"valid\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : null;\n  return tidy(() => {\n    if (null == s && (s = imageDataFormat()), checkDataFormat(s), 3 !== e.rank && 4 !== e.rank) throw new ValueError(\"conv2dWithBiasActivation expects input to be of rank 3 or 4, but received \".concat(e.rank, \".\"));\n    if (3 !== t.rank && 4 !== t.rank) throw new ValueError(\"conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received \".concat(e.rank, \".\"));\n    var l = preprocessConv2DInput(e, s);\n    if (\"causal\" === a) throw new NotImplementedError(\"The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.\");\n    return l = conv2d$2({\n      x: l,\n      filter: t,\n      strides: r,\n      pad: \"same\" === a ? \"same\" : \"valid\",\n      dilations: o,\n      dataFormat: \"NHWC\",\n      bias: n,\n      activation: i\n    }), \"channelsFirst\" === s && (l = transpose$2(l, [0, 3, 1, 2])), l;\n  });\n}\n\nfunction conv3dWithBias(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : [1, 1, 1];\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : \"valid\";\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 ? arguments[6] : undefined;\n  return tidy(() => {\n    if (null == s && (s = imageDataFormat()), checkDataFormat(s), 4 !== e.rank && 5 !== e.rank) throw new ValueError(\"conv3dWithBias expects input to be of rank 4 or 5, but received \".concat(e.rank, \".\"));\n    if (4 !== t.rank && 5 !== t.rank) throw new ValueError(\"conv3dWithBias expects kernel to be of rank 4 or 5, but received \".concat(e.rank, \".\"));\n    var i = preprocessConv3DInput(e, s);\n    if (\"causal\" === a) throw new NotImplementedError(\"The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.\");\n    return i = conv3d$1(i, t, r, \"same\" === a ? \"same\" : \"valid\", \"NDHWC\", o), null != n && (i = biasAdd(i, n)), \"channelsFirst\" === s && (i = transpose$2(i, [0, 4, 1, 2, 3])), i;\n  });\n}\n\nSoftmax.className = \"Softmax\", registerClass(Softmax);\n\nclass BaseConv extends Layer {\n  constructor(e, t) {\n    if (super(t), this.bias = null, this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", BaseConv.verifyArgs(t), this.rank = e, assertPositiveInteger(this.rank, \"rank\"), 1 !== this.rank && 2 !== this.rank && 3 !== this.rank) throw new NotImplementedError(\"Convolution layer for rank other than 1, 2, or 3 (\".concat(this.rank, \") is not implemented yet.\"));\n    if (this.kernelSize = normalizeArray(t.kernelSize, e, \"kernelSize\"), this.strides = normalizeArray(null == t.strides ? 1 : t.strides, e, \"strides\"), this.padding = null == t.padding ? \"valid\" : t.padding, checkPaddingMode(this.padding), this.dataFormat = null == t.dataFormat ? \"channelsLast\" : t.dataFormat, checkDataFormat(this.dataFormat), this.activation = getActivation(t.activation), this.useBias = null == t.useBias || t.useBias, this.biasInitializer = getInitializer(t.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.biasConstraint = getConstraint(t.biasConstraint), this.biasRegularizer = getRegularizer(t.biasRegularizer), this.activityRegularizer = getRegularizer(t.activityRegularizer), this.dilationRate = normalizeArray(null == t.dilationRate ? 1 : t.dilationRate, e, \"dilationRate\"), 1 === this.rank && Array.isArray(this.dilationRate) && 1 !== this.dilationRate.length) throw new ValueError(\"dilationRate must be a number or an array of a single number for 1D convolution, but received \".concat(JSON.stringify(this.dilationRate)));\n\n    if (2 === this.rank) {\n      if (\"number\" == typeof this.dilationRate) this.dilationRate = [this.dilationRate, this.dilationRate];else if (2 !== this.dilationRate.length) throw new ValueError(\"dilationRate must be a number or array of two numbers for 2D convolution, but received \".concat(JSON.stringify(this.dilationRate)));\n    } else if (3 === this.rank) if (\"number\" == typeof this.dilationRate) this.dilationRate = [this.dilationRate, this.dilationRate, this.dilationRate];else if (3 !== this.dilationRate.length) throw new ValueError(\"dilationRate must be a number or array of three numbers for 3D convolution, but received \".concat(JSON.stringify(this.dilationRate)));\n  }\n\n  static verifyArgs(e) {\n    if (assert$3(\"kernelSize\" in e, \"required key 'kernelSize' not in config\"), \"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 3)) throw new ValueError(\"BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n  getConfig() {\n    var e = {\n      kernelSize: this.kernelSize,\n      strides: this.strides,\n      padding: this.padding,\n      dataFormat: this.dataFormat,\n      dilationRate: this.dilationRate,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      biasConstraint: serializeConstraint(this.biasConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass Conv extends BaseConv {\n  constructor(e, t) {\n    super(e, t), this.kernel = null, Conv.verifyArgs(t), this.filters = t.filters, assertPositiveInteger(this.filters, \"filters\"), this.kernelInitializer = getInitializer(t.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.kernelConstraint = getConstraint(t.kernelConstraint), this.kernelRegularizer = getRegularizer(t.kernelRegularizer);\n  }\n\n  build(e) {\n    e = getExactlyOneShape(e);\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError(\"The channel dimension of the input should be defined. Found \".concat(e[t]));\n    var n = e[t],\n        r = this.kernelSize.concat([n, this.filters]);\n    this.kernel = this.addWeight(\"kernel\", r, null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [{\n      ndim: this.rank + 2,\n      axes: {\n        [t]: n\n      }\n    }], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t;\n      e = getExactlyOneTensor(e);\n      var n = null == this.bias ? null : this.bias.read(),\n          r = mapActivationToFusedKernel(this.activation.getClassName());\n      if (null != r && 2 === this.rank) t = conv2dWithBiasActivation(e, this.kernel.read(), n, this.strides, this.padding, this.dataFormat, this.dilationRate, r);else {\n        if (1 === this.rank) t = conv1dWithBias(e, this.kernel.read(), n, this.strides[0], this.padding, this.dataFormat, this.dilationRate[0]);else if (2 === this.rank) t = conv2dWithBiasActivation(e, this.kernel.read(), n, this.strides, this.padding, this.dataFormat, this.dilationRate);else {\n          if (3 !== this.rank) throw new NotImplementedError(\"convolutions greater than 3D are not implemented yet.\");\n          t = conv3dWithBias(e, this.kernel.read(), n, this.strides, this.padding, this.dataFormat, this.dilationRate);\n        }\n        null != this.activation && (t = this.activation.apply(t));\n      }\n      return t;\n    });\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape(e);\n    var t = [],\n        n = \"channelsLast\" === this.dataFormat ? e.slice(1, e.length - 1) : e.slice(2);\n\n    for (var _e723 = 0; _e723 < n.length; ++_e723) {\n      var _r233 = convOutputLength(n[_e723], this.kernelSize[_e723], this.padding, this.strides[_e723], \"number\" == typeof this.dilationRate ? this.dilationRate : this.dilationRate[_e723]);\n\n      t.push(_r233);\n    }\n\n    var r = [e[0]];\n    return \"channelsLast\" === this.dataFormat ? (r = r.concat(t), r.push(this.filters)) : (r.push(this.filters), r = r.concat(t)), r;\n  }\n\n  getConfig() {\n    var e = {\n      filters: this.filters,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  static verifyArgs(e) {\n    if (!(\"filters\" in e) || \"number\" != typeof e.filters || e.filters < 1) throw new ValueError(\"Convolution layer expected config.filters to be a 'number' > 0 but got \".concat(JSON.stringify(e.filters)));\n  }\n\n}\n\nclass Conv2D extends Conv {\n  constructor(e) {\n    super(2, e), Conv2D.verifyArgs(e);\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, e;\n  }\n\n  static verifyArgs(e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 2)) throw new ValueError(\"Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n}\n\nConv2D.className = \"Conv2D\", registerClass(Conv2D);\n\nclass Conv3D extends Conv {\n  constructor(e) {\n    super(3, e), Conv3D.verifyArgs(e);\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, e;\n  }\n\n  static verifyArgs(e) {\n    if (\"number\" != typeof e.kernelSize && (!Array.isArray(e.kernelSize) || 1 !== e.kernelSize.length && 3 !== e.kernelSize.length)) throw new ValueError(\"Conv3D expects config.kernelSize to be number or [number, number, number], but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n}\n\nConv3D.className = \"Conv3D\", registerClass(Conv3D);\n\nclass Conv2DTranspose extends Conv2D {\n  constructor(e) {\n    if (super(e), this.inputSpec = [new InputSpec({\n      ndim: 4\n    })], \"same\" !== this.padding && \"valid\" !== this.padding) throw new ValueError(\"Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode \".concat(this.padding));\n  }\n\n  build(e) {\n    if (4 !== (e = getExactlyOneShape(e)).length) throw new ValueError(\"Input should have rank 4; Received input shape: \" + JSON.stringify(e));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError(\"The channel dimension of the inputs should be defined. Found `None`.\");\n    var n = e[t],\n        r = this.kernelSize.concat([this.filters, n]);\n    this.kernel = this.addWeight(\"kernel\", r, \"float32\", this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [new InputSpec({\n      ndim: 4,\n      axes: {\n        [t]: n\n      }\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t = getExactlyOneTensor(e);\n      if (4 !== t.shape.length) throw new ValueError(\"Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-\".concat(t.shape.length));\n      var n = t.shape;\n      var r, a;\n      \"channelsFirst\" === this.dataFormat ? (r = 2, a = 3) : (r = 1, a = 2);\n      var s = n[a],\n          o = this.kernelSize[1],\n          i = this.strides[1],\n          l = [n[0], deconvLength(n[r], this.strides[0], this.kernelSize[0], this.padding), deconvLength(s, i, o, this.padding), this.filters];\n      \"channelsLast\" !== this.dataFormat && (t = transpose$2(t, [0, 2, 3, 1]));\n      var u = conv2dTranspose$1(t, this.kernel.read(), l, this.strides, this.padding);\n      return \"channelsLast\" !== this.dataFormat && (u = transpose$2(u, [0, 3, 1, 2])), null != this.bias && (u = biasAdd(u, this.bias.read(), this.dataFormat)), null != this.activation && (u = this.activation.apply(u)), u;\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape(e)).slice();\n    var n, r, a;\n    \"channelsFirst\" === this.dataFormat ? (n = 1, r = 2, a = 3) : (n = 3, r = 1, a = 2);\n    var s = this.kernelSize[0],\n        o = this.kernelSize[1],\n        i = this.strides[0],\n        l = this.strides[1];\n    return t[n] = this.filters, t[r] = deconvLength(t[r], i, s, this.padding), t[a] = deconvLength(t[a], l, o, this.padding), t;\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.dilationRate, e;\n  }\n\n}\n\nConv2DTranspose.className = \"Conv2DTranspose\", registerClass(Conv2DTranspose);\n\nclass Conv3DTranspose extends Conv3D {\n  constructor(e) {\n    if (super(e), this.inputSpec = [new InputSpec({\n      ndim: 5\n    })], \"same\" !== this.padding && \"valid\" !== this.padding) throw new ValueError(\"Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode \".concat(this.padding));\n  }\n\n  build(e) {\n    if (5 !== (e = getExactlyOneShape(e)).length) throw new ValueError(\"Input should have rank 5; Received input shape: \" + JSON.stringify(e));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t]) throw new ValueError(\"The channel dimension of the inputs should be defined. Found `None`.\");\n    var n = e[t],\n        r = this.kernelSize.concat([this.filters, n]);\n    this.kernel = this.addWeight(\"kernel\", r, \"float32\", this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint)), this.inputSpec = [new InputSpec({\n      ndim: 5,\n      axes: {\n        [t]: n\n      }\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t = getExactlyOneTensor(e);\n      if (5 !== t.shape.length) throw new ValueError(\"Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-\".concat(t.shape.length));\n      var n = t.shape;\n      var r, a, s;\n      \"channelsFirst\" === this.dataFormat ? (s = 2, r = 3, a = 4) : (s = 1, r = 2, a = 3);\n      var o = n[r],\n          i = n[a],\n          l = this.kernelSize[1],\n          u = this.kernelSize[2],\n          c = this.strides[1],\n          p = this.strides[2],\n          d = [n[0], deconvLength(n[s], this.strides[0], this.kernelSize[0], this.padding), deconvLength(o, c, l, this.padding), deconvLength(i, p, u, this.padding), this.filters];\n      \"channelsLast\" !== this.dataFormat && (t = transpose$2(t, [0, 2, 3, 4, 1]));\n      var h = conv3dTranspose$1(t, this.kernel.read(), d, this.strides, this.padding);\n      return \"channelsLast\" !== this.dataFormat && (h = transpose$2(h, [0, 4, 1, 2, 3])), null !== this.bias && (h = biasAdd(h, this.bias.read(), this.dataFormat)), null !== this.activation && (h = this.activation.apply(h)), h;\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape(e)).slice();\n    var n, r, a, s;\n    \"channelsFirst\" === this.dataFormat ? (n = 1, r = 2, a = 3, s = 4) : (n = 4, r = 1, a = 2, s = 3);\n    var o = this.kernelSize[0],\n        i = this.kernelSize[1],\n        l = this.kernelSize[2],\n        u = this.strides[0],\n        c = this.strides[1],\n        p = this.strides[2];\n    return t[n] = this.filters, t[r] = deconvLength(t[r], u, o, this.padding), t[a] = deconvLength(t[a], c, i, this.padding), t[s] = deconvLength(t[s], p, l, this.padding), t;\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.dilationRate, e;\n  }\n\n}\n\nConv3DTranspose.className = \"Conv3DTranspose\", registerClass(Conv3DTranspose);\n\nclass SeparableConv extends Conv {\n  constructor(e, t) {\n    if (super(e, t), this.DEFAULT_DEPTHWISE_INITIALIZER = \"glorotUniform\", this.DEFAULT_POINTWISE_INITIALIZER = \"glorotUniform\", this.depthwiseKernel = null, this.pointwiseKernel = null, null == t.filters) throw new ValueError(\"The `filters` configuration field is required by SeparableConv, but is unspecified.\");\n    if (null != t.kernelInitializer || null != t.kernelRegularizer || null != t.kernelConstraint) throw new ValueError(\"Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.\");\n    if (null != t.padding && \"same\" !== t.padding && \"valid\" !== t.padding) throw new ValueError(\"SeparableConv\".concat(this.rank, \"D supports only padding modes: 'same' and 'valid', but received \").concat(JSON.stringify(t.padding)));\n    this.depthMultiplier = null == t.depthMultiplier ? 1 : t.depthMultiplier, this.depthwiseInitializer = getInitializer(t.depthwiseInitializer || this.DEFAULT_DEPTHWISE_INITIALIZER), this.depthwiseRegularizer = getRegularizer(t.depthwiseRegularizer), this.depthwiseConstraint = getConstraint(t.depthwiseConstraint), this.pointwiseInitializer = getInitializer(t.depthwiseInitializer || this.DEFAULT_POINTWISE_INITIALIZER), this.pointwiseRegularizer = getRegularizer(t.pointwiseRegularizer), this.pointwiseConstraint = getConstraint(t.pointwiseConstraint);\n  }\n\n  build(e) {\n    if ((e = getExactlyOneShape(e)).length < this.rank + 2) throw new ValueError(\"Inputs to SeparableConv\".concat(this.rank, \"D should have rank \").concat(this.rank + 2, \", but received input shape: \").concat(JSON.stringify(e)));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[t] || e[t] < 0) throw new ValueError(\"The channel dimension of the inputs should be defined, but found \".concat(JSON.stringify(e[t])));\n    var n = e[t],\n        r = this.kernelSize.concat([n, this.depthMultiplier]),\n        a = [];\n\n    for (var _e724 = 0; _e724 < this.rank; ++_e724) {\n      a.push(1);\n    }\n\n    a.push(n * this.depthMultiplier, this.filters);\n    var s = !0;\n    this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", r, \"float32\", this.depthwiseInitializer, this.depthwiseRegularizer, s, this.depthwiseConstraint), this.pointwiseKernel = this.addWeight(\"pointwise_kernel\", a, \"float32\", this.pointwiseInitializer, this.pointwiseRegularizer, s, this.pointwiseConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [this.filters], \"float32\", this.biasInitializer, this.biasRegularizer, s, this.biasConstraint) : null, this.inputSpec = [new InputSpec({\n      ndim: this.rank + 2,\n      axes: {\n        [t]: n\n      }\n    })], this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t;\n      if (e = getExactlyOneTensor(e), 1 === this.rank) throw new NotImplementedError(\"1D separable convolution is not implemented yet.\");\n      return 2 === this.rank && (\"channelsFirst\" === this.dataFormat && (e = transpose$2(e, [0, 2, 3, 1])), t = separableConv2d$1(e, this.depthwiseKernel.read(), this.pointwiseKernel.read(), this.strides, this.padding, this.dilationRate, \"NHWC\")), this.useBias && (t = biasAdd(t, this.bias.read(), this.dataFormat)), null != this.activation && (t = this.activation.apply(t)), \"channelsFirst\" === this.dataFormat && (t = transpose$2(t, [0, 3, 1, 2])), t;\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, delete e.kernelInitializer, delete e.kernelRegularizer, delete e.kernelConstraint, e.depthwiseInitializer = serializeInitializer(this.depthwiseInitializer), e.pointwiseInitializer = serializeInitializer(this.pointwiseInitializer), e.depthwiseRegularizer = serializeRegularizer(this.depthwiseRegularizer), e.pointwiseRegularizer = serializeRegularizer(this.pointwiseRegularizer), e.depthwiseConstraint = serializeConstraint(this.depthwiseConstraint), e.pointwiseConstraint = serializeConstraint(this.pointwiseConstraint), e;\n  }\n\n}\n\nSeparableConv.className = \"SeparableConv\";\n\nclass SeparableConv2D extends SeparableConv {\n  constructor(e) {\n    super(2, e);\n  }\n\n}\n\nSeparableConv2D.className = \"SeparableConv2D\", registerClass(SeparableConv2D);\n\nclass Conv1D extends Conv {\n  constructor(e) {\n    super(1, e), Conv1D.verifyArgs(e), this.inputSpec = [{\n      ndim: 3\n    }];\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return delete e.rank, delete e.dataFormat, e;\n  }\n\n  static verifyArgs(e) {\n    if (\"number\" != typeof e.kernelSize && !checkArrayTypeAndLength(e.kernelSize, \"number\", 1, 1)) throw new ValueError(\"Conv1D expects config.kernelSize to be number or number[] with length 1, but received \".concat(JSON.stringify(e.kernelSize), \".\"));\n  }\n\n}\n\nConv1D.className = \"Conv1D\", registerClass(Conv1D);\n\nclass Cropping2D extends Layer {\n  constructor(e) {\n    super(e), this.cropping = \"number\" == typeof e.cropping ? [[e.cropping, e.cropping], [e.cropping, e.cropping]] : \"number\" == typeof e.cropping[0] ? [[e.cropping[0], e.cropping[0]], [e.cropping[1], e.cropping[1]]] : e.cropping, this.dataFormat = void 0 === e.dataFormat ? \"channelsLast\" : e.dataFormat, this.inputSpec = [{\n      ndim: 4\n    }];\n  }\n\n  computeOutputShape(e) {\n    return \"channelsFirst\" === this.dataFormat ? [e[0], e[1], e[2] - this.cropping[0][0] - this.cropping[0][1], e[3] - this.cropping[1][0] - this.cropping[1][1]] : [e[0], e[1] - this.cropping[0][0] - this.cropping[0][1], e[2] - this.cropping[1][0] - this.cropping[1][1], e[3]];\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      if (e = getExactlyOneTensor(e), \"channelsLast\" === this.dataFormat) {\n        var _t515 = sliceAlongAxis(e, this.cropping[0][0], e.shape[1] - this.cropping[0][0] - this.cropping[0][1], 2);\n\n        return sliceAlongAxis(_t515, this.cropping[1][0], e.shape[2] - this.cropping[1][1] - this.cropping[1][0], 3);\n      }\n\n      {\n        var _t516 = sliceAlongAxis(e, this.cropping[0][0], e.shape[2] - this.cropping[0][0] - this.cropping[0][1], 3);\n\n        return sliceAlongAxis(_t516, this.cropping[1][0], e.shape[3] - this.cropping[1][1] - this.cropping[1][0], 4);\n      }\n    });\n  }\n\n  getConfig() {\n    var e = {\n      cropping: this.cropping,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nCropping2D.className = \"Cropping2D\", registerClass(Cropping2D);\n\nclass UpSampling2D extends Layer {\n  constructor(e) {\n    super(e), this.DEFAULT_SIZE = [2, 2], this.inputSpec = [{\n      ndim: 4\n    }], this.size = null == e.size ? this.DEFAULT_SIZE : e.size, this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat(this.dataFormat), this.interpolation = null == e.interpolation ? \"nearest\" : e.interpolation, checkInterpolationFormat(this.interpolation);\n  }\n\n  computeOutputShape(e) {\n    return \"channelsFirst\" === this.dataFormat ? [e[0], e[1], null == e[2] ? null : this.size[0] * e[2], null == e[3] ? null : this.size[1] * e[3]] : [e[0], null == e[1] ? null : this.size[0] * e[1], null == e[2] ? null : this.size[1] * e[2], e[3]];\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t = getExactlyOneTensor(e);\n      var n = t.shape;\n\n      if (\"channelsFirst\" === this.dataFormat) {\n        t = transpose$2(t, [0, 2, 3, 1]);\n\n        var _e725 = this.size[0] * n[2],\n            r = this.size[1] * n[3],\n            a = \"nearest\" === this.interpolation ? image$1.resizeNearestNeighbor(t, [_e725, r]) : image$1.resizeBilinear(t, [_e725, r]);\n\n        return transpose$2(a, [0, 3, 1, 2]);\n      }\n\n      {\n        var _e726 = this.size[0] * n[1],\n            _r234 = this.size[1] * n[2];\n\n        return \"nearest\" === this.interpolation ? image$1.resizeNearestNeighbor(t, [_e726, _r234]) : image$1.resizeBilinear(t, [_e726, _r234]);\n      }\n    });\n  }\n\n  getConfig() {\n    var e = {\n      size: this.size,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction depthwiseConv2d$1(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : [1, 1];\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : \"valid\";\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  return tidy(() => {\n    null == a && (a = imageDataFormat()), checkDataFormat(a);\n    var o = preprocessConv2DInput(e, a);\n    if (4 !== e.rank) throw new ValueError(\"Input for depthwiseConv2d is required to be 4-D, but is instead \".concat(e.rank, \"-D\"));\n    if (4 !== t.rank) throw new ValueError(\"depthwiseKernel is required to be 4-D, but is instead \".concat(t.rank, \"-D\"));\n    return o = depthwiseConv2d$3(o, t, n, \"same\" === r ? \"same\" : \"valid\", \"NHWC\", s), \"channelsFirst\" === a && (o = transpose$2(o, [0, 3, 1, 2])), o;\n  });\n}\n\nUpSampling2D.className = \"UpSampling2D\", registerClass(UpSampling2D);\n\nclass DepthwiseConv2D extends BaseConv {\n  constructor(e) {\n    super(2, e), this.depthwiseKernel = null, this.depthMultiplier = null == e.depthMultiplier ? 1 : e.depthMultiplier, this.depthwiseInitializer = getInitializer(e.depthwiseInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.depthwiseConstraint = getConstraint(e.depthwiseConstraint), this.depthwiseRegularizer = getRegularizer(e.depthwiseRegularizer);\n  }\n\n  build(e) {\n    if ((e = getExactlyOneShape(e)).length < 4) throw new ValueError(\"Inputs to DepthwiseConv2D should have rank 4. Received input shape: \".concat(JSON.stringify(e), \".\"));\n    var t = \"channelsFirst\" === this.dataFormat ? 1 : 3;\n    if (null == e[t] || e[t] < 0) throw new ValueError(\"The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (\".concat(e[t], \").\"));\n    var n = e[t];\n    this.depthwiseKernel = this.addWeight(\"depthwise_kernel\", [this.kernelSize[0], this.kernelSize[1], n, this.depthMultiplier], null, this.depthwiseInitializer, this.depthwiseRegularizer, !0, this.depthwiseConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [n * this.depthMultiplier], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t = depthwiseConv2d$1(e = getExactlyOneTensor(e), this.depthwiseKernel.read(), this.strides, this.padding, this.dataFormat, null);\n      return this.useBias && (t = biasAdd(t, this.bias.read(), this.dataFormat)), null != this.activation && (t = this.activation.apply(t)), t;\n    });\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[3] : e[2],\n        n = \"channelsFirst\" === this.dataFormat ? e[1] * this.depthMultiplier : e[3] * this.depthMultiplier,\n        r = convOutputLength(\"channelsFirst\" === this.dataFormat ? e[2] : e[1], this.kernelSize[0], this.padding, this.strides[0]),\n        a = convOutputLength(t, this.kernelSize[1], this.padding, this.strides[1]);\n    return \"channelsFirst\" === this.dataFormat ? [e[0], n, r, a] : [e[0], r, a, n];\n  }\n\n  getConfig() {\n    var e = super.getConfig();\n    return e.depthMultiplier = this.depthMultiplier, e.depthwiseInitializer = serializeInitializer(this.depthwiseInitializer), e.depthwiseRegularizer = serializeRegularizer(this.depthwiseRegularizer), e.depthwiseConstraint = serializeConstraint(this.depthwiseRegularizer), e;\n  }\n\n}\n\nfunction standardizeArgs(e, t, n, r) {\n  if (Array.isArray(e)) {\n    if (null != t || null != n) throw new ValueError(\"When inputs is an array, neither initialState or constants should be provided\");\n    null != r && (n = e.slice(e.length - r, e.length), e = e.slice(0, e.length - r)), e.length > 1 && (t = e.slice(1, e.length)), e = e[0];\n  }\n\n  function a(e) {\n    return null == e || Array.isArray(e) ? e : [e];\n  }\n\n  return {\n    inputs: e,\n    initialState: t = a(t),\n    constants: n = a(n)\n  };\n}\n\nfunction rnn$1(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = arguments.length > 4 ? arguments[4] : undefined;\n  var s = arguments.length > 5 ? arguments[5] : undefined;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !1;\n  var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : !1;\n  return tidy(() => {\n    var l = t.shape.length;\n    if (l < 3) throw new ValueError(\"Input should be at least 3D, but is \".concat(l, \"D.\"));\n    var u = [1, 0].concat(range$3(2, l));\n    if (t = transpose$2(t, u), null != s) throw new NotImplementedError(\"The rnn() functoin of the deeplearn.js backend does not support constants yet.\");\n    o && console.warn(\"Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend.\"), null != a && ((a = cast$3(cast$3(a, \"bool\"), \"float32\")).rank === l - 1 && (a = expandDims$3(a, -1)), a = transpose$2(a, u)), r && (t = reverse$2(t, 0), null != a && (a = reverse$2(a, 0)));\n    var c = [];\n    var p,\n        d = n;\n    var h = t.shape[0],\n        m = unstack(t);\n    var f, g;\n    null != a && (f = unstack(a));\n\n    var _loop47 = function _loop47(_t517) {\n      var n = m[_t517],\n          r = tidy(() => e(n, d));\n      if (null == a) p = r[0], d = r[1];else {\n        var _e727 = tidy(() => {\n          var e = f[_t517],\n              n = sub$2(onesLike$2(e), e);\n          return {\n            output: add$2(mul(r[0], e), mul(d[0], n)),\n            newStates: d.map((t, a) => add$2(mul(r[1][a], e), mul(t, n)))\n          };\n        });\n\n        p = _e727.output, d = _e727.newStates;\n      }\n      i && c.push(p);\n    };\n\n    for (var _t517 = 0; _t517 < h; ++_t517) {\n      _loop47(_t517);\n    }\n\n    return i && (g = stack(c, 1)), [p, g, d];\n  });\n}\n\nDepthwiseConv2D.className = \"DepthwiseConv2D\", registerClass(DepthwiseConv2D);\n\nclass RNN extends Layer {\n  constructor(e) {\n    var t;\n    if (super(e), null == e.cell) throw new ValueError(\"cell property is missing for the constructor of RNN.\");\n    if (t = Array.isArray(e.cell) ? new StackedRNNCells({\n      cells: e.cell\n    }) : e.cell, null == t.stateSize) throw new ValueError(\"The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).\");\n    this.cell = t, this.returnSequences = null != e.returnSequences && e.returnSequences, this.returnState = null != e.returnState && e.returnState, this.goBackwards = null != e.goBackwards && e.goBackwards, this._stateful = null != e.stateful && e.stateful, this.unroll = null != e.unroll && e.unroll, this.supportsMasking = !0, this.inputSpec = [new InputSpec({\n      ndim: 3\n    })], this.stateSpec = null, this.states_ = null, this.numConstants = null, this.keptStates = [];\n  }\n\n  getStates() {\n    return null == this.states_ ? range$3(0, Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1).map(e => null) : this.states_;\n  }\n\n  setStates(e) {\n    this.states_ = e;\n  }\n\n  computeOutputShape(e) {\n    isArrayOfShapes(e) && (e = e[0]), e = e;\n    var t = this.cell.stateSize;\n    Array.isArray(t) || (t = [t]);\n    var n = t[0];\n    var r;\n\n    if (r = this.returnSequences ? [e[0], e[1], n] : [e[0], n], this.returnState) {\n      var _n289 = [];\n\n      for (var _r235 of t) {\n        _n289.push([e[0], _r235]);\n      }\n\n      return [r].concat(_n289);\n    }\n\n    return r;\n  }\n\n  computeMask(e, t) {\n    return tidy(() => {\n      Array.isArray(t) && (t = t[0]);\n      var e = this.returnSequences ? t : null;\n\n      if (this.returnState) {\n        var _t518 = this.states.map(e => null);\n\n        return [e].concat(_t518);\n      }\n\n      return e;\n    });\n  }\n\n  get states() {\n    if (null == this.states_) {\n      var _e728 = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1,\n          t = [];\n\n      for (var n = 0; n < _e728; ++n) {\n        t.push(null);\n      }\n\n      return t;\n    }\n\n    return this.states_;\n  }\n\n  set states(e) {\n    this.states_ = e;\n  }\n\n  build(e) {\n    if (null != this.numConstants) throw new NotImplementedError(\"Constants support is not implemented in RNN yet.\");\n    isArrayOfShapes(e) && (e = e[0]), e = e;\n    var t = this.stateful ? e[0] : null,\n        n = e.slice(2);\n    this.inputSpec[0] = new InputSpec({\n      shape: [t, null, ...n]\n    });\n    var r = [e[0]].concat(e.slice(2));\n    var a;\n\n    if (this.cell.build(r), a = Array.isArray(this.cell.stateSize) ? this.cell.stateSize : [this.cell.stateSize], null != this.stateSpec) {\n      if (!arraysEqual(this.stateSpec.map(e => e.shape[e.shape.length - 1]), a)) throw new ValueError(\"An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=\".concat(this.stateSpec, \"; However cell.stateSize is \").concat(this.cell.stateSize));\n    } else this.stateSpec = a.map(e => new InputSpec({\n      shape: [null, e]\n    }));\n\n    this.stateful && this.resetStates();\n  }\n\n  resetStates(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    tidy(() => {\n      if (!this.stateful) throw new AttributeError(\"Cannot call resetStates() on an RNN Layer that is not stateful.\");\n      var n = this.inputSpec[0].shape[0];\n      if (null == n) throw new ValueError(\"If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.\");\n      if (null == this.states_) this.states_ = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.map(e => zeros$2([n, e])) : [zeros$2([n, this.cell.stateSize])];else if (null == e) dispose(this.states_), null != this.keptStates && (dispose(this.keptStates), this.keptStates = []), Array.isArray(this.cell.stateSize) ? this.states_ = this.cell.stateSize.map(e => zeros$2([n, e])) : this.states_[0] = zeros$2([n, this.cell.stateSize]);else {\n        if (Array.isArray(e) || (e = [e]), e.length !== this.states_.length) throw new ValueError(\"Layer \".concat(this.name, \" expects \").concat(this.states_.length, \" state(s), but it received \").concat(e.length, \" state value(s). Input received: \").concat(e));\n        !0 === t ? this.keptStates.push(this.states_.slice()) : dispose(this.states_);\n\n        for (var _t519 = 0; _t519 < this.states_.length; ++_t519) {\n          var r = e[_t519],\n              a = Array.isArray(this.cell.stateSize) ? this.cell.stateSize[_t519] : this.cell.stateSize,\n              s = [n, a];\n          if (!arraysEqual(r.shape, s)) throw new ValueError(\"State \".concat(_t519, \" is incompatible with layer \").concat(this.name, \": expected shape=\").concat(s, \", received shape=\").concat(r.shape));\n          this.states_[_t519] = r;\n        }\n      }\n      this.states_ = this.states_.map(e => keep(e.clone()));\n    });\n  }\n\n  apply(e, t) {\n    var n = null == t ? null : t.initialState,\n        r = null == t ? null : t.constants;\n    null == t && (t = {});\n    var a = standardizeArgs(e, n, r, this.numConstants);\n    e = a.inputs, n = a.initialState, r = a.constants;\n    var s = [],\n        o = [];\n\n    if (null != n) {\n      t.initialState = n, s = s.concat(n), this.stateSpec = [];\n\n      for (var _e729 of n) {\n        this.stateSpec.push(new InputSpec({\n          shape: _e729.shape\n        }));\n      }\n\n      o = o.concat(this.stateSpec);\n    }\n\n    if (null != r && (t.constants = r, s = s.concat(r), this.numConstants = r.length), s[0] instanceof SymbolicTensor) {\n      var _n290 = [e].concat(s),\n          _r236 = this.inputSpec.concat(o),\n          _a155 = this.inputSpec;\n\n      this.inputSpec = _r236;\n      var i = super.apply(_n290, t);\n      return this.inputSpec = _a155, i;\n    }\n\n    return super.apply(e, t);\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var n = null == t ? null : t.mask,\n          r = null == t ? null : t.training;\n      var a = null == t ? null : t.initialState;\n      e = getExactlyOneTensor(e), null == a && (a = this.stateful ? this.states_ : this.getInitialState(e));\n      var s = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n      if (a.length !== s) throw new ValueError(\"RNN Layer has \".concat(s, \" state(s) but was passed \").concat(a.length, \" initial state(s).\"));\n      this.unroll && console.warn(\"Ignoring unroll = true for RNN layer, due to imperative backend.\");\n      var o = {\n        training: r\n      },\n          i = rnn$1((e, t) => {\n        var n = this.cell.call([e].concat(t), o);\n        return [n[0], n.slice(1)];\n      }, e, a, this.goBackwards, n, null, this.unroll, this.returnSequences),\n          l = i[0],\n          u = i[1],\n          c = i[2];\n      this.stateful && this.resetStates(c, r);\n      var p = this.returnSequences ? u : l;\n      return this.returnState ? [p].concat(c) : p;\n    });\n  }\n\n  getInitialState(e) {\n    return tidy(() => {\n      var t = zeros$2(e.shape);\n      return t = sum$2(t, [1, 2]), t = expandDims$2(t), Array.isArray(this.cell.stateSize) ? this.cell.stateSize.map(e => e > 1 ? tile$2(t, [1, e]) : t) : this.cell.stateSize > 1 ? [tile$2(t, [1, this.cell.stateSize])] : [t];\n    });\n  }\n\n  get trainableWeights() {\n    return this.trainable ? this.cell.trainableWeights : [];\n  }\n\n  get nonTrainableWeights() {\n    return this.trainable ? this.cell.nonTrainableWeights : this.cell.weights;\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    super.setFastWeightInitDuringBuild(e), null != this.cell && this.cell.setFastWeightInitDuringBuild(e);\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      returnSequences: this.returnSequences,\n      returnState: this.returnState,\n      goBackwards: this.goBackwards,\n      stateful: this.stateful,\n      unroll: this.unroll\n    };\n    null != this.numConstants && (t.numConstants = this.numConstants);\n    var n = this.cell.getConfig();\n    return this.getClassName() === RNN.className && (t.cell = {\n      className: this.cell.getClassName(),\n      config: n\n    }), Object.assign({}, n, e, t);\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = deserialize(t.cell, n);\n    return new e(Object.assign(t, {\n      cell: r\n    }));\n  }\n\n}\n\nRNN.className = \"RNN\", registerClass(RNN);\n\nclass RNNCell extends Layer {}\n\nclass SimpleRNNCell extends RNNCell {\n  constructor(e) {\n    super(e), this.DEFAULT_ACTIVATION = \"tanh\", this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", this.units = e.units, assertPositiveInteger(this.units, \"units\"), this.activation = getActivation(null == e.activation ? this.DEFAULT_ACTIVATION : e.activation), this.useBias = null == e.useBias || e.useBias, this.kernelInitializer = getInitializer(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.recurrentInitializer = getInitializer(e.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER), this.biasInitializer = getInitializer(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.kernelRegularizer = getRegularizer(e.kernelRegularizer), this.recurrentRegularizer = getRegularizer(e.recurrentRegularizer), this.biasRegularizer = getRegularizer(e.biasRegularizer), this.kernelConstraint = getConstraint(e.kernelConstraint), this.recurrentConstraint = getConstraint(e.recurrentConstraint), this.biasConstraint = getConstraint(e.biasConstraint), this.dropout = min$2([1, max$2([0, null == e.dropout ? 0 : e.dropout])]), this.recurrentDropout = min$2([1, max$2([0, null == e.recurrentDropout ? 0 : e.recurrentDropout])]), this.stateSize = this.units, this.dropoutMask = null, this.recurrentDropoutMask = null;\n  }\n\n  build(e) {\n    e = getExactlyOneShape(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      if (2 !== (e = e).length) throw new ValueError(\"SimpleRNNCell expects 2 input Tensors, got \".concat(e.length, \".\"));\n      var n = e[1];\n      e = e[0];\n      var r = null != t.training && t.training;\n      var a;\n      0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(e),\n        rate: this.dropout,\n        training: r\n      })), 0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(n),\n        rate: this.recurrentDropout,\n        training: r\n      }));\n      var s = this.dropoutMask,\n          o = this.recurrentDropoutMask;\n      a = dot$1(null != s ? mul(e, s) : e, this.kernel.read()), null != this.bias && (a = biasAdd(a, this.bias.read())), null != o && (n = mul(n, o));\n      var i = add$2(a, dot$1(n, this.recurrentKernel.read()));\n      return null != this.activation && (i = this.activation.apply(i)), [i, i];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout\n    };\n    return Object.assign({}, e, t);\n  }\n\n}\n\nSimpleRNNCell.className = \"SimpleRNNCell\", registerClass(SimpleRNNCell);\n\nclass SimpleRNN extends RNN {\n  constructor(e) {\n    e.cell = new SimpleRNNCell(e), super(e);\n  }\n\n  call(e, t) {\n    return tidy(() => (null != this.cell.dropoutMask && (dispose(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), super.call(e, {\n      mask: null == t ? null : t.mask,\n      training: null == t ? null : t.training,\n      initialState: null == t ? null : t.initialState\n    })));\n  }\n\n  static fromConfig(e, t) {\n    return new e(t);\n  }\n\n}\n\nSimpleRNN.className = \"SimpleRNN\", registerClass(SimpleRNN);\n\nclass GRUCell extends RNNCell {\n  constructor(e) {\n    if (super(e), this.DEFAULT_ACTIVATION = \"tanh\", this.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", e.resetAfter) throw new ValueError(\"GRUCell does not support reset_after parameter set to true.\");\n    this.units = e.units, assertPositiveInteger(this.units, \"units\"), this.activation = getActivation(void 0 === e.activation ? this.DEFAULT_ACTIVATION : e.activation), this.recurrentActivation = getActivation(void 0 === e.recurrentActivation ? this.DEFAULT_RECURRENT_ACTIVATION : e.recurrentActivation), this.useBias = null == e.useBias || e.useBias, this.kernelInitializer = getInitializer(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.recurrentInitializer = getInitializer(e.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER), this.biasInitializer = getInitializer(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.kernelRegularizer = getRegularizer(e.kernelRegularizer), this.recurrentRegularizer = getRegularizer(e.recurrentRegularizer), this.biasRegularizer = getRegularizer(e.biasRegularizer), this.kernelConstraint = getConstraint(e.kernelConstraint), this.recurrentConstraint = getConstraint(e.recurrentConstraint), this.biasConstraint = getConstraint(e.biasConstraint), this.dropout = min$2([1, max$2([0, null == e.dropout ? 0 : e.dropout])]), this.recurrentDropout = min$2([1, max$2([0, null == e.recurrentDropout ? 0 : e.recurrentDropout])]), this.implementation = e.implementation, this.stateSize = this.units, this.dropoutMask = null, this.recurrentDropoutMask = null;\n  }\n\n  build(e) {\n    e = getExactlyOneShape(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], 3 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 3 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.bias = this.useBias ? this.addWeight(\"bias\", [3 * this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      if (2 !== (e = e).length) throw new ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \".concat(e.length, \".\"));\n      var n = null != t.training && t.training;\n      var r = e[1];\n      e = e[0], 0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(e),\n        rate: this.dropout,\n        training: n,\n        count: 3\n      })), 0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(r),\n        rate: this.recurrentDropout,\n        training: n,\n        count: 3\n      }));\n      var a = this.recurrentDropoutMask;\n      var s, o, i;\n      0 < this.dropout && this.dropout < 1 && (e = mul(e, this.dropoutMask[0]));\n      var l = dot$1(e, this.kernel.read());\n      this.useBias && (l = biasAdd(l, this.bias.read())), 0 < this.recurrentDropout && this.recurrentDropout < 1 && (r = mul(r, a[0]));\n      var u = this.recurrentKernel.read(),\n          [c, p] = split$2(u, [2 * this.units, this.units], u.rank - 1),\n          d = dot$1(r, c),\n          [h, m, f] = split$2(l, 3, l.rank - 1),\n          [g, $] = split$2(d, 2, d.rank - 1);\n      s = this.recurrentActivation.apply(add$2(h, g)), o = this.recurrentActivation.apply(add$2(m, $));\n      var y = dot$1(mul(o, r), p);\n      i = this.activation.apply(add$2(f, y));\n      var b = add$2(mul(s, r), mul(add$2(1, neg$2(s)), i));\n      return [b, b];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation,\n      resetAfter: !1\n    };\n    return Object.assign({}, e, t);\n  }\n\n}\n\nGRUCell.className = \"GRUCell\", registerClass(GRUCell);\n\nclass GRU extends RNN {\n  constructor(e) {\n    0 === e.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), e.cell = new GRUCell(e), super(e);\n  }\n\n  call(e, t) {\n    return tidy(() => (null != this.cell.dropoutMask && (dispose(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), super.call(e, {\n      mask: null == t ? null : t.mask,\n      training: null == t ? null : t.training,\n      initialState: null == t ? null : t.initialState\n    })));\n  }\n\n  static fromConfig(e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }\n\n}\n\nGRU.className = \"GRU\", registerClass(GRU);\n\nclass LSTMCell extends RNNCell {\n  constructor(e) {\n    super(e), this.DEFAULT_ACTIVATION = \"tanh\", this.DEFAULT_RECURRENT_ACTIVATION = \"hardSigmoid\", this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_RECURRENT_INITIALIZER = \"orthogonal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", this.units = e.units, assertPositiveInteger(this.units, \"units\"), this.activation = getActivation(void 0 === e.activation ? this.DEFAULT_ACTIVATION : e.activation), this.recurrentActivation = getActivation(void 0 === e.recurrentActivation ? this.DEFAULT_RECURRENT_ACTIVATION : e.recurrentActivation), this.useBias = null == e.useBias || e.useBias, this.kernelInitializer = getInitializer(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.recurrentInitializer = getInitializer(e.recurrentInitializer || this.DEFAULT_RECURRENT_INITIALIZER), this.biasInitializer = getInitializer(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.unitForgetBias = e.unitForgetBias, this.kernelRegularizer = getRegularizer(e.kernelRegularizer), this.recurrentRegularizer = getRegularizer(e.recurrentRegularizer), this.biasRegularizer = getRegularizer(e.biasRegularizer), this.kernelConstraint = getConstraint(e.kernelConstraint), this.recurrentConstraint = getConstraint(e.recurrentConstraint), this.biasConstraint = getConstraint(e.biasConstraint), this.dropout = min$2([1, max$2([0, null == e.dropout ? 0 : e.dropout])]), this.recurrentDropout = min$2([1, max$2([0, null == e.recurrentDropout ? 0 : e.recurrentDropout])]), this.implementation = e.implementation, this.stateSize = [this.units, this.units], this.dropoutMask = null, this.recurrentDropoutMask = null;\n  }\n\n  build(e) {\n    var t;\n    var n;\n\n    if (e = getExactlyOneShape(e), this.kernel = this.addWeight(\"kernel\", [e[e.length - 1], 4 * this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.recurrentKernel = this.addWeight(\"recurrent_kernel\", [this.units, 4 * this.units], null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias) {\n      if (this.unitForgetBias) {\n        var _e730 = this.biasInitializer,\n            r = this.units;\n        n = new ((t = class extends Initializer {\n          apply(t, n) {\n            var a = _e730.apply([r]),\n                s = new Ones().apply([r]),\n                o = _e730.apply([2 * r]);\n\n            return concatAlongFirstAxis(concatAlongFirstAxis(a, s), o);\n          }\n\n        }).className = \"CustomInit\", t)();\n      } else n = this.biasInitializer;\n\n      this.bias = this.addWeight(\"bias\", [4 * this.units], null, n, this.biasRegularizer, !0, this.biasConstraint);\n    } else this.bias = null;\n\n    this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var n = null != t.training && t.training;\n      if (3 !== (e = e).length) throw new ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \".concat(e.length, \".\"));\n      var r = e[1];\n      var a = e[2];\n      e = e[0], 0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(e),\n        rate: this.dropout,\n        training: n,\n        count: 4\n      })), 0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(r),\n        rate: this.recurrentDropout,\n        training: n,\n        count: 4\n      }));\n      var s = this.recurrentDropoutMask;\n      var o, i, l, u;\n      0 < this.dropout && this.dropout < 1 && (e = mul(e, this.dropoutMask[0]));\n      var c = dot$1(e, this.kernel.read());\n      0 < this.recurrentDropout && this.recurrentDropout < 1 && (r = mul(r, s[0])), c = add$2(c, dot$1(r, this.recurrentKernel.read())), this.useBias && (c = biasAdd(c, this.bias.read()));\n      var [p, d, h, m] = split$2(c, 4, c.rank - 1);\n      o = this.recurrentActivation.apply(p), i = this.recurrentActivation.apply(d), l = add$2(mul(i, a), mul(o, this.activation.apply(h))), u = this.recurrentActivation.apply(m);\n      var f = mul(u, this.activation.apply(l));\n      return [f, f, l];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      recurrentActivation: serializeActivation(this.recurrentActivation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      unitForgetBias: this.unitForgetBias,\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint),\n      dropout: this.dropout,\n      recurrentDropout: this.recurrentDropout,\n      implementation: this.implementation\n    };\n    return Object.assign({}, e, t);\n  }\n\n}\n\nLSTMCell.className = \"LSTMCell\", registerClass(LSTMCell);\n\nclass LSTM extends RNN {\n  constructor(e) {\n    0 === e.implementation && console.warn(\"`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call.\"), e.cell = new LSTMCell(e), super(e);\n  }\n\n  call(e, t) {\n    return tidy(() => (null != this.cell.dropoutMask && (dispose(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), super.call(e, {\n      mask: null == t ? null : t.mask,\n      training: null == t ? null : t.training,\n      initialState: null == t ? null : t.initialState\n    })));\n  }\n\n  static fromConfig(e, t) {\n    return 0 === t.implmentation && (t.implementation = 1), new e(t);\n  }\n\n}\n\nLSTM.className = \"LSTM\", registerClass(LSTM);\n\nclass StackedRNNCells extends RNNCell {\n  constructor(e) {\n    super(e), this.cells = e.cells;\n  }\n\n  get stateSize() {\n    var e = [];\n\n    for (var t of this.cells.slice().reverse()) {\n      Array.isArray(t.stateSize) ? e.push(...t.stateSize) : e.push(t.stateSize);\n    }\n\n    return e;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var n = (e = e).slice(1);\n      var r = [];\n\n      for (var _e731 of this.cells.slice().reverse()) {\n        Array.isArray(_e731.stateSize) ? r.push(n.splice(0, _e731.stateSize.length)) : r.push(n.splice(0, 1));\n      }\n\n      r.reverse();\n      var a = [];\n      var s;\n\n      for (var o = 0; o < this.cells.length; ++o) {\n        var i = this.cells[o];\n        n = r[o], s = 0 === o ? [e[0]].concat(n) : [s[0]].concat(n), s = i.call(s, t), a.push(s.slice(1));\n      }\n\n      n = [];\n\n      for (var _e732 of a.slice().reverse()) {\n        n.push(..._e732);\n      }\n\n      return [s[0]].concat(n);\n    });\n  }\n\n  build(e) {\n    var t;\n    isArrayOfShapes(e) && (e = e[0]), e = e, this.cells.forEach((n, r) => {\n      nameScope(\"RNNCell_\".concat(r), () => {\n        n.build(e), t = Array.isArray(n.stateSize) ? n.stateSize[0] : n.stateSize, e = [e[0], t];\n      });\n    }), this.built = !0;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = this.cells.map(e => ({\n      className: e.getClassName(),\n      config: e.getConfig()\n    }));\n    return Object.assign({}, e, {\n      cells: t\n    });\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = [];\n\n    for (var _e733 of t.cells) {\n      r.push(deserialize(_e733, n));\n    }\n\n    return new e({\n      cells: r\n    });\n  }\n\n  get trainableWeights() {\n    if (!this.trainable) return [];\n    var e = [];\n\n    for (var t of this.cells) {\n      e.push(...t.trainableWeights);\n    }\n\n    return e;\n  }\n\n  get nonTrainableWeights() {\n    var e = [];\n\n    for (var t of this.cells) {\n      e.push(...t.nonTrainableWeights);\n    }\n\n    if (!this.trainable) {\n      var _t520 = [];\n\n      for (var _e734 of this.cells) {\n        _t520.push(..._e734.trainableWeights);\n      }\n\n      return _t520.concat(e);\n    }\n\n    return e;\n  }\n\n  getWeights() {\n    var e = [];\n\n    for (var t of this.cells) {\n      e.push(...t.weights);\n    }\n\n    return batchGetValue(e);\n  }\n\n  setWeights(e) {\n    var t = [];\n\n    for (var n of this.cells) {\n      var r = e.splice(n.weights.length);\n\n      for (var _e735 = 0; _e735 < n.weights.length; ++_e735) {\n        t.push([n.weights[_e735], r[_e735]]);\n      }\n    }\n\n    batchSetValue(t);\n  }\n\n}\n\nfunction generateDropoutMask(e) {\n  var {\n    ones: t,\n    rate: n,\n    training: r = !1,\n    count: a = 1\n  } = e,\n      s = () => dropout$1(t(), n),\n      o = () => inTrainPhase(s, t, r);\n\n  return !a || a <= 1 ? keep(o().clone()) : Array(a).fill(void 0).map(o).map(e => keep(e.clone()));\n}\n\nStackedRNNCells.className = \"StackedRNNCells\", registerClass(StackedRNNCells);\n\nvar __rest = function __rest(e, t) {\n  var n = {};\n\n  for (var r in e) {\n    Object.prototype.hasOwnProperty.call(e, r) && t.indexOf(r) < 0 && (n[r] = e[r]);\n  }\n\n  if (null != e && \"function\" == typeof Object.getOwnPropertySymbols) {\n    var a = 0;\n\n    for (r = Object.getOwnPropertySymbols(e); a < r.length; a++) {\n      t.indexOf(r[a]) < 0 && Object.prototype.propertyIsEnumerable.call(e, r[a]) && (n[r[a]] = e[r[a]]);\n    }\n  }\n\n  return n;\n};\n\nclass ConvRNN2D extends RNN {\n  constructor(e) {\n    if (e.unroll) throw new NotImplementedError(\"Unrolling is not possible with convolutional RNNs.\");\n    if (Array.isArray(e.cell)) throw new NotImplementedError(\"It is not possible at the moment to stack convolutional cells.\");\n    super(e), this.inputSpec = [new InputSpec({\n      ndim: 5\n    })];\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      if (null != this.cell.dropoutMask && (dispose(this.cell.dropoutMask), this.cell.dropoutMask = null), null != this.cell.recurrentDropoutMask && (dispose(this.cell.recurrentDropoutMask), this.cell.recurrentDropoutMask = null), t && t.constants) throw new ValueError(\"ConvRNN2D cell does not support constants\");\n      return super.call(e, {\n        mask: null == t ? null : t.mask,\n        training: null == t ? null : t.training,\n        initialState: null == t ? null : t.initialState\n      });\n    });\n  }\n\n  computeOutputShape(e) {\n    var t = this.computeSingleOutputShape(e);\n    return this.returnSequences || (t = [t[0], ...t.slice(2)]), this.returnState && (t = [t, ...Array(2).fill([e[0], ...t.slice(-3)])]), t;\n  }\n\n  getInitialState(e) {\n    return tidy(() => {\n      var {\n        stateSize: t\n      } = this.cell,\n          n = this.computeSingleOutputShape(e.shape),\n          r = zeros$2([n[0], ...n.slice(2)]);\n      return Array.isArray(t) ? Array(t.length).fill(r) : [r];\n    });\n  }\n\n  resetStates(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    tidy(() => {\n      if (!this.stateful) throw new AttributeError(\"Cannot call resetStates() on an RNN Layer that is not stateful.\");\n      var n = this.inputSpec[0].shape,\n          r = this.computeSingleOutputShape(n),\n          a = [r[0], ...r.slice(2)];\n      if (null == n[0]) throw new ValueError(\"If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \\n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.\");\n      if (null == this.getStates()) this.states_ = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.map(() => zeros$2(a)) : [zeros$2(a)];else if (null == e) dispose(this.states_), null != this.keptStates && (dispose(this.keptStates), this.keptStates = []), Array.isArray(this.cell.stateSize) ? this.states_ = this.cell.stateSize.map(() => zeros$2(a)) : this.states_[0] = zeros$2(a);else {\n        if (Array.isArray(e) || (e = [e]), e.length !== this.states_.length) throw new ValueError(\"Layer \".concat(this.name, \" expects \").concat(this.states_.length, \" state(s), but it received \").concat(e.length, \" state value(s). Input received: \").concat(e));\n        t ? this.keptStates.push(this.states_.slice()) : dispose(this.states_);\n\n        for (var _t521 = 0; _t521 < this.states_.length; ++_t521) {\n          var _n291 = e[_t521],\n              _r237 = a;\n          if (!arraysEqual(_n291.shape, _r237)) throw new ValueError(\"State \".concat(_t521, \" is incompatible with layer \").concat(this.name, \": expected shape=\").concat(_r237, \", received shape=\").concat(_n291.shape));\n          this.states_[_t521] = _n291;\n        }\n      }\n      this.states_ = this.states_.map(e => keep(e.clone()));\n    });\n  }\n\n  computeSingleOutputShape(e) {\n    var {\n      dataFormat: t,\n      filters: n,\n      kernelSize: r,\n      padding: a,\n      strides: s,\n      dilationRate: o\n    } = this.cell,\n        i = \"channelsFirst\" === t,\n        l = e[i ? 4 : 3],\n        u = convOutputLength(e[i ? 3 : 2], r[0], a, s[0], o[0]),\n        c = convOutputLength(l, r[1], a, s[1], o[1]);\n    return [...e.slice(0, 2), ...(i ? [n, u, c] : [u, c, n])];\n  }\n\n}\n\nConvRNN2D.className = \"ConvRNN2D\";\n\nclass ConvLSTM2DCell extends LSTMCell {\n  constructor(e) {\n    var {\n      filters: t,\n      kernelSize: n,\n      strides: r,\n      padding: a,\n      dataFormat: s,\n      dilationRate: o\n    } = e;\n    super(Object.assign({}, e, {\n      units: t\n    })), this.filters = t, assertPositiveInteger(this.filters, \"filters\"), this.kernelSize = normalizeArray(n, 2, \"kernelSize\"), this.kernelSize.forEach(e => assertPositiveInteger(e, \"kernelSize\")), this.strides = normalizeArray(r || 1, 2, \"strides\"), this.strides.forEach(e => assertPositiveInteger(e, \"strides\")), this.padding = a || \"valid\", checkPaddingMode(this.padding), this.dataFormat = s || \"channelsLast\", checkDataFormat(this.dataFormat), this.dilationRate = normalizeArray(o || 1, 2, \"dilationRate\"), this.dilationRate.forEach(e => assertPositiveInteger(e, \"dilationRate\"));\n  }\n\n  build(e) {\n    var t;\n    e = getExactlyOneShape(e);\n    var n = \"channelsFirst\" === this.dataFormat ? 1 : e.length - 1;\n    if (null == e[n]) throw new ValueError(\"The channel dimension of the input should be defined. Found \".concat(e[n]));\n    var r = this.kernelSize.concat([e[n], 4 * this.filters]);\n    this.kernel = this.addWeight(\"kernel\", r, null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint);\n    var a = this.kernelSize.concat([this.filters, 4 * this.filters]);\n\n    if (this.recurrentKernel = this.addWeight(\"recurrent_kernel\", a, null, this.recurrentInitializer, this.recurrentRegularizer, !0, this.recurrentConstraint), this.useBias) {\n      var _e736;\n\n      if (this.unitForgetBias) {\n        var _n292 = this.biasInitializer,\n            _r238 = this.filters;\n        _e736 = new ((t = class extends Initializer {\n          apply(e, t) {\n            return concatenate$1([_n292.apply([_r238]), ones$1([_r238]), _n292.apply([2 * _r238])]);\n          }\n\n        }).className = \"CustomInit\", t)();\n      } else _e736 = this.biasInitializer;\n\n      this.bias = this.addWeight(\"bias\", [4 * this.filters], null, _e736, this.biasRegularizer, !0, this.biasConstraint);\n    }\n\n    this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      if (3 !== e.length) throw new ValueError(\"ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got \".concat(e.length, \".\"));\n      var n = t.training || !1,\n          r = e[0],\n          a = e[1],\n          s = e[2];\n      0 < this.dropout && this.dropout < 1 && null == this.dropoutMask && (this.dropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(r),\n        rate: this.dropout,\n        training: n,\n        count: 4\n      }));\n\n      var o = this.dropoutMask,\n          i = (e, t, n) => t && t[n] ? mul(t[n], e) : e;\n\n      var l = i(r, o, 0),\n          u = i(r, o, 1),\n          c = i(r, o, 2),\n          p = i(r, o, 3);\n      0 < this.recurrentDropout && this.recurrentDropout < 1 && null == this.recurrentDropoutMask && (this.recurrentDropoutMask = generateDropoutMask({\n        ones: () => onesLike$2(a),\n        rate: this.recurrentDropout,\n        training: n,\n        count: 4\n      }));\n      var d = this.recurrentDropoutMask;\n      var h = i(a, d, 0),\n          m = i(a, d, 1),\n          f = i(a, d, 2),\n          g = i(a, d, 3);\n      var [$, y, b, x] = split$2(this.kernel.read(), 4, 3),\n          [v, I, C, S] = this.useBias ? split$2(this.bias.read(), 4) : [null, null, null, null];\n      l = this.inputConv(l, $, v, this.padding), u = this.inputConv(u, y, I, this.padding), c = this.inputConv(c, b, C, this.padding), p = this.inputConv(p, x, S, this.padding);\n      var [k, T, N, w] = split$2(this.recurrentKernel.read(), 4, 3);\n      h = this.recurrentConv(h, k), m = this.recurrentConv(m, T), f = this.recurrentConv(f, N), g = this.recurrentConv(g, w);\n      var E = this.recurrentActivation.apply(add$2(l, h)),\n          A = this.recurrentActivation.apply(add$2(u, m)),\n          D = add$2(mul(A, s), mul(E, this.activation.apply(add$2(c, f)))),\n          R = mul(this.recurrentActivation.apply(add$2(p, g)), this.activation.apply(D));\n      return [R, R, D];\n    });\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = __rest(e, [\"units\"]);\n\n    return Object.assign({}, t, {\n      filters: this.filters,\n      kernelSize: this.kernelSize,\n      padding: this.padding,\n      dataFormat: this.dataFormat,\n      dilationRate: this.dilationRate,\n      strides: this.strides\n    });\n  }\n\n  inputConv(e, t, n, r) {\n    var a = conv2d$3(e, t, this.strides, r || \"valid\", \"channelsFirst\" === this.dataFormat ? \"NCHW\" : \"NHWC\", this.dilationRate);\n    return n ? biasAdd(a, n, this.dataFormat) : a;\n  }\n\n  recurrentConv(e, t) {\n    return conv2d$3(e, t, 1, \"same\", \"channelsFirst\" === this.dataFormat ? \"NCHW\" : \"NHWC\");\n  }\n\n}\n\nConvLSTM2DCell.className = \"ConvLSTM2DCell\", registerClass(ConvLSTM2DCell);\n\nclass ConvLSTM2D extends ConvRNN2D {\n  constructor(e) {\n    var t = new ConvLSTM2DCell(e);\n    super(Object.assign({}, e, {\n      cell: t\n    }));\n  }\n\n  static fromConfig(e, t) {\n    return new e(t);\n  }\n\n}\n\nConvLSTM2D.className = \"ConvLSTM2D\", registerClass(ConvLSTM2D);\n\nclass Dropout extends Layer {\n  constructor(e) {\n    super(e), this.rate = Math.max(Math.min(e.rate, 1), 0), this.noiseShape = e.noiseShape, this.seed = e.seed, this.supportsMasking = !0;\n  }\n\n  getNoiseShape(e) {\n    if (null == this.noiseShape) return this.noiseShape;\n    var t = e.shape,\n        n = [];\n\n    for (var _e737 = 0; _e737 < this.noiseShape.length; ++_e737) {\n      n.push(null == this.noiseShape[_e737] ? t[_e737] : this.noiseShape[_e737]);\n    }\n\n    return n;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e);\n\n      if (0 < this.rate && this.rate < 1) {\n        var _e738 = null != t.training && t.training,\n            r = this.getNoiseShape(n);\n\n        return inTrainPhase(() => dropout$1(n, this.rate, r, this.seed), () => n, _e738);\n      }\n\n      return e;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      rate: this.rate,\n      noiseShape: this.noiseShape,\n      seed: this.seed\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  dispose() {\n    return super.dispose();\n  }\n\n}\n\nDropout.className = \"Dropout\", registerClass(Dropout);\n\nclass SpatialDropout1D extends Dropout {\n  constructor(e) {\n    super(e), this.inputSpec = [{\n      ndim: 3\n    }];\n  }\n\n  getNoiseShape(e) {\n    var t = e.shape;\n    return [t[0], 1, t[2]];\n  }\n\n}\n\nSpatialDropout1D.className = \"SpatialDropout1D\", registerClass(SpatialDropout1D);\n\nclass Dense extends Layer {\n  constructor(e) {\n    if (super(e), this.activation = null, this.useBias = !0, this.kernel = null, this.bias = null, this.DEFAULT_KERNEL_INITIALIZER = \"glorotNormal\", this.DEFAULT_BIAS_INITIALIZER = \"zeros\", null == e.batchInputShape && null == e.inputShape && null != e.inputDim) {\n      var t = null;\n      null != e.batchSize && (t = e.batchSize), this.batchInputShape = [t, e.inputDim];\n    }\n\n    this.units = e.units, assertPositiveInteger(this.units, \"units\"), this.activation = getActivation(e.activation), null != e.useBias && (this.useBias = e.useBias), this.kernelInitializer = getInitializer(e.kernelInitializer || this.DEFAULT_KERNEL_INITIALIZER), this.biasInitializer = getInitializer(e.biasInitializer || this.DEFAULT_BIAS_INITIALIZER), this.kernelConstraint = getConstraint(e.kernelConstraint), this.biasConstraint = getConstraint(e.biasConstraint), this.kernelRegularizer = getRegularizer(e.kernelRegularizer), this.biasRegularizer = getRegularizer(e.biasRegularizer), this.activityRegularizer = getRegularizer(e.activityRegularizer), this.supportsMasking = !0, this.inputSpec = [{\n      minNDim: 2\n    }];\n  }\n\n  build(e) {\n    var t = (e = getExactlyOneShape(e))[e.length - 1];\n    null == this.kernel && (this.kernel = this.addWeight(\"kernel\", [t, this.units], null, this.kernelInitializer, this.kernelRegularizer, !0, this.kernelConstraint), this.useBias && (this.bias = this.addWeight(\"bias\", [this.units], null, this.biasInitializer, this.biasRegularizer, !0, this.biasConstraint))), this.inputSpec = [{\n      minNDim: 2,\n      axes: {\n        [-1]: t\n      }\n    }], this.built = !0;\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape(e)).slice();\n    return t[t.length - 1] = this.units, t;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e),\n          r = mapActivationToFusedKernel(this.activation.getClassName());\n      var a;\n      return null != r ? a = dot$1(n, this.kernel.read(), r, this.bias ? this.bias.read() : null) : (a = dot$1(n, this.kernel.read()), null != this.bias && (a = biasAdd(a, this.bias.read())), null != this.activation && (a = this.activation.apply(a))), a;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      units: this.units,\n      activation: serializeActivation(this.activation),\n      useBias: this.useBias,\n      kernelInitializer: serializeInitializer(this.kernelInitializer),\n      biasInitializer: serializeInitializer(this.biasInitializer),\n      kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n      biasRegularizer: serializeRegularizer(this.biasRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      kernelConstraint: serializeConstraint(this.kernelConstraint),\n      biasConstraint: serializeConstraint(this.biasConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nDense.className = \"Dense\", registerClass(Dense);\n\nclass Flatten extends Layer {\n  constructor(e) {\n    super(e = e || {}), this.inputSpec = [{\n      minNDim: 3\n    }], this.dataFormat = e.dataFormat;\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape(e);\n\n    for (var t of e.slice(1)) {\n      if (null == t) throw new ValueError(\"The shape of the input to \\\"Flatten\\\" is not fully defined (got \".concat(e.slice(1), \"). Make sure to pass a complete \\\"input_shape\\\" or \\\"batch_input_shape\\\" argument to the first layer in your model.\"));\n    }\n\n    return [e[0], arrayProd(e, 1)];\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e);\n\n      if (\"channelsFirst\" === this.dataFormat && n.rank > 1) {\n        var _e739 = [0];\n\n        for (var _t522 = 2; _t522 < n.rank; ++_t522) {\n          _e739.push(_t522);\n        }\n\n        _e739.push(1), n = transpose$2(n, _e739);\n      }\n\n      return batchFlatten(n);\n    });\n  }\n\n  getConfig() {\n    var e = {};\n    null != this.dataFormat && (e.dataFormat = this.dataFormat);\n    var t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nFlatten.className = \"Flatten\", registerClass(Flatten);\n\nclass Activation extends Layer {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.activation = getActivation(e.activation);\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e);\n      return this.activation.apply(n);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      activation: serializeActivation(this.activation)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nActivation.className = \"Activation\", registerClass(Activation);\n\nclass RepeatVector extends Layer {\n  constructor(e) {\n    super(e), this.n = e.n, this.inputSpec = [{\n      ndim: 2\n    }];\n  }\n\n  computeOutputShape(e) {\n    return [e[0], this.n, e[1]];\n  }\n\n  call(e, t) {\n    return tidy(() => repeat$1(e = getExactlyOneTensor(e), this.n));\n  }\n\n  getConfig() {\n    var e = {\n      n: this.n\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nRepeatVector.className = \"RepeatVector\", registerClass(RepeatVector);\n\nclass Reshape extends Layer {\n  constructor(e) {\n    super(e), this.targetShape = e.targetShape;\n\n    for (var _e740 = 0; _e740 < this.targetShape.length; ++_e740) {\n      this.isUnknown(this.targetShape[_e740]) && (this.targetShape[_e740] = null);\n    }\n  }\n\n  isUnknown(e) {\n    return e < 0 || null == e;\n  }\n\n  fixUnknownDimension(e, t) {\n    var n = \"Total size of new array must be unchanged.\",\n        r = t.slice();\n    var a = 1,\n        s = null;\n\n    for (var _e741 = 0; _e741 < r.length; ++_e741) {\n      var _t523 = r[_e741];\n\n      if (this.isUnknown(_t523)) {\n        if (null !== s) throw new ValueError(\"Can only specifiy one unknown dimension.\");\n        s = _e741;\n      } else a *= _t523;\n    }\n\n    var o = arrayProd(e);\n\n    if (null !== s) {\n      if (0 === a || o % a != 0) throw new ValueError(n);\n      r[s] = o / a;\n    } else if (o !== a) throw new ValueError(n);\n\n    return r;\n  }\n\n  computeOutputShape(e) {\n    var t = !1;\n\n    for (var n = 0; n < e.length; ++n) {\n      if (this.isUnknown(e[n])) {\n        t = !0;\n        break;\n      }\n    }\n\n    return t ? e.slice(0, 1).concat(this.targetShape) : e.slice(0, 1).concat(this.fixUnknownDimension(e.slice(1), this.targetShape));\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e),\n          r = n.shape,\n          a = r.slice(0, 1).concat(this.fixUnknownDimension(r.slice(1), this.targetShape));\n      return reshape$3(n, a);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      targetShape: this.targetShape\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nReshape.className = \"Reshape\", registerClass(Reshape);\n\nclass Permute extends Layer {\n  constructor(e) {\n    if (super(e), null == e.dims) throw new Error(\"Required configuration field `dims` is missing during Permute constructor call.\");\n    if (!Array.isArray(e.dims)) throw new Error(\"Permute constructor requires `dims` to be an Array, but received \".concat(e.dims, \" instead.\"));\n    var t = range$3(1, e.dims.length + 1);\n    if (!arraysEqual(e.dims.slice().sort(), t)) throw new Error(\"Invalid permutation `dims`: \" + JSON.stringify(e.dims) + \" `dims` must contain consecutive integers starting from 1.\");\n    this.dims = e.dims, this.dimsIncludingBatch = [0].concat(this.dims), this.inputSpec = [new InputSpec({\n      ndim: this.dims.length + 1\n    })];\n  }\n\n  computeOutputShape(e) {\n    var t = (e = getExactlyOneShape(e)).slice();\n    return this.dims.forEach((n, r) => {\n      t[r + 1] = e[n];\n    }), t;\n  }\n\n  call(e, t) {\n    return transpose$2(getExactlyOneTensor(e), this.dimsIncludingBatch);\n  }\n\n  getConfig() {\n    var e = {\n      dims: this.dims\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nPermute.className = \"Permute\", registerClass(Permute);\n\nclass Masking extends Layer {\n  constructor(e) {\n    super(null == e ? {} : e), this.supportsMasking = !0, this.maskValue = null != e ? null == e.maskValue ? 0 : e.maskValue : 0;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      maskValue: this.maskValue\n    };\n    return Object.assign(t, e), t;\n  }\n\n  computeMask(e, t) {\n    var n = getExactlyOneTensor(e);\n    return any$2(notEqual$2(n, this.maskValue), -1);\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e),\n          r = any$2(notEqual$2(n, this.maskValue), -1, !0);\n      return mul(n, cast$3(r, n.dtype));\n    });\n  }\n\n}\n\nMasking.className = \"Masking\", registerClass(Masking);\n\nclass Embedding extends Layer {\n  constructor(e) {\n    if (super(e), this.embeddings = null, this.DEFAULT_EMBEDDINGS_INITIALIZER = \"randomUniform\", null == e.batchInputShape && null == e.inputShape) {\n      var t = null;\n      null != e.batchSize && (t = e.batchSize), this.batchInputShape = null == e.inputLength ? [t, null] : [t].concat(toList(e.inputLength));\n    }\n\n    this.inputDim = e.inputDim, assertPositiveInteger(this.inputDim, \"inputDim\"), this.outputDim = e.outputDim, assertPositiveInteger(this.outputDim, \"outputDim\"), this.embeddingsInitializer = getInitializer(e.embeddingsInitializer || this.DEFAULT_EMBEDDINGS_INITIALIZER), this.embeddingsRegularizer = getRegularizer(e.embeddingsRegularizer), this.activityRegularizer = getRegularizer(e.activityRegularizer), this.embeddingsConstraint = getConstraint(e.embeddingsConstraint), this.maskZero = e.maskZero, this.supportsMasking = e.maskZero, this.inputLength = e.inputLength;\n  }\n\n  build(e) {\n    this.embeddings = this.addWeight(\"embeddings\", [this.inputDim, this.outputDim], this.dtype, this.embeddingsInitializer, this.embeddingsRegularizer, !0, this.embeddingsConstraint), this.built = !0;\n  }\n\n  warnOnIncompatibleInputShape(e) {}\n\n  computeMask(e, t) {\n    return tidy(() => this.maskZero ? (e = getExactlyOneTensor(e), notEqual$2(e, zerosLike$2(e))) : null);\n  }\n\n  computeOutputShape(e) {\n    if (e = getExactlyOneShape(e), null == this.inputLength) return [...e, this.outputDim];\n    var t = toList(this.inputLength);\n    if (t.length !== e.length - 1) throw new ValueError(\"\\\"inputLength\\\" is \".concat(this.inputLength, \", but received input shape has shape \").concat(e));\n    {\n      var n = 0;\n\n      for (var r = 0; r < t.length; ++r) {\n        var a = t[r],\n            s = e[r + 1];\n        if (null != a && null != s && a !== s) throw new ValueError(\"\\\"inputLength\\\" is \".concat(this.inputLength, \", but received input shape has shape \").concat(e));\n        null == a && (t[n] = s), n++;\n      }\n    }\n    return [e[0], ...t, this.outputDim];\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e);\n      \"int32\" !== n.dtype && (n = cast$2(n, \"int32\"));\n      var r = gather(this.embeddings.read(), reshape$3(n, [n.size]));\n      return reshape$3(r, getExactlyOneShape(this.computeOutputShape(n.shape)));\n    });\n  }\n\n  getConfig() {\n    var e = {\n      inputDim: this.inputDim,\n      outputDim: this.outputDim,\n      embeddingsInitializer: serializeInitializer(this.embeddingsInitializer),\n      embeddingsRegularizer: serializeRegularizer(this.embeddingsRegularizer),\n      activityRegularizer: serializeRegularizer(this.activityRegularizer),\n      embeddingsConstraint: serializeConstraint(this.embeddingsConstraint),\n      maskZero: this.maskZero,\n      inputLength: this.inputLength\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nEmbedding.className = \"Embedding\", registerClass(Embedding);\n\nclass Merge extends Layer {\n  constructor(e) {\n    super(e || {}), this.supportsMasking = !0;\n  }\n\n  mergeFunction(e) {\n    throw new NotImplementedError();\n  }\n\n  computeElementwiseOpOutputShape(e, t) {\n    if (null == e || null == t) return null;\n    if (e.length < t.length) return this.computeElementwiseOpOutputShape(t, e);\n    if (0 === t.length) return e;\n    var n = e.slice(0, e.length - t.length);\n\n    for (var r = 0; r < t.length; ++r) {\n      var a = e[e.length - t.length + r],\n          s = t[r];\n      if (null == a || null == s || a < 0 || s < 0) n.push(null);else if (1 === a) n.push(s);else if (1 === s) n.push(a);else {\n        if (a !== s) throw new ValueError(\"Operands could not be broadcast together with shapes \" + JSON.stringify(e) + \" \" + JSON.stringify(t));\n        n.push(a);\n      }\n    }\n\n    return n;\n  }\n\n  build(e) {\n    if (Array.isArray(e) && !Array.isArray(e[0]) && (e = [getExactlyOneShape(e)]), (e = e).length < 2) throw new ValueError(\"A merge layer should be called on an Array of at least 2 inputs. Got \".concat(e.length, \" input(s).\"));\n    var t = [];\n\n    for (var _n293 of e) {\n      null != _n293 && null !== _n293[0] && t.push(_n293[0]);\n    }\n\n    if (t = unique$2(t), t.length > 1) throw new ValueError(\"Can not merge tensors with different batch sizes. Got tensors with shapes: \".concat(JSON.stringify(e), \".\"));\n    var n = null == e[0] ? null : e[0].slice(1);\n\n    for (var _t524 = 1; _t524 < e.length; ++_t524) {\n      var _r239 = null == e[_t524] ? null : e[_t524].slice(1);\n\n      n = this.computeElementwiseOpOutputShape(n, _r239);\n    }\n\n    var r = e.map(e => e.length);\n    this.reshapeRequired = -1 !== e.indexOf(null) || 1 !== unique$2(r).length;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      if (e = e, this.reshapeRequired) {\n        var _t525 = [],\n            n = e.map(e => e.rank);\n\n        if (-1 === n.indexOf(null)) {\n          var r = max$2(n);\n\n          for (var _n294 of e) {\n            var _e742 = _n294.rank;\n\n            for (var _t526 = 0; _t526 < r - _e742; ++_t526) {\n              _n294 = expandDims$2(_n294, 1);\n            }\n\n            _t525.push(_n294);\n          }\n\n          return this.mergeFunction(_t525);\n        }\n\n        {\n          var _n295 = !1;\n\n          for (var _r241 of e) {\n            var _e743 = _r241.rank;\n\n            if (null == _e743) {\n              var _e744 = _r241.shape,\n                  _a156 = _e744[0],\n                  s = _e744.slice(1).concat([_a156]);\n\n              var o = reshape$3(_r241, [_a156].concat(arrayProd(_e744.slice(1))));\n              o = transpose$2(o, [1, 0]), o = reshape$3(o, s), _t525.push(o), _n295 = !0;\n            } else if (_e743 > 1) {\n              var _a157 = range$3(1, _e743).concat([0]);\n\n              _t525.push(transpose$2(_r241, _a157)), _n295 = !0;\n            } else _t525.push(_r241);\n          }\n\n          var _r240 = this.mergeFunction(_t525);\n\n          var a = _r240.rank;\n          if (_n295) if (null == a) {\n            var _e745 = _r240.shape,\n                _t527 = _e745[_e745.length - 1],\n                _n296 = [_t527].concat(_e745.slice(0, _e745.length - 1));\n\n            _r240 = reshape$3(transpose$2(reshape$3(_r240, [-1, _t527]), [1, 0]), _n296);\n          } else if (a > 1) {\n            var _e746 = [a - 1].concat(range$3(0, a - 1));\n\n            _r240 = transpose$2(_r240, _e746);\n          }\n          return _r240;\n        }\n      }\n\n      return this.mergeFunction(e);\n    });\n  }\n\n  computeOutputShape(e) {\n    var t;\n    t = null == (e = e)[0] ? null : e[0].slice(1);\n\n    for (var _n297 = 1; _n297 < e.length; ++_n297) {\n      var r = null == e[_n297] ? null : e[_n297].slice(1);\n      t = this.computeElementwiseOpOutputShape(t, r);\n    }\n\n    var n = [];\n\n    for (var _t528 of e) {\n      null != _t528 && null !== _t528[0] && n.push(_t528[0]);\n    }\n\n    return n = unique$2(n), t = 1 === n.length ? n.concat(t) : [null].concat(t), t;\n  }\n\n  computeMask(e, t) {\n    return tidy(() => {\n      if (null == t) return null;\n      if (!Array.isArray(t)) throw new ValueError(\"`mask` should be an Array\");\n      if (!Array.isArray(e)) throw new ValueError(\"`inputs` should be an Array\");\n      if (t.length !== e.length) throw new ValueError(\"The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (\".concat(e.length, \" vs \").concat(t.length, \")\"));\n      if (t.every(e => null == e)) return null;\n      var n = (t = t.map(e => null == e ? e : expandDims$3(e, 0)))[0];\n\n      for (var _e747 = 1; _e747 < t.length - 1; ++_e747) {\n        n = logicalAnd$2(n, t[_e747]);\n      }\n\n      return n;\n    });\n  }\n\n}\n\nclass Add extends Merge {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy(() => {\n      var t = e[0].clone();\n\n      for (var n = 1; n < e.length; ++n) {\n        t = add$2(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nAdd.className = \"Add\", registerClass(Add);\n\nclass Multiply extends Merge {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy(() => {\n      var t = e[0].clone();\n\n      for (var n = 1; n < e.length; ++n) {\n        t = mul(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nMultiply.className = \"Multiply\", registerClass(Multiply);\n\nclass Average extends Merge {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy(() => {\n      var t = e[0].clone();\n\n      for (var n = 1; n < e.length; ++n) {\n        t = add$2(t, e[n]);\n      }\n\n      return mul(1 / e.length, t);\n    });\n  }\n\n}\n\nAverage.className = \"Average\", registerClass(Average);\n\nclass Maximum extends Merge {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy(() => {\n      var t = e[0];\n\n      for (var n = 1; n < e.length; ++n) {\n        t = maximum$3(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nMaximum.className = \"Maximum\", registerClass(Maximum);\n\nclass Minimum extends Merge {\n  constructor(e) {\n    super(e);\n  }\n\n  mergeFunction(e) {\n    return tidy(() => {\n      var t = e[0];\n\n      for (var n = 1; n < e.length; ++n) {\n        t = minimum$3(t, e[n]);\n      }\n\n      return t;\n    });\n  }\n\n}\n\nMinimum.className = \"Minimum\", registerClass(Minimum);\n\nclass Concatenate extends Merge {\n  constructor(e) {\n    super(e), this.DEFAULT_AXIS = -1, null == e && (e = {}), this.axis = null == e.axis ? this.DEFAULT_AXIS : e.axis, this.supportsMasking = !0, this.reshapeRequired = !1;\n  }\n\n  build(e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0]) || 1 === e.length) throw new ValueError(\"A `Concatenate` layer should be called on a list of at least 2 inputs\");\n    e = e;\n    var t = !0;\n\n    for (var _n298 of e) {\n      if (null != _n298) {\n        t = !1;\n        break;\n      }\n    }\n\n    if (t) return;\n    var n = [];\n\n    for (var _t529 = 0; _t529 < e.length; ++_t529) {\n      var r = e[_t529].slice();\n\n      r.splice(this.axis, 1);\n      var a = !1;\n\n      for (var _e748 of n) {\n        if (arraysEqual(_e748, r)) {\n          a = !0;\n          break;\n        }\n      }\n\n      a || n.push(r);\n    }\n\n    if (n.length > 1) throw new ValueError(\"A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: \" + JSON.stringify(e));\n  }\n\n  mergeFunction(e) {\n    return tidy(() => concatenate$1(e, this.axis));\n  }\n\n  computeOutputShape(e) {\n    if (!Array.isArray(e) || !Array.isArray(e[0])) throw new ValueError(\"A `Concatenate` layer should be called on a list of inputs.\");\n    var t = e,\n        n = t[0].slice(),\n        r = this.axis < 0 ? n.length + this.axis : this.axis;\n\n    for (var _e749 of t.slice(1)) {\n      if (null == n[r] || null == _e749[r]) {\n        n[r] = null;\n        break;\n      }\n\n      n[r] += _e749[r];\n    }\n\n    return n;\n  }\n\n  computeMask(e, t) {\n    if (null == t) return null;\n    if (!Array.isArray(t)) throw new ValueError(\"`mask` should be an array for Concatenate\");\n    if (!Array.isArray(e)) throw new ValueError(\"`inputs` should be an array for Concatenate\");\n    if (t.length !== e.length) throw new ValueError(\"Mismatch in the length of mask (\".concat(t.length, \") and the legnth of inputs (\").concat(e.length, \")\"));\n    return tidy(() => {\n      var n = !0;\n      if (t.forEach(e => {\n        null == e || (n = !1);\n      }), n) return null;\n      var r = [];\n\n      for (var _n299 = 0; _n299 < e.length; ++_n299) {\n        r.push(null == t[_n299] ? cast$3(onesLike$2(e[_n299]), \"bool\") : t[_n299].rank < e[_n299].rank ? expandDims$3(t[_n299], -1) : t[_n299]);\n      }\n\n      var a = concat$2(r, this.axis);\n      return all$2(a, -1, !1);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction interpretAxis(e, t) {\n  for (; e < 0;) {\n    e += t;\n  }\n\n  return e;\n}\n\nfunction batchDot(e, t, n) {\n  if (e.shape.length > 3 || t.shape.length > 3) throw new NotImplementedError(\"batchDot is not implemented for tensors of 4D or higher rank yet\");\n  if (assert$4(e.shape.length >= 2, () => \"batchDot requires the rank of x to be >= 2, but got \".concat(e.shape.length)), assert$4(e.shape.length >= 2, () => \"batchDot requires the rank of y to be >= 2, but got \".concat(t.shape.length)), \"number\" == typeof n && (n = [n, n]), \"complex64\" === e.dtype || \"complex64\" === t.dtype) throw new NotImplementedError(\"batchDot is not implemented for complex64-type Tensors yet.\");\n  var r = e.shape.length,\n      a = t.shape.length;\n  null == n && (n = [r - 1, a - 2]);\n  var s = n;\n  return tidy(() => {\n    var n, o;\n\n    if (r > a) {\n      n = r - a;\n      var _e750 = [];\n\n      for (var _t530 = 0; _t530 < n; ++_t530) {\n        _e750.push(1);\n      }\n\n      t = reshape$3(t, t.shape.concat(_e750));\n    } else if (a > r) {\n      n = a - r;\n      var _t531 = [];\n\n      for (var _e751 = 0; _e751 < n; ++_e751) {\n        _t531.push(1);\n      }\n\n      e = reshape$3(e, e.shape.concat(_t531));\n    } else n = 0;\n\n    if (o = 2 === e.shape.length && 2 === t.shape.length ? s[0] === s[1] ? sum$2(mul(e, t), s[0]) : sum$2(mul(transpose$2(e, [1, 0]), t), s[1]) : matMul$1(e, t, s[0] !== e.shape.length - 1, s[1] === t.shape.length - 1), n > 0) {\n      var _e752;\n\n      _e752 = r > a ? r + a - 3 : r - 1;\n      var _t532 = [];\n\n      for (var _r242 = _e752; _r242 < _e752 + n; ++_r242) {\n        _t532.push(_r242);\n      }\n\n      o = squeeze(o, _t532);\n    }\n\n    return 1 === o.shape.length && (o = expandDims$3(o, 1)), o;\n  });\n}\n\nConcatenate.className = \"Concatenate\", registerClass(Concatenate);\n\nclass Dot extends Merge {\n  constructor(e) {\n    super(e), this.axes = e.axes, this.normalize = null != e.normalize && e.normalize, this.supportsMasking = !0, this.reshapeRequired = !1;\n  }\n\n  build(e) {\n    assert$4(Array.isArray(e) && 2 === e.length && Array.isArray(e[0]) && Array.isArray(e[1]), () => \"A `Dot` layer should be called on a list of exactly 2 inputs.\");\n    var t = e[0],\n        n = e[1];\n    if (t.length > 3 || n.length > 3) throw new NotImplementedError(\"Dot layer does not support tensors of 4D or higher rank yet.\");\n    var r = this.interpretAxes(t, n);\n    if (t[r[0]] !== n[r[1]]) throw new ValueError(\"Dimension incompatibility: \".concat(t[r[0]], \" !== \").concat(n[r[1]]));\n  }\n\n  mergeFunction(e) {\n    if (2 !== e.length) throw new ValueError(\"A `Dot` layer must be called on exactly 2 inputs, but received \".concat(e.length, \" input(s).\"));\n    var t,\n        n = e[0],\n        r = e[1];\n    return t = Array.isArray(this.axes) ? this.axes.map((t, n) => interpretAxis(t, e[n].shape.length)) : [interpretAxis(this.axes, n.shape.length), interpretAxis(this.axes, r.shape.length)], this.normalize && (n = l2Normalize(n, t[0]), r = l2Normalize(r, t[1])), batchDot(n, r, t);\n  }\n\n  interpretAxes(e, t) {\n    var n;\n    return n = Array.isArray(this.axes) ? this.axes : [interpretAxis(this.axes, e.length), interpretAxis(this.axes, t.length)], n;\n  }\n\n  computeOutputShape(e) {\n    assert$4(Array.isArray(e) && 2 === e.length && Array.isArray(e[0]) && Array.isArray(e[1]), () => \"A `Dot` layer should be called on a list of exactly 2 inputs.\");\n    var t = e[0].slice(),\n        n = e[1].slice();\n    if (t.length > 3 || n.length > 3) throw new NotImplementedError(\"Dot layer does not support tensors of 4D or higher rank yet.\");\n    var r = this.interpretAxes(t, n);\n    t.splice(r[0], 1), n.splice(r[1], 1), n.splice(0, 1);\n    var a = t.concat(n);\n    return 1 === a.length && a.push(1), a;\n  }\n\n  computeMask(e, t) {\n    return null;\n  }\n\n  getConfig() {\n    var e = {\n      axes: this.axes,\n      normalize: this.normalize\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nDot.className = \"Dot\", registerClass(Dot);\n\nclass GaussianNoise extends Layer {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.stddev = e.stddev;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      stddev: this.stddev\n    };\n    return Object.assign(t, e), t;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e);\n      return inTrainPhase(() => add$2(randomNormal$1(n.shape, 0, this.stddev), n), () => n, t.training || !1);\n    });\n  }\n\n}\n\nGaussianNoise.className = \"GaussianNoise\", registerClass(GaussianNoise);\n\nclass GaussianDropout extends Layer {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.rate = e.rate;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      rate: this.rate\n    };\n    return Object.assign(t, e), t;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t);\n      var n = getExactlyOneTensor(e);\n      return this.rate > 0 && this.rate < 1 ? inTrainPhase(() => {\n        var e = Math.sqrt(this.rate / (1 - this.rate));\n        return mul(n, randomNormal$1(n.shape, 1, e));\n      }, () => n, t.training || !1) : n;\n    });\n  }\n\n}\n\nGaussianDropout.className = \"GaussianDropout\", registerClass(GaussianDropout);\n\nclass AlphaDropout extends Layer {\n  constructor(e) {\n    super(e), this.supportsMasking = !0, this.rate = e.rate, this.noiseShape = e.noiseShape;\n  }\n\n  _getNoiseShape(e) {\n    return this.noiseShape || getExactlyOneTensor(e).shape;\n  }\n\n  computeOutputShape(e) {\n    return e;\n  }\n\n  getConfig() {\n    var e = super.getConfig(),\n        t = {\n      rate: this.rate\n    };\n    return Object.assign(t, e), t;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        var n = this._getNoiseShape(e);\n\n        return inTrainPhase(() => {\n          var t = getExactlyOneTensor(e),\n              r = -1.7580993408473766;\n          var a = greaterEqual$2(randomUniform$1(n), this.rate);\n          a = cast$2(a, \"float32\");\n          var s = ((1 - this.rate) * (1 + this.rate * r ** 2)) ** -.5,\n              o = -s * r * this.rate,\n              i = add$2(mul(t, a), mul(add$2(a, -1), r));\n          return add$2(mul(i, s), o);\n        }, () => getExactlyOneTensor(e), t.training || !1);\n      }\n\n      return e;\n    });\n  }\n\n}\n\nfunction batchNormalization$1(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : .001;\n  var o;\n  if (2 === e.rank) o = batchNorm2d(e, t, n, r, a, s);else if (3 === e.rank) o = batchNorm3d(e, t, n, r, a, s);else {\n    if (4 !== e.rank) throw new NotImplementedError(\"batchNormalization is not implemented for array of rank \".concat(e.rank, \" yet\"));\n    o = batchNorm4d(e, t, n, r, a, s);\n  }\n  return o;\n}\n\nfunction regularNormalizeBatchInTraining(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .001;\n  return tidy(() => {\n    var s = moments(e, r),\n        o = s.mean,\n        i = s.variance;\n    return [batchNormalization$1(e, o, i, n, t, a), o, i];\n  });\n}\n\nfunction broadcastNormalizeBatchInTraining(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .001;\n  return tidy(() => {\n    var s = moments(e, r),\n        o = s.mean,\n        i = s.variance,\n        l = [];\n\n    for (var _t533 of range$3(0, e.rank)) {\n      -1 !== r.indexOf(_t533) ? l.push(1) : l.push(e.shape[_t533]);\n    }\n\n    var u = reshape$3(o, l),\n        c = reshape$3(i, l),\n        p = null == t ? null : reshape$3(t, l),\n        d = null == n ? null : reshape$3(n, l);\n    return [batchNormalization$1(e, u, c, d, p, a), o, i];\n  });\n}\n\nfunction normalizeBatchInTraining(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : .001;\n  return arraysEqual(r.slice().sort(), range$3(0, e.rank - 1)) ? regularNormalizeBatchInTraining(e, t, n, r, a) : broadcastNormalizeBatchInTraining(e, t, n, r, a);\n}\n\nAlphaDropout.className = \"AlphaDropout\", registerClass(AlphaDropout);\n\nclass BatchNormalization extends Layer {\n  constructor(e) {\n    null == e && (e = {}), super(e), this.supportsMasking = !0, this.axis = null == e.axis ? -1 : e.axis, this.momentum = null == e.momentum ? .99 : e.momentum, this.epsilon = null == e.epsilon ? .001 : e.epsilon, this.center = null == e.center || e.center, this.scale = null == e.scale || e.scale, this.betaInitializer = getInitializer(e.betaInitializer || \"zeros\"), this.gammaInitializer = getInitializer(e.gammaInitializer || \"ones\"), this.movingMeanInitializer = getInitializer(e.movingMeanInitializer || \"zeros\"), this.movingVarianceInitializer = getInitializer(e.movingVarianceInitializer || \"ones\"), this.betaConstraint = getConstraint(e.betaConstraint), this.gammaConstraint = getConstraint(e.gammaConstraint), this.betaRegularizer = getRegularizer(e.betaRegularizer), this.gammaRegularizer = getRegularizer(e.gammaRegularizer);\n  }\n\n  build(e) {\n    e = getExactlyOneShape(e);\n    var t = this.axis >= 0 ? this.axis : this.axis + e.length,\n        n = e[t];\n    if (null == n) throw new ValueError(\"Axis \".concat(t, \" of input tensor should have a defined dimension but the layer received an input with shape \").concat(JSON.stringify(e), \".\"));\n    this.inputSpec = [new InputSpec({\n      ndim: e.length,\n      axes: {\n        [t]: n\n      }\n    })];\n    var r = [n];\n    this.scale && (this.gamma = this.addWeight(\"gamma\", r, null, this.gammaInitializer, this.gammaRegularizer, !0, this.gammaConstraint)), this.center && (this.beta = this.addWeight(\"beta\", r, null, this.betaInitializer, this.betaRegularizer, !0, this.betaConstraint)), this.movingMean = this.addWeight(\"moving_mean\", r, null, this.movingMeanInitializer, null, !1), this.movingVariance = this.addWeight(\"moving_variance\", r, null, this.movingVarianceInitializer, null, !1), this.built = !0;\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var n = null != t.training && t.training,\n          r = getExactlyOneTensor(e),\n          a = r.shape,\n          s = a.length,\n          o = range$3(0, s),\n          i = this.axis >= 0 ? this.axis : this.axis + s;\n      o.splice(i, 1);\n      var l = pyListRepeat(1, s);\n      l[i] = a[i];\n      var u = o.slice();\n      u.sort();\n      var c = !arraysEqual(u, range$3(0, s).slice(0, s - 1));\n      if (!n) return (() => {\n        if (c) {\n          var _e753 = reshape$3(this.movingMean.read(), l),\n              _t534 = reshape$3(this.movingVariance.read(), l),\n              _n300 = this.center ? reshape$3(this.beta.read(), l) : null,\n              _a158 = this.scale ? reshape$3(this.gamma.read(), l) : null;\n\n          return batchNormalization$1(r, _e753, _t534, _n300, _a158, this.epsilon);\n        }\n\n        return batchNormalization$1(r, this.movingMean.read(), this.movingVariance.read(), null == this.beta ? null : this.beta.read(), null == this.gamma ? null : this.gamma.read(), this.epsilon);\n      })();\n\n      var [p, d, h] = normalizeBatchInTraining(r, this.gamma.read(), this.beta.read(), o, this.epsilon),\n          m = (e, t, n) => {\n        tidy(() => {\n          var r = 1 - n,\n              a = e.read(),\n              s = mul(sub$2(a, t), r);\n          e.write(sub$2(a, s));\n        });\n      };\n\n      return (() => {\n        m(this.movingMean, d, this.momentum), m(this.movingVariance, h, this.momentum);\n      })(), p;\n    });\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nBatchNormalization.className = \"BatchNormalization\", registerClass(BatchNormalization);\n\nclass LayerNormalization extends Layer {\n  constructor(e) {\n    if (null == e && (e = {}), super(e), this.axis = null == e.axis ? -1 : e.axis, \"number\" == typeof this.axis) {\n      if (!Number.isInteger(this.axis)) throw new Error(\"Expected axis to be an integer, but received \".concat(this.axis));\n    } else {\n      if (!Array.isArray(this.axis)) throw new Error(\"Expected axis to be an integer or an array of integers, but received \".concat(JSON.stringify(this.axis)));\n\n      for (var _e754 of this.axis) {\n        if (!Number.isInteger(_e754)) throw new Error(\"Expected axis to be an array of integers, but received \".concat(JSON.stringify(this.axis)));\n      }\n    }\n\n    this.epsilon = null == e.epsilon ? .001 : e.epsilon, this.center = null == e.center || e.center, this.scale = null == e.scale || e.scale, this.betaInitializer = getInitializer(e.betaInitializer || \"zeros\"), this.gammaInitializer = getInitializer(e.gammaInitializer || \"ones\"), this.betaRegularizer = getRegularizer(e.betaRegularizer), this.gammaRegularizer = getRegularizer(e.gammaRegularizer), this.supportsMasking = !0;\n  }\n\n  build(e) {\n    var t = (e = getExactlyOneShape(e)).length;\n    \"number\" == typeof this.axis && (this.axis = [this.axis]);\n\n    for (var _e755 = 0; _e755 < this.axis.length; ++_e755) {\n      this.axis[_e755] < 0 && (this.axis[_e755] += t);\n    }\n\n    for (var _e756 of this.axis) {\n      if (_e756 < 0 || _e756 >= t) throw new Error(\"Invalid axis: \".concat(_e756));\n    }\n\n    if (this.axis.length !== unique$2(this.axis).length) throw new Error(\"Found duplicate axes in: \".concat(this.axis));\n    var n = this.axis.map(t => e[t]);\n    this.gamma = this.scale ? this.addWeight(\"gamma\", n, \"float32\", this.gammaInitializer, this.gammaRegularizer, !0) : null, this.beta = this.center ? this.addWeight(\"beta\", n, \"float32\", this.betaInitializer, this.betaRegularizer, !0) : null, this.built = !0;\n  }\n\n  call(e, t) {\n    var n = getExactlyOneTensor(e),\n        r = n.shape,\n        a = r.length;\n    return tidy(() => {\n      var {\n        mean: e,\n        variance: t\n      } = moments(n, this.axis, !0);\n      var s = pyListRepeat(1, a);\n\n      for (var _e757 of this.axis) {\n        s[_e757] = r[_e757];\n      }\n\n      var o = e => null != e && e.shape.length !== a && this.axis !== [a - 1] ? reshape$3(e, s) : e;\n\n      var i = o(this.gamma.read()),\n          l = o(this.beta.read());\n      var u = [],\n          c = [];\n\n      for (var _e758 = 0; _e758 < a; ++_e758) {\n        -1 !== this.axis.indexOf(_e758) ? (u.push(r[_e758]), c.push(1)) : (u.push(1), c.push(r[_e758]));\n      }\n\n      return e = tile$3(e, u), t = tile$3(t, u), i = tile$3(i, c), l = tile$3(l, c), batchNormalization$1(n, e, t, l, i, this.epsilon);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction spatial2dPadding(e, t, n) {\n  return tidy(() => {\n    if (4 !== e.rank) throw new ValueError(\"temporalPadding expects input tensor to be 4-D, but received a \".concat(e.rank, \"-D tensor.\"));\n    if (null == t && (t = [[1, 1], [1, 1]]), 2 !== t.length || 2 !== t[0].length || 2 !== t[1].length) throw new ValueError(\"spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.\");\n    if (null == n && (n = imageDataFormat()), \"channelsLast\" !== n && \"channelsFirst\" !== n) throw new ValueError(\"Unknown data format: \".concat(n, \". Supported data formats are 'channelsLast' and 'channelsFirst.\"));\n    var r;\n    return r = \"channelsFirst\" === n ? [[0, 0], [0, 0], t[0], t[1]] : [[0, 0], t[0], t[1], [0, 0]], pad(e, r);\n  });\n}\n\nLayerNormalization.className = \"LayerNormalization\", registerClass(LayerNormalization);\n\nclass ZeroPadding2D extends Layer {\n  constructor(e) {\n    if (null == e && (e = {}), super(e), this.dataFormat = null == e.dataFormat ? imageDataFormat() : e.dataFormat, null == e.padding) this.padding = [[1, 1], [1, 1]];else if (\"number\" == typeof e.padding) this.padding = [[e.padding, e.padding], [e.padding, e.padding]];else {\n      if (e.padding = e.padding, 2 !== e.padding.length) throw new ValueError(\"ZeroPadding2D expects padding to be a length-2 array, but received a length-\".concat(e.padding.length, \" array.\"));\n      var t, n;\n      if (\"number\" == typeof e.padding[0]) t = [e.padding[0], e.padding[0]], n = [e.padding[1], e.padding[1]];else {\n        if (e.padding = e.padding, 2 !== e.padding[0].length) throw new ValueError(\"ZeroPadding2D expects height padding to be a length-2 array, but received a length-\".concat(e.padding[0].length, \" array.\"));\n        if (t = e.padding[0], 2 !== e.padding[1].length) throw new ValueError(\"ZeroPadding2D expects width padding to be a length-2 array, but received a length-\".concat(e.padding[1].length, \" array.\"));\n        n = e.padding[1];\n      }\n      this.padding = [t, n];\n    }\n    this.inputSpec = [new InputSpec({\n      ndim: 4\n    })];\n  }\n\n  computeOutputShape(e) {\n    var t, n;\n    return e = getExactlyOneShape(e), \"channelsFirst\" === this.dataFormat ? (t = null != e[2] && e[2] >= 0 ? e[2] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[3] && e[3] >= 0 ? e[3] + this.padding[1][0] + this.padding[1][1] : null, [e[0], e[1], t, n]) : (t = null != e[1] && e[1] >= 0 ? e[1] + this.padding[0][0] + this.padding[0][1] : null, n = null != e[2] && e[2] >= 0 ? e[2] + this.padding[1][0] + this.padding[1][1] : null, [e[0], t, n, e[3]]);\n  }\n\n  call(e, t) {\n    return tidy(() => spatial2dPadding(getExactlyOneTensor(e), this.padding, this.dataFormat));\n  }\n\n  getConfig() {\n    var e = {\n      padding: this.padding,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nfunction pool2d(e, t, n, r, a, s) {\n  return tidy(() => {\n    var o;\n    checkDataFormat(a), checkPoolMode(s), checkPaddingMode(r), null == n && (n = [1, 1]), null == r && (r = \"valid\"), null == a && (a = imageDataFormat()), null == s && (s = \"max\"), e = preprocessConv2DInput(e, a);\n    var i = \"same\" === r ? \"same\" : \"valid\";\n    return o = \"max\" === s ? maxPool$2(e, t, n, i) : avgPool$2(e, t, n, i), \"channelsFirst\" === a && (o = transpose$2(o, [0, 3, 1, 2])), o;\n  });\n}\n\nfunction pool3d$1(e, t, n, r, a, s) {\n  return tidy(() => {\n    var o;\n    checkDataFormat(a), checkPoolMode(s), checkPaddingMode(r), null == n && (n = [1, 1, 1]), null == r && (r = \"valid\"), null == a && (a = imageDataFormat()), null == s && (s = \"max\"), e = preprocessConv3DInput(e, a);\n    var i = \"same\" === r ? \"same\" : \"valid\";\n    return o = \"max\" === s ? maxPool3d$1(e, t, n, i) : avgPool3d$1(e, t, n, i), \"channelsFirst\" === a && (o = transpose$2(o, [0, 4, 1, 2, 3])), o;\n  });\n}\n\nZeroPadding2D.className = \"ZeroPadding2D\", registerClass(ZeroPadding2D);\n\nclass Pooling1D extends Layer {\n  constructor(e) {\n    if (null == e.poolSize && (e.poolSize = 2), super(e), \"number\" == typeof e.poolSize) this.poolSize = [e.poolSize];else {\n      if (!Array.isArray(e.poolSize) || 1 !== e.poolSize.length || \"number\" != typeof e.poolSize[0]) throw new ValueError(\"poolSize for 1D convolutional layer must be a number or an Array of a single number, but received \".concat(JSON.stringify(e.poolSize)));\n      this.poolSize = e.poolSize;\n    }\n    if (assertPositiveInteger(this.poolSize, \"poolSize\"), null == e.strides) this.strides = this.poolSize;else if (\"number\" == typeof e.strides) this.strides = [e.strides];else {\n      if (!Array.isArray(e.strides) || 1 !== e.strides.length || \"number\" != typeof e.strides[0]) throw new ValueError(\"strides for 1D convolutional layer must be a number or an Array of a single number, but received \".concat(JSON.stringify(e.strides)));\n      this.strides = e.strides;\n    }\n    assertPositiveInteger(this.strides, \"strides\"), this.padding = null == e.padding ? \"valid\" : e.padding, checkPaddingMode(this.padding), this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n  }\n\n  computeOutputShape(e) {\n    var t = convOutputLength((e = getExactlyOneShape(e))[1], this.poolSize[0], this.padding, this.strides[0]);\n    return [e[0], t, e[2]];\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      this.invokeCallHook(e, t), e = expandDims$2(getExactlyOneTensor(e), 2);\n      var n = this.poolingFunction(getExactlyOneTensor(e), [this.poolSize[0], 1], [this.strides[0], 1], this.padding, \"channelsLast\");\n      return squeeze(n, [2]);\n    });\n  }\n\n  getConfig() {\n    var e = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass MaxPooling1D extends Pooling1D {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat(a), checkPaddingMode(r), pool2d(e, t, n, r, a, \"max\");\n  }\n\n}\n\nMaxPooling1D.className = \"MaxPooling1D\", registerClass(MaxPooling1D);\n\nclass AveragePooling1D extends Pooling1D {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat(a), checkPaddingMode(r), pool2d(e, t, n, r, a, \"avg\");\n  }\n\n}\n\nAveragePooling1D.className = \"AveragePooling1D\", registerClass(AveragePooling1D);\n\nclass Pooling2D extends Layer {\n  constructor(e) {\n    if (null == e.poolSize && (e.poolSize = [2, 2]), super(e), this.poolSize = Array.isArray(e.poolSize) ? e.poolSize : [e.poolSize, e.poolSize], null == e.strides) this.strides = this.poolSize;else if (Array.isArray(e.strides)) {\n      if (2 !== e.strides.length) throw new ValueError(\"If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length \".concat(e.strides.length, \".\"));\n      this.strides = e.strides;\n    } else this.strides = [e.strides, e.strides];\n    assertPositiveInteger(this.poolSize, \"poolSize\"), assertPositiveInteger(this.strides, \"strides\"), this.padding = null == e.padding ? \"valid\" : e.padding, this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat(this.dataFormat), checkPaddingMode(this.padding), this.inputSpec = [new InputSpec({\n      ndim: 4\n    })];\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2];\n    return t = convOutputLength(t, this.poolSize[0], this.padding, this.strides[0]), n = convOutputLength(n, this.poolSize[1], this.padding, this.strides[1]), \"channelsFirst\" === this.dataFormat ? [e[0], e[1], t, n] : [e[0], t, n, e[3]];\n  }\n\n  call(e, t) {\n    return tidy(() => (this.invokeCallHook(e, t), this.poolingFunction(getExactlyOneTensor(e), this.poolSize, this.strides, this.padding, this.dataFormat)));\n  }\n\n  getConfig() {\n    var e = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass MaxPooling2D extends Pooling2D {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat(a), checkPaddingMode(r), pool2d(e, t, n, r, a, \"max\");\n  }\n\n}\n\nMaxPooling2D.className = \"MaxPooling2D\", registerClass(MaxPooling2D);\n\nclass AveragePooling2D extends Pooling2D {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat(a), checkPaddingMode(r), pool2d(e, t, n, r, a, \"avg\");\n  }\n\n}\n\nAveragePooling2D.className = \"AveragePooling2D\", registerClass(AveragePooling2D);\n\nclass Pooling3D extends Layer {\n  constructor(e) {\n    if (null == e.poolSize && (e.poolSize = [2, 2, 2]), super(e), this.poolSize = Array.isArray(e.poolSize) ? e.poolSize : [e.poolSize, e.poolSize, e.poolSize], null == e.strides) this.strides = this.poolSize;else if (Array.isArray(e.strides)) {\n      if (3 !== e.strides.length) throw new ValueError(\"If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length \".concat(e.strides.length, \".\"));\n      this.strides = e.strides;\n    } else this.strides = [e.strides, e.strides, e.strides];\n    assertPositiveInteger(this.poolSize, \"poolSize\"), assertPositiveInteger(this.strides, \"strides\"), this.padding = null == e.padding ? \"valid\" : e.padding, this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat(this.dataFormat), checkPaddingMode(this.padding), this.inputSpec = [new InputSpec({\n      ndim: 5\n    })];\n  }\n\n  computeOutputShape(e) {\n    e = getExactlyOneShape(e);\n    var t = \"channelsFirst\" === this.dataFormat ? e[2] : e[1],\n        n = \"channelsFirst\" === this.dataFormat ? e[3] : e[2],\n        r = \"channelsFirst\" === this.dataFormat ? e[4] : e[3];\n    return t = convOutputLength(t, this.poolSize[0], this.padding, this.strides[0]), n = convOutputLength(n, this.poolSize[1], this.padding, this.strides[1]), r = convOutputLength(r, this.poolSize[2], this.padding, this.strides[2]), \"channelsFirst\" === this.dataFormat ? [e[0], e[1], t, n, r] : [e[0], t, n, r, e[4]];\n  }\n\n  call(e, t) {\n    return tidy(() => (this.invokeCallHook(e, t), this.poolingFunction(getExactlyOneTensor(e), this.poolSize, this.strides, this.padding, this.dataFormat)));\n  }\n\n  getConfig() {\n    var e = {\n      poolSize: this.poolSize,\n      padding: this.padding,\n      strides: this.strides,\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass MaxPooling3D extends Pooling3D {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat(a), checkPaddingMode(r), pool3d$1(e, t, n, r, a, \"max\");\n  }\n\n}\n\nMaxPooling3D.className = \"MaxPooling3D\", registerClass(MaxPooling3D);\n\nclass AveragePooling3D extends Pooling3D {\n  constructor(e) {\n    super(e);\n  }\n\n  poolingFunction(e, t, n, r, a) {\n    return checkDataFormat(a), checkPaddingMode(r), pool3d$1(e, t, n, r, a, \"avg\");\n  }\n\n}\n\nAveragePooling3D.className = \"AveragePooling3D\", registerClass(AveragePooling3D);\n\nclass GlobalPooling1D extends Layer {\n  constructor(e) {\n    super(e), this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n  }\n\n  computeOutputShape(e) {\n    return [e[0], e[2]];\n  }\n\n  call(e, t) {\n    throw new NotImplementedError();\n  }\n\n}\n\nclass GlobalAveragePooling1D extends GlobalPooling1D {\n  constructor(e) {\n    super(e || {});\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t = getExactlyOneTensor(e);\n      return mean$1(t, 1);\n    });\n  }\n\n}\n\nGlobalAveragePooling1D.className = \"GlobalAveragePooling1D\", registerClass(GlobalAveragePooling1D);\n\nclass GlobalMaxPooling1D extends GlobalPooling1D {\n  constructor(e) {\n    super(e || {});\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var t = getExactlyOneTensor(e);\n      return max$3(t, 1);\n    });\n  }\n\n}\n\nGlobalMaxPooling1D.className = \"GlobalMaxPooling1D\", registerClass(GlobalMaxPooling1D);\n\nclass GlobalPooling2D extends Layer {\n  constructor(e) {\n    super(e), this.dataFormat = null == e.dataFormat ? \"channelsLast\" : e.dataFormat, checkDataFormat(this.dataFormat), this.inputSpec = [new InputSpec({\n      ndim: 4\n    })];\n  }\n\n  computeOutputShape(e) {\n    return e = e, \"channelsLast\" === this.dataFormat ? [e[0], e[3]] : [e[0], e[1]];\n  }\n\n  call(e, t) {\n    throw new NotImplementedError();\n  }\n\n  getConfig() {\n    var e = {\n      dataFormat: this.dataFormat\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n}\n\nclass GlobalAveragePooling2D extends GlobalPooling2D {\n  call(e, t) {\n    return tidy(() => {\n      var t = getExactlyOneTensor(e);\n      return mean$1(t, \"channelsLast\" === this.dataFormat ? [1, 2] : [2, 3]);\n    });\n  }\n\n}\n\nGlobalAveragePooling2D.className = \"GlobalAveragePooling2D\", registerClass(GlobalAveragePooling2D);\n\nclass GlobalMaxPooling2D extends GlobalPooling2D {\n  call(e, t) {\n    return tidy(() => {\n      var t = getExactlyOneTensor(e);\n      return max$3(t, \"channelsLast\" === this.dataFormat ? [1, 2] : [2, 3]);\n    });\n  }\n\n}\n\nGlobalMaxPooling2D.className = \"GlobalMaxPooling2D\", registerClass(GlobalMaxPooling2D);\n\nclass Wrapper extends Layer {\n  constructor(e) {\n    super(e), this.layer = e.layer;\n  }\n\n  build(e) {\n    this.built = !0;\n  }\n\n  get trainable() {\n    return null != this.layer && this.layer.trainable;\n  }\n\n  set trainable(e) {\n    null != this.layer && (this.layer.trainable = e);\n  }\n\n  get trainableWeights() {\n    return this.layer.trainableWeights;\n  }\n\n  get nonTrainableWeights() {\n    return this.layer.nonTrainableWeights;\n  }\n\n  get updates() {\n    return this.layer._updates;\n  }\n\n  get losses() {\n    return this.layer.losses;\n  }\n\n  getWeights() {\n    return this.layer.getWeights();\n  }\n\n  setWeights(e) {\n    this.layer.setWeights(e);\n  }\n\n  getConfig() {\n    var e = {\n      layer: {\n        className: this.layer.getClassName(),\n        config: this.layer.getConfig()\n      }\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    super.setFastWeightInitDuringBuild(e), null != this.layer && this.layer.setFastWeightInitDuringBuild(e);\n  }\n\n  static fromConfig(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = deserialize(t.layer, n);\n    delete t.layer;\n    var a = {\n      layer: r\n    };\n    return Object.assign(a, t), new e(a);\n  }\n\n}\n\nclass TimeDistributed extends Wrapper {\n  constructor(e) {\n    super(e), this.supportsMasking = !0;\n  }\n\n  build(e) {\n    if ((e = getExactlyOneShape(e)).length < 3) throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received input shape \".concat(JSON.stringify(e)));\n    this.inputSpec = [{\n      shape: e\n    }];\n    var t = [e[0]].concat(e.slice(2));\n    this.layer.built || (this.layer.build(t), this.layer.built = !0), super.build(e);\n  }\n\n  computeOutputShape(e) {\n    var t = [(e = getExactlyOneShape(e))[0]].concat(e.slice(2)),\n        n = this.layer.computeOutputShape(t);\n    return [n[0], e[1]].concat(n.slice(1));\n  }\n\n  call(e, t) {\n    return tidy(() => rnn$1((e, n) => [getExactlyOneTensor(this.layer.call(e, t)), []], e = getExactlyOneTensor(e), [], !1, null, null, !1, !0)[1]);\n  }\n\n}\n\nfunction checkBidirectionalMergeMode(e) {\n  checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, \"BidirectionalMergeMode\", e);\n}\n\nTimeDistributed.className = \"TimeDistributed\", registerClass(TimeDistributed);\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE = \"concat\";\n\nclass Bidirectional extends Wrapper {\n  constructor(e) {\n    super(e);\n    var t = e.layer.getConfig(),\n        n = {};\n    n.className = e.layer.getClassName(), n.config = t, this.forwardLayer = deserialize(n), t.goBackwards = !0 !== t.goBackwards;\n    var r = {};\n    if (r.className = e.layer.getClassName(), r.config = t, this.backwardLayer = deserialize(r), this.forwardLayer.name = \"forward_\" + this.forwardLayer.name, this.backwardLayer.name = \"backward_\" + this.backwardLayer.name, this.mergeMode = void 0 === e.mergeMode ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : e.mergeMode, checkBidirectionalMergeMode(this.mergeMode), e.weights) throw new NotImplementedError(\"weights support is not implemented for Bidirectional layer yet.\");\n    this._stateful = e.layer.stateful, this.returnSequences = e.layer.returnSequences, this.returnState = e.layer.returnState, this.supportsMasking = !0, this._trainable = !0, this.inputSpec = e.layer.inputSpec, this.numConstants = null;\n  }\n\n  get trainable() {\n    return this._trainable;\n  }\n\n  set trainable(e) {\n    this._trainable = e, null != this.forwardLayer && (this.forwardLayer.trainable = e), null != this.backwardLayer && (this.backwardLayer.trainable = e);\n  }\n\n  getWeights() {\n    return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n  }\n\n  setWeights(e) {\n    var t = Math.floor(e.length / 2);\n    this.forwardLayer.setWeights(e.slice(0, t)), this.backwardLayer.setWeights(e.slice(t));\n  }\n\n  computeOutputShape(e) {\n    var t,\n        n,\n        r,\n        a = this.forwardLayer.computeOutputShape(e);\n    return Array.isArray(a) && Array.isArray(a[0]) || (a = [a]), a = a, this.returnState ? (r = a.slice(1), t = a[0]) : t = a[0], t = t, \"concat\" === this.mergeMode ? (t[t.length - 1] *= 2, n = [t]) : n = null == this.mergeMode ? [t, t.slice()] : [t], this.returnState ? null == this.mergeMode ? n.concat(r).concat(r.slice()) : [t].concat(r).concat(r.slice()) : singletonOrArray(n);\n  }\n\n  apply(e, t) {\n    var n = null == t ? null : t.initialState,\n        r = null == t ? null : t.constants;\n    null == t && (t = {});\n    var a = standardizeArgs(e, n, r, this.numConstants);\n    if (e = a.inputs, n = a.initialState, r = a.constants, Array.isArray(e) && (n = e.slice(1), e = e[0]), (null == n || 0 === n.length) && null == r) return super.apply(e, t);\n    var s = [],\n        o = [];\n\n    if (null != n) {\n      var _e759 = n.length;\n      if (_e759 % 2 > 0) throw new ValueError(\"When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.\");\n      t.initialState = n, s.push(...n);\n\n      var _r243 = n.map(e => new InputSpec({\n        shape: e.shape\n      }));\n\n      this.forwardLayer.stateSpec = _r243.slice(0, _e759 / 2), this.backwardLayer.stateSpec = _r243.slice(_e759 / 2), o.push(..._r243);\n    }\n\n    if (null != r) throw new NotImplementedError(\"Support for constants in Bidirectional layers is not implemented yet.\");\n    var i = s[0] instanceof SymbolicTensor;\n\n    for (var _e760 of s) {\n      if (_e760 instanceof SymbolicTensor !== i) throw new ValueError(\"The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors\");\n    }\n\n    if (i) {\n      var _n301 = [e].concat(s),\n          _r244 = this.inputSpec.concat(o),\n          _a159 = this.inputSpec;\n\n      this.inputSpec = _r244;\n\n      var _i46 = super.apply(_n301, t);\n\n      return this.inputSpec = _a159, _i46;\n    }\n\n    return super.apply(e, t);\n  }\n\n  call(e, t) {\n    return tidy(() => {\n      var n = t.initialState;\n      var r, a, s, o;\n      if (null == n) r = this.forwardLayer.call(e, t), a = this.backwardLayer.call(e, t);else {\n        var _s116 = n.slice(0, n.length / 2),\n            _o76 = n.slice(n.length / 2);\n\n        r = this.forwardLayer.call(e, Object.assign(t, {\n          initialState: _s116\n        })), a = this.backwardLayer.call(e, Object.assign(t, {\n          initialState: _o76\n        }));\n      }\n      return this.returnState && (Array.isArray(r) && (s = r.slice(1).concat(a.slice(1))), r = r[0], a = a[0]), this.returnSequences && (a = reverse$2(a, 1)), \"concat\" === this.mergeMode ? o = concatenate$1([r, a]) : \"sum\" === this.mergeMode ? o = add$2(r, a) : \"ave\" === this.mergeMode ? o = mul(.5, add$2(r, a)) : \"mul\" === this.mergeMode ? o = mul(r, a) : null == this.mergeMode && (o = [r, a]), this.returnState ? null == this.mergeMode ? o.concat(s) : [o].concat(s) : o;\n    });\n  }\n\n  resetStates(e) {\n    this.forwardLayer.resetStates(), this.backwardLayer.resetStates();\n  }\n\n  build(e) {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(e);\n    }), nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(e);\n    }), this.built = !0;\n  }\n\n  computeMask(e, t) {\n    var n;\n\n    if (Array.isArray(t) && (t = t[0]), n = this.returnSequences ? null == this.mergeMode ? [t, t] : t : null == this.mergeMode ? [null, null] : null, this.returnState) {\n      var _e761 = this.forwardLayer.states.map(e => null);\n\n      return Array.isArray(n) ? n.concat(_e761).concat(_e761) : [n].concat(_e761).concat(_e761);\n    }\n\n    return n;\n  }\n\n  get trainableWeights() {\n    return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n  }\n\n  get nonTrainableWeights() {\n    return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n  }\n\n  setFastWeightInitDuringBuild(e) {\n    super.setFastWeightInitDuringBuild(e), null != this.forwardLayer && this.forwardLayer.setFastWeightInitDuringBuild(e), null != this.backwardLayer && this.backwardLayer.setFastWeightInitDuringBuild(e);\n  }\n\n  getConfig() {\n    var e = {\n      mergeMode: this.mergeMode\n    },\n        t = super.getConfig();\n    return Object.assign(e, t), e;\n  }\n\n  static fromConfig(e, t) {\n    var n = deserialize(t.layer);\n    if (delete t.layer, null != t.numConstants) throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants present is not supported yet.\");\n    var r = t;\n    return r.layer = n, new e(r);\n  }\n\n}\n\nfunction inputLayer(e) {\n  return new InputLayer(e);\n}\n\nfunction elu$2(e) {\n  return new ELU$3(e);\n}\n\nfunction reLU(e) {\n  return new ReLU(e);\n}\n\nfunction leakyReLU(e) {\n  return new LeakyReLU(e);\n}\n\nfunction prelu$2(e) {\n  return new PReLU(e);\n}\n\nfunction softmax$2(e) {\n  return new Softmax(e);\n}\n\nfunction thresholdedReLU(e) {\n  return new ThresholdedReLU(e);\n}\n\nfunction conv1d(e) {\n  return new Conv1D(e);\n}\n\nfunction conv2d$1(e) {\n  return new Conv2D(e);\n}\n\nfunction conv2dTranspose(e) {\n  return new Conv2DTranspose(e);\n}\n\nfunction conv3d(e) {\n  return new Conv3D(e);\n}\n\nfunction conv3dTranspose(e) {\n  return new Conv3DTranspose(e);\n}\n\nfunction separableConv2d(e) {\n  return new SeparableConv2D(e);\n}\n\nfunction cropping2D(e) {\n  return new Cropping2D(e);\n}\n\nfunction upSampling2d(e) {\n  return new UpSampling2D(e);\n}\n\nfunction depthwiseConv2d(e) {\n  return new DepthwiseConv2D(e);\n}\n\nfunction activation(e) {\n  return new Activation(e);\n}\n\nfunction dense(e) {\n  return new Dense(e);\n}\n\nfunction dropout(e) {\n  return new Dropout(e);\n}\n\nfunction spatialDropout1d(e) {\n  return new SpatialDropout1D(e);\n}\n\nfunction flatten$1(e) {\n  return new Flatten(e);\n}\n\nfunction repeatVector(e) {\n  return new RepeatVector(e);\n}\n\nfunction reshape$2(e) {\n  return new Reshape(e);\n}\n\nfunction permute(e) {\n  return new Permute(e);\n}\n\nfunction embedding(e) {\n  return new Embedding(e);\n}\n\nfunction add$1(e) {\n  return new Add(e);\n}\n\nfunction average(e) {\n  return new Average(e);\n}\n\nfunction concatenate(e) {\n  return new Concatenate(e);\n}\n\nfunction maximum$2(e) {\n  return new Maximum(e);\n}\n\nfunction minimum$2(e) {\n  return new Minimum(e);\n}\n\nfunction multiply$2(e) {\n  return new Multiply(e);\n}\n\nfunction dot(e) {\n  return new Dot(e);\n}\n\nfunction batchNormalization(e) {\n  return new BatchNormalization(e);\n}\n\nfunction layerNormalization(e) {\n  return new LayerNormalization(e);\n}\n\nfunction zeroPadding2d(e) {\n  return new ZeroPadding2D(e);\n}\n\nfunction averagePooling1d(e) {\n  return new AveragePooling1D(e);\n}\n\nfunction avgPool1d(e) {\n  return averagePooling1d(e);\n}\n\nfunction avgPooling1d(e) {\n  return averagePooling1d(e);\n}\n\nfunction averagePooling2d(e) {\n  return new AveragePooling2D(e);\n}\n\nfunction avgPool2d(e) {\n  return averagePooling2d(e);\n}\n\nfunction avgPooling2d(e) {\n  return averagePooling2d(e);\n}\n\nfunction averagePooling3d(e) {\n  return new AveragePooling3D(e);\n}\n\nfunction avgPool3d(e) {\n  return averagePooling3d(e);\n}\n\nfunction avgPooling3d(e) {\n  return averagePooling3d(e);\n}\n\nfunction globalAveragePooling1d(e) {\n  return new GlobalAveragePooling1D(e);\n}\n\nfunction globalAveragePooling2d(e) {\n  return new GlobalAveragePooling2D(e);\n}\n\nfunction globalMaxPooling1d(e) {\n  return new GlobalMaxPooling1D(e);\n}\n\nfunction globalMaxPooling2d(e) {\n  return new GlobalMaxPooling2D(e);\n}\n\nfunction maxPooling1d(e) {\n  return new MaxPooling1D(e);\n}\n\nfunction maxPooling2d(e) {\n  return new MaxPooling2D(e);\n}\n\nfunction maxPooling3d(e) {\n  return new MaxPooling3D(e);\n}\n\nfunction gru(e) {\n  return new GRU(e);\n}\n\nfunction gruCell(e) {\n  return new GRUCell(e);\n}\n\nfunction lstm(e) {\n  return new LSTM(e);\n}\n\nfunction lstmCell(e) {\n  return new LSTMCell(e);\n}\n\nfunction simpleRNN(e) {\n  return new SimpleRNN(e);\n}\n\nfunction simpleRNNCell(e) {\n  return new SimpleRNNCell(e);\n}\n\nfunction convLstm2d(e) {\n  return new ConvLSTM2D(e);\n}\n\nfunction convLstm2dCell(e) {\n  return new ConvLSTM2DCell(e);\n}\n\nfunction rnn(e) {\n  return new RNN(e);\n}\n\nfunction stackedRNNCells(e) {\n  return new StackedRNNCells(e);\n}\n\nfunction bidirectional(e) {\n  return new Bidirectional(e);\n}\n\nfunction timeDistributed(e) {\n  return new TimeDistributed(e);\n}\n\nBidirectional.className = \"Bidirectional\", registerClass(Bidirectional);\nvar globalMaxPool1d = globalMaxPooling1d,\n    globalMaxPool2d = globalMaxPooling2d,\n    maxPool1d = maxPooling1d,\n    maxPool2d = maxPooling2d;\n\nfunction gaussianNoise(e) {\n  return new GaussianNoise(e);\n}\n\nfunction gaussianDropout(e) {\n  return new GaussianDropout(e);\n}\n\nfunction alphaDropout(e) {\n  return new AlphaDropout(e);\n}\n\nfunction masking(e) {\n  return new Masking(e);\n}\n\nvar exports_layers = {\n  __proto__: null,\n  inputLayer,\n  elu: elu$2,\n  reLU,\n  leakyReLU,\n  prelu: prelu$2,\n  softmax: softmax$2,\n  thresholdedReLU,\n  conv1d,\n  conv2d: conv2d$1,\n  conv2dTranspose,\n  conv3d,\n  conv3dTranspose,\n  separableConv2d,\n  cropping2D,\n  upSampling2d,\n  depthwiseConv2d,\n  activation,\n  dense,\n  dropout,\n  spatialDropout1d,\n  flatten: flatten$1,\n  repeatVector,\n  reshape: reshape$2,\n  permute,\n  embedding,\n  add: add$1,\n  average,\n  concatenate,\n  maximum: maximum$2,\n  minimum: minimum$2,\n  multiply: multiply$2,\n  dot,\n  batchNormalization,\n  layerNormalization,\n  zeroPadding2d,\n  averagePooling1d,\n  avgPool1d,\n  avgPooling1d,\n  averagePooling2d,\n  avgPool2d,\n  avgPooling2d,\n  averagePooling3d,\n  avgPool3d,\n  avgPooling3d,\n  globalAveragePooling1d,\n  globalAveragePooling2d,\n  globalMaxPooling1d,\n  globalMaxPooling2d,\n  maxPooling1d,\n  maxPooling2d,\n  maxPooling3d,\n  gru,\n  gruCell,\n  lstm,\n  lstmCell,\n  simpleRNN,\n  simpleRNNCell,\n  convLstm2d,\n  convLstm2dCell,\n  rnn,\n  stackedRNNCells,\n  bidirectional,\n  timeDistributed,\n  globalMaxPool1d,\n  globalMaxPool2d,\n  maxPool1d,\n  maxPool2d,\n  Layer,\n  RNN,\n  RNNCell,\n  input,\n  gaussianNoise,\n  gaussianDropout,\n  alphaDropout,\n  masking\n};\n\nfunction binaryAccuracy(e, t) {\n  return binaryAccuracy$1(e, t);\n}\n\nfunction binaryCrossentropy(e, t) {\n  return binaryCrossentropy$1(e, t);\n}\n\nfunction sparseCategoricalAccuracy(e, t) {\n  return sparseCategoricalAccuracy$1(e, t);\n}\n\nfunction categoricalAccuracy(e, t) {\n  return categoricalAccuracy$1(e, t);\n}\n\nfunction categoricalCrossentropy(e, t) {\n  return categoricalCrossentropy$1(e, t);\n}\n\nfunction precision(e, t) {\n  return precision$1(e, t);\n}\n\nfunction recall(e, t) {\n  return recall$1(e, t);\n}\n\nfunction cosineProximity(e, t) {\n  return cosineProximity$1(e, t);\n}\n\nfunction meanAbsoluteError(e, t) {\n  return meanAbsoluteError$1(e, t);\n}\n\nfunction meanAbsolutePercentageError(e, t) {\n  return meanAbsolutePercentageError$1(e, t);\n}\n\nfunction MAPE(e, t) {\n  return meanAbsolutePercentageError$1(e, t);\n}\n\nfunction mape(e, t) {\n  return meanAbsolutePercentageError$1(e, t);\n}\n\nfunction meanSquaredError(e, t) {\n  return meanSquaredError$1(e, t);\n}\n\nfunction MSE(e, t) {\n  return meanSquaredError$1(e, t);\n}\n\nfunction mse(e, t) {\n  return meanSquaredError$1(e, t);\n}\n\nvar exports_metrics = {\n  __proto__: null,\n  binaryAccuracy,\n  binaryCrossentropy,\n  sparseCategoricalAccuracy,\n  categoricalAccuracy,\n  categoricalCrossentropy,\n  precision,\n  recall,\n  cosineProximity,\n  meanAbsoluteError,\n  meanAbsolutePercentageError,\n  MAPE,\n  mape,\n  meanSquaredError,\n  MSE,\n  mse\n},\n    exports_models = {\n  __proto__: null,\n  modelFromJSON\n};\n\nfunction l1l2(e) {\n  return new L1L2(e);\n}\n\nfunction l1(e) {\n  return l1$1(e);\n}\n\nfunction l2(e) {\n  return l2$1(e);\n}\n\nvar exports_regularizers = {\n  __proto__: null,\n  l1l2,\n  l1,\n  l2\n};\n\nclass Callback extends BaseCallback {\n  constructor() {\n    super(...arguments), this.model = null;\n  }\n\n  setModel(e) {\n    if (!(e instanceof LayersModel)) throw new Error(\"model must be a LayersModel, not some other Container\");\n    this.model = e;\n  }\n\n}\n\nfunction less$2(e, t) {\n  return e < t;\n}\n\nfunction greater$2(e, t) {\n  return e > t;\n}\n\nclass EarlyStopping extends Callback {\n  constructor(e) {\n    if (super(), null == e && (e = {}), e.restoreBestWeights) throw new NotImplementedError(\"restoreBestWeights = True is not implemented in EarlyStopping yet.\");\n    this.monitor = e.monitor || \"val_loss\", this.minDelta = Math.abs(e.minDelta || 0), this.patience = e.patience || 0, this.verbose = e.verbose || 0, this.mode = e.mode || \"auto\", this.baseline = e.baseline, -1 === [\"auto\", \"min\", \"max\"].indexOf(this.mode) && (console.warn(\"EarlyStopping mode '\".concat(this.mode, \"' is invalid. Falling back to mode 'auto'.\")), this.mode = \"auto\"), this.monitorFunc = \"min\" === this.mode ? less$2 : \"max\" === this.mode || -1 !== this.monitor.indexOf(\"acc\") ? greater$2 : less$2, this.monitorFunc === less$2 && (this.minDelta *= -1);\n  }\n\n  onTrainBegin(e) {\n    var _this147 = this;\n\n    return _asyncToGenerator(function* () {\n      _this147.wait = 0, _this147.stoppedEpoch = 0, _this147.best = null != _this147.baseline ? _this147.baseline : _this147.monitorFunc === less$2 ? Infinity : -Infinity;\n    })();\n  }\n\n  onEpochEnd(e, t) {\n    var _this148 = this;\n\n    return _asyncToGenerator(function* () {\n      yield resolveScalarsInLogs(t);\n\n      var n = _this148.getMonitorValue(t);\n\n      null != n && (_this148.monitorFunc(n - _this148.minDelta, _this148.best) ? (_this148.best = n, _this148.wait = 0) : (_this148.wait++, _this148.wait >= _this148.patience && (_this148.stoppedEpoch = e, _this148.model.stopTraining = !0)));\n    })();\n  }\n\n  onTrainEnd(e) {\n    var _this149 = this;\n\n    return _asyncToGenerator(function* () {\n      _this149.stoppedEpoch > 0 && _this149.verbose && console.log(\"Epoch \".concat(_this149.stoppedEpoch, \": early stopping.\"));\n    })();\n  }\n\n  getMonitorValue(e) {\n    null == e && (e = {});\n    var t = e[this.monitor];\n    return null == t && console.warn(\"Metric for EarlyStopping \".concat(this.monitor, \" is not available. Available metrics are: \").concat(Object.keys(e))), t;\n  }\n\n}\n\nfunction earlyStopping(e) {\n  return new EarlyStopping(e);\n}\n\nvar callbacks = {\n  earlyStopping\n};\nvar DataType, SaverDef;\n!function (e) {\n  e[e.DT_INVALID = 0] = \"DT_INVALID\", e[e.DT_FLOAT = 1] = \"DT_FLOAT\", e[e.DT_DOUBLE = 2] = \"DT_DOUBLE\", e[e.DT_INT32 = 3] = \"DT_INT32\", e[e.DT_UINT8 = 4] = \"DT_UINT8\", e[e.DT_INT16 = 5] = \"DT_INT16\", e[e.DT_INT8 = 6] = \"DT_INT8\", e[e.DT_STRING = 7] = \"DT_STRING\", e[e.DT_COMPLEX64 = 8] = \"DT_COMPLEX64\", e[e.DT_INT64 = 9] = \"DT_INT64\", e[e.DT_BOOL = 10] = \"DT_BOOL\", e[e.DT_QINT8 = 11] = \"DT_QINT8\", e[e.DT_QUINT8 = 12] = \"DT_QUINT8\", e[e.DT_QINT32 = 13] = \"DT_QINT32\", e[e.DT_BFLOAT16 = 14] = \"DT_BFLOAT16\", e[e.DT_FLOAT_REF = 101] = \"DT_FLOAT_REF\", e[e.DT_DOUBLE_REF = 102] = \"DT_DOUBLE_REF\", e[e.DT_INT32_REF = 103] = \"DT_INT32_REF\", e[e.DT_UINT8_REF = 104] = \"DT_UINT8_REF\", e[e.DT_INT16_REF = 105] = \"DT_INT16_REF\", e[e.DT_INT8_REF = 106] = \"DT_INT8_REF\", e[e.DT_STRING_REF = 107] = \"DT_STRING_REF\", e[e.DT_COMPLEX64_REF = 108] = \"DT_COMPLEX64_REF\", e[e.DT_INT64_REF = 109] = \"DT_INT64_REF\", e[e.DT_BOOL_REF = 110] = \"DT_BOOL_REF\", e[e.DT_QINT8_REF = 111] = \"DT_QINT8_REF\", e[e.DT_QUINT8_REF = 112] = \"DT_QUINT8_REF\", e[e.DT_QINT32_REF = 113] = \"DT_QINT32_REF\", e[e.DT_BFLOAT16_REF = 114] = \"DT_BFLOAT16_REF\";\n}(DataType || (DataType = {})), function (e) {\n  var t;\n  (t = e.CheckpointFormatVersion || (e.CheckpointFormatVersion = {}))[t.LEGACY = 0] = \"LEGACY\", t[t.V1 = 1] = \"V1\", t[t.V2 = 2] = \"V2\";\n}(SaverDef || (SaverDef = {}));\nvar CUSTOM_OPS = {};\n\nfunction registerOp(e, t) {\n  CUSTOM_OPS[e] = {\n    tfOpName: e,\n    category: \"custom\",\n    inputs: [],\n    attrs: [],\n    customExecutor: t\n  };\n}\n\nfunction getRegisteredOp(e) {\n  return CUSTOM_OPS[e];\n}\n\nfunction deregisterOp(e) {\n  delete CUSTOM_OPS[e];\n}\n\nfunction getParamValue(e, t, n, r, a) {\n  var s = t.inputParams[e];\n\n  if (s && void 0 !== s.inputIndexStart) {\n    var _e762 = s.inputIndexStart,\n        _o77 = 0 === s.inputIndexEnd ? void 0 : void 0 === s.inputIndexEnd ? _e762 + 1 : s.inputIndexEnd;\n\n    if (\"tensor\" === s.type) return getTensor(t.inputNames[s.inputIndexStart], n, r, a);\n    if (\"tensors\" === s.type) return t.inputNames.slice(_e762, _o77).map(e => getTensor(e, n, r, a));\n    var i = getTensor(t.inputNames.slice(_e762)[0], n, r, a),\n        l = i.dataSync();\n    return \"number\" === s.type ? l[0] : toNestedArray(i.shape, l);\n  }\n\n  var o = t.attrParams[e];\n  return o && o.value;\n}\n\nfunction getTensor(e, t, n, r) {\n  var [a, s] = parseNodeName(e);\n\n  if (null != r) {\n    var _e763 = r.getHashTableHandleByName(a);\n\n    if (null != _e763) return _e763;\n  }\n\n  var o = n.currentContextIds.find(e => !!t[getNodeNameWithContextId(a, e)]);\n  return void 0 !== o ? t[getNodeNameWithContextId(a, o)][s] : void 0;\n}\n\nfunction getTensorsForCurrentContenxt(e, t, n) {\n  return t[getNodeNameWithContextId(e, n.currentContextId)];\n}\n\nfunction getNodeNameAndIndex(e, t) {\n  var [n, r, a] = parseNodeName(e);\n  return [getNodeNameWithContextId(n, t && t.currentContextId), r, a];\n}\n\nfunction getNodeNameWithContextId(e, t) {\n  return t ? \"\".concat(e, \"-\").concat(t) : e;\n}\n\nfunction parseNodeName(e) {\n  var t = e.split(\":\");\n  if (1 === t.length) return [e, 0, void 0];\n  var n = 3 === t.length ? t[1] : void 0;\n  return [t[0], Number(t[t.length - 1]), n];\n}\n\nfunction getPadding(e, t, n) {\n  var r = getParamValue(\"pad\", e, t, n);\n\n  if (\"explicit\" === r) {\n    r = getParamValue(\"explicitPaddings\", e, t, n);\n    var a = [[0, 0], [0, 0], [0, 0], [0, 0]];\n\n    for (var _e764 = 0; _e764 < 4; _e764++) {\n      a[_e764][0] = r[2 * _e764], a[_e764][1] = r[2 * _e764 + 1];\n    }\n\n    return a;\n  }\n\n  return r;\n}\n\nfunction cloneTensor(e) {\n  return e.kept ? e : clone(e);\n}\n\nvar json$i = [{\n  tfOpName: \"Add\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"AddV2\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"AddN\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"BiasAdd\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sub\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"RealDiv\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Div\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"DivNoNan\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FloorDiv\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Mul\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Maximum\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Minimum\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Pow\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"SquaredDifference\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Mod\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FloorMod\",\n  category: \"arithmetic\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}];\nvar arithmetic = {\n  __proto__: null,\n  json: json$i\n};\nvar json$h = [{\n  tfOpName: \"Abs\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Acos\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Asin\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Atan\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Atan2\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"y\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Ceil\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ClipByValue\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"clipValueMin\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"clipValueMax\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Complex\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"real\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"imag\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ComplexAbs\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Cos\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Cosh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Elu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Exp\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Floor\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Log\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Imag\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"outputType\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Neg\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Real\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"outputType\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Prelu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"alpha\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Relu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Relu6\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Selu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sigmoid\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sin\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sinh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sqrt\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Rsqrt\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Square\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Tan\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Tanh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Sign\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Round\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Expm1\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Log1p\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Reciprocal\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Softplus\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Asinh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Acosh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Atanh\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Erf\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Prod\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axes\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\",\n    notSupported: !0\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LeakyRelu\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"alpha\",\n    name: \"alpha\",\n    type: \"number\",\n    defaultValue: .2\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"IsNan\",\n  category: \"basic_math\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}];\nvar basicMath = {\n  __proto__: null,\n  json: json$h\n};\nvar json$g = [{\n  tfOpName: \"EmptyTensorList\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"elementShape\",\n    type: \"shape\"\n  }, {\n    start: 1,\n    name: \"maxNumElements\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"LoopCond\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"pred\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Switch\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"data\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"pred\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Merge\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"Enter\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"frame_name\",\n    name: \"frameName\",\n    type: \"string\"\n  }, {\n    tfName: \"is_constant\",\n    name: \"isConstant\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Exit\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"NextIteration\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArrayV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"size\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"element_shape\",\n    name: \"elementShape\",\n    type: \"shape\"\n  }, {\n    tfName: \"dynamic_size\",\n    name: \"dynamicSize\",\n    type: \"bool\"\n  }, {\n    tfName: \"clear_after_read\",\n    name: \"clearAfterRead\",\n    type: \"bool\"\n  }, {\n    tfName: \"identical_element_shapes\",\n    name: \"identicalElementShapes\",\n    type: \"bool\"\n  }, {\n    tfName: \"tensor_array_name\",\n    name: \"name\",\n    type: \"string\"\n  }]\n}, {\n  tfOpName: \"TensorArrayWriteV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"index\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArrayReadV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"index\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArrayGatherV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"element_shape\",\n    name: \"elementShape\",\n    type: \"shape\"\n  }]\n}, {\n  tfOpName: \"TensorArrayScatterV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorArrayConcatV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"element_shape_except0\",\n    name: \"elementShapeExcept0\",\n    type: \"shape\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"TensorArraySplitV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"lengths\",\n    type: \"number[]\"\n  }, {\n    start: 3,\n    name: \"flowIn\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorArraySizeV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"flowIn\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"TensorArrayCloseV3\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorArrayId\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"StatelessIf\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"cond\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"then_branch\",\n    name: \"thenBranch\",\n    type: \"func\"\n  }, {\n    tfName: \"else_branch\",\n    name: \"elseBranch\",\n    type: \"func\"\n  }]\n}, {\n  tfOpName: \"If\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"cond\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"then_branch\",\n    name: \"thenBranch\",\n    type: \"func\"\n  }, {\n    tfName: \"else_branch\",\n    name: \"elseBranch\",\n    type: \"func\"\n  }]\n}, {\n  tfOpName: \"StatelessWhile\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"cond\",\n    name: \"cond\",\n    type: \"func\"\n  }, {\n    tfName: \"body\",\n    name: \"body\",\n    type: \"func\"\n  }]\n}, {\n  tfOpName: \"While\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"cond\",\n    name: \"cond\",\n    type: \"func\"\n  }, {\n    tfName: \"body\",\n    name: \"body\",\n    type: \"func\"\n  }]\n}, {\n  tfOpName: \"TensorListScatter\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"elementShape\",\n    type: \"shape\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListScatterV2\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"elementShape\",\n    type: \"shape\"\n  }, {\n    start: 3,\n    name: \"numElements\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListGather\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorListId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"elementShape\",\n    type: \"shape\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListGetItem\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorListId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"index\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"elementShape\",\n    type: \"shape\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListSetItem\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorListId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"index\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListReserve\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"elementShape\",\n    type: \"shape\"\n  }, {\n    start: 1,\n    name: \"numElements\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListFromTensor\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"elementShape\",\n    type: \"shape\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListStack\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorListId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"elementShape\",\n    type: \"shape\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }, {\n    tfName: \"num_elements\",\n    name: \"numElements\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListSplit\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"elementShape\",\n    type: \"shape\"\n  }, {\n    start: 2,\n    name: \"lengths\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListConcat\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorListId\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"element_shape\",\n    name: \"elementShape\",\n    type: \"shape\"\n  }, {\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListPopBack\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorListId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"elementShape\",\n    type: \"shape\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TensorListPushBack\",\n  category: \"control\",\n  inputs: [{\n    start: 0,\n    name: \"tensorListId\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"element_dtype\",\n    name: \"elementDType\",\n    type: \"dtype\"\n  }]\n}];\nvar control = {\n  __proto__: null,\n  json: json$g\n};\nvar json$f = [{\n  tfOpName: \"AvgPool\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"MaxPool\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: [],\n    notSupported: !0\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"MaxPoolWithArgmax\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"include_batch_in_index\",\n    name: \"includeBatchInIndex\",\n    type: \"bool\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"AvgPool3D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"MaxPool3D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"ksize\",\n    name: \"kernelSize\",\n    type: \"number[]\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Conv1D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"stride\",\n    name: \"stride\",\n    type: \"number\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NWC\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"dilation\",\n    name: \"dilation\",\n    type: \"number\",\n    defaultValue: 1\n  }]\n}, {\n  tfOpName: \"Conv2D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"useCudnnOnGpu\",\n    name: \"useCudnnOnGpu\",\n    type: \"bool\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: []\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"_FusedConv2D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"num_args\",\n    name: \"numArgs\",\n    type: \"number\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: []\n  }, {\n    tfName: \"use_cudnn_on_gpu\",\n    name: \"useCudnnOnGpu\",\n    type: \"bool\",\n    defaultValue: !0\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\",\n    defaultValue: [1, 1, 1, 1]\n  }, {\n    tfName: \"fused_ops\",\n    name: \"fusedOps\",\n    type: \"string[]\",\n    defaultValue: []\n  }, {\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: 1e-4\n  }, {\n    tfName: \"leakyrelu_alpha\",\n    name: \"leakyreluAlpha\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Conv2DBackpropInput\",\n  category: \"convolution\",\n  inputs: [{\n    start: 2,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }, {\n    start: 0,\n    name: \"outputShape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: []\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"DepthwiseConv2d\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"input\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: []\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"DepthwiseConv2dNative\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"input\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: []\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"FusedDepthwiseConv2dNative\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"num_args\",\n    name: \"numArgs\",\n    type: \"number\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\",\n    defaultValue: [1, 1, 1, 1]\n  }, {\n    tfName: \"fused_ops\",\n    name: \"fusedOps\",\n    type: \"string[]\",\n    defaultValue: []\n  }, {\n    tfName: \"explicit_paddings\",\n    name: \"explicitPaddings\",\n    type: \"number[]\",\n    defaultValue: []\n  }]\n}, {\n  tfOpName: \"Conv3D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    defaultValue: \"NHWC\"\n  }, {\n    tfName: \"dilations\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Dilation2D\",\n  category: \"convolution\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"filter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"strides\",\n    name: \"strides\",\n    type: \"number[]\"\n  }, {\n    tfName: \"rates\",\n    name: \"dilations\",\n    type: \"number[]\"\n  }, {\n    tfName: \"padding\",\n    name: \"pad\",\n    type: \"string\"\n  }]\n}];\nvar convolution = {\n  __proto__: null,\n  json: json$f\n};\nvar json$e = [{\n  tfOpName: \"Fill\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }, {\n    start: 1,\n    name: \"value\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"LinSpace\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"start\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"stop\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"num\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"OneHot\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"depth\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"onValue\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    start: 3,\n    name: \"offValue\",\n    type: \"number\",\n    defaultValue: 0\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    name: \"axis\",\n    type: \"number\",\n    notSupported: !0\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Ones\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"OnesLike\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"RandomUniform\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"minval\",\n    name: \"minval\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"maxval\",\n    name: \"maxval\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"seed\",\n    name: \"seed\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"seed2\",\n    name: \"seed2\",\n    type: \"number\",\n    defaultValue: 0,\n    notSupported: !0\n  }, {\n    tfName: \"T\",\n    name: \"T\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Range\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"start\",\n    type: \"number\"\n  }, {\n    start: 1,\n    name: \"stop\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"step\",\n    type: \"number\",\n    defaultValue: 0\n  }],\n  attrs: [{\n    tfName: \"Tidx\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"TruncatedNormal\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"means\",\n    name: \"mean\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"stddev\",\n    name: \"stdDev\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"seed\",\n    name: \"seed\",\n    type: \"number\"\n  }, {\n    tfName: \"seed2\",\n    name: \"seed2\",\n    type: \"number\",\n    defaultValue: 0,\n    notSupported: !0\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"T\",\n    name: \"T\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Zeros\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"ZerosLike\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"Multinomial\",\n  category: \"creation\",\n  inputs: [{\n    start: 0,\n    name: \"logits\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"numSamples\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"seed\",\n    name: \"seed\",\n    type: \"number\"\n  }, {\n    tfName: \"seed2\",\n    name: \"seed2\",\n    type: \"number\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }, {\n    tfName: \"output_dtype\",\n    name: \"output_dtype\",\n    type: \"dtype\"\n  }]\n}];\nvar creation = {\n  __proto__: null,\n  json: json$e\n};\nvar json$d = [{\n  tfOpName: \"NonMaxSuppressionV2\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scores\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"maxOutputSize\",\n    type: \"number\"\n  }, {\n    start: 3,\n    name: \"iouThreshold\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"NonMaxSuppressionV3\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scores\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"maxOutputSize\",\n    type: \"number\"\n  }, {\n    start: 3,\n    name: \"iouThreshold\",\n    type: \"number\"\n  }, {\n    start: 4,\n    name: \"scoreThreshold\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"NonMaxSuppressionV4\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scores\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"maxOutputSize\",\n    type: \"number\"\n  }, {\n    start: 3,\n    name: \"iouThreshold\",\n    type: \"number\"\n  }, {\n    start: 4,\n    name: \"scoreThreshold\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"T_threshold\",\n    name: \"threshold\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"pad_to_max_output_size\",\n    name: \"padToMaxOutputSize\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"NonMaxSuppressionV5\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scores\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"maxOutputSize\",\n    type: \"number\"\n  }, {\n    start: 3,\n    name: \"iouThreshold\",\n    type: \"number\"\n  }, {\n    start: 4,\n    name: \"scoreThreshold\",\n    type: \"number\"\n  }, {\n    start: 5,\n    name: \"softNmsSigma\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Where\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"condition\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ListDiff\",\n  category: \"dynamic\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"y\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}];\nvar dynamic = {\n  __proto__: null,\n  json: json$d\n};\nvar json$c = [{\n  tfOpName: \"TopKV2\",\n  category: \"evaluation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"k\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"sorted\",\n    name: \"sorted\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Unique\",\n  category: \"evaluation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"UniqueV2\",\n  category: \"evaluation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}];\nvar evaluation = {\n  __proto__: null,\n  json: json$c\n};\nvar json$b = [{\n  tfOpName: \"PlaceholderWithDefault\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"default\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"shape\",\n    name: \"shape\",\n    type: \"shape\"\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"Placeholder\",\n  category: \"graph\",\n  attrs: [{\n    tfName: \"shape\",\n    name: \"shape\",\n    type: \"shape\"\n  }, {\n    tfName: \"dtype\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"Const\",\n  category: \"graph\"\n}, {\n  tfOpName: \"Identity\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"IdentityN\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"x\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"Snapshot\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Rank\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Size\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"Shape\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"ShapeN\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"x\",\n    type: \"tensors\"\n  }]\n}, {\n  tfOpName: \"Print\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"data\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"message\",\n    name: \"message\",\n    type: \"string\"\n  }, {\n    tfName: \"first_n\",\n    name: \"firstN\",\n    type: \"number\",\n    notSupported: !0\n  }, {\n    tfName: \"summarize\",\n    name: \"summarize\",\n    type: \"number\",\n    defaultValue: 3\n  }]\n}, {\n  tfOpName: \"NoOp\",\n  category: \"graph\",\n  inputs: []\n}, {\n  tfOpName: \"StopGradient\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"FakeQuantWithMinMaxVars\",\n  category: \"graph\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"min\",\n    name: \"min\",\n    type: \"number\"\n  }, {\n    tfName: \"max\",\n    name: \"max\",\n    type: \"number\"\n  }]\n}];\nvar graph = {\n  __proto__: null,\n  json: json$b\n};\nvar json$a = [{\n  tfOpName: \"HashTable\",\n  category: \"hash_table\",\n  inputs: [],\n  attrs: [{\n    tfName: \"shared_name\",\n    name: \"sharedName\",\n    type: \"string\"\n  }, {\n    tfName: \"use_node_name_sharing\",\n    name: \"useNodeNameSharing\",\n    type: \"bool\"\n  }, {\n    tfName: \"key_dtype\",\n    name: \"keyDType\",\n    type: \"dtype\"\n  }, {\n    tfName: \"value_dtype\",\n    name: \"valueDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"HashTableV2\",\n  category: \"hash_table\",\n  inputs: [],\n  attrs: [{\n    tfName: \"shared_name\",\n    name: \"sharedName\",\n    type: \"string\"\n  }, {\n    tfName: \"use_node_name_sharing\",\n    name: \"useNodeNameSharing\",\n    type: \"bool\"\n  }, {\n    tfName: \"key_dtype\",\n    name: \"keyDType\",\n    type: \"dtype\"\n  }, {\n    tfName: \"value_dtype\",\n    name: \"valueDType\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"LookupTableImport\",\n  category: \"hash_table\",\n  inputs: [{\n    start: 0,\n    name: \"tableHandle\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"keys\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"values\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"Tin\",\n    name: \"tIn\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"tOut\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LookupTableImportV2\",\n  category: \"hash_table\",\n  inputs: [{\n    start: 0,\n    name: \"tableHandle\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"keys\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"values\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"Tin\",\n    name: \"tIn\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"tOut\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LookupTableFind\",\n  category: \"hash_table\",\n  inputs: [{\n    start: 0,\n    name: \"tableHandle\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"keys\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"defaultValue\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"Tin\",\n    name: \"tIn\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"tOut\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LookupTableFindV2\",\n  category: \"hash_table\",\n  inputs: [{\n    start: 0,\n    name: \"tableHandle\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"keys\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"defaultValue\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"Tin\",\n    name: \"tIn\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"Tout\",\n    name: \"tOut\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LookupTableSize\",\n  category: \"hash_table\",\n  inputs: [{\n    start: 0,\n    name: \"tableHandle\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"LookupTableSizeV2\",\n  category: \"hash_table\",\n  inputs: [{\n    start: 0,\n    name: \"tableHandle\",\n    type: \"tensor\"\n  }]\n}];\nvar hashTable = {\n  __proto__: null,\n  json: json$a\n};\nvar json$9 = [{\n  tfOpName: \"ResizeBilinear\",\n  category: \"image\",\n  inputs: [{\n    start: 0,\n    name: \"images\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"size\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"align_corners\",\n    name: \"alignCorners\",\n    type: \"bool\"\n  }, {\n    tfName: \"half_pixel_centers\",\n    name: \"halfPixelCenters\",\n    type: \"bool\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"ResizeNearestNeighbor\",\n  category: \"image\",\n  inputs: [{\n    start: 0,\n    name: \"images\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"size\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"align_corners\",\n    name: \"alignCorners\",\n    type: \"bool\"\n  }, {\n    tfName: \"half_pixel_centers\",\n    name: \"halfPixelCenters\",\n    type: \"bool\"\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"CropAndResize\",\n  category: \"image\",\n  inputs: [{\n    start: 0,\n    name: \"image\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"boxes\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"boxInd\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"cropSize\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"method\",\n    name: \"method\",\n    type: \"string\"\n  }, {\n    tfName: \"extrapolation_value\",\n    name: \"extrapolationValue\",\n    type: \"number\"\n  }]\n}];\nvar image = {\n  __proto__: null,\n  json: json$9\n};\nvar json$8 = [{\n  tfOpName: \"Equal\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"NotEqual\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Greater\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"GreaterEqual\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Less\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LessEqual\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LogicalAnd\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LogicalNot\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LogicalOr\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Select\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"condition\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"SelectV2\",\n  category: \"logical\",\n  inputs: [{\n    start: 0,\n    name: \"condition\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}];\nvar logical = {\n  __proto__: null,\n  json: json$8\n};\nvar json$7 = [{\n  tfOpName: \"_FusedMatMul\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    end: 0,\n    name: \"args\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"num_args\",\n    name: \"numArgs\",\n    type: \"number\"\n  }, {\n    tfName: \"fused_ops\",\n    name: \"fusedOps\",\n    type: \"string[]\",\n    defaultValue: []\n  }, {\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: 1e-4\n  }, {\n    tfName: \"transpose_a\",\n    name: \"transposeA\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"transpose_b\",\n    name: \"transposeB\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"MatMul\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"transpose_a\",\n    name: \"transposeA\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"transpose_b\",\n    name: \"transposeB\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"BatchMatMul\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"adj_x\",\n    name: \"transposeA\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"adj_y\",\n    name: \"transposeB\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"BatchMatMulV2\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"a\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"b\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"adj_x\",\n    name: \"transposeA\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"adj_y\",\n    name: \"transposeB\",\n    type: \"bool\",\n    defaultValue: !1\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Transpose\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"perm\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Einsum\",\n  category: \"matrices\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"equation\",\n    name: \"equation\",\n    type: \"string\"\n  }, {\n    tfName: \"N\",\n    name: \"n\",\n    type: \"number\",\n    defaultValue: 2\n  }, {\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}];\nvar matrices = {\n  __proto__: null,\n  json: json$7\n};\nvar json$6 = [{\n  tfOpName: \"FusedBatchNorm\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scale\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"offset\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"mean\",\n    type: \"tensor\"\n  }, {\n    start: 4,\n    name: \"variance\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: .001\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FusedBatchNormV2\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scale\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"offset\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"mean\",\n    type: \"tensor\"\n  }, {\n    start: 4,\n    name: \"variance\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: .001\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"FusedBatchNormV3\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"scale\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"offset\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"mean\",\n    type: \"tensor\"\n  }, {\n    start: 4,\n    name: \"variance\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"epsilon\",\n    name: \"epsilon\",\n    type: \"number\",\n    defaultValue: .001\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"LRN\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"depth_radius\",\n    name: \"radius\",\n    type: \"number\",\n    defaultValue: 5\n  }, {\n    tfName: \"bias\",\n    name: \"bias\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"alpha\",\n    name: \"alpha\",\n    type: \"number\",\n    defaultValue: 1\n  }, {\n    tfName: \"beta\",\n    name: \"beta\",\n    type: \"number\",\n    defaultValue: .5\n  }]\n}, {\n  tfOpName: \"Softmax\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"LogSoftmax\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"SparseToDense\",\n  category: \"normalization\",\n  inputs: [{\n    start: 0,\n    name: \"sparseIndices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"outputShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"sparseValues\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"defaultValue\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"validate_indices\",\n    name: \"validateIndices\",\n    type: \"bool\",\n    defaultValue: !0,\n    notSupported: !0\n  }]\n}];\nvar normalization = {\n  __proto__: null,\n  json: json$6\n};\nvar json$5 = [{\n  tfOpName: \"Bincount\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"size\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"weights\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"DenseBincount\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"size\",\n    type: \"number\"\n  }, {\n    start: 2,\n    name: \"weights\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"binary_output\",\n    name: \"binaryOutput\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Max\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Mean\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Min\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Sum\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"All\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Any\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"ArgMax\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"ArgMin\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"Prod\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"keep_dims\",\n    name: \"keepDims\",\n    type: \"bool\"\n  }]\n}, {\n  tfOpName: \"Cumsum\",\n  category: \"reduction\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"exclusive\",\n    name: \"exclusive\",\n    type: \"bool\"\n  }, {\n    tfName: \"reverse\",\n    name: \"reverse\",\n    type: \"bool\"\n  }]\n}];\nvar reduction = {\n  __proto__: null,\n  json: json$5\n};\nvar json$4 = [{\n  tfOpName: \"ConcatV2\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    end: -1,\n    name: \"tensors\",\n    type: \"tensors\"\n  }, {\n    start: -1,\n    name: \"axis\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"N\",\n    name: \"n\",\n    type: \"number\",\n    defaultValue: 2\n  }]\n}, {\n  tfOpName: \"Concat\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 1,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }, {\n    start: 0,\n    name: \"axis\",\n    type: \"number\"\n  }],\n  attrs: [{\n    tfName: \"N\",\n    name: \"n\",\n    type: \"number\",\n    defaultValue: 2\n  }]\n}, {\n  tfOpName: \"GatherV2\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }],\n  attrs: [{\n    tfName: \"batch_dims\",\n    name: \"batchDims\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Gather\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"validate_indices\",\n    name: \"validateIndices\",\n    type: \"bool\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Reverse\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"dims\",\n    type: \"bool[]\"\n  }]\n}, {\n  tfOpName: \"ReverseV2\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Slice\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"begin\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"size\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"StridedSlice\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"begin\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"end\",\n    type: \"number[]\"\n  }, {\n    start: 3,\n    name: \"strides\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"begin_mask\",\n    name: \"beginMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"end_mask\",\n    name: \"endMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"new_axis_mask\",\n    name: \"newAxisMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"ellipsis_mask\",\n    name: \"ellipsisMask\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"shrink_axis_mask\",\n    name: \"shrinkAxisMask\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Pack\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    end: 0,\n    name: \"tensors\",\n    type: \"tensors\"\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Unpack\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"tensor\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    tfName: \"num\",\n    name: \"num\",\n    type: \"number\",\n    defaultValue: 0,\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"Tile\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"reps\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Split\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }, {\n    start: 1,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"num_split\",\n    name: \"numOrSizeSplits\",\n    type: \"number\",\n    defaultValue: 1\n  }]\n}, {\n  tfOpName: \"SplitV\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"numOrSizeSplits\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"axis\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"ScatterNd\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"values\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"shape\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"GatherNd\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"SparseToDense\",\n  category: \"slice_join\",\n  inputs: [{\n    start: 0,\n    name: \"sparseIndices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"outputShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"sparseValues\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"defaultValue\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"validate_indices\",\n    name: \"validateIndices\",\n    type: \"bool\",\n    defaultValue: !1,\n    notSupported: !0\n  }]\n}];\nvar sliceJoin = {\n  __proto__: null,\n  json: json$4\n};\nvar json$3 = [{\n  tfOpName: \"SparseFillEmptyRows\",\n  category: \"sparse\",\n  inputs: [{\n    start: 0,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"values\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"denseShape\",\n    type: \"tensor\"\n  }, {\n    start: 3,\n    name: \"defaultValue\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"SparseReshape\",\n  category: \"sparse\",\n  inputs: [{\n    start: 0,\n    name: \"inputIndices\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"inputShape\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"newShape\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"T\",\n    name: \"dtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"SparseSegmentMean\",\n  category: \"sparse\",\n  inputs: [{\n    start: 0,\n    name: \"data\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"segmentIds\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"SparseSegmentSum\",\n  category: \"sparse\",\n  inputs: [{\n    start: 0,\n    name: \"data\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"indices\",\n    type: \"tensor\"\n  }, {\n    start: 2,\n    name: \"segmentIds\",\n    type: \"tensor\"\n  }]\n}];\nvar sparse = {\n  __proto__: null,\n  json: json$3\n};\nvar json$2 = [{\n  tfOpName: \"FFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"IFFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }]\n}, {\n  tfOpName: \"RFFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"fft_length\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}, {\n  tfOpName: \"IRFFT\",\n  category: \"spectral\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"fft_length\",\n    type: \"number\",\n    notSupported: !0\n  }]\n}];\nvar spectral = {\n  __proto__: null,\n  json: json$2\n};\nvar json$1 = [{\n  tfOpName: \"StringNGrams\",\n  category: \"string\",\n  inputs: [{\n    start: 0,\n    name: \"data\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"dataSplits\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"separator\",\n    name: \"separator\",\n    type: \"string\"\n  }, {\n    tfName: \"ngram_widths\",\n    name: \"nGramWidths\",\n    type: \"number[]\"\n  }, {\n    tfName: \"left_pad\",\n    name: \"leftPad\",\n    type: \"string\"\n  }, {\n    tfName: \"right_pad\",\n    name: \"rightPad\",\n    type: \"string\"\n  }, {\n    tfName: \"pad_width\",\n    name: \"padWidth\",\n    type: \"number\"\n  }, {\n    tfName: \"preserve_short_sequences\",\n    name: \"preserveShortSequences\",\n    type: \"bool\"\n  }],\n  outputs: [\"ngrams\", \"ngrams_splits\"]\n}, {\n  tfOpName: \"StringSplit\",\n  category: \"string\",\n  inputs: [{\n    start: 0,\n    name: \"input\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"delimiter\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"skip_empty\",\n    name: \"skipEmpty\",\n    type: \"bool\"\n  }],\n  outputs: [\"indices\", \"values\", \"shape\"]\n}, {\n  tfOpName: \"StringToHashBucketFast\",\n  category: \"string\",\n  inputs: [{\n    start: 0,\n    name: \"input\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"num_buckets\",\n    name: \"numBuckets\",\n    type: \"number\"\n  }]\n}];\nvar string = {\n  __proto__: null,\n  json: json$1\n};\nvar json = [{\n  tfOpName: \"Cast\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"SrcT\",\n    name: \"sdtype\",\n    type: \"dtype\",\n    notSupported: !0\n  }, {\n    tfName: \"DstT\",\n    name: \"dtype\",\n    type: \"dtype\"\n  }]\n}, {\n  tfOpName: \"ExpandDims\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"axis\",\n    type: \"number\"\n  }]\n}, {\n  tfOpName: \"MirrorPad\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"padding\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"mode\",\n    name: \"mode\",\n    type: \"string\"\n  }]\n}, {\n  tfOpName: \"Pad\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"padding\",\n    type: \"number[]\"\n  }],\n  attrs: [{\n    tfName: \"constant_value\",\n    name: \"constantValue\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"PadV2\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"padding\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"constantValue\",\n    type: \"number\",\n    defaultValue: 0\n  }]\n}, {\n  tfOpName: \"Reshape\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"shape\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"Squeeze\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"axis\",\n    tfDeprecatedName: \"squeeze_dims\",\n    name: \"axis\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"SpaceToBatchND\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"blockShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"paddings\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"BatchToSpaceND\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"blockShape\",\n    type: \"number[]\"\n  }, {\n    start: 2,\n    name: \"crops\",\n    type: \"number[]\"\n  }]\n}, {\n  tfOpName: \"DepthToSpace\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }],\n  attrs: [{\n    tfName: \"block_size\",\n    name: \"blockSize\",\n    type: \"number\"\n  }, {\n    tfName: \"data_format\",\n    name: \"dataFormat\",\n    type: \"string\"\n  }]\n}, {\n  tfOpName: \"BroadcastTo\",\n  category: \"transformation\",\n  inputs: [{\n    start: 0,\n    name: \"x\",\n    type: \"tensor\"\n  }, {\n    start: 1,\n    name: \"shape\",\n    type: \"number[]\"\n  }],\n  attrs: []\n}];\nvar transformation = {\n  __proto__: null,\n  json\n};\n\nclass OperationMapper {\n  static get Instance() {\n    return this._instance || (this._instance = new this());\n  }\n\n  constructor() {\n    var e = [].concat(...[arithmetic, basicMath, control, convolution, creation, dynamic, evaluation, graph, hashTable, image, logical, matrices, normalization, reduction, sliceJoin, sparse, spectral, string, transformation].map(e => e.json));\n    this.opMappers = e.reduce((e, t) => (e[t.tfOpName] = t, e), {});\n  }\n\n  transformGraph(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    var n = [],\n        r = [],\n        a = [],\n        s = e.node.reduce((e, t) => (e[t.name] = this.mapNode(t), t.op.startsWith(\"Placeholder\") ? n.push(e[t.name]) : \"Const\" === t.op ? r.push(e[t.name]) : null != t.input && 0 !== t.input.length || a.push(e[t.name]), e), {});\n    var o = [];\n    var i = [];\n    var l = {},\n        u = {};\n    null != t && (l = this.mapSignatureEntries(t.inputs), u = this.mapSignatureEntries(t.outputs));\n    var c = Object.keys(s);\n    c.forEach(e => {\n      var t = s[e];\n      t.inputNames.forEach((e, n) => {\n        var [r,, a] = getNodeNameAndIndex(e),\n            o = s[r];\n\n        if (null != o.outputs) {\n          var _e765 = o.outputs.indexOf(a);\n\n          -1 !== _e765 && (t.inputNames[n] = \"\".concat(r, \":\").concat(_e765));\n        }\n\n        t.inputs.push(o), o.children.push(t);\n      });\n    }), 0 === Object.keys(u).length ? c.forEach(e => {\n      var t = s[e];\n      0 === t.children.length && i.push(t);\n    }) : Object.keys(u).forEach(e => {\n      var [t] = getNodeNameAndIndex(e),\n          n = s[t];\n      null != n && (n.signatureKey = u[e], i.push(n));\n    }), Object.keys(l).length > 0 ? Object.keys(l).forEach(e => {\n      var [t] = getNodeNameAndIndex(e),\n          n = s[t];\n      n && (n.signatureKey = l[e], o.push(n));\n    }) : o = n;\n    var p = {};\n    null != e.library && null != e.library.function && (p = e.library.function.reduce((e, t) => (e[t.signature.name] = this.mapFunction(t), e), {}));\n    var d = {\n      nodes: s,\n      inputs: o,\n      outputs: i,\n      weights: r,\n      placeholders: n,\n      signature: t,\n      functions: p\n    };\n    return a.length > 0 && (d.initNodes = a), d;\n  }\n\n  mapSignatureEntries(e) {\n    return Object.keys(e || {}).reduce((t, n) => (t[e[n].name] = n, t), {});\n  }\n\n  mapNode(e) {\n    var t = getRegisteredOp(e.op) || this.opMappers[e.op] || {};\n    null == e.attr && (e.attr = {});\n    var n = {\n      name: e.name,\n      op: e.op,\n      category: t.category,\n      inputNames: (e.input || []).map(e => e.startsWith(\"^\") ? e.substr(1) : e),\n      inputs: [],\n      children: [],\n      inputParams: {},\n      attrParams: {},\n      rawAttrs: e.attr,\n      outputs: t.outputs\n    };\n    return null != t.inputs && (n.inputParams = t.inputs.reduce((e, t) => (e[t.name] = {\n      type: t.type,\n      inputIndexStart: t.start,\n      inputIndexEnd: t.end\n    }, e), {})), null != t.attrs && (n.attrParams = t.attrs.reduce((t, n) => {\n      var r = n.type;\n      var a;\n\n      switch (n.type) {\n        case \"string\":\n          a = getStringParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getStringParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"string[]\":\n          a = getStringArrayParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getStringArrayParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"number\":\n          a = getNumberParam(e.attr, n.tfName, n.defaultValue || 0), void 0 === a && n.tfDeprecatedName && (a = getNumberParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"number[]\":\n          a = getNumericArrayParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getNumericArrayParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"bool\":\n          a = getBoolParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getBoolParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"bool[]\":\n          a = getBoolArrayParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getBoolArrayParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"shape\":\n          a = getTensorShapeParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getTensorShapeParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"shape[]\":\n          a = getTensorShapeArrayParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getTensorShapeArrayParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"dtype\":\n          a = getDtypeParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getDtypeParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"dtype[]\":\n          a = getDtypeArrayParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getDtypeArrayParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"func\":\n          a = getFuncParam(e.attr, n.tfName, n.defaultValue), void 0 === a && n.tfDeprecatedName && (a = getFuncParam(e.attr, n.tfDeprecatedName, n.defaultValue));\n          break;\n\n        case \"tensor\":\n        case \"tensors\":\n          break;\n\n        default:\n          throw new Error(\"Unsupported param type: \".concat(n.type, \" for op: \").concat(e.op));\n      }\n\n      return t[n.name] = {\n        value: a,\n        type: r\n      }, t;\n    }, {})), n;\n  }\n\n  mapFunction(e) {\n    var t = e.nodeDef,\n        n = [];\n    var r = {};\n    null != t && (r = t.reduce((e, t) => (e[t.name] = this.mapNode(t), \"Const\" === t.op && n.push(e[t.name]), e), {}));\n    var a = [],\n        s = [];\n    e.signature.inputArg.forEach(e => {\n      var [t] = getNodeNameAndIndex(e.name),\n          n = {\n        name: t,\n        op: \"Placeholder\",\n        inputs: [],\n        inputNames: [],\n        category: \"graph\",\n        inputParams: {},\n        attrParams: {\n          dtype: {\n            value: parseDtypeParam(e.type),\n            type: \"dtype\"\n          }\n        },\n        children: []\n      };\n      n.signatureKey = e.name, a.push(n), r[t] = n;\n    }), Object.keys(r).forEach(e => {\n      var t = r[e];\n      t.inputNames.forEach((e, n) => {\n        var [a,, s] = getNodeNameAndIndex(e),\n            o = r[a];\n\n        if (null != o.outputs) {\n          var _e766 = o.outputs.indexOf(s);\n\n          -1 !== _e766 && (t.inputNames[n] = \"\".concat(a, \":\").concat(_e766));\n        }\n\n        t.inputs.push(o), o.children.push(t);\n      });\n    });\n    var o = e.ret;\n    e.signature.outputArg.forEach(e => {\n      var [t, n] = getNodeNameAndIndex(o[e.name]),\n          a = r[t];\n      null != a && (a.defaultOutput = n, s.push(a));\n    });\n    var i = this.mapArgsToSignature(e);\n    return {\n      nodes: r,\n      inputs: a,\n      outputs: s,\n      weights: n,\n      placeholders: [],\n      signature: i\n    };\n  }\n\n  mapArgsToSignature(e) {\n    return {\n      methodName: e.signature.name,\n      inputs: e.signature.inputArg.reduce((e, t) => (e[t.name] = this.mapArgToTensorInfo(t), e), {}),\n      outputs: e.signature.outputArg.reduce((t, n) => (t[n.name] = this.mapArgToTensorInfo(n, e.ret), t), {})\n    };\n  }\n\n  mapArgToTensorInfo(e, t) {\n    var n = e.name;\n    return null != t && (n = t[n]), {\n      name: n,\n      dtype: e.type\n    };\n  }\n\n}\n\nfunction decodeBase64(e) {\n  var t = env().global;\n  if (void 0 !== t.atob) return t.atob(e);\n  if (\"undefined\" != typeof Buffer) return new Buffer(e, \"base64\").toString();\n  throw new Error(\"Unable to decode base64 in this environment. Missing built-in atob() or Buffer()\");\n}\n\nfunction parseStringParam(e, t) {\n  var n = Array.isArray(e) ? String.fromCharCode.apply(null, e) : decodeBase64(e);\n  return t ? n : n.toLowerCase();\n}\n\nfunction getStringParam(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = e[t];\n  return null != a ? parseStringParam(a.s, r) : n;\n}\n\nfunction getBoolParam(e, t, n) {\n  var r = e[t];\n  return r ? r.b : n;\n}\n\nfunction getNumberParam(e, t, n) {\n  var r = e[t] || {},\n      a = null != r.i ? r.i : null != r.f ? r.f : n;\n  return \"number\" == typeof a ? a : parseInt(a, 10);\n}\n\nfunction parseDtypeParam(e) {\n  switch (\"string\" == typeof e && (e = DataType[e]), e) {\n    case DataType.DT_FLOAT:\n      return \"float32\";\n\n    case DataType.DT_INT32:\n    case DataType.DT_INT64:\n    case DataType.DT_INT8:\n    case DataType.DT_UINT8:\n      return \"int32\";\n\n    case DataType.DT_BOOL:\n      return \"bool\";\n\n    case DataType.DT_DOUBLE:\n      return \"float32\";\n\n    case DataType.DT_STRING:\n      return \"string\";\n\n    default:\n      return null;\n  }\n}\n\nfunction getFuncParam(e, t, n) {\n  var r = e[t];\n  return r && r.func ? r.func.name : n;\n}\n\nfunction getDtypeParam(e, t, n) {\n  var r = e[t];\n  return r && r.type ? parseDtypeParam(r.type) : n;\n}\n\nfunction getDtypeArrayParam(e, t, n) {\n  var r = e[t];\n  return r && r.list && r.list.type ? r.list.type.map(e => parseDtypeParam(e)) : n;\n}\n\nfunction parseTensorShapeParam(e) {\n  if (!e.unknownRank) return null != e.dim ? e.dim.map(e => \"number\" == typeof e.size ? e.size : parseInt(e.size, 10)) : [];\n}\n\nfunction getTensorShapeParam(e, t, n) {\n  var r = e[t];\n  return r && r.shape ? parseTensorShapeParam(r.shape) : n;\n}\n\nfunction getNumericArrayParam(e, t, n) {\n  var r = e[t];\n  return r ? ((r.list.f && r.list.f.length ? r.list.f : r.list.i) || []).map(e => \"number\" == typeof e ? e : parseInt(e, 10)) : n;\n}\n\nfunction getStringArrayParam(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = e[t];\n  return a && a.list && a.list.s ? a.list.s.map(e => parseStringParam(e, r)) : n;\n}\n\nfunction getTensorShapeArrayParam(e, t, n) {\n  var r = e[t];\n  return r && r.list && r.list.shape ? r.list.shape.map(e => parseTensorShapeParam(e)) : n;\n}\n\nfunction getBoolArrayParam(e, t, n) {\n  var r = e[t];\n  return r && r.list && r.list.b ? r.list.b : n;\n}\n\nclass NodeValueImpl {\n  constructor(e, t, n) {\n    this.node = e, this.tensorMap = t, this.context = n, this.inputs = [], this.attrs = {}, this.inputs = e.inputNames.map(e => this.getInput(e)), null != e.rawAttrs && (this.attrs = Object.keys(e.rawAttrs).reduce((e, t) => (e[t] = this.getAttr(t), e), {}));\n  }\n\n  getInput(e) {\n    return getTensor(e, this.tensorMap, this.context);\n  }\n\n  getAttr(e, t) {\n    var n = this.node.rawAttrs[e];\n    if (null != n.tensor) return getTensor(e, this.tensorMap, this.context);\n    if (null != n.i || null != n.f) return getNumberParam(this.node.rawAttrs, e, t);\n    if (null != n.s) return getStringParam(this.node.rawAttrs, e, t);\n    if (null != n.b) return getBoolParam(this.node.rawAttrs, e, t);\n    if (null != n.shape) return getTensorShapeParam(this.node.rawAttrs, e, t);\n    if (null != n.type) return getDtypeParam(this.node.rawAttrs, e, t);\n\n    if (null != n.list) {\n      if (null != n.list.i || null != n.list.f) return getNumericArrayParam(this.node.rawAttrs, e, t);\n      if (null != n.list.s) return getStringArrayParam(this.node.rawAttrs, e, t);\n      if (null != n.list.shape) return getTensorShapeArrayParam(this.node.rawAttrs, e, t);\n      if (null != n.list.b) return getBoolArrayParam(this.node.rawAttrs, e, t);\n      if (null != n.list.type) return getDtypeArrayParam(this.node.rawAttrs, e, t);\n    }\n\n    return t;\n  }\n\n}\n\nvar executeOp$j = (e, t, n) => {\n  switch (e.op) {\n    case \"BiasAdd\":\n    case \"AddV2\":\n    case \"Add\":\n      return [add$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"AddN\":\n      return [addN$2(getParamValue(\"tensors\", e, t, n))];\n\n    case \"FloorMod\":\n    case \"Mod\":\n      return [mod$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Mul\":\n      return [mul(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"RealDiv\":\n    case \"Div\":\n      return [div$1(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"DivNoNan\":\n      return [divNoNan(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"FloorDiv\":\n      return [floorDiv$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Sub\":\n      return [sub$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Minimum\":\n      return [minimum$3(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Maximum\":\n      return [maximum$3(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Pow\":\n      return [pow$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"SquaredDifference\":\n      return [squaredDifference$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$i = (e, t, n) => {\n  switch (e.op) {\n    case \"Abs\":\n    case \"ComplexAbs\":\n      return [abs$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Acos\":\n      return [acos$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Acosh\":\n      return [acosh$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Asin\":\n      return [asin$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Asinh\":\n      return [asinh$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Atan\":\n      return [atan$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Atan2\":\n      return [atan2$2(getParamValue(\"x\", e, t, n), getParamValue(\"y\", e, t, n))];\n\n    case \"Atanh\":\n      return [atanh$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Ceil\":\n      return [ceil$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Complex\":\n      return [complex$2(getParamValue(\"real\", e, t, n), getParamValue(\"imag\", e, t, n))];\n\n    case \"Cos\":\n      return [cos$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Cosh\":\n      return [cosh$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Elu\":\n      return [elu$4(getParamValue(\"x\", e, t, n))];\n\n    case \"Erf\":\n      return [erf$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Exp\":\n      return [exp$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Expm1\":\n      return [expm1$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Floor\":\n      return [floor$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Log\":\n      return [log$3(getParamValue(\"x\", e, t, n))];\n\n    case \"Log1p\":\n      return [log1p$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Imag\":\n      return [imag$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Neg\":\n      return [neg$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Reciprocal\":\n      return [reciprocal$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Real\":\n      return [real$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Relu\":\n      return [relu$3(getParamValue(\"x\", e, t, n))];\n\n    case \"Round\":\n      return [round$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Selu\":\n      return [selu$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Sigmoid\":\n      return [sigmoid$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Sin\":\n      return [sin$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Sign\":\n      return [sign$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Sinh\":\n      return [sinh$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Softplus\":\n      return [softplus$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Sqrt\":\n      return [sqrt$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Square\":\n      return [square$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Tanh\":\n      return [tanh$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Tan\":\n      return [tan$2(getParamValue(\"x\", e, t, n))];\n\n    case \"ClipByValue\":\n      return [clipByValue$1(getParamValue(\"x\", e, t, n), getParamValue(\"clipValueMin\", e, t, n), getParamValue(\"clipValueMax\", e, t, n))];\n\n    case \"Relu6\":\n      return [relu6$2(getParamValue(\"x\", e, t, n))];\n\n    case \"Rsqrt\":\n      return [rsqrt$2(getTensor(e.inputNames[0], t, n))];\n\n    case \"Prod\":\n      return [prod$2(getParamValue(\"x\", e, t, n), getParamValue(\"axes\", e, t, n))];\n\n    case \"LeakyRelu\":\n      return [leakyRelu$2(getParamValue(\"x\", e, t, n), getParamValue(\"alpha\", e, t, n))];\n\n    case \"Prelu\":\n      return [prelu$3(getParamValue(\"x\", e, t, n), getParamValue(\"alpha\", e, t, n))];\n\n    case \"IsNan\":\n      return [isNaN$3(getTensor(e.inputNames[0], t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n};\n\nfunction assertShapesMatchAllowUndefinedSize(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"\";\n\n  if (\"number\" != typeof e && \"number\" != typeof t) {\n    assert$4(e.length === t.length, () => n + \" Shapes \".concat(e, \" and \").concat(t, \" must match\"));\n\n    for (var r = 0; r < e.length; r++) {\n      var a = e[r],\n          s = t[r];\n      assert$4(a < 0 || s < 0 || a === s, () => n + \" Shapes \".concat(e, \" and \").concat(t, \" must match\"));\n    }\n  }\n}\n\nfunction fullDefinedShape(e) {\n  return \"number\" != typeof e && !e.some(e => e < 0);\n}\n\nfunction inferElementShape(e, t, n) {\n  var r = mergeElementShape(e, n);\n  var a = !fullDefinedShape(r);\n  if (a && 0 === t.length) throw new Error(\"Tried to calculate elements of an empty list with non-fully-defined elementShape: \".concat(r));\n  if (a && t.forEach(e => {\n    r = mergeElementShape(e.shape, r);\n  }), !fullDefinedShape(r)) throw new Error(\"Non-fully-defined elementShape: \".concat(r));\n  return r;\n}\n\nfunction mergeElementShape(e, t) {\n  if (\"number\" == typeof e) return t;\n  if (\"number\" == typeof t) return e;\n  if (e.length !== t.length) throw new Error(\"Incompatible ranks during merge: \".concat(e, \" vs. \").concat(t));\n  var n = [];\n\n  for (var r = 0; r < e.length; ++r) {\n    var a = e[r],\n        s = t[r];\n    if (a >= 0 && s >= 0 && a !== s) throw new Error(\"Incompatible shape during merge: \".concat(e, \" vs. \").concat(t));\n    n[r] = a >= 0 ? a : s;\n  }\n\n  return n;\n}\n\nclass TensorArray {\n  constructor(e, t, n, r, a, s, o) {\n    this.name = e, this.dtype = t, this.maxSize = n, this.elementShape = r, this.identicalElementShapes = a, this.dynamicSize = s, this.clearAfterRead = o, this.tensors = [], this.closed_ = !1, this.idTensor = scalar(0), keep(this.idTensor);\n  }\n\n  get id() {\n    return this.idTensor.id;\n  }\n\n  get closed() {\n    return this.closed_;\n  }\n\n  clearAndClose(e) {\n    this.tensors.forEach(t => {\n      null != e && e.has(t.tensor.id) || t.tensor.dispose();\n    }), this.tensors = [], this.closed_ = !0, this.idTensor.dispose();\n  }\n\n  size() {\n    return this.tensors.length;\n  }\n\n  read(e) {\n    if (this.closed_) throw new Error(\"TensorArray \".concat(this.name, \" has already been closed.\"));\n    if (e < 0 || e >= this.size()) throw new Error(\"Tried to read from index \".concat(e, \", but array size is: \").concat(this.size()));\n    var t = this.tensors[e];\n    if (t.cleared) throw new Error(\"TensorArray \".concat(this.name, \": Could not read index \").concat(e, \" twice because it was cleared after a previous read (perhaps try setting clear_after_read = false?).\"));\n    return this.clearAfterRead && (t.cleared = !0), t.read = !0, t.tensor;\n  }\n\n  readMany(e) {\n    return e.map(e => this.read(e));\n  }\n\n  write(e, t) {\n    if (this.closed_) throw new Error(\"TensorArray \".concat(this.name, \" has already been closed.\"));\n    if (e < 0 || !this.dynamicSize && e >= this.maxSize) throw new Error(\"Tried to write to index \".concat(e, \", but array is not resizeable and size is: \").concat(this.maxSize));\n    var n = this.tensors[e] || {};\n    if (t.dtype !== this.dtype) throw new Error(\"TensorArray \".concat(this.name, \": Could not write to TensorArray index \").concat(e, \",\\n          because the value dtype is \").concat(t.dtype, \", but TensorArray dtype is \").concat(this.dtype, \".\"));\n    if (0 !== this.size() || null != this.elementShape && 0 !== this.elementShape.length || (this.elementShape = t.shape), assertShapesMatchAllowUndefinedSize(this.elementShape, t.shape, \"TensorArray \".concat(this.name, \": Could not write to TensorArray index \").concat(e, \".\")), n.read) throw new Error(\"TensorArray \".concat(this.name, \": Could not write to TensorArray index \").concat(e, \", because it has already been read.\"));\n    if (n.written) throw new Error(\"TensorArray \".concat(this.name, \": Could not write to TensorArray index \").concat(e, \", because it has already been written.\"));\n    n.tensor = t, keep(t), n.written = !0, this.tensors[e] = n;\n  }\n\n  writeMany(e, t) {\n    if (e.length !== t.length) throw new Error(\"TensorArray \".concat(this.name, \": could not write multiple tensors,because the index size: \").concat(e.length, \" is not the same as tensors size: \").concat(t.length, \".\"));\n    e.forEach((e, n) => this.write(e, t[n]));\n  }\n\n  gather(e, t) {\n    if (t && t !== this.dtype) throw new Error(\"TensorArray dtype is \".concat(this.dtype, \" but gather requested dtype \").concat(t));\n    if (e) e = e.slice(0, this.size());else {\n      e = [];\n\n      for (var _t535 = 0; _t535 < this.size(); _t535++) {\n        e.push(_t535);\n      }\n    }\n    if (0 === e.length) return tensor([], [0].concat(this.elementShape));\n    var n = this.readMany(e);\n    return assertShapesMatchAllowUndefinedSize(this.elementShape, n[0].shape, \"TensorArray shape mismatch: \"), stack(n, 0);\n  }\n\n  concat(e) {\n    if (e && e !== this.dtype) throw new Error(\"TensorArray dtype is \".concat(this.dtype, \" but concat requested dtype \").concat(e));\n    if (0 === this.size()) return tensor([], [0].concat(this.elementShape));\n    var t = [];\n\n    for (var _e767 = 0; _e767 < this.size(); _e767++) {\n      t.push(_e767);\n    }\n\n    var n = this.readMany(t);\n    return assertShapesMatchAllowUndefinedSize(this.elementShape, n[0].shape, \"TensorArray shape mismatch: tensor array shape (\".concat(this.elementShape, \") vs first tensor shape (\").concat(n[0].shape, \")\")), concat$2(n, 0);\n  }\n\n  scatter(e, t) {\n    if (t.dtype !== this.dtype) throw new Error(\"TensorArray dtype is \".concat(this.dtype, \" but tensor has dtype \").concat(t.dtype));\n    if (e.length !== t.shape[0]) throw new Error(\"Expected len(indices) == tensor.shape[0], but saw: \".concat(e.length, \" vs. \").concat(t.shape[0]));\n    var n = Math.max(...e);\n    if (!this.dynamicSize && n >= this.maxSize) throw new Error(\"Max index must be < array size (\".concat(n, \"  vs. \").concat(this.maxSize, \")\"));\n    this.writeMany(e, unstack(t, 0));\n  }\n\n  split(e, t) {\n    if (t.dtype !== this.dtype) throw new Error(\"TensorArray dtype is \".concat(this.dtype, \" but tensor has dtype \").concat(t.dtype));\n    var n = 0;\n    var r = e.map(e => (n += e, n));\n    if (n !== t.shape[0]) throw new Error(\"Expected sum of lengths to be equal to\\n          tensor.shape[0], but sum of lengths is\\n        \".concat(n, \", and tensor's shape is: \").concat(t.shape));\n    if (!this.dynamicSize && e.length !== this.maxSize) throw new Error(\"TensorArray's size is not equal to the size of lengths (\".concat(this.maxSize, \" vs. \").concat(e.length, \"), and the TensorArray is not marked as dynamically resizeable\"));\n    var a = 0 === n ? 0 : t.size / n,\n        s = [];\n    tidy(() => {\n      t = reshape$3(t, [1, n, a]);\n\n      for (var _n302 = 0; _n302 < e.length; ++_n302) {\n        s[_n302] = reshape$3(slice$2(t, [0, 0 === _n302 ? 0 : r[_n302 - 1], 0], [1, e[_n302], a]), this.elementShape);\n      }\n\n      return s;\n    });\n    var o = [];\n\n    for (var _t536 = 0; _t536 < e.length; _t536++) {\n      o[_t536] = _t536;\n    }\n\n    this.writeMany(o, s);\n  }\n\n}\n\nclass TensorList {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : -1;\n    this.tensors = e, this.elementShape = t, this.elementDtype = n, null != e && e.forEach(e => {\n      if (n !== e.dtype) throw new Error(\"Invalid data types; op elements \".concat(n, \", but list elements \").concat(e.dtype));\n      assertShapesMatchAllowUndefinedSize(t, e.shape, \"TensorList shape mismatch: \"), keep(e);\n    }), this.idTensor = scalar(0), this.maxNumElements = r, keep(this.idTensor);\n  }\n\n  get id() {\n    return this.idTensor.id;\n  }\n\n  copy() {\n    return new TensorList([...this.tensors], this.elementShape, this.elementDtype);\n  }\n\n  clearAndClose(e) {\n    this.tensors.forEach(t => {\n      null != e && e.has(t.id) || t.dispose();\n    }), this.tensors.length = 0, this.idTensor.dispose();\n  }\n\n  size() {\n    return this.tensors.length;\n  }\n\n  stack(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : -1;\n    if (t !== this.elementDtype) throw new Error(\"Invalid data types; op elements \".concat(t, \", but list elements \").concat(this.elementDtype));\n    if (-1 !== n && this.tensors.length !== n) throw new Error(\"Operation expected a list with \".concat(n, \" elements but got a list with \").concat(this.tensors.length, \" elements.\"));\n    assertShapesMatchAllowUndefinedSize(e, this.elementShape, \"TensorList shape mismatch: \");\n    var r = inferElementShape(this.elementShape, this.tensors, e);\n    return tidy(() => {\n      var e = this.tensors.map(e => reshape$3(e, r));\n      return stack(e, 0);\n    });\n  }\n\n  popBack(e, t) {\n    if (t !== this.elementDtype) throw new Error(\"Invalid data types; op elements \".concat(t, \", but list elements \").concat(this.elementDtype));\n    if (0 === this.size()) throw new Error(\"Trying to pop from an empty list.\");\n    var n = inferElementShape(this.elementShape, this.tensors, e),\n        r = this.tensors.pop();\n    return assertShapesMatchAllowUndefinedSize(r.shape, e, \"TensorList shape mismatch: \"), reshape$3(r, n);\n  }\n\n  pushBack(e) {\n    if (e.dtype !== this.elementDtype) throw new Error(\"Invalid data types; op elements \".concat(e.dtype, \", but list elements \").concat(this.elementDtype));\n    if (assertShapesMatchAllowUndefinedSize(e.shape, this.elementShape, \"TensorList shape mismatch: \"), this.maxNumElements === this.size()) throw new Error(\"Trying to push element into a full list.\");\n    keep(e), this.tensors.push(e);\n  }\n\n  resize(e) {\n    if (e < 0) throw new Error(\"TensorListResize expects size to be non-negative. Got: \".concat(e));\n    if (-1 !== this.maxNumElements && e > this.maxNumElements) throw new Error(\"TensorListResize input size \".concat(e, \" is greater maxNumElement \").concat(this.maxNumElements, \".\"));\n    this.tensors.length = e;\n  }\n\n  getItem(e, t, n) {\n    if (n !== this.elementDtype) throw new Error(\"Invalid data types; op elements \".concat(n, \", but list elements \").concat(this.elementDtype));\n    if (e < 0 || e > this.tensors.length) throw new Error(\"Trying to access element \".concat(e, \" in a list with \").concat(this.tensors.length, \" elements.\"));\n    if (null == this.tensors[e]) throw new Error(\"element at index \".concat(e, \" is null.\"));\n    assertShapesMatchAllowUndefinedSize(this.tensors[e].shape, t, \"TensorList shape mismatch: \");\n    var r = inferElementShape(this.elementShape, this.tensors, t);\n    return reshape$3(this.tensors[e], r);\n  }\n\n  setItem(e, t) {\n    if (t.dtype !== this.elementDtype) throw new Error(\"Invalid data types; op elements \".concat(t.dtype, \", but list elements \").concat(this.elementDtype));\n    if (e < 0 || -1 !== this.maxNumElements && e >= this.maxNumElements) throw new Error(\"Trying to set element \".concat(e, \" in a list with max \").concat(this.maxNumElements, \" elements.\"));\n    assertShapesMatchAllowUndefinedSize(this.elementShape, t.shape, \"TensorList shape mismatch: \"), keep(t), this.tensors[e] = t;\n  }\n\n  gather(e, t, n) {\n    if (t !== this.elementDtype) throw new Error(\"Invalid data types; op elements \".concat(t, \", but list elements \").concat(this.elementDtype));\n    assertShapesMatchAllowUndefinedSize(this.elementShape, n, \"TensorList shape mismatch: \"), e = e.slice(0, this.size());\n    var r = inferElementShape(this.elementShape, this.tensors, n);\n    return 0 === e.length ? tensor([], [0].concat(r)) : tidy(() => {\n      var t = e.map(e => reshape$3(this.tensors[e], r));\n      return stack(t, 0);\n    });\n  }\n\n  concat(e, t) {\n    if (e && e !== this.elementDtype) throw new Error(\"TensorList dtype is \".concat(this.elementDtype, \" but concat requested dtype \").concat(e));\n    assertShapesMatchAllowUndefinedSize(this.elementShape, t, \"TensorList shape mismatch: \");\n    var n = inferElementShape(this.elementShape, this.tensors, t);\n    return 0 === this.size() ? tensor([], [0].concat(n)) : tidy(() => {\n      var e = this.tensors.map(e => reshape$3(e, n));\n      return concat$2(e, 0);\n    });\n  }\n\n}\n\nfunction fromTensor(e, t, n) {\n  var r = e.dtype;\n  if (e.shape.length < 1) throw new Error(\"Tensor must be at least a vector, but saw shape: \".concat(e.shape));\n  if (e.dtype !== n) throw new Error(\"Invalid data types; op elements \".concat(e.dtype, \", but list elements \").concat(n));\n  assertShapesMatchAllowUndefinedSize(e.shape.slice(1), t, \"TensorList shape mismatch: \");\n  var a = unstack(e);\n  return new TensorList(a, t, r);\n}\n\nfunction reserve(e, t, n) {\n  return new TensorList([], e, t, n);\n}\n\nfunction scatter(e, t, n, r) {\n  if (t.length !== e.shape[0]) throw new Error(\"Expected len(indices) == tensor.shape[0], but saw: \".concat(t.length, \" vs. \").concat(e.shape[0]));\n  var a = Math.max(...t);\n  if (null != r && -1 !== r && a >= r) throw new Error(\"Max index must be < array size (\".concat(a, \"  vs. \").concat(r, \")\"));\n  var s = new TensorList([], n, e.dtype, r),\n      o = unstack(e, 0);\n  return t.forEach((e, t) => {\n    s.setItem(e, o[t]);\n  }), s;\n}\n\nfunction split$1(e, t, n) {\n  var r = 0;\n  var a = t.map(e => (r += e, r));\n  if (r !== e.shape[0]) throw new Error(\"Expected sum of lengths to be equal to\\n          tensor.shape[0], but sum of lengths is\\n        \".concat(r, \", and tensor's shape is: \").concat(e.shape));\n  var s = mergeElementShape(e.shape.slice(1), n),\n      o = 0 === r ? 0 : e.size / r,\n      i = tidy(() => {\n    var n = [];\n    e = reshape$3(e, [1, r, o]);\n\n    for (var _r245 = 0; _r245 < t.length; ++_r245) {\n      n[_r245] = reshape$3(slice$2(e, [0, 0 === _r245 ? 0 : a[_r245 - 1], 0], [1, t[_r245], o]), s);\n    }\n\n    return e.dispose(), n;\n  }),\n      l = new TensorList([], n, e.dtype, t.length);\n\n  for (var _e768 = 0; _e768 < i.length; _e768++) {\n    l.setItem(_e768, i[_e768]);\n  }\n\n  return l;\n}\n\nvar executeOp$h = /*#__PURE__*/function () {\n  var _ref32 = _asyncToGenerator(function* (e, t, n) {\n    switch (e.op) {\n      case \"If\":\n      case \"StatelessIf\":\n        {\n          var r = getParamValue(\"thenBranch\", e, t, n),\n              a = getParamValue(\"elseBranch\", e, t, n),\n              s = getParamValue(\"cond\", e, t, n),\n              o = getParamValue(\"args\", e, t, n);\n          return (yield s.data())[0] ? n.functionMap[r].executeFunctionAsync(o, n.tensorArrayMap, n.tensorListMap) : n.functionMap[a].executeFunctionAsync(o, n.tensorArrayMap, n.tensorListMap);\n        }\n\n      case \"While\":\n      case \"StatelessWhile\":\n        {\n          var _ret2 = yield* function* () {\n            var r = getParamValue(\"body\", e, t, n),\n                a = getParamValue(\"cond\", e, t, n),\n                s = getParamValue(\"args\", e, t, n),\n                o = yield n.functionMap[a].executeFunctionAsync(s, n.tensorArrayMap, n.tensorListMap),\n                i = s.map(e => e.id);\n            var l = yield o[0].data();\n            o.forEach(e => {\n              e.kept || -1 !== i.indexOf(e.id) || e.dispose();\n            });\n            var u = s;\n\n            var _loop48 = function* _loop48() {\n              var e = u;\n              u = yield n.functionMap[r].executeFunctionAsync(u, n.tensorArrayMap, n.tensorListMap);\n              var t = u.map(e => e.id);\n              e.forEach(e => {\n                e.kept || -1 !== i.indexOf(e.id) || -1 !== t.indexOf(e.id) || e.dispose();\n              });\n              var s = yield n.functionMap[a].executeFunctionAsync(u, n.tensorArrayMap, n.tensorListMap);\n              l = yield s[0].data(), s.forEach(e => {\n                e.kept || -1 !== i.indexOf(e.id) || -1 !== t.indexOf(e.id) || e.dispose();\n              });\n            };\n\n            for (; l[0];) {\n              yield* _loop48();\n            }\n\n            return {\n              v: u\n            };\n          }();\n\n          if (typeof _ret2 === \"object\") return _ret2.v;\n        }\n\n      case \"LoopCond\":\n        return [cloneTensor(getParamValue(\"pred\", e, t, n))];\n\n      case \"Switch\":\n        {\n          var _r246 = getParamValue(\"pred\", e, t, n);\n\n          var _a160 = getParamValue(\"data\", e, t, n);\n\n          return _a160.kept || (_a160 = cloneTensor(_a160)), (yield _r246.data())[0] ? [void 0, _a160] : [_a160, void 0];\n        }\n\n      case \"Merge\":\n        {\n          var _r247 = e.inputNames.find(e => void 0 !== getTensor(e, t, n));\n\n          return _r247 ? [cloneTensor(getTensor(_r247, t, n))] : void 0;\n        }\n\n      case \"Enter\":\n        {\n          var _r248 = getParamValue(\"frameName\", e, t, n),\n              _a161 = getParamValue(\"tensor\", e, t, n);\n\n          return n.enterFrame(_r248), [cloneTensor(_a161)];\n        }\n\n      case \"Exit\":\n        {\n          var _r249 = getParamValue(\"tensor\", e, t, n);\n\n          return n.exitFrame(), [cloneTensor(_r249)];\n        }\n\n      case \"NextIteration\":\n        {\n          var _r250 = getParamValue(\"tensor\", e, t, n);\n\n          return n.nextIteration(), [cloneTensor(_r250)];\n        }\n\n      case \"TensorArrayV3\":\n        {\n          var _r251 = getParamValue(\"size\", e, t, n),\n              _a162 = getParamValue(\"dtype\", e, t, n),\n              _s117 = getParamValue(\"elementShape\", e, t, n),\n              _o78 = getParamValue(\"dynamicSize\", e, t, n),\n              i = getParamValue(\"clearAfterRead\", e, t, n),\n              l = getParamValue(\"identicalElementShapes\", e, t, n),\n              u = getParamValue(\"name\", e, t, n),\n              c = new TensorArray(u, _a162, _r251, _s117, l, _o78, i);\n\n          return n.addTensorArray(c), [c.idTensor, scalar(1)];\n        }\n\n      case \"TensorArrayWriteV3\":\n        {\n          var _r252 = getParamValue(\"tensorArrayId\", e, t, n),\n              _a163 = getParamValue(\"index\", e, t, n),\n              _s118 = getParamValue(\"tensor\", e, t, n),\n              _o79 = n.getTensorArray(_r252.id);\n\n          return _o79.write(_a163, _s118), [_o79.idTensor];\n        }\n\n      case \"TensorArrayReadV3\":\n        {\n          var _r253 = getParamValue(\"tensorArrayId\", e, t, n),\n              _a164 = getParamValue(\"index\", e, t, n);\n\n          return [n.getTensorArray(_r253.id).read(_a164)];\n        }\n\n      case \"TensorArrayGatherV3\":\n        {\n          var _r254 = getParamValue(\"tensorArrayId\", e, t, n),\n              _a165 = getParamValue(\"indices\", e, t, n),\n              _s119 = getParamValue(\"dtype\", e, t, n);\n\n          return [n.getTensorArray(_r254.id).gather(_a165, _s119)];\n        }\n\n      case \"TensorArrayScatterV3\":\n        {\n          var _r255 = getParamValue(\"tensorArrayId\", e, t, n),\n              _a166 = getParamValue(\"indices\", e, t, n),\n              _s120 = getParamValue(\"tensor\", e, t, n),\n              _o80 = n.getTensorArray(_r255.id);\n\n          return _o80.scatter(_a166, _s120), [_o80.idTensor];\n        }\n\n      case \"TensorArrayConcatV3\":\n        {\n          var _r256 = getParamValue(\"tensorArrayId\", e, t, n),\n              _a167 = n.getTensorArray(_r256.id),\n              _s121 = getParamValue(\"dtype\", e, t, n);\n\n          return [_a167.concat(_s121)];\n        }\n\n      case \"TensorArraySplitV3\":\n        {\n          var _r257 = getParamValue(\"tensorArrayId\", e, t, n),\n              _a168 = getParamValue(\"tensor\", e, t, n),\n              _s122 = getParamValue(\"lengths\", e, t, n),\n              _o81 = n.getTensorArray(_r257.id);\n\n          return _o81.split(_s122, _a168), [_o81.idTensor];\n        }\n\n      case \"TensorArraySizeV3\":\n        {\n          var _r258 = getParamValue(\"tensorArrayId\", e, t, n);\n\n          return [scalar(n.getTensorArray(_r258.id).size(), \"int32\")];\n        }\n\n      case \"TensorArrayCloseV3\":\n        {\n          var _r259 = getParamValue(\"tensorArrayId\", e, t, n),\n              _a169 = n.getTensorArray(_r259.id);\n\n          return _a169.clearAndClose(), [_a169.idTensor];\n        }\n\n      case \"TensorListSetItem\":\n        {\n          var _r260 = getParamValue(\"tensorListId\", e, t, n),\n              _a170 = getParamValue(\"index\", e, t, n),\n              _s123 = getParamValue(\"tensor\", e, t, n),\n              _o82 = n.getTensorList(_r260.id);\n\n          return _o82.setItem(_a170, _s123), [_o82.idTensor];\n        }\n\n      case \"TensorListGetItem\":\n        {\n          var _r261 = getParamValue(\"tensorListId\", e, t, n),\n              _a171 = getParamValue(\"index\", e, t, n),\n              _s124 = getParamValue(\"elementShape\", e, t, n),\n              _o83 = getParamValue(\"elementDType\", e, t, n);\n\n          return [n.getTensorList(_r261.id).getItem(_a171, _s124, _o83)];\n        }\n\n      case \"TensorListScatterV2\":\n      case \"TensorListScatter\":\n        {\n          var _r262 = getParamValue(\"indices\", e, t, n),\n              _a172 = scatter(getParamValue(\"tensor\", e, t, n), _r262, getParamValue(\"elementShape\", e, t, n), getParamValue(\"numElements\", e, t, n));\n\n          return n.addTensorList(_a172), [_a172.idTensor];\n        }\n\n      case \"TensorListReserve\":\n      case \"EmptyTensorList\":\n        {\n          var _r263 = getParamValue(\"elementShape\", e, t, n),\n              _a173 = getParamValue(\"elementDType\", e, t, n);\n\n          var _s125;\n\n          _s125 = \"TensorListReserve\" === e.op ? \"numElements\" : \"maxNumElements\";\n\n          var _o84 = reserve(_r263, _a173, getParamValue(_s125, e, t, n));\n\n          return n.addTensorList(_o84), [_o84.idTensor];\n        }\n\n      case \"TensorListGather\":\n        {\n          var _r264 = getParamValue(\"tensorListId\", e, t, n),\n              _a174 = getParamValue(\"indices\", e, t, n),\n              _s126 = getParamValue(\"elementShape\", e, t, n),\n              _o85 = getParamValue(\"elementDType\", e, t, n);\n\n          return [n.getTensorList(_r264.id).gather(_a174, _o85, _s126)];\n        }\n\n      case \"TensorListStack\":\n        {\n          var _r265 = getParamValue(\"tensorListId\", e, t, n),\n              _a175 = getParamValue(\"elementShape\", e, t, n),\n              _s127 = getParamValue(\"elementDType\", e, t, n),\n              _o86 = getParamValue(\"numElements\", e, t, n);\n\n          return [n.getTensorList(_r265.id).stack(_a175, _s127, _o86)];\n        }\n\n      case \"TensorListFromTensor\":\n        {\n          var _r266 = fromTensor(getParamValue(\"tensor\", e, t, n), getParamValue(\"elementShape\", e, t, n), getParamValue(\"elementDType\", e, t, n));\n\n          return n.addTensorList(_r266), [_r266.idTensor];\n        }\n\n      case \"TensorListConcat\":\n        {\n          var _r267 = getParamValue(\"tensorListId\", e, t, n),\n              _a176 = n.getTensorList(_r267.id),\n              _s128 = getParamValue(\"dtype\", e, t, n),\n              _o87 = getParamValue(\"elementShape\", e, t, n);\n\n          return [_a176.concat(_s128, _o87)];\n        }\n\n      case \"TensorListPushBack\":\n        {\n          var _r268 = getParamValue(\"tensorListId\", e, t, n),\n              _a177 = getParamValue(\"tensor\", e, t, n),\n              _s129 = n.getTensorList(_r268.id);\n\n          return _s129.pushBack(_a177), [_s129.idTensor];\n        }\n\n      case \"TensorListPopBack\":\n        {\n          var _r269 = getParamValue(\"tensorListId\", e, t, n),\n              _a178 = getParamValue(\"elementShape\", e, t, n),\n              _s130 = getParamValue(\"elementDType\", e, t, n);\n\n          return [n.getTensorList(_r269.id).popBack(_a178, _s130)];\n        }\n\n      case \"TensorListSplit\":\n        {\n          var _r270 = getParamValue(\"tensor\", e, t, n),\n              _a179 = getParamValue(\"elementShape\", e, t, n),\n              _s131 = split$1(_r270, getParamValue(\"lengths\", e, t, n), _a179);\n\n          return n.addTensorList(_s131), [_s131.idTensor];\n        }\n\n      default:\n        throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n    }\n  });\n\n  return function executeOp$h(_x122, _x123, _x124) {\n    return _ref32.apply(this, arguments);\n  };\n}();\n\nfunction fusedConvAndDepthWiseParams(e, t, n) {\n  var [r, a] = getParamValue(\"fusedOps\", e, t, n),\n      s = \"biasadd\" === r,\n      o = !s,\n      i = \"prelu\" === a,\n      l = \"fusedbatchnorm\" === r,\n      u = getParamValue(\"numArgs\", e, t, n);\n\n  if (s) {\n    if (i && 2 !== u) throw new Error(\"FusedConv2d and DepthwiseConv2d with BiasAdd and Prelu must have two extra arguments: bias and alpha.\");\n    if (!i && s && 1 !== u) throw new Error(\"FusedConv2d and DepthwiseConv2d with BiasAdd must have one extra argument: bias.\");\n  }\n\n  if (l) throw new Error(\"FusedConv2d and DepthwiseConv2d with FusedBatchNorm is not supported\");\n  var c = getParamValue(\"strides\", e, t, n),\n      p = getPadding(e, t, n),\n      d = getParamValue(\"dataFormat\", e, t, n).toUpperCase(),\n      h = getParamValue(\"dilations\", e, t, n);\n  var [m, f] = getParamValue(\"args\", e, t, n);\n  return o && (f = m, m = void 0), {\n    stride: c,\n    pad: p,\n    dataFormat: d,\n    dilations: h,\n    biasArg: m,\n    preluArg: f,\n    activationFunc: a,\n    leakyreluAlpha: getParamValue(\"leakyreluAlpha\", e, t, n)\n  };\n}\n\nvar executeOp$g = (e, t, n) => {\n  switch (e.op) {\n    case \"Conv1D\":\n      {\n        var r = getParamValue(\"stride\", e, t, n),\n            a = getParamValue(\"pad\", e, t, n),\n            s = getParamValue(\"dataFormat\", e, t, n).toUpperCase(),\n            o = getParamValue(\"dilation\", e, t, n);\n        return [conv1d$1(getParamValue(\"x\", e, t, n), getParamValue(\"filter\", e, t, n), r, a, s, o)];\n      }\n\n    case \"Conv2D\":\n      {\n        var _r271 = getParamValue(\"strides\", e, t, n),\n            _a180 = getPadding(e, t, n),\n            _s132 = getParamValue(\"dataFormat\", e, t, n).toUpperCase(),\n            _o88 = getParamValue(\"dilations\", e, t, n);\n\n        return [conv2d$3(getParamValue(\"x\", e, t, n), getParamValue(\"filter\", e, t, n), [_r271[1], _r271[2]], _a180, _s132, [_o88[1], _o88[2]])];\n      }\n\n    case \"_FusedConv2D\":\n      {\n        var {\n          stride: _r272,\n          pad: _a181,\n          dataFormat: _s133,\n          dilations: _o89,\n          biasArg: i,\n          preluArg: l,\n          activationFunc: u,\n          leakyreluAlpha: c\n        } = fusedConvAndDepthWiseParams(e, t, n);\n        return [conv2d$2({\n          x: getParamValue(\"x\", e, t, n),\n          filter: getParamValue(\"filter\", e, t, n),\n          strides: [_r272[1], _r272[2]],\n          pad: _a181,\n          dataFormat: _s133,\n          dilations: [_o89[1], _o89[2]],\n          bias: i,\n          activation: u,\n          preluActivationWeights: l,\n          leakyreluAlpha: c\n        })];\n      }\n\n    case \"FusedDepthwiseConv2dNative\":\n      {\n        var {\n          stride: _r273,\n          pad: _a182,\n          dataFormat: _s134,\n          dilations: _o90,\n          biasArg: _i47,\n          preluArg: _l38,\n          activationFunc: _u33,\n          leakyreluAlpha: _c23\n        } = fusedConvAndDepthWiseParams(e, t, n);\n        return [depthwiseConv2d$2({\n          x: getParamValue(\"x\", e, t, n),\n          filter: getParamValue(\"filter\", e, t, n),\n          strides: [_r273[1], _r273[2]],\n          pad: _a182,\n          dataFormat: _s134,\n          dilations: [_o90[1], _o90[2]],\n          bias: _i47,\n          activation: _u33,\n          preluActivationWeights: _l38,\n          leakyreluAlpha: _c23\n        })];\n      }\n\n    case \"Conv2DBackpropInput\":\n    case \"Conv2dTranspose\":\n      {\n        var _r274 = getParamValue(\"outputShape\", e, t, n),\n            _a183 = getParamValue(\"strides\", e, t, n),\n            _s135 = getPadding(e, t, n);\n\n        return [conv2dTranspose$1(getParamValue(\"x\", e, t, n), getParamValue(\"filter\", e, t, n), _r274, [_a183[1], _a183[2]], _s135)];\n      }\n\n    case \"DepthwiseConv2dNative\":\n    case \"DepthwiseConv2d\":\n      {\n        var _r275 = getParamValue(\"strides\", e, t, n),\n            _a184 = getPadding(e, t, n),\n            _s136 = getParamValue(\"dilations\", e, t, n),\n            _o91 = getParamValue(\"dataFormat\", e, t, n).toUpperCase();\n\n        return [depthwiseConv2d$3(getParamValue(\"input\", e, t, n), getParamValue(\"filter\", e, t, n), [_r275[1], _r275[2]], _a184, _o91, [_s136[1], _s136[2]])];\n      }\n\n    case \"Conv3D\":\n      {\n        var _r276 = getParamValue(\"strides\", e, t, n),\n            _a185 = getParamValue(\"pad\", e, t, n),\n            _s137 = getParamValue(\"dataFormat\", e, t, n).toUpperCase(),\n            _o92 = getParamValue(\"dilations\", e, t, n);\n\n        return [conv3d$1(getParamValue(\"x\", e, t, n), getParamValue(\"filter\", e, t, n), [_r276[1], _r276[2], _r276[3]], _a185, _s137, [_o92[1], _o92[2], _o92[3]])];\n      }\n\n    case \"AvgPool\":\n      {\n        var _r277 = getParamValue(\"strides\", e, t, n),\n            _a186 = getParamValue(\"pad\", e, t, n),\n            _s138 = getParamValue(\"kernelSize\", e, t, n);\n\n        return [avgPool$2(getParamValue(\"x\", e, t, n), [_s138[1], _s138[2]], [_r277[1], _r277[2]], _a186)];\n      }\n\n    case \"MaxPool\":\n      {\n        var _r278 = getParamValue(\"strides\", e, t, n),\n            _a187 = getParamValue(\"pad\", e, t, n),\n            _s139 = getParamValue(\"kernelSize\", e, t, n);\n\n        return [maxPool$2(getParamValue(\"x\", e, t, n), [_s139[1], _s139[2]], [_r278[1], _r278[2]], _a187)];\n      }\n\n    case \"MaxPoolWithArgmax\":\n      {\n        var _r279 = getParamValue(\"strides\", e, t, n),\n            _a188 = getParamValue(\"pad\", e, t, n),\n            _s140 = getParamValue(\"kernelSize\", e, t, n),\n            _o93 = getParamValue(\"includeBatchInIndex\", e, t, n),\n            {\n          result: _i48,\n          indexes: _l39\n        } = maxPoolWithArgmax(getParamValue(\"x\", e, t, n), [_s140[1], _s140[2]], [_r279[1], _r279[2]], _a188, _o93);\n\n        return [_i48, _l39];\n      }\n\n    case \"AvgPool3D\":\n      {\n        var _r280 = getParamValue(\"strides\", e, t, n),\n            _a189 = getParamValue(\"pad\", e, t, n),\n            _s141 = getParamValue(\"kernelSize\", e, t, n);\n\n        return [avgPool3d$1(getParamValue(\"x\", e, t, n), [_s141[1], _s141[2], _s141[3]], [_r280[1], _r280[2], _r280[3]], _a189)];\n      }\n\n    case \"MaxPool3D\":\n      {\n        var _r281 = getParamValue(\"strides\", e, t, n),\n            _a190 = getParamValue(\"pad\", e, t, n),\n            _s142 = getParamValue(\"kernelSize\", e, t, n);\n\n        return [maxPool3d$1(getParamValue(\"x\", e, t, n), [_s142[1], _s142[2], _s142[3]], [_r281[1], _r281[2], _r281[3]], _a190)];\n      }\n\n    case \"Dilation2D\":\n      {\n        var _r282 = getParamValue(\"strides\", e, t, n),\n            _a191 = getParamValue(\"pad\", e, t, n),\n            _s143 = getParamValue(\"dilations\", e, t, n),\n            _o94 = _r282[1],\n            _i49 = _r282[2],\n            _l40 = _s143[1],\n            _u34 = _s143[2];\n\n        return [dilation2d(getParamValue(\"x\", e, t, n), getParamValue(\"filter\", e, t, n), [_o94, _i49], _a191, [_l40, _u34], \"NHWC\")];\n      }\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$f = (e, t, n) => {\n  switch (e.op) {\n    case \"Fill\":\n      {\n        var r = getParamValue(\"shape\", e, t, n),\n            a = getParamValue(\"dtype\", e, t, n);\n        return [fill$2(r, getParamValue(\"value\", e, t, n), a)];\n      }\n\n    case \"LinSpace\":\n      return [linspace(getParamValue(\"start\", e, t, n), getParamValue(\"stop\", e, t, n), getParamValue(\"num\", e, t, n))];\n\n    case \"Multinomial\":\n      {\n        var _r283 = getParamValue(\"logits\", e, t, n),\n            _a192 = getParamValue(\"numSamples\", e, t, n),\n            s = getParamValue(\"seed\", e, t, n);\n\n        return [multinomial$2(_r283, _a192, s)];\n      }\n\n    case \"OneHot\":\n      {\n        var _r284 = getParamValue(\"indices\", e, t, n),\n            _a193 = getParamValue(\"depth\", e, t, n),\n            _s144 = getParamValue(\"onValue\", e, t, n),\n            o = getParamValue(\"offValue\", e, t, n);\n\n        return [oneHot$2(_r284, _a193, _s144, o)];\n      }\n\n    case \"Ones\":\n      return [ones$1(getParamValue(\"shape\", e, t, n), getParamValue(\"dtype\", e, t, n))];\n\n    case \"OnesLike\":\n      return [onesLike$2(getParamValue(\"x\", e, t, n))];\n\n    case \"RandomUniform\":\n      return [randomUniform$1(getParamValue(\"shape\", e, t, n), getParamValue(\"minval\", e, t, n), getParamValue(\"maxval\", e, t, n), getParamValue(\"dtype\", e, t, n))];\n\n    case \"Range\":\n      return [range$4(getParamValue(\"start\", e, t, n), getParamValue(\"stop\", e, t, n), getParamValue(\"step\", e, t, n), getParamValue(\"dtype\", e, t, n))];\n\n    case \"TruncatedNormal\":\n      {\n        var _r285 = getParamValue(\"shape\", e, t, n),\n            _a194 = getParamValue(\"mean\", e, t, n),\n            _s145 = getParamValue(\"stdDev\", e, t, n),\n            _o95 = getParamValue(\"seed\", e, t, n);\n\n        return [truncatedNormal$1(_r285, _a194, _s145, getParamValue(\"dtype\", e, t, n), _o95)];\n      }\n\n    case \"Zeros\":\n      return [zeros$2(getParamValue(\"shape\", e, t, n), getParamValue(\"dtype\", e, t, n))];\n\n    case \"ZerosLike\":\n      return [zerosLike$2(getParamValue(\"x\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n};\n\nfunction nmsParams(e, t, n) {\n  return {\n    boxes: getParamValue(\"boxes\", e, t, n),\n    scores: getParamValue(\"scores\", e, t, n),\n    maxOutputSize: getParamValue(\"maxOutputSize\", e, t, n),\n    iouThreshold: getParamValue(\"iouThreshold\", e, t, n),\n    scoreThreshold: getParamValue(\"scoreThreshold\", e, t, n),\n    softNmsSigma: getParamValue(\"softNmsSigma\", e, t, n)\n  };\n}\n\nvar executeOp$e = /*#__PURE__*/function () {\n  var _ref33 = _asyncToGenerator(function* (e, t, n) {\n    switch (e.op) {\n      case \"NonMaxSuppressionV5\":\n        {\n          var {\n            boxes: r,\n            scores: a,\n            maxOutputSize: s,\n            iouThreshold: o,\n            scoreThreshold: i,\n            softNmsSigma: l\n          } = nmsParams(e, t, n),\n              u = yield image$1.nonMaxSuppressionWithScoreAsync(r, a, s, o, i, l);\n          return [u.selectedIndices, u.selectedScores];\n        }\n\n      case \"NonMaxSuppressionV4\":\n        {\n          var {\n            boxes: _r286,\n            scores: _a195,\n            maxOutputSize: _s146,\n            iouThreshold: _o96,\n            scoreThreshold: _i50\n          } = nmsParams(e, t, n),\n              _l41 = getParamValue(\"padToMaxOutputSize\", e, t, n),\n              _u35 = yield image$1.nonMaxSuppressionPaddedAsync(_r286, _a195, _s146, _o96, _i50, _l41);\n\n          return [_u35.selectedIndices, _u35.validOutputs];\n        }\n\n      case \"NonMaxSuppressionV3\":\n      case \"NonMaxSuppressionV2\":\n        {\n          var {\n            boxes: _r287,\n            scores: _a196,\n            maxOutputSize: _s147,\n            iouThreshold: _o97,\n            scoreThreshold: _i51\n          } = nmsParams(e, t, n);\n          return [yield image$1.nonMaxSuppressionAsync(_r287, _a196, _s147, _o97, _i51)];\n        }\n\n      case \"Where\":\n        {\n          var _r288 = cast$3(getParamValue(\"condition\", e, t, n), \"bool\"),\n              _a197 = [yield whereAsync(_r288)];\n\n          return _r288.dispose(), _a197;\n        }\n\n      case \"ListDiff\":\n        return setdiff1dAsync(getParamValue(\"x\", e, t, n), getParamValue(\"y\", e, t, n));\n\n      default:\n        throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n    }\n  });\n\n  return function executeOp$e(_x125, _x126, _x127) {\n    return _ref33.apply(this, arguments);\n  };\n}(),\n    executeOp$d = (e, t, n) => {\n  switch (e.op) {\n    case \"TopKV2\":\n      {\n        var r = getParamValue(\"x\", e, t, n),\n            a = getParamValue(\"k\", e, t, n),\n            s = getParamValue(\"sorted\", e, t, n),\n            o = topk(r, a, s);\n        return [o.values, o.indices];\n      }\n\n    case \"Unique\":\n      {\n        var _r289 = getParamValue(\"x\", e, t, n),\n            _a198 = unique$3(_r289);\n\n        return [_a198.values, _a198.indices];\n      }\n\n    case \"UniqueV2\":\n      {\n        var _r290 = getParamValue(\"x\", e, t, n),\n            _a199 = getParamValue(\"axis\", e, t, n),\n            _s148 = unique$3(_r290, _a199);\n\n        return [_s148.values, _s148.indices];\n      }\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$c = (e, t, n) => {\n  switch (e.op) {\n    case \"Const\":\n      return t[e.name];\n\n    case \"PlaceholderWithDefault\":\n      var r = getParamValue(\"default\", e, t, n);\n      return [getTensor(e.name, t, n) || r];\n\n    case \"Placeholder\":\n      return [getTensor(e.name, t, n)];\n\n    case \"Identity\":\n    case \"StopGradient\":\n    case \"FakeQuantWithMinMaxVars\":\n      return [cloneTensor(getParamValue(\"x\", e, t, n))];\n\n    case \"IdentityN\":\n      return getParamValue(\"x\", e, t, n).map(e => cloneTensor(e));\n\n    case \"Snapshot\":\n      return [cloneTensor(getParamValue(\"x\", e, t, n))];\n\n    case \"Shape\":\n      return [tensor1d(getParamValue(\"x\", e, t, n).shape, \"int32\")];\n\n    case \"ShapeN\":\n      return getParamValue(\"x\", e, t, n).map(e => tensor1d(e.shape));\n\n    case \"Size\":\n      return [scalar(getParamValue(\"x\", e, t, n).size, \"int32\")];\n\n    case \"Rank\":\n      return [scalar(getParamValue(\"x\", e, t, n).rank, \"int32\")];\n\n    case \"NoOp\":\n      return [scalar(1)];\n\n    case \"Print\":\n      var a = getParamValue(\"x\", e, t, n),\n          s = getParamValue(\"data\", e, t, n),\n          o = getParamValue(\"message\", e, t, n),\n          i = getParamValue(\"summarize\", e, t, n);\n      console.warn(\"The graph has a tf.print() operation,usually used for debugging, which slows down performance.\"), console.log(o);\n\n      for (var _e769 = 0; _e769 < s.length; _e769++) {\n        console.log(Array.prototype.slice.call(s[_e769].dataSync()).slice(0, i));\n      }\n\n      return [a];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n};\n\nclass HashTable {\n  constructor(e, t) {\n    this.keyDType = e, this.valueDType = t, this.handle = scalar(0), this.tensorMap = new Map(), keep(this.handle);\n  }\n\n  get id() {\n    return this.handle.id;\n  }\n\n  clearAndClose() {\n    this.tensorMap.forEach(e => e.dispose()), this.tensorMap.clear(), this.handle.dispose();\n  }\n\n  size() {\n    return this.tensorMap.size;\n  }\n\n  tensorSize() {\n    return scalar(this.size(), \"int32\");\n  }\n\n  import(e, t) {\n    var _this150 = this;\n\n    return _asyncToGenerator(function* () {\n      _this150.checkKeyAndValueTensor(e, t);\n\n      var n = yield e.data();\n      return _this150.tensorMap.forEach(e => e.dispose()), _this150.tensorMap.clear(), tidy(() => {\n        var e = unstack(t),\n            r = n.length,\n            a = e.length;\n        assert$4(r === a, () => \"The number of elements doesn't match, keys has \".concat(r, \" elements, the values has \").concat(a, \" elements.\"));\n\n        for (var _t537 = 0; _t537 < r; _t537++) {\n          var _r291 = n[_t537],\n              _a200 = e[_t537];\n          keep(_a200), _this150.tensorMap.set(_r291, _a200);\n        }\n\n        return _this150.handle;\n      });\n    })();\n  }\n\n  find(e, t) {\n    var _this151 = this;\n\n    return _asyncToGenerator(function* () {\n      _this151.checkKeyAndValueTensor(e, t);\n\n      var n = yield e.data();\n      return tidy(() => {\n        var e = [];\n\n        for (var r = 0; r < n.length; r++) {\n          var a = _this151.findWithDefault(n[r], t);\n\n          e.push(a);\n        }\n\n        return stack(e);\n      });\n    })();\n  }\n\n  findWithDefault(e, t) {\n    var n = this.tensorMap.get(e);\n    return null != n ? n : t;\n  }\n\n  checkKeyAndValueTensor(e, t) {\n    if (e.dtype !== this.keyDType) throw new Error(\"Expect key dtype \".concat(this.keyDType, \", but got \").concat(e.dtype));\n    if (t.dtype !== this.valueDType) throw new Error(\"Expect value dtype \".concat(this.valueDType, \", but got \").concat(t.dtype));\n  }\n\n}\n\nvar executeOp$b = /*#__PURE__*/function () {\n  var _ref34 = _asyncToGenerator(function* (e, t, n, r) {\n    switch (e.op) {\n      case \"HashTable\":\n      case \"HashTableV2\":\n        {\n          var a = getParamValue(\"keyDType\", e, t, n),\n              s = getParamValue(\"valueDType\", e, t, n),\n              o = new HashTable(a, s);\n          return r.addHashTable(e.name, o), [o.handle];\n        }\n\n      case \"LookupTableImport\":\n      case \"LookupTableImportV2\":\n        {\n          var _a201 = getParamValue(\"tableHandle\", e, t, n, r),\n              _s149 = getParamValue(\"keys\", e, t, n),\n              _o98 = getParamValue(\"values\", e, t, n),\n              i = r.getHashTableById(_a201.id);\n\n          return [yield i.import(_s149, _o98)];\n        }\n\n      case \"LookupTableFind\":\n      case \"LookupTableFindV2\":\n        {\n          var _a202 = getParamValue(\"tableHandle\", e, t, n, r),\n              _s150 = getParamValue(\"keys\", e, t, n),\n              _o99 = getParamValue(\"defaultValue\", e, t, n),\n              _i52 = r.getHashTableById(_a202.id);\n\n          return [yield _i52.find(_s150, _o99)];\n        }\n\n      case \"LookupTableSize\":\n      case \"LookupTableSizeV2\":\n        {\n          var _a203 = getParamValue(\"tableHandle\", e, t, n, r);\n\n          return [r.getHashTableById(_a203.id).tensorSize()];\n        }\n\n      default:\n        throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n    }\n  });\n\n  return function executeOp$b(_x128, _x129, _x130, _x131) {\n    return _ref34.apply(this, arguments);\n  };\n}(),\n    executeOp$a = (e, t, n) => {\n  switch (e.op) {\n    case \"ResizeBilinear\":\n      {\n        var r = getParamValue(\"images\", e, t, n),\n            a = getParamValue(\"size\", e, t, n),\n            s = getParamValue(\"alignCorners\", e, t, n),\n            o = getParamValue(\"halfPixelCenters\", e, t, n);\n        return [image$1.resizeBilinear(r, [a[0], a[1]], s, o)];\n      }\n\n    case \"ResizeNearestNeighbor\":\n      {\n        var _r292 = getParamValue(\"images\", e, t, n),\n            _a204 = getParamValue(\"size\", e, t, n),\n            _s151 = getParamValue(\"alignCorners\", e, t, n),\n            _o100 = getParamValue(\"halfPixelCenters\", e, t, n);\n\n        return [image$1.resizeNearestNeighbor(_r292, [_a204[0], _a204[1]], _s151, _o100)];\n      }\n\n    case \"CropAndResize\":\n      {\n        var _r293 = getParamValue(\"image\", e, t, n),\n            _a205 = getParamValue(\"boxes\", e, t, n),\n            _s152 = getParamValue(\"boxInd\", e, t, n),\n            _o101 = getParamValue(\"cropSize\", e, t, n),\n            i = getParamValue(\"method\", e, t, n),\n            l = getParamValue(\"extrapolationValue\", e, t, n);\n\n        return [image$1.cropAndResize(_r293, _a205, _s152, _o101, i, l)];\n      }\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$9 = (e, t, n) => {\n  switch (e.op) {\n    case \"Equal\":\n      return [equal$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"NotEqual\":\n      return [notEqual$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Greater\":\n      return [greater$3(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"GreaterEqual\":\n      return [greaterEqual$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Less\":\n      return [less$3(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"LessEqual\":\n      return [lessEqual$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"LogicalAnd\":\n      return [logicalAnd$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"LogicalNot\":\n      return [logicalNot$2(getParamValue(\"a\", e, t, n))];\n\n    case \"LogicalOr\":\n      return [logicalOr$2(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    case \"Select\":\n    case \"SelectV2\":\n      return [where(getParamValue(\"condition\", e, t, n), getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$8 = (e, t, n) => {\n  switch (e.op) {\n    case \"BatchMatMul\":\n    case \"BatchMatMulV2\":\n    case \"MatMul\":\n      return [matMul$1(getParamValue(\"a\", e, t, n), getParamValue(\"b\", e, t, n), getParamValue(\"transposeA\", e, t, n), getParamValue(\"transposeB\", e, t, n))];\n\n    case \"Einsum\":\n      return [einsum$2(getParamValue(\"equation\", e, t, n), ...getParamValue(\"tensors\", e, t, n))];\n\n    case \"Transpose\":\n      return [transpose$2(getParamValue(\"x\", e, t, n), getParamValue(\"perm\", e, t, n))];\n\n    case \"_FusedMatMul\":\n      var [r, a] = getParamValue(\"fusedOps\", e, t, n),\n          s = \"biasadd\" === r,\n          o = \"prelu\" === a,\n          i = getParamValue(\"numArgs\", e, t, n),\n          l = getParamValue(\"leakyreluAlpha\", e, t, n);\n\n      if (s) {\n        if (o && 2 !== i) throw new Error(\"Fused MatMul with BiasAdd and Prelu must have two extra arguments: bias and alpha.\");\n        if (!o && 1 !== i) throw new Error(\"Fused MatMul with BiasAdd must have one extra argument: bias.\");\n      }\n\n      var [u, c] = getParamValue(\"args\", e, t, n);\n      return [matMul({\n        a: getParamValue(\"a\", e, t, n),\n        b: getParamValue(\"b\", e, t, n),\n        transposeA: getParamValue(\"transposeA\", e, t, n),\n        transposeB: getParamValue(\"transposeB\", e, t, n),\n        bias: u,\n        activation: a,\n        preluActivationWeights: c,\n        leakyreluAlpha: l\n      })];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$7 = (e, t, n) => {\n  switch (e.op) {\n    case \"FusedBatchNorm\":\n    case \"FusedBatchNormV2\":\n    case \"FusedBatchNormV3\":\n      return [batchNorm$2(getParamValue(\"x\", e, t, n), getParamValue(\"mean\", e, t, n), getParamValue(\"variance\", e, t, n), getParamValue(\"offset\", e, t, n), getParamValue(\"scale\", e, t, n), getParamValue(\"epsilon\", e, t, n))];\n\n    case \"LRN\":\n      return [localResponseNormalization(getParamValue(\"x\", e, t, n), getParamValue(\"radius\", e, t, n), getParamValue(\"bias\", e, t, n), getParamValue(\"alpha\", e, t, n), getParamValue(\"beta\", e, t, n))];\n\n    case \"Softmax\":\n      return [softmax$3(getParamValue(\"x\", e, t, n))];\n\n    case \"LogSoftmax\":\n      return [logSoftmax(getParamValue(\"x\", e, t, n))];\n\n    case \"SparseToDense\":\n      return [sparseToDense$2(getParamValue(\"sparseIndices\", e, t, n), getParamValue(\"outputShape\", e, t, n), getParamValue(\"sparseValues\", e, t, n), getParamValue(\"defaultValue\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$6 = (e, t, n) => {\n  switch (e.op) {\n    case \"Max\":\n      {\n        var _r294 = getParamValue(\"axis\", e, t, n),\n            _a206 = getParamValue(\"keepDims\", e, t, n);\n\n        return [max$3(getParamValue(\"x\", e, t, n), _r294, _a206)];\n      }\n\n    case \"Mean\":\n      {\n        var _r295 = getParamValue(\"axis\", e, t, n),\n            _a207 = getParamValue(\"keepDims\", e, t, n);\n\n        return [mean$1(getParamValue(\"x\", e, t, n), _r295, _a207)];\n      }\n\n    case \"Min\":\n      {\n        var _r296 = getParamValue(\"axis\", e, t, n),\n            _a208 = getParamValue(\"keepDims\", e, t, n);\n\n        return [min$3(getParamValue(\"x\", e, t, n), _r296, _a208)];\n      }\n\n    case \"Sum\":\n      {\n        var _r297 = getParamValue(\"axis\", e, t, n),\n            _a209 = getParamValue(\"keepDims\", e, t, n);\n\n        return [sum$2(getParamValue(\"x\", e, t, n), _r297, _a209)];\n      }\n\n    case \"All\":\n      {\n        var _r298 = getParamValue(\"axis\", e, t, n),\n            _a210 = getParamValue(\"keepDims\", e, t, n);\n\n        return [all$2(getParamValue(\"x\", e, t, n), _r298, _a210)];\n      }\n\n    case \"Any\":\n      {\n        var _r299 = getParamValue(\"axis\", e, t, n),\n            _a211 = getParamValue(\"keepDims\", e, t, n);\n\n        return [any$2(getParamValue(\"x\", e, t, n), _r299, _a211)];\n      }\n\n    case \"ArgMax\":\n      {\n        var _r300 = getParamValue(\"axis\", e, t, n);\n\n        return [argMax$2(getParamValue(\"x\", e, t, n), _r300)];\n      }\n\n    case \"ArgMin\":\n      {\n        var _r301 = getParamValue(\"axis\", e, t, n);\n\n        return [argMin$2(getParamValue(\"x\", e, t, n), _r301)];\n      }\n\n    case \"Prod\":\n      {\n        var _r302 = getParamValue(\"axis\", e, t, n),\n            _a212 = getParamValue(\"keepDims\", e, t, n);\n\n        return [prod$2(getParamValue(\"x\", e, t, n), _r302, _a212)];\n      }\n\n    case \"Cumsum\":\n      {\n        var _r303 = getParamValue(\"axis\", e, t, n),\n            _a213 = getParamValue(\"exclusive\", e, t, n),\n            _s153 = getParamValue(\"reverse\", e, t, n);\n\n        return [cumsum$2(getParamValue(\"x\", e, t, n), _r303, _a213, _s153)];\n      }\n\n    case \"Bincount\":\n      var r = getParamValue(\"x\", e, t, n),\n          a = getParamValue(\"weights\", e, t, n),\n          s = getParamValue(\"size\", e, t, n);\n      return [bincount$2(r, a, s)];\n\n    case \"DenseBincount\":\n      {\n        var _r304 = getParamValue(\"x\", e, t, n),\n            _a214 = getParamValue(\"weights\", e, t, n),\n            _s154 = getParamValue(\"size\", e, t, n),\n            o = getParamValue(\"binaryOutput\", e, t, n);\n\n        return [denseBincount$2(_r304, _a214, _s154, o)];\n      }\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$5 = (e, t, n) => {\n  switch (e.op) {\n    case \"ConcatV2\":\n    case \"Concat\":\n      {\n        var r = getParamValue(\"n\", e, t, n),\n            a = getParamValue(\"axis\", e, t, n);\n        var s = getParamValue(\"tensors\", e, t, n);\n        return s = s.slice(0, r), [concat$2(s, a)];\n      }\n\n    case \"Gather\":\n      {\n        var _r305 = getParamValue(\"x\", e, t, n),\n            _a215 = getParamValue(\"indices\", e, t, n);\n\n        return [gather$1(_r305, cast$3(_a215, \"int32\"), 0)];\n      }\n\n    case \"GatherV2\":\n      {\n        var _r306 = getParamValue(\"axis\", e, t, n),\n            _a216 = getParamValue(\"batchDims\", e, t, n),\n            _s155 = getParamValue(\"x\", e, t, n),\n            o = getParamValue(\"indices\", e, t, n);\n\n        return [gather$1(_s155, cast$3(o, \"int32\"), _r306, _a216)];\n      }\n\n    case \"Reverse\":\n      {\n        var _r307 = getParamValue(\"dims\", e, t, n),\n            _a217 = [];\n\n        for (var _e770 = 0; _e770 < _r307.length; _e770++) {\n          _r307[_e770] && _a217.push(_e770);\n        }\n\n        var _s156 = getParamValue(\"x\", e, t, n);\n\n        return [reverse$2(_s156, _a217)];\n      }\n\n    case \"ReverseV2\":\n      {\n        var _r308 = getParamValue(\"axis\", e, t, n),\n            _a218 = getParamValue(\"x\", e, t, n);\n\n        return [reverse$2(_a218, _r308)];\n      }\n\n    case \"Slice\":\n      {\n        var _r309 = getParamValue(\"begin\", e, t, n),\n            _a219 = getParamValue(\"size\", e, t, n);\n\n        return [slice$2(getParamValue(\"x\", e, t, n), _r309, _a219)];\n      }\n\n    case \"StridedSlice\":\n      {\n        var _r310 = getParamValue(\"begin\", e, t, n),\n            _a220 = getParamValue(\"end\", e, t, n),\n            _s157 = getParamValue(\"strides\", e, t, n),\n            _o102 = getParamValue(\"beginMask\", e, t, n),\n            i = getParamValue(\"endMask\", e, t, n),\n            l = getParamValue(\"ellipsisMask\", e, t, n),\n            u = getParamValue(\"newAxisMask\", e, t, n),\n            c = getParamValue(\"shrinkAxisMask\", e, t, n),\n            _p24 = getParamValue(\"x\", e, t, n);\n\n        return [stridedSlice$2(_p24, _r310, _a220, _s157, _o102, i, l, u, c)];\n      }\n\n    case \"Pack\":\n      return tidy(() => {\n        var r = getParamValue(\"axis\", e, t, n),\n            a = getParamValue(\"tensors\", e, t, n),\n            s = a[0].shape,\n            o = squeeze(a[0]).shape,\n            i = a.map(e => {\n          var t = arraysEqual(e.shape, s);\n          if (!t && !arraysEqual(squeeze(e).shape, o)) throw new Error(\"the input tensors shape does not match\");\n          return t ? e : reshape$3(e, s);\n        });\n        return [stack(i, r)];\n      });\n\n    case \"Unpack\":\n      {\n        var _r311 = getParamValue(\"axis\", e, t, n),\n            _a221 = getParamValue(\"tensor\", e, t, n);\n\n        return unstack(_a221, _r311);\n      }\n\n    case \"Tile\":\n      {\n        var _r312 = getParamValue(\"reps\", e, t, n);\n\n        return [tile$3(getParamValue(\"x\", e, t, n), _r312)];\n      }\n\n    case \"Split\":\n    case \"SplitV\":\n      {\n        var _r313 = getParamValue(\"axis\", e, t, n),\n            _a222 = getParamValue(\"numOrSizeSplits\", e, t, n),\n            _s158 = getParamValue(\"x\", e, t, n);\n\n        return split$2(_s158, _a222, _r313);\n      }\n\n    case \"ScatterNd\":\n      {\n        var _r314 = getParamValue(\"indices\", e, t, n),\n            _a223 = getParamValue(\"values\", e, t, n),\n            _s159 = getParamValue(\"shape\", e, t, n);\n\n        return [scatterND(_r314, _a223, _s159)];\n      }\n\n    case \"GatherNd\":\n      {\n        var _r315 = getParamValue(\"x\", e, t, n),\n            _a224 = getParamValue(\"indices\", e, t, n);\n\n        return [gatherND(_r315, _a224)];\n      }\n\n    case \"SparseToDense\":\n      {\n        var _r316 = getParamValue(\"sparseIndices\", e, t, n),\n            _a225 = getParamValue(\"outputShape\", e, t, n),\n            _s160 = getParamValue(\"sparseValues\", e, t, n),\n            _o103 = getParamValue(\"defaultValue\", e, t, n);\n\n        return [sparseToDense$2(_r316, _s160, _a225, _s160.dtype === _o103.dtype ? _o103 : cast$3(_o103, _s160.dtype))];\n      }\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$4 = (e, t, n) => {\n  switch (e.op) {\n    case \"SparseFillEmptyRows\":\n      {\n        var {\n          outputIndices: r,\n          outputValues: a,\n          emptyRowIndicator: s,\n          reverseIndexMap: o\n        } = sparse$1.sparseFillEmptyRows(getParamValue(\"indices\", e, t, n), getParamValue(\"values\", e, t, n), getParamValue(\"denseShape\", e, t, n), getParamValue(\"defaultValue\", e, t, n));\n        return [r, a, s, o];\n      }\n\n    case \"SparseReshape\":\n      {\n        var {\n          outputIndices: _r317,\n          outputShape: _a226\n        } = sparse$1.sparseReshape(getParamValue(\"inputIndices\", e, t, n), getParamValue(\"inputShape\", e, t, n), getParamValue(\"newShape\", e, t, n));\n        return [_r317, _a226];\n      }\n\n    case \"SparseSegmentMean\":\n      return [sparse$1.sparseSegmentMean(getParamValue(\"data\", e, t, n), getParamValue(\"indices\", e, t, n), getParamValue(\"segmentIds\", e, t, n))];\n\n    case \"SparseSegmentSum\":\n      return [sparse$1.sparseSegmentSum(getParamValue(\"data\", e, t, n), getParamValue(\"indices\", e, t, n), getParamValue(\"segmentIds\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$3 = (e, t, n) => {\n  switch (e.op) {\n    case \"FFT\":\n      return [fft$2(getParamValue(\"x\", e, t, n))];\n\n    case \"IFFT\":\n      return [ifft$2(getParamValue(\"x\", e, t, n))];\n\n    case \"RFFT\":\n      return [rfft(getParamValue(\"x\", e, t, n))];\n\n    case \"IRFFT\":\n      return [irfft(getParamValue(\"x\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$2 = (e, t, n) => {\n  switch (e.op) {\n    case \"StringNGrams\":\n      {\n        var {\n          nGrams: r,\n          nGramsSplits: a\n        } = string$1.stringNGrams(getParamValue(\"data\", e, t, n), getParamValue(\"dataSplits\", e, t, n), getParamValue(\"separator\", e, t, n), getParamValue(\"nGramWidths\", e, t, n), getParamValue(\"leftPad\", e, t, n), getParamValue(\"rightPad\", e, t, n), getParamValue(\"padWidth\", e, t, n), getParamValue(\"preserveShortSequences\", e, t, n));\n        return [r, a];\n      }\n\n    case \"StringSplit\":\n      {\n        var {\n          indices: _r318,\n          values: _a227,\n          shape: s\n        } = string$1.stringSplit(getParamValue(\"input\", e, t, n), getParamValue(\"delimiter\", e, t, n), getParamValue(\"skipEmpty\", e, t, n));\n        return [_r318, _a227, s];\n      }\n\n    case \"StringToHashBucketFast\":\n      return [string$1.stringToHashBucketFast(getParamValue(\"input\", e, t, n), getParamValue(\"numBuckets\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n},\n    executeOp$1 = (e, t, n) => {\n  switch (e.op) {\n    case \"Cast\":\n      return [cast$3(getParamValue(\"x\", e, t, n), getParamValue(\"dtype\", e, t, n))];\n\n    case \"ExpandDims\":\n      {\n        var r = getParamValue(\"axis\", e, t, n);\n        return [expandDims$3(getParamValue(\"x\", e, t, n), r)];\n      }\n\n    case \"Squeeze\":\n      {\n        var _r319 = getParamValue(\"axis\", e, t, n);\n\n        return [squeeze(getParamValue(\"x\", e, t, n), _r319)];\n      }\n\n    case \"Reshape\":\n      return [reshape$3(getParamValue(\"x\", e, t, n), getParamValue(\"shape\", e, t, n))];\n\n    case \"MirrorPad\":\n      return [mirrorPad$1(getParamValue(\"x\", e, t, n), getParamValue(\"padding\", e, t, n), getParamValue(\"mode\", e, t, n))];\n\n    case \"PadV2\":\n    case \"Pad\":\n      return [pad(getParamValue(\"x\", e, t, n), getParamValue(\"padding\", e, t, n), getParamValue(\"constantValue\", e, t, n))];\n\n    case \"SpaceToBatchND\":\n      {\n        var _r320 = getParamValue(\"blockShape\", e, t, n),\n            a = getParamValue(\"paddings\", e, t, n);\n\n        return [spaceToBatchND$2(getParamValue(\"x\", e, t, n), _r320, a)];\n      }\n\n    case \"BatchToSpaceND\":\n      {\n        var _r321 = getParamValue(\"blockShape\", e, t, n),\n            _a228 = getParamValue(\"crops\", e, t, n);\n\n        return [batchToSpaceND$2(getParamValue(\"x\", e, t, n), _r321, _a228)];\n      }\n\n    case \"DepthToSpace\":\n      {\n        var _r322 = getParamValue(\"blockSize\", e, t, n),\n            _a229 = getParamValue(\"dataFormat\", e, t, n).toUpperCase();\n\n        return [depthToSpace$2(getParamValue(\"x\", e, t, n), _r322, _a229)];\n      }\n\n    case \"BroadcastTo\":\n      return [broadcastTo(getParamValue(\"x\", e, t, n), getParamValue(\"shape\", e, t, n))];\n\n    default:\n      throw TypeError(\"Node type \".concat(e.op, \" is not implemented\"));\n  }\n};\n\nfunction executeOp(e, t, n, r) {\n  var a = ((e, t, n) => {\n    switch (e.category) {\n      case \"arithmetic\":\n        return tidy(() => executeOp$j(e, t, n));\n\n      case \"basic_math\":\n        return tidy(() => executeOp$i(e, t, n));\n\n      case \"control\":\n        return executeOp$h(e, t, n);\n\n      case \"convolution\":\n        return tidy(() => executeOp$g(e, t, n));\n\n      case \"creation\":\n        return tidy(() => executeOp$f(e, t, n));\n\n      case \"dynamic\":\n        return executeOp$e(e, t, n);\n\n      case \"evaluation\":\n        return tidy(() => executeOp$d(e, t, n));\n\n      case \"image\":\n        return tidy(() => executeOp$a(e, t, n));\n\n      case \"graph\":\n        return tidy(() => executeOp$c(e, t, n));\n\n      case \"logical\":\n        return tidy(() => executeOp$9(e, t, n));\n\n      case \"matrices\":\n        return tidy(() => executeOp$8(e, t, n));\n\n      case \"normalization\":\n        return tidy(() => executeOp$7(e, t, n));\n\n      case \"reduction\":\n        return tidy(() => executeOp$6(e, t, n));\n\n      case \"slice_join\":\n        return tidy(() => executeOp$5(e, t, n));\n\n      case \"sparse\":\n        return tidy(() => executeOp$4(e, t, n));\n\n      case \"spectral\":\n        return tidy(() => executeOp$3(e, t, n));\n\n      case \"string\":\n        return tidy(() => executeOp$2(e, t, n));\n\n      case \"transformation\":\n        return tidy(() => executeOp$1(e, t, n));\n\n      case \"hash_table\":\n        return executeOp$b(e, t, n, r);\n\n      case \"custom\":\n        var _a230 = getRegisteredOp(e.op);\n\n        if (_a230 && _a230.customExecutor) return _a230.customExecutor(new NodeValueImpl(e, t, n));\n        throw TypeError(\"Custom op \".concat(e.op, \" is not registered.\"));\n\n      default:\n        throw TypeError(\"Unknown op '\".concat(e.op, \"'. File an issue at https://github.com/tensorflow/tfjs/issues so we can add it, or register a custom execution with tf.registerOp()\"));\n    }\n  })(e, t, n);\n\n  return isPromise(a) ? a.then(e => [].concat(e)) : [].concat(a);\n}\n\nclass ExecutionContext {\n  constructor() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : {};\n    this.weightMap = e, this.tensorArrayMap = t, this.tensorListMap = n, this.functionMap = r, this.rootContext = {\n      id: 0,\n      frameName: \"\",\n      iterationId: 0\n    }, this.contexts = [this.rootContext], this.lastId = 0, this.generateCurrentContextIds();\n  }\n\n  newFrame(e, t) {\n    return {\n      id: e,\n      frameName: t,\n      iterationId: 0\n    };\n  }\n\n  set currentContext(e) {\n    this.contexts !== e && (this.contexts = e, this.generateCurrentContextIds());\n  }\n\n  get currentContext() {\n    return this.contexts;\n  }\n\n  get currentContextId() {\n    return this._currentContextIds[0];\n  }\n\n  get currentContextIds() {\n    return this._currentContextIds;\n  }\n\n  generateCurrentContextIds() {\n    var e = [];\n\n    for (var t = 0; t < this.contexts.length - 1; t++) {\n      var n = this.contexts.slice(0, this.contexts.length - t);\n      e.push(this.contextIdforContexts(n));\n    }\n\n    e.push(\"\"), this._currentContextIds = e;\n  }\n\n  contextIdforContexts(e) {\n    return e ? e.map(e => 0 === e.id && 0 === e.iterationId ? \"\" : \"\".concat(e.frameName, \"-\").concat(e.iterationId)).join(\"/\") : \"\";\n  }\n\n  enterFrame(e) {\n    this.contexts && (this.lastId++, this.contexts = this.contexts.slice(), this.contexts.push(this.newFrame(this.lastId, e)), this._currentContextIds.unshift(this.contextIdforContexts(this.contexts)));\n  }\n\n  exitFrame() {\n    if (!(this.contexts && this.contexts.length > 1)) throw new Error(\"Cannot exit frame, the context is empty\");\n    this.contexts = this.contexts.slice(), this.contexts.splice(-1), this.currentContextIds.shift();\n  }\n\n  nextIteration() {\n    if (!(this.contexts && this.contexts.length > 0)) throw new Error(\"Cannot increase frame iteration, the context is empty\");\n    {\n      this.contexts = this.contexts.slice(), this.lastId++;\n\n      var _e771 = Object.assign({}, this.contexts[this.contexts.length - 1]);\n\n      _e771.iterationId += 1, _e771.id = this.lastId, this.contexts.splice(-1, 1, _e771), this._currentContextIds.splice(0, 1, this.contextIdforContexts(this.contexts));\n    }\n  }\n\n  getWeight(e) {\n    return this.weightMap[e];\n  }\n\n  addTensorArray(e) {\n    this.tensorArrayMap[e.id] = e;\n  }\n\n  getTensorArray(e) {\n    return this.tensorArrayMap[e];\n  }\n\n  addTensorList(e) {\n    this.tensorListMap[e.id] = e;\n  }\n\n  getTensorList(e) {\n    return this.tensorListMap[e];\n  }\n\n  dispose(e) {\n    for (var t in this.tensorArrayMap) {\n      this.tensorArrayMap[t].clearAndClose(e);\n    }\n\n    for (var _t538 in this.tensorListMap) {\n      this.tensorListMap[_t538].clearAndClose(e);\n    }\n  }\n\n}\n\nfunction getExecutionSubgraph(e, t, n, r) {\n  var a = new Set(),\n      s = [];\n  var o = null,\n      i = null;\n  var l = new Set(),\n      u = Object.keys(e).map(e => parseNodeName(e)[0]);\n  var c = [];\n  null != r && (c = r.map(e => parseNodeName(e.name)[0]));\n  var p = [...t];\n\n  for (; p.length > 0;) {\n    var _e772 = p.pop();\n\n    (isControlFlow(_e772) || isDynamicShape(_e772) || isHashTable(_e772)) && null == o && (o = _e772, i = o.children.map(e => e.name).filter(e => a.has(e))), a.add(_e772.name), null == n[_e772.name] && -1 === u.indexOf(_e772.name) && -1 === c.indexOf(_e772.name) && (0 !== _e772.inputs.length ? _e772.inputs.forEach(e => {\n      l.has(e.name) || (l.add(e.name), p.push(e));\n    }) : s.push(_e772.name));\n  }\n\n  return {\n    inputs: e,\n    outputs: t,\n    usedNodes: a,\n    missingInputs: s,\n    dynamicNode: o,\n    syncInputs: i\n  };\n}\n\nfunction getNodesInTopologicalOrder(e, t, n) {\n  var {\n    usedNodes: r,\n    inputs: a\n  } = n,\n      s = [],\n      o = Object.keys(a).map(e => parseNodeName(e)[0]).map(t => e.nodes[t]),\n      i = e.initNodes;\n  o.forEach(e => {\n    r.has(e.name) && s.push(e);\n  }), e.weights.forEach(e => {\n    r.has(e.name) && s.push(e);\n  }), null != i && i.forEach(e => {\n    r.has(e.name) && s.push(e);\n  });\n  var l = new Set(),\n      u = [];\n\n  for (; s.length > 0;) {\n    var _e773 = s.pop();\n\n    l.add(_e773.name), t[_e773.name] || u.push(_e773), _e773.children.forEach(e => {\n      !l.has(e.name) && r.has(e.name) && e.inputs.every(e => l.has(e.name)) && s.push(e);\n    });\n  }\n\n  return u;\n}\n\nvar CONTROL_FLOW_OPS = [\"Switch\", \"Merge\", \"Enter\", \"Exit\", \"NextIteration\", \"StatelessIf\", \"StatelessWhile\", \"if\", \"While\"],\n    DYNAMIC_SHAPE_OPS = [\"NonMaxSuppressionV2\", \"NonMaxSuppressionV3\", \"NonMaxSuppressionV5\", \"Where\"],\n    HASH_TABLE_OPS = [\"HashTable\", \"HashTableV2\", \"LookupTableImport\", \"LookupTableImportV2\", \"LookupTableFind\", \"LookupTableFindV2\", \"LookupTableSize\", \"LookupTableSizeV2\"];\n\nfunction isControlFlow(e) {\n  return CONTROL_FLOW_OPS.indexOf(e.op) >= 0;\n}\n\nfunction isDynamicShape(e) {\n  return DYNAMIC_SHAPE_OPS.indexOf(e.op) >= 0;\n}\n\nfunction isHashTable(e) {\n  return HASH_TABLE_OPS.indexOf(e.op) >= 0;\n}\n\nclass GraphExecutor {\n  constructor(e, t) {\n    this.graph = e, this.parent = t, this.compiledMap = new Map(), this._weightMap = {}, this.SEPERATOR = \",\", this._functions = {}, this._functionExecutorMap = {}, this._outputs = e.outputs, this._inputs = e.inputs, this._initNodes = e.initNodes, this._signature = e.signature, this._functions = e.functions, null != e.functions && Object.keys(e.functions).forEach(t => {\n      this._functionExecutorMap[t] = new GraphExecutor(e.functions[t], this);\n    });\n  }\n\n  get weightIds() {\n    return this.parent ? this.parent.weightIds : this._weightIds;\n  }\n\n  get functionExecutorMap() {\n    return this.parent ? this.parent.functionExecutorMap : this._functionExecutorMap;\n  }\n\n  get weightMap() {\n    return this.parent ? this.parent.weightMap : this._weightMap;\n  }\n\n  set weightMap(e) {\n    var t = Object.keys(e).map(t => e[t].map(e => e.id));\n    this._weightIds = [].concat(...t), this._weightMap = e;\n  }\n\n  set resourceManager(e) {\n    this._resourceManager = e;\n  }\n\n  get inputs() {\n    return this._inputs.map(e => ({\n      name: e.name,\n      shape: e.attrParams.shape ? e.attrParams.shape.value : void 0,\n      dtype: e.attrParams.dtype ? e.attrParams.dtype.value : void 0\n    }));\n  }\n\n  get outputs() {\n    return this._outputs.map(e => ({\n      name: e.name,\n      shape: e.attrParams.shape ? e.attrParams.shape.value : void 0,\n      dtype: e.attrParams.dtype ? e.attrParams.dtype.value : void 0\n    }));\n  }\n\n  get inputNodes() {\n    return this._inputs.map(e => e.signatureKey || e.name);\n  }\n\n  get outputNodes() {\n    return this._outputs.map(e => {\n      var t = e.signatureKey || e.name;\n      return e.defaultOutput ? \"\".concat(t, \":\").concat(e.defaultOutput) : t;\n    });\n  }\n\n  get functions() {\n    return Object.keys(this._functions).reduce((e, t) => (e[t] = this._functions[t].signature, e), {});\n  }\n\n  getCompilationKey(e, t) {\n    var n = e.map(e => e.name).sort(),\n        r = t.map(e => e.name).sort();\n    return n.join(this.SEPERATOR) + \"--\" + r.join(this.SEPERATOR);\n  }\n\n  compile(e, t) {\n    var n = getExecutionSubgraph(e, t, this.weightMap, this._initNodes),\n        {\n      missingInputs: r,\n      dynamicNode: a,\n      syncInputs: s\n    } = n;\n    if (null != a) throw new Error(\"This execution contains the node '\".concat(a.name, \"', which has the dynamic op '\").concat(a.op, \"'. Please use model.executeAsync() instead. Alternatively, to avoid the dynamic ops, specify the inputs [\").concat(s, \"]\"));\n\n    if (r.length > 0) {\n      var _n303 = t.map(e => e.name),\n          _a231 = Object.keys(e);\n\n      throw new Error(\"Cannot compute the outputs [\".concat(_n303, \"] from the provided inputs [\").concat(_a231, \"]. Missing the following inputs: [\").concat(r, \"]\"));\n    }\n\n    return getNodesInTopologicalOrder(this.graph, this.weightMap, n);\n  }\n\n  execute(e, t) {\n    e = this.mapInputs(e);\n    var n = Object.keys(e).sort();\n    this.checkInputs(e), this.checkInputShapeAndType(e), t = this.mapOutputs(t), this.checkOutputs(t);\n    var r = n.map(e => this.graph.nodes[parseNodeName(e)[0]]),\n        a = t.map(e => parseNodeName(e)[0]);\n    var s = a.map(e => this.graph.nodes[e]);\n    0 === s.length && (s = this._outputs);\n    var o = this.getCompilationKey(r, s);\n    var i = this.compiledMap.get(o);\n    null == i && (i = this.compile(e, s), this.compiledMap.set(o, i));\n    var l = {},\n        u = {};\n    return tidy(() => {\n      var n = new ExecutionContext(this.weightMap, l, u, this.functionExecutorMap),\n          r = Object.assign({}, this.weightMap);\n      Object.keys(e).forEach(t => {\n        var [n, a] = parseNodeName(t),\n            s = [];\n        s[a] = e[t], r[n] = s;\n      });\n      var s = this.getFrozenTensorIds(r),\n          o = {};\n\n      for (var _e774 = 0; _e774 < i.length; _e774++) {\n        var _t539 = i[_e774];\n\n        if (!r[_t539.name]) {\n          var _e775 = executeOp(_t539, r, n, this._resourceManager);\n\n          if (isPromise(_e775)) throw new Error(\"The execution of the op '\".concat(_t539.op, \"' returned a promise. Please use model.executeAsync() instead.\"));\n          r[_t539.name] = _e775, this.checkTensorForDisposal(_t539.name, _t539, r, n, s, a, o);\n        }\n      }\n\n      return null == this.parent && n.dispose(s), t.map(e => getTensor(e, r, n));\n    });\n  }\n\n  getFrozenTensorIds(e) {\n    var t = [].concat.apply([], Object.keys(e).map(t => e[t]).map(e => e.map(e => e.id)));\n    return new Set(t);\n  }\n\n  checkTensorForDisposal(e, t, n, r, a, s, o) {\n    \"control\" !== t.category && -1 === s.indexOf(e) && (n[e].forEach(e => {\n      null != e && (o[e.id] = (o[e.id] || 0) + t.children.length);\n    }), t.inputs.forEach(e => {\n      if (\"control\" !== e.category) {\n        var _t540 = getTensorsForCurrentContenxt(e.name, n, r);\n\n        null != _t540 && _t540.forEach(e => {\n          if (e && !e.kept && !a.has(e.id)) {\n            var _t541 = o[e.id];\n            1 === _t541 ? (e.dispose(), delete o[e.id]) : null != _t541 && o[e.id]--;\n          }\n        });\n      }\n    }));\n  }\n\n  executeAsync(e, t) {\n    var _this152 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this152._executeAsync(e, t);\n    })();\n  }\n\n  _executeAsync(e, t) {\n    var _arguments7 = arguments,\n        _this153 = this;\n\n    return _asyncToGenerator(function* () {\n      var n = _arguments7.length > 2 && _arguments7[2] !== undefined ? _arguments7[2] : !1;\n      var r = _arguments7.length > 3 && _arguments7[3] !== undefined ? _arguments7[3] : {};\n      var a = _arguments7.length > 4 && _arguments7[4] !== undefined ? _arguments7[4] : {};\n      n || (e = _this153.mapInputs(e), _this153.checkInputs(e), _this153.checkInputShapeAndType(e), t = _this153.mapOutputs(t), _this153.checkOutputs(t));\n      var s = new ExecutionContext(_this153.weightMap, r, a, _this153.functionExecutorMap),\n          o = yield _this153.executeWithControlFlow(e, s, t, n),\n          i = t.map(e => getTensor(e, o, s)),\n          l = i.map(e => e.id),\n          u = Object.keys(e).map(t => e[t].id),\n          c = new Set([...l, ...u, ..._this153.weightIds]);\n      return Object.keys(o).forEach(e => {\n        o[e].forEach(e => {\n          !e || e.kept || e.isDisposed || c.has(e.id) || e.dispose();\n        });\n      }), null == _this153.parent && s.dispose(c), i;\n    })();\n  }\n\n  executeFunctionAsync(e, t, n) {\n    var _this154 = this;\n\n    return _asyncToGenerator(function* () {\n      var r = e.reduce((e, t, n) => (e[_this154.inputs[n].name] = t, e), {});\n      return _this154._executeAsync(r, _this154.outputNodes, !0, t, n);\n    })();\n  }\n\n  executeWithControlFlow(e, t, n, r) {\n    var _this155 = this;\n\n    return _asyncToGenerator(function* () {\n      var a = Object.keys(e),\n          s = a.map(e => _this155.graph.nodes[parseNodeName(e)[0]]),\n          o = n.map(e => parseNodeName(e)[0]);\n      var i = o.map(e => _this155.graph.nodes[e]);\n      0 === i.length && (i = _this155._outputs);\n      var {\n        usedNodes: l,\n        missingInputs: u,\n        dynamicNode: c,\n        syncInputs: p\n      } = getExecutionSubgraph(e, i, _this155.weightMap, _this155._initNodes),\n          d = [...s, ..._this155.graph.weights, ...(_this155._initNodes || [])].map(e => ({\n        node: e,\n        contexts: t.currentContext\n      })),\n          h = Object.assign({}, _this155.weightMap);\n      Object.keys(e).forEach(t => {\n        var [n, r] = parseNodeName(t),\n            a = [];\n        a[r] = e[t], h[n] = a;\n      });\n\n      var m = {},\n          f = _this155.getFrozenTensorIds(h),\n          g = {};\n\n      for (; d.length > 0;) {\n        var _e776 = _this155.processStack(s, d, t, h, g, f, o, m, l);\n\n        yield Promise.all(_e776);\n      }\n\n      null != c || r || console.warn(\"This model execution did not contain any nodes with control flow or dynamic output shapes. You can use model.execute() instead.\");\n      var $ = i.filter(e => !isControlFlow(e) && !getTensor(e.name, h, t)).map(e => e.name);\n\n      if ($.length > 0) {\n        var _e777 = \"\";\n        throw null != c && (_e777 = \"Alternatively, to avoid the dynamic ops, use model.execute() and specify the inputs [\".concat(p, \"]\")), new Error(\"Cannot compute the outputs [\".concat($, \"] from the provided inputs [\").concat(a, \"]. Consider providing the following inputs: [\").concat(u, \"]. \").concat(_e777));\n      }\n\n      return h;\n    })();\n  }\n\n  processStack(e, t, n, r, a, s, o, i, l) {\n    var _this156 = this;\n\n    var u = [];\n\n    var _loop49 = function _loop49() {\n      var e = t.pop();\n      n.currentContext = e.contexts;\n      var c = \"\";\n\n      if (\"Enter\" === e.node.op && getParamValue(\"isConstant\", e.node, r, n) && ([c] = getNodeNameAndIndex(e.node.name, n)), null == r[e.node.name]) {\n        var _p25 = executeOp(e.node, r, n, _this156._resourceManager);\n\n        c || ([c] = getNodeNameAndIndex(e.node.name, n));\n        var d = n.currentContext;\n        isPromise(_p25) ? u.push(_p25.then(u => (r[c] = u, n.currentContext = d, _this156.checkTensorForDisposal(c, e.node, r, n, s, o, i), _this156.processChildNodes(e.node, t, n, r, a, l), u))) : (r[c] = _p25, _this156.checkTensorForDisposal(c, e.node, r, n, s, o, i), _this156.processChildNodes(e.node, t, n, r, a, l));\n      } else _this156.processChildNodes(e.node, t, n, r, a, l);\n    };\n\n    for (; t.length > 0;) {\n      _loop49();\n    }\n\n    return u;\n  }\n\n  processChildNodes(e, t, n, r, a, s) {\n    e.children.forEach(e => {\n      var [o] = getNodeNameAndIndex(e.name, n);\n      !a[o] && s.has(e.name) && (\"Merge\" === e.op ? e.inputNames.some(e => !!getTensor(e, r, n)) && (a[o] = !0, t.push({\n        contexts: n.currentContext,\n        node: e\n      })) : e.inputNames.every(e => !!getTensor(e, r, n)) && (a[o] = !0, t.push({\n        contexts: n.currentContext,\n        node: e\n      })));\n    });\n  }\n\n  dispose() {\n    Object.keys(this.weightMap).forEach(e => this.weightMap[e].forEach(e => e.dispose()));\n  }\n\n  checkInputShapeAndType(e) {\n    Object.keys(e).forEach(t => {\n      var n = e[t],\n          [r] = parseNodeName(t),\n          a = this.graph.nodes[r];\n\n      if (a.attrParams.shape && a.attrParams.shape.value) {\n        var _e778 = a.attrParams.shape.value;\n        assert$4(_e778.length === n.shape.length && n.shape.every((t, n) => -1 === _e778[n] || _e778[n] === t), () => \"The shape of dict['\".concat(a.name, \"'] provided in model.execute(dict) must be [\").concat(_e778, \"], but was [\").concat(n.shape, \"]\"));\n      }\n\n      a.attrParams.dtype && a.attrParams.dtype.value && assert$4(n.dtype === a.attrParams.dtype.value, () => \"The dtype of dict['\".concat(a.name, \"'] provided in model.execute(dict) must be \").concat(a.attrParams.dtype.value, \", but was \").concat(n.dtype));\n    });\n  }\n\n  mapInputs(e) {\n    var t = {};\n\n    for (var n in e) {\n      null != this._signature && null != this._signature.inputs && null != this._signature.inputs[n] ? t[this._signature.inputs[n].name] = e[n] : t[n] = e[n];\n    }\n\n    return t;\n  }\n\n  checkInputs(e) {\n    var t = Object.keys(e).filter(e => {\n      var [t] = parseNodeName(e);\n      return null == this.graph.nodes[t];\n    });\n    if (t.length > 0) throw new Error(\"The dict provided in model.execute(dict) has keys: [\".concat(t, \"] that are not part of graph\"));\n  }\n\n  mapOutputs(e) {\n    return e.map(e => null != this._signature && null != this._signature.outputs && null != this._signature.outputs[e] ? this._signature.outputs[e].name : e, {});\n  }\n\n  checkOutputs(e) {\n    e.forEach(e => {\n      var [t] = parseNodeName(e);\n      if (!this.graph.nodes[t]) throw new Error(\"The output '\".concat(e, \"' is not found in the graph\"));\n    });\n  }\n\n}\n\nclass ResourceManager {\n  constructor() {\n    var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    this.hashTableNameToHandle = e, this.hashTableMap = t;\n  }\n\n  addHashTable(e, t) {\n    this.hashTableNameToHandle[e] = t.handle, this.hashTableMap[t.id] = t;\n  }\n\n  getHashTableHandleByName(e) {\n    return this.hashTableNameToHandle[e];\n  }\n\n  getHashTableById(e) {\n    return this.hashTableMap[e];\n  }\n\n  dispose() {\n    for (var _e779 in this.hashTableMap) {\n      this.hashTableMap[_e779].clearAndClose(), delete this.hashTableMap[_e779];\n    }\n\n    for (var _e780 in this.hashTableNameToHandle) {\n      this.hashTableNameToHandle[_e780].dispose(), delete this.hashTableNameToHandle[_e780];\n    }\n  }\n\n}\n\nvar TFHUB_SEARCH_PARAM = \"?tfjs-format=file\",\n    DEFAULT_MODEL_NAME = \"model.json\";\n\nclass GraphModel {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    this.modelUrl = e, this.loadOptions = t, this.version = \"n/a\", null == t && (this.loadOptions = {}), this.resourceManager = new ResourceManager();\n  }\n\n  get modelVersion() {\n    return this.version;\n  }\n\n  get inputNodes() {\n    return this.executor.inputNodes;\n  }\n\n  get outputNodes() {\n    return this.executor.outputNodes;\n  }\n\n  get inputs() {\n    return this.executor.inputs;\n  }\n\n  get outputs() {\n    return this.executor.outputs;\n  }\n\n  get weights() {\n    return this.executor.weightMap;\n  }\n\n  get metadata() {\n    return this.artifacts.userDefinedMetadata;\n  }\n\n  get modelSignature() {\n    return this.signature;\n  }\n\n  findIOHandler() {\n    var e = this.modelUrl;\n    if (null != e.load) this.handler = e;else if (null != this.loadOptions.requestInit) this.handler = browserHTTPRequest(e, this.loadOptions);else {\n      var t = getLoadHandlers(e, this.loadOptions);\n      if (0 === t.length) t.push(browserHTTPRequest(e, this.loadOptions));else if (t.length > 1) throw new Error(\"Found more than one (\".concat(t.length, \") load handlers for URL '\").concat([e], \"'\"));\n      this.handler = t[0];\n    }\n  }\n\n  load() {\n    var _this157 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this157.findIOHandler(), null == _this157.handler.load) throw new Error(\"Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.\");\n      var e = yield _this157.handler.load();\n      return _this157.loadSync(e);\n    })();\n  }\n\n  loadSync(e) {\n    this.artifacts = e;\n    var t = this.artifacts.modelTopology;\n    var n;\n    n = null != this.artifacts.userDefinedMetadata && null != this.artifacts.userDefinedMetadata.signature ? this.artifacts.userDefinedMetadata.signature : this.artifacts.signature, this.signature = n, this.version = \"\".concat(t.versions.producer, \".\").concat(t.versions.minConsumer);\n    var r = decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n\n    if (this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(t, this.signature)), this.executor.weightMap = this.convertTensorMapToTensorsMap(r), this.executor.resourceManager = this.resourceManager, null != e.modelInitializer && null != e.modelInitializer.node) {\n      var _t542 = OperationMapper.Instance.transformGraph(e.modelInitializer);\n\n      this.initializer = new GraphExecutor(_t542), this.initializer.weightMap = this.executor.weightMap, this.initializer.resourceManager = this.resourceManager, this.initializer.executeAsync({}, []);\n    }\n\n    return !0;\n  }\n\n  save(e, t) {\n    var _this158 = this;\n\n    return _asyncToGenerator(function* () {\n      if (\"string\" == typeof e) {\n        var _t543 = getSaveHandlers(e);\n\n        if (0 === _t543.length) throw new Error(\"Cannot find any save handlers for URL '\".concat(e, \"'\"));\n        if (_t543.length > 1) throw new Error(\"Found more than one (\".concat(_t543.length, \") save handlers for URL '\").concat(e, \"'\"));\n        e = _t543[0];\n      }\n\n      if (null == e.save) throw new Error(\"GraphModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.\");\n      return e.save(_this158.artifacts);\n    })();\n  }\n\n  predict(e, t) {\n    return this.execute(e, this.outputNodes);\n  }\n\n  normalizeInputs(e) {\n    if (!(e instanceof Tensor || Array.isArray(e))) return e;\n    if ((e = Array.isArray(e) ? e : [e]).length !== this.inputNodes.length) throw new Error(\"Input tensor count mismatch,the graph model has \".concat(this.inputNodes.length, \" placeholders, while there are \").concat(e.length, \" input tensors.\"));\n    return this.inputNodes.reduce((t, n, r) => (t[n] = e[r], t), {});\n  }\n\n  normalizeOutputs(e) {\n    return e = e || this.outputNodes, Array.isArray(e) ? e : [e];\n  }\n\n  execute(e, t) {\n    e = this.normalizeInputs(e), t = this.normalizeOutputs(t);\n    var n = this.executor.execute(e, t);\n    return n.length > 1 ? n : n[0];\n  }\n\n  executeAsync(e, t) {\n    var _this159 = this;\n\n    return _asyncToGenerator(function* () {\n      e = _this159.normalizeInputs(e), t = _this159.normalizeOutputs(t);\n      var n = yield _this159.executor.executeAsync(e, t);\n      return n.length > 1 ? n : n[0];\n    })();\n  }\n\n  convertTensorMapToTensorsMap(e) {\n    return Object.keys(e).reduce((t, n) => (t[n] = [e[n]], t), {});\n  }\n\n  dispose() {\n    this.executor.dispose(), this.initializer && this.initializer.dispose(), this.resourceManager.dispose();\n  }\n\n}\n\nfunction loadGraphModel(_x132) {\n  return _loadGraphModel.apply(this, arguments);\n}\n\nfunction _loadGraphModel() {\n  _loadGraphModel = _asyncToGenerator(function* (e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    if (null == e) throw new Error(\"modelUrl in loadGraphModel() cannot be null. Please provide a url or an IOHandler that loads the model\");\n    null == t && (t = {}), t.fromTFHub && null == e.load && (e.endsWith(\"/\") || (e += \"/\"), e = \"\".concat(e).concat(DEFAULT_MODEL_NAME).concat(TFHUB_SEARCH_PARAM));\n    var n = new GraphModel(e, t);\n    return yield n.load(), n;\n  });\n  return _loadGraphModel.apply(this, arguments);\n}\n\nvar version$5 = \"3.8.0\";\n\nfunction deepMap(e, t) {\n  return deepMapInternal(e, t);\n}\n\nfunction deepMapInternal(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : new Map();\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : new Set();\n  if (null == e) return null;\n  if (r.has(e)) throw new Error(\"Circular references are not supported.\");\n  if (n.has(e)) return n.get(e);\n  var a = t(e);\n  if (a.recurse && null !== a.value) throw new Error(\"A deep map function may not return both a value and recurse=true.\");\n\n  if (a.recurse) {\n    if (isIterable(e)) {\n      var _a232 = Array.isArray(e) ? [] : {};\n\n      r.add(e);\n\n      for (var s in e) {\n        var o = deepMapInternal(e[s], t, n, r);\n        _a232[s] = o;\n      }\n\n      return r.delete(e), _a232;\n    }\n\n    throw new Error(\"Can't recurse into non-iterable type: \".concat(e));\n  }\n\n  return n.set(e, a.value), a.value;\n}\n\nfunction deepZip(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : zipToList;\n  return deepZipInternal(e, t);\n}\n\nfunction deepZipInternal(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : new Set();\n  var r = e[0];\n  if (n.has(r)) throw new Error(\"Circular references are not supported.\");\n  var a = t(e);\n  if (a.recurse && null !== a.value) throw new Error(\"A deep zip function may not return both a value and recurse=true.\");\n\n  if (a.recurse) {\n    if (isIterable(r)) {\n      var _a233 = Array.isArray(r) ? [] : {};\n\n      n.add(r);\n\n      var _loop50 = function _loop50(s) {\n        var r = deepZipInternal(e.map(e => e[s]), t, n);\n        _a233[s] = r;\n      };\n\n      for (var s in r) {\n        _loop50(s);\n      }\n\n      return n.delete(r), _a233;\n    }\n\n    throw new Error(\"Can't recurse into non-iterable type: \".concat(r));\n  }\n\n  return a.value;\n}\n\nfunction zipToList(e) {\n  return null === e ? null : isIterable(e[0]) ? {\n    value: null,\n    recurse: !0\n  } : {\n    value: e,\n    recurse: !1\n  };\n}\n\nfunction deepMapAndAwaitAll(_x133, _x134) {\n  return _deepMapAndAwaitAll.apply(this, arguments);\n}\n\nfunction _deepMapAndAwaitAll() {\n  _deepMapAndAwaitAll = _asyncToGenerator(function* (e, t) {\n    var n = new Map();\n    deepMapInternal(e, t, n);\n\n    for (var _e1163 of Array.from(n.keys())) {\n      var _t790 = n.get(_e1163);\n\n      if (isPromise(_t790)) {\n        var r = yield _t790;\n        n.set(_e1163, r);\n      }\n    }\n\n    return deepMapInternal(e, t, n);\n  });\n  return _deepMapAndAwaitAll.apply(this, arguments);\n}\n\nfunction isIterable(e) {\n  return null != e && !ArrayBuffer.isView(e) && (Array.isArray(e) || \"object\" == typeof e && !(e instanceof Tensor));\n}\n\nfunction canTensorify(e) {\n  return null == e || isPrimitive(e) || Array.isArray(e) || \"object\" == typeof e && e instanceof Tensor || isTypedArray(e);\n}\n\nfunction isPrimitive(e) {\n  return null === e || \"object\" != typeof e && \"function\" != typeof e;\n}\n\nfunction deepClone(e) {\n  return deepMap(e, cloneIfTensor);\n}\n\nfunction cloneIfTensor(e) {\n  return e instanceof Tensor ? {\n    value: e.clone(),\n    recurse: !1\n  } : isIterable(e) ? {\n    value: null,\n    recurse: !0\n  } : {\n    value: e,\n    recurse: !1\n  };\n}\n\nclass RingBuffer {\n  constructor(e) {\n    if (this.capacity = e, this.begin = 0, this.end = 0, null == e) throw new RangeError(\"Can't create a ring buffer of unknown capacity.\");\n    if (e < 1) throw new RangeError(\"Can't create ring buffer of capacity < 1.\");\n    this.data = new Array(e), this.doubledCapacity = 2 * e;\n  }\n\n  wrap(e) {\n    for (; e < 0;) {\n      e += this.doubledCapacity;\n    }\n\n    return e % this.doubledCapacity;\n  }\n\n  get(e) {\n    if (e < 0) throw new RangeError(\"Can't get item at a negative index.\");\n    return this.data[e % this.capacity];\n  }\n\n  set(e, t) {\n    if (e < 0) throw new RangeError(\"Can't set item at a negative index.\");\n    this.data[e % this.capacity] = t;\n  }\n\n  length() {\n    var e = this.end - this.begin;\n    return e < 0 && (e = this.doubledCapacity + e), e;\n  }\n\n  isFull() {\n    return this.length() === this.capacity;\n  }\n\n  isEmpty() {\n    return 0 === this.length();\n  }\n\n  push(e) {\n    if (this.isFull()) throw new RangeError(\"Ring buffer is full.\");\n    this.set(this.end, e), this.end = this.wrap(this.end + 1);\n  }\n\n  pushAll(e) {\n    for (var t of e) {\n      this.push(t);\n    }\n  }\n\n  pop() {\n    if (this.isEmpty()) throw new RangeError(\"Ring buffer is empty.\");\n    this.end = this.wrap(this.end - 1);\n    var e = this.get(this.end);\n    return this.set(this.end, void 0), e;\n  }\n\n  unshift(e) {\n    if (this.isFull()) throw new RangeError(\"Ring buffer is full.\");\n    this.begin = this.wrap(this.begin - 1), this.set(this.begin, e);\n  }\n\n  shift() {\n    if (this.isEmpty()) throw new RangeError(\"Ring buffer is empty.\");\n    var e = this.get(this.begin);\n    return this.set(this.begin, void 0), this.begin = this.wrap(this.begin + 1), e;\n  }\n\n  shuffleExcise(e) {\n    if (this.isEmpty()) throw new RangeError(\"Ring buffer is empty.\");\n    var t = this.wrap(this.begin + e),\n        n = this.get(t);\n    return this.set(t, this.pop()), n;\n  }\n\n}\n\nclass GrowingRingBuffer extends RingBuffer {\n  constructor() {\n    super(GrowingRingBuffer.INITIAL_CAPACITY);\n  }\n\n  isFull() {\n    return !1;\n  }\n\n  push(e) {\n    super.isFull() && this.expand(), super.push(e);\n  }\n\n  unshift(e) {\n    super.isFull() && this.expand(), super.unshift(e);\n  }\n\n  expand() {\n    var e = 2 * this.capacity,\n        t = new Array(e),\n        n = this.length();\n\n    for (var _e781 = 0; _e781 < n; _e781++) {\n      t[_e781] = this.get(this.wrap(this.begin + _e781));\n    }\n\n    this.data = t, this.capacity = e, this.doubledCapacity = 2 * this.capacity, this.begin = 0, this.end = n;\n  }\n\n}\n\nfunction iteratorFromItems(e) {\n  return new ArrayIterator(e);\n}\n\nfunction iteratorFromFunction(e) {\n  return new FunctionCallIterator(e);\n}\n\nfunction iteratorFromConcatenated(e, t) {\n  return new ChainedIterator(e, t);\n}\n\nfunction iteratorFromZipped(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : ZipMismatchMode.FAIL;\n  return new ZipIterator(e, t);\n}\n\nGrowingRingBuffer.INITIAL_CAPACITY = 32;\n\nclass LazyIterator {\n  toArray() {\n    var _this160 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [];\n      var t = yield _this160.next();\n\n      for (; !t.done;) {\n        e.push(t.value), t = yield _this160.next();\n      }\n\n      return e;\n    })();\n  }\n\n  toArrayForTest() {\n    var _this161 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = _this161.prefetch(100),\n          t = [];\n\n      var n = yield e.next();\n\n      for (; !n.done;) {\n        t.push(n.value), n = yield e.next();\n      }\n\n      return t;\n    })();\n  }\n\n  resolveFully() {\n    var _this162 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this162.next();\n\n      for (; !e.done;) {\n        e = yield _this162.next();\n      }\n    })();\n  }\n\n  resolveWhile(e) {\n    var _this163 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = yield _this163.next(),\n          n = e(t.value);\n\n      for (; !t.done && n;) {\n        t = yield _this163.next(), n = e(t.value);\n      }\n    })();\n  }\n\n  handleErrors(e) {\n    return new ErrorHandlingLazyIterator(this, e);\n  }\n\n  filter(e) {\n    return new FilterIterator(this, e);\n  }\n\n  map(e) {\n    return new MapIterator(this, e);\n  }\n\n  mapAsync(e) {\n    return new AsyncMapIterator(this, e);\n  }\n\n  serialMapAsync(e) {\n    return new AsyncMapIterator(this, e).serial();\n  }\n\n  flatmap(e) {\n    return new FlatmapIterator(this, e);\n  }\n\n  forEachAsync(e) {\n    var _this164 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this164.map(e).resolveFully();\n    })();\n  }\n\n  serialForEach(e) {\n    var _this165 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this165.serialMapAsync(e).resolveWhile(e => !0 === e);\n    })();\n  }\n\n  rowMajorBatch(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    return new RowMajorBatchIterator(this, e, t);\n  }\n\n  columnMajorBatch(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : zipToList;\n    return this.rowMajorBatch(e, t).map(e => deepZip(e, n));\n  }\n\n  concatenate(e, t) {\n    return new ChainedIterator(iteratorFromItems([this, e]), t);\n  }\n\n  take(e) {\n    return e < 0 || null == e ? this : new TakeIterator(this, e);\n  }\n\n  skip(e) {\n    return e < 0 || null == e ? this : new SkipIterator(this, e);\n  }\n\n  prefetch(e) {\n    return new PrefetchIterator(this, e);\n  }\n\n  shuffle(e, t) {\n    return new ShuffleIterator(this, e, t);\n  }\n\n  serial() {\n    return new SerialIterator(this);\n  }\n\n}\n\nclass ArrayIterator extends LazyIterator {\n  constructor(e) {\n    super(), this.items = e, this.trav = 0;\n  }\n\n  summary() {\n    return \"Array of \".concat(this.items.length, \" items\");\n  }\n\n  next() {\n    var _this166 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this166.trav >= _this166.items.length) return {\n        value: null,\n        done: !0\n      };\n      var e = _this166.items[_this166.trav];\n      return _this166.trav++, {\n        value: deepClone(e),\n        done: !1\n      };\n    })();\n  }\n\n}\n\nclass FunctionCallIterator extends LazyIterator {\n  constructor(e) {\n    super(), this.nextFn = e;\n  }\n\n  summary() {\n    return \"Function call\";\n  }\n\n  next() {\n    var _this167 = this;\n\n    return _asyncToGenerator(function* () {\n      try {\n        return _this167.nextFn();\n      } catch (e) {\n        throw e.message = \"Error thrown while iterating through a dataset: \".concat(e.message), e;\n      }\n    })();\n  }\n\n}\n\nclass SerialIterator extends LazyIterator {\n  constructor(e) {\n    super(), this.upstream = e, this.lastRead = Promise.resolve({\n      value: null,\n      done: !1\n    });\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Serial\");\n  }\n\n  next() {\n    var _this168 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this168.lastRead = _this168.lastRead.then(() => _this168.serialNext()), _this168.lastRead;\n    })();\n  }\n\n  serialNext() {\n    var _this169 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this169.upstream.next();\n    })();\n  }\n\n}\n\nclass SkipIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.maxCount = t, this.count = 0, this.lastRead = Promise.resolve({\n      value: null,\n      done: !1\n    });\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Skip\");\n  }\n\n  next() {\n    var _this170 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this170.lastRead = _this170.lastRead.then(() => _this170.serialNext()), _this170.lastRead;\n    })();\n  }\n\n  serialNext() {\n    var _this171 = this;\n\n    return _asyncToGenerator(function* () {\n      for (; _this171.count++ < _this171.maxCount;) {\n        var _e782 = yield _this171.upstream.next();\n\n        if (_e782.done) return _e782;\n        dispose(_e782.value);\n      }\n\n      return _this171.upstream.next();\n    })();\n  }\n\n}\n\nclass TakeIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.maxCount = t, this.count = 0;\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Take\");\n  }\n\n  next() {\n    var _this172 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this172.count++ >= _this172.maxCount ? {\n        value: null,\n        done: !0\n      } : _this172.upstream.next();\n    })();\n  }\n\n}\n\nclass RowMajorBatchIterator extends LazyIterator {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n    super(), this.upstream = e, this.batchSize = t, this.enableSmallLastBatch = n, this.lastRead = Promise.resolve({\n      value: null,\n      done: !1\n    });\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> RowMajorBatch\");\n  }\n\n  next() {\n    var _this173 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this173.lastRead = _this173.lastRead.then(() => _this173.serialNext()), _this173.lastRead;\n    })();\n  }\n\n  serialNext() {\n    var _this174 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [];\n\n      for (; e.length < _this174.batchSize;) {\n        var t = yield _this174.upstream.next();\n        if (t.done) return _this174.enableSmallLastBatch && e.length > 0 ? {\n          value: e,\n          done: !1\n        } : {\n          value: null,\n          done: !0\n        };\n        e.push(t.value);\n      }\n\n      return {\n        value: e,\n        done: !1\n      };\n    })();\n  }\n\n}\n\nclass FilterIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.predicate = t, this.lastRead = Promise.resolve({\n      value: null,\n      done: !1\n    });\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Filter\");\n  }\n\n  next() {\n    var _this175 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this175.lastRead = _this175.lastRead.then(() => _this175.serialNext()), _this175.lastRead;\n    })();\n  }\n\n  serialNext() {\n    var _this176 = this;\n\n    return _asyncToGenerator(function* () {\n      for (;;) {\n        var _e783 = yield _this176.upstream.next();\n\n        if (_e783.done || _this176.predicate(_e783.value)) return _e783;\n        dispose(_e783.value);\n      }\n    })();\n  }\n\n}\n\nclass MapIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.transform = t;\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Map\");\n  }\n\n  next() {\n    var _this177 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this177.upstream.next();\n      if (e.done) return {\n        value: null,\n        done: !0\n      };\n\n      var t = getTensorsInContainer(e.value),\n          n = _this177.transform(e.value),\n          r = getTensorsInContainer(n);\n\n      for (var _e784 of t) {\n        isTensorInList(_e784, r) || _e784.dispose();\n      }\n\n      return {\n        value: n,\n        done: !1\n      };\n    })();\n  }\n\n}\n\nclass ErrorHandlingLazyIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.handler = t, this.count = 0, this.lastRead = Promise.resolve({\n      value: null,\n      done: !1\n    });\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> handleErrors\");\n  }\n\n  next() {\n    var _this178 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this178.lastRead = _this178.lastRead.then(() => _this178.serialNext()), _this178.lastRead;\n    })();\n  }\n\n  serialNext() {\n    var _this179 = this;\n\n    return _asyncToGenerator(function* () {\n      for (;;) {\n        try {\n          return yield _this179.upstream.next();\n        } catch (e) {\n          if (!_this179.handler(e)) return {\n            value: null,\n            done: !0\n          };\n        }\n      }\n    })();\n  }\n\n}\n\nclass AsyncMapIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.transform = t;\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> AsyncMap\");\n  }\n\n  next() {\n    var _this180 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this180.upstream.next();\n      if (e.done) return {\n        value: null,\n        done: !0\n      };\n      var t = getTensorsInContainer(e.value),\n          n = yield _this180.transform(e.value),\n          r = getTensorsInContainer(n);\n\n      for (var _e785 of t) {\n        isTensorInList(_e785, r) || _e785.dispose();\n      }\n\n      return {\n        value: n,\n        done: !1\n      };\n    })();\n  }\n\n}\n\nclass OneToManyIterator extends LazyIterator {\n  constructor() {\n    super(), this.outputQueue = new GrowingRingBuffer(), this.lastRead = Promise.resolve({\n      value: null,\n      done: !1\n    });\n  }\n\n  next() {\n    var _this181 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this181.lastRead = _this181.lastRead.then(() => _this181.serialNext()), _this181.lastRead;\n    })();\n  }\n\n  serialNext() {\n    var _this182 = this;\n\n    return _asyncToGenerator(function* () {\n      for (; 0 === _this182.outputQueue.length();) {\n        if (!(yield _this182.pump())) return {\n          value: null,\n          done: !0\n        };\n      }\n\n      return {\n        value: _this182.outputQueue.shift(),\n        done: !1\n      };\n    })();\n  }\n\n}\n\nclass FlatmapIterator extends OneToManyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.transform = t;\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Flatmap\");\n  }\n\n  pump() {\n    var _this183 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this183.upstream.next();\n      if (e.done) return !1;\n\n      var t = getTensorsInContainer(e.value),\n          n = _this183.transform(e.value),\n          r = getTensorsInContainer(n);\n\n      _this183.outputQueue.pushAll(n);\n\n      for (var _e786 of t) {\n        isTensorInList(_e786, r) || _e786.dispose();\n      }\n\n      return !0;\n    })();\n  }\n\n}\n\nclass ChainedIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.baseErrorHandler = t, this.lastRead = null, this.iterator = null, this.moreIterators = e;\n  }\n\n  summary() {\n    return \"TODO: fill in upstream of chained summaries -> Chained\";\n  }\n\n  next() {\n    var _this184 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this184.lastRead = _this184.readFromChain(_this184.lastRead), _this184.lastRead;\n    })();\n  }\n\n  readFromChain(e) {\n    var _this185 = this;\n\n    return _asyncToGenerator(function* () {\n      if (yield e, null == _this185.iterator) {\n        var _e787 = yield _this185.moreIterators.next();\n\n        if (_e787.done) return {\n          value: null,\n          done: !0\n        };\n        _this185.iterator = _e787.value, null != _this185.baseErrorHandler && (_this185.iterator = _this185.iterator.handleErrors(_this185.baseErrorHandler));\n      }\n\n      var t = yield _this185.iterator.next();\n      return t.done ? (_this185.iterator = null, _this185.readFromChain(e)) : t;\n    })();\n  }\n\n}\n\nvar ZipMismatchMode;\n!function (e) {\n  e[e.FAIL = 0] = \"FAIL\", e[e.SHORTEST = 1] = \"SHORTEST\", e[e.LONGEST = 2] = \"LONGEST\";\n}(ZipMismatchMode || (ZipMismatchMode = {}));\n\nclass ZipIterator extends LazyIterator {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : ZipMismatchMode.FAIL;\n    super(), this.iterators = e, this.mismatchMode = t, this.count = 0, this.currentPromise = null;\n  }\n\n  summary() {\n    return \"{TODO: fill in upstream of zip summaries} -> Zip\";\n  }\n\n  nextState(e) {\n    var _this186 = this;\n\n    return _asyncToGenerator(function* () {\n      yield e;\n      var t = 0,\n          n = 0;\n      var r = yield deepMapAndAwaitAll(_this186.iterators, function (e) {\n        return e instanceof LazyIterator ? {\n          value: e.next().then(e => (t++, e.done && n++, e.value)),\n          recurse: !1\n        } : {\n          value: null,\n          recurse: !0\n        };\n      });\n      if (t === n) return {\n        value: null,\n        done: !0\n      };\n      if (n > 0) switch (_this186.mismatchMode) {\n        case ZipMismatchMode.FAIL:\n          throw new Error(\"Zipped streams should have the same length. Mismatched at element \".concat(_this186.count, \".\"));\n\n        case ZipMismatchMode.SHORTEST:\n          return {\n            value: null,\n            done: !0\n          };\n      }\n      return _this186.count++, {\n        value: r,\n        done: !1\n      };\n    })();\n  }\n\n  next() {\n    var _this187 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this187.currentPromise = _this187.nextState(_this187.currentPromise), _this187.currentPromise;\n    })();\n  }\n\n}\n\nclass PrefetchIterator extends LazyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.bufferSize = t, this.buffer = new RingBuffer(t);\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Prefetch\");\n  }\n\n  refill() {\n    for (; !this.buffer.isFull();) {\n      var _e788 = this.upstream.next();\n\n      this.buffer.push(_e788);\n    }\n  }\n\n  next() {\n    return this.refill(), this.buffer.shift();\n  }\n\n}\n\nclass ShuffleIterator extends PrefetchIterator {\n  constructor(e, t, n) {\n    super(e, t), this.upstream = e, this.windowSize = t, this.upstreamExhausted = !1, this.random = seedrandom.alea(n || now().toString()), this.lastRead = Promise.resolve({\n      value: null,\n      done: !1\n    });\n  }\n\n  next() {\n    var _this188 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this188.lastRead = _this188.lastRead.then(() => _this188.serialNext()), _this188.lastRead;\n    })();\n  }\n\n  randomInt(e) {\n    return Math.floor(this.random() * e);\n  }\n\n  chooseIndex() {\n    return this.randomInt(this.buffer.length());\n  }\n\n  serialNext() {\n    var _this189 = this;\n\n    return _asyncToGenerator(function* () {\n      for (_this189.upstreamExhausted || _this189.refill(); !_this189.buffer.isEmpty();) {\n        var _e789 = _this189.chooseIndex(),\n            t = yield _this189.buffer.shuffleExcise(_e789);\n\n        if (!t.done) return _this189.refill(), t;\n        _this189.upstreamExhausted = !0;\n      }\n\n      return {\n        value: null,\n        done: !0\n      };\n    })();\n  }\n\n}\n\nclass Dataset {\n  constructor() {\n    this.size = null;\n  }\n\n  batch(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = this;\n    var r;\n    return assert$4(e > 0, () => \"batchSize needs to be positive, but it is\\n      \".concat(e)), r = Infinity === this.size || null == this.size ? this.size : t ? Math.ceil(this.size / e) : Math.floor(this.size / e), datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield n.iterator()).columnMajorBatch(e, t, deepBatchConcat);\n    }), r);\n  }\n\n  concatenate(e) {\n    var t = this;\n    var n;\n    return n = Infinity === this.size || Infinity === e.size ? Infinity : null != this.size && null != e.size ? this.size + e.size : null, datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield t.iterator()).concatenate(yield e.iterator());\n    }), n);\n  }\n\n  filter(e) {\n    var t = this;\n    var n;\n    return n = Infinity === this.size ? Infinity : null, datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield t.iterator()).filter(t => tidy(() => e(t)));\n    }), n);\n  }\n\n  forEachAsync(e) {\n    var _this190 = this;\n\n    return _asyncToGenerator(function* () {\n      return (yield _this190.iterator()).forEachAsync(e);\n    })();\n  }\n\n  map(e) {\n    var t = this;\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield t.iterator()).map(t => tidy(() => e(t)));\n    }), this.size);\n  }\n\n  mapAsync(e) {\n    var t = this;\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield t.iterator()).mapAsync(e);\n    }), this.size);\n  }\n\n  prefetch(e) {\n    if (null == e) throw new RangeError(\"`Dataset.prefetch()` requires bufferSize to be specified.\");\n    var t = this;\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield t.iterator()).prefetch(e);\n    }), this.size);\n  }\n\n  repeat(e) {\n    var t = this;\n    var n;\n    return n = null != this.size && e > 0 ? this.size * e : 0 === e ? 0 : null != this.size && (void 0 === e || e < 0) ? Infinity : null, datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return iteratorFromConcatenated(iteratorFromFunction( /*#__PURE__*/_asyncToGenerator(function* () {\n        return {\n          value: yield t.iterator(),\n          done: !1\n        };\n      })).take(e));\n    }), n);\n  }\n\n  skip(e) {\n    var t = this;\n    var n;\n    return n = null != this.size && e >= 0 && this.size >= e ? this.size - e : null != this.size && (this.size < e || void 0 === e || e < 0) ? 0 : null, datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield t.iterator()).skip(e);\n    }), n);\n  }\n\n  shuffle(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n    if (null == e || e < 0) throw null == this.size ? new RangeError(\"`Dataset.shuffle()` requires bufferSize to be specified.\") : new RangeError(\"`Dataset.shuffle()` requires bufferSize to be specified.  If your data fits in main memory (for regular JS objects), and/or GPU memory (for `tf.Tensor`s), consider setting bufferSize to the dataset size (\".concat(this.size, \" elements)\"));\n    var r = this,\n        a = seedrandom.alea(t || now().toString());\n    return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      var t = a.int32();\n      return n && (t += a.int32()), (yield r.iterator()).shuffle(e, t.toString());\n    }), this.size);\n  }\n\n  take(e) {\n    var t = this;\n    var n;\n    return n = null != this.size && this.size > e ? e : null != this.size && this.size <= e ? this.size : null, datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n      return (yield t.iterator()).take(e);\n    }), n);\n  }\n\n  toArray() {\n    var _this191 = this;\n\n    return _asyncToGenerator(function* () {\n      if (Infinity === _this191.size) throw new Error(\"Can not convert infinite data stream to array.\");\n      return (yield _this191.iterator()).toArray();\n    })();\n  }\n\n  toArrayForTest() {\n    var _this192 = this;\n\n    return _asyncToGenerator(function* () {\n      if (Infinity === _this192.size) throw new Error(\"Can not convert infinite data stream to array.\");\n      return (yield _this192.iterator()).toArrayForTest();\n    })();\n  }\n\n}\n\nfunction datasetFromIteratorFn(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  return new class extends Dataset {\n    constructor() {\n      super(...arguments), this.size = t;\n    }\n\n    iterator() {\n      return _asyncToGenerator(function* () {\n        return e();\n      })();\n    }\n\n  }();\n}\n\nfunction array(e) {\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n    return iteratorFromItems(e);\n  }), e.length);\n}\n\nfunction zip(e) {\n  if (!isIterable(e)) throw new Error(\"The argument to zip() must be an object or array.\");\n  var t;\n  if (Array.isArray(e)) for (var n = 0; n < e.length; n++) {\n    t = null == t ? e[n].size : Math.min(t, e[n].size);\n  } else if (e instanceof Object) for (var _n304 in e) {\n    t = null == t ? e[_n304].size : Math.min(t, e[_n304].size);\n  }\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n    return iteratorFromZipped(yield deepMapAndAwaitAll(e, e => {\n      if (e instanceof Dataset) return {\n        value: e.iterator(),\n        recurse: !1\n      };\n      if (isIterable(e)) return {\n        value: null,\n        recurse: !0\n      };\n      throw new Error(\"Leaves of the structure passed to zip() must be Datasets, not primitives.\");\n    }), ZipMismatchMode.SHORTEST);\n  }), t);\n}\n\nfunction deepBatchConcat(e) {\n  return null === e ? null : canTensorify(e[0]) ? {\n    value: batchConcat(e),\n    recurse: !1\n  } : {\n    value: null,\n    recurse: !0\n  };\n}\n\nfunction batchConcat(e) {\n  if (0 === e.length) throw new Error(\"Can't make a batch of zero elements.\");\n  return e[0] instanceof Tensor ? stack(e) : tensor(e);\n}\n\nDataset.MAX_BUFFER_SIZE = 1e4;\n\nclass TextLineDataset extends Dataset {\n  constructor(e) {\n    super(), this.input = e;\n  }\n\n  iterator() {\n    var _this193 = this;\n\n    return _asyncToGenerator(function* () {\n      return (yield _this193.input.iterator()).decodeUTF8().split(\"\\n\").map(e => (e.endsWith(\"\\r\") && (e = e.slice(0, -1)), e));\n    })();\n  }\n\n}\n\nvar CODE_QUOTE = '\"',\n    STATE_OUT = Symbol(\"out\"),\n    STATE_FIELD = Symbol(\"field\"),\n    STATE_QUOTE = Symbol(\"quote\"),\n    STATE_QUOTE_AFTER_QUOTE = Symbol(\"quoteafterquote\"),\n    STATE_WITHIN_QUOTE_IN_QUOTE = Symbol(\"quoteinquote\");\n\nclass CSVDataset extends Dataset {\n  constructor(e, t) {\n    super(), this.input = e, this.hasHeader = !0, this.fullColumnNames = null, this.columnNamesValidated = !1, this.columnConfigs = null, this.configuredColumnsOnly = !1, this.delimiter = \",\", this.delimWhitespace = !1, this.base = new TextLineDataset(e), t || (t = {}), this.hasHeader = !1 !== t.hasHeader, this.fullColumnNames = t.columnNames, this.columnConfigs = t.columnConfigs, this.configuredColumnsOnly = t.configuredColumnsOnly, t.delimWhitespace ? (assert$4(null == t.delimiter, () => \"Delimiter should not be provided when delimWhitespace is true.\"), this.delimWhitespace = !0, this.delimiter = \" \") : this.delimiter = t.delimiter ? t.delimiter : \",\";\n  }\n\n  columnNames() {\n    var _this194 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this194.columnNamesValidated || (yield _this194.setColumnNames()), _this194.configuredColumnsOnly ? Object.keys(_this194.columnConfigs) : _this194.fullColumnNames;\n    })();\n  }\n\n  setColumnNames() {\n    var _this195 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this195.maybeReadHeaderLine();\n      if (!_this195.fullColumnNames && !e) throw new Error(\"Column names must be provided if there is no header line.\");\n      _this195.fullColumnNames && e && assert$4(e.length === _this195.fullColumnNames.length, () => \"The length of provided columnNames (\" + _this195.fullColumnNames.length.toString() + \") does not match the length of the header line read from file (\" + e.length.toString() + \").\"), _this195.fullColumnNames || (_this195.fullColumnNames = e);\n\n      var t = _this195.fullColumnNames.reduce((e, t) => (e[t] = e[t] + 1 || 1, e), {}),\n          n = Object.keys(t).filter(e => t[e] > 1);\n\n      if (assert$4(0 === n.length, () => \"Duplicate column names found: \" + n.toString()), _this195.columnConfigs) for (var _e790 of Object.keys(_this195.columnConfigs)) {\n        if (-1 === _this195.fullColumnNames.indexOf(_e790)) throw new Error('The key \"' + _e790 + '\" provided in columnConfigs does not match any of the column names (' + _this195.fullColumnNames.toString() + \").\");\n      }\n      _this195.columnNamesValidated = !0;\n    })();\n  }\n\n  maybeReadHeaderLine() {\n    var _this196 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this196.hasHeader) {\n        var _e791 = yield _this196.base.iterator(),\n            t = yield _e791.next();\n\n        if (t.done) throw new Error(\"No data was found for CSV parsing.\");\n        return _this196.parseRow(t.value, !1);\n      }\n\n      return null;\n    })();\n  }\n\n  iterator() {\n    var _this197 = this;\n\n    return _asyncToGenerator(function* () {\n      _this197.columnNamesValidated || (yield _this197.setColumnNames());\n      var e = yield _this197.base.iterator();\n      return _this197.hasHeader && (e = e.skip(1)), e.map(e => _this197.makeDataElement(e));\n    })();\n  }\n\n  makeDataElement(e) {\n    var t = this.parseRow(e),\n        n = {},\n        r = {};\n\n    for (var a = 0; a < this.fullColumnNames.length; a++) {\n      var s = this.fullColumnNames[a],\n          o = this.columnConfigs ? this.columnConfigs[s] : null;\n\n      if (!this.configuredColumnsOnly || o) {\n        var i = t[a];\n        var l = null;\n        if (\"\" === i) {\n          if (o && void 0 !== o.default) l = o.default;else {\n            if (o && (o.required || o.isLabel)) throw new Error(\"Required column \".concat(s, \" is empty in this line: \").concat(e));\n            l = void 0;\n          }\n        } else {\n          var _e792 = Number(i);\n\n          if (isNaN(_e792)) l = o && \"bool\" === o.dtype ? this.getBoolean(i) : i;else if (o && o.dtype) switch (o.dtype) {\n            case \"float32\":\n              l = _e792;\n              break;\n\n            case \"int32\":\n              l = Math.floor(_e792);\n              break;\n\n            case \"bool\":\n              l = this.getBoolean(i);\n              break;\n\n            default:\n              l = _e792;\n          } else l = _e792;\n        }\n        o && o.isLabel ? r[s] = l : n[s] = l;\n      }\n    }\n\n    return 0 === Object.keys(r).length ? n : {\n      xs: n,\n      ys: r\n    };\n  }\n\n  getBoolean(e) {\n    return \"1\" === e || \"true\" === e.toLowerCase() ? 1 : 0;\n  }\n\n  parseRow(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !0;\n    var n = [];\n    var r = 0;\n    var a = e.length;\n    var s = STATE_OUT;\n\n    for (var _t544 = 0; _t544 < a; _t544++) {\n      switch (s) {\n        case STATE_OUT:\n          switch (e.charAt(_t544)) {\n            case CODE_QUOTE:\n              r = _t544 + 1, s = STATE_QUOTE;\n              break;\n\n            case this.delimiter:\n              if (r = _t544 + 1, \" \" === this.delimiter && this.delimWhitespace) break;\n              n.push(\"\"), s = STATE_OUT;\n              break;\n\n            default:\n              s = STATE_FIELD, r = _t544;\n          }\n\n          break;\n\n        case STATE_FIELD:\n          switch (e.charAt(_t544)) {\n            case this.delimiter:\n              n.push(e.substring(r, _t544)), s = STATE_OUT, r = _t544 + 1;\n          }\n\n          break;\n\n        case STATE_QUOTE:\n          switch (e.charAt(_t544)) {\n            case CODE_QUOTE:\n              s = STATE_QUOTE_AFTER_QUOTE;\n          }\n\n          break;\n\n        case STATE_QUOTE_AFTER_QUOTE:\n          switch (e.charAt(_t544)) {\n            case this.delimiter:\n              n.push(e.substring(r, _t544 - 1)), s = STATE_OUT, r = _t544 + 1;\n              break;\n\n            case CODE_QUOTE:\n              s = STATE_QUOTE;\n              break;\n\n            default:\n              s = STATE_WITHIN_QUOTE_IN_QUOTE;\n          }\n\n          break;\n\n        case STATE_WITHIN_QUOTE_IN_QUOTE:\n          switch (e.charAt(_t544)) {\n            case CODE_QUOTE:\n              s = STATE_QUOTE;\n          }\n\n      }\n    }\n\n    if (n.push(s === STATE_QUOTE_AFTER_QUOTE ? e.substring(r, a - 1) : e.substring(r)), t && n.length !== this.fullColumnNames.length) throw new Error(\"Invalid row in csv file. Should have \".concat(this.fullColumnNames.length, \" elements in a row, but got \").concat(n));\n    return n;\n  }\n\n}\n\nclass MicrophoneIterator extends LazyIterator {\n  constructor(e) {\n    super(), this.microphoneConfig = e, this.isClosed = !1, this.fftSize = e.fftSize || 1024;\n    var t = Math.log2(this.fftSize);\n    if (this.fftSize < 0 || t < 4 || t > 14 || !Number.isInteger(t)) throw new Error(\"Invalid fftSize: it must be a power of 2 between 2 to 4 and 2 to 14, but got \".concat(this.fftSize));\n    if (this.numFrames = e.numFramesPerSpectrogram || 43, this.sampleRateHz = e.sampleRateHz, this.columnTruncateLength = e.columnTruncateLength || this.fftSize, this.audioTrackConstraints = e.audioTrackConstraints, this.smoothingTimeConstant = e.smoothingTimeConstant || 0, this.includeSpectrogram = !1 !== e.includeSpectrogram, this.includeWaveform = !0 === e.includeWaveform, !this.includeSpectrogram && !this.includeWaveform) throw new Error(\"Both includeSpectrogram and includeWaveform are false. At least one type of data should be returned.\");\n  }\n\n  summary() {\n    return \"microphone\";\n  }\n\n  static create() {\n    var _arguments8 = arguments;\n    return _asyncToGenerator(function* () {\n      var e = _arguments8.length > 0 && _arguments8[0] !== undefined ? _arguments8[0] : {};\n      if (env().get(\"IS_NODE\")) throw new Error(\"microphone API is only supported in browser environment.\");\n      var t = new MicrophoneIterator(e);\n      return yield t.start(), t;\n    })();\n  }\n\n  start() {\n    var _this198 = this;\n\n    return _asyncToGenerator(function* () {\n      try {\n        _this198.stream = yield navigator.mediaDevices.getUserMedia({\n          audio: null == _this198.audioTrackConstraints || _this198.audioTrackConstraints,\n          video: !1\n        });\n      } catch (e) {\n        throw new Error(\"Error thrown while initializing video stream: \".concat(e.message));\n      }\n\n      if (!_this198.stream) throw new Error(\"Could not obtain audio from microphone.\");\n      var e = window.AudioContext || window.webkitAudioContext;\n\n      if (_this198.audioContext = new e(), _this198.sampleRateHz) {\n        if (_this198.audioContext.sampleRate !== _this198.sampleRateHz) throw new Error(\"Mismatch in sampling rate: Expected: \".concat(_this198.sampleRateHz, \"; Actual: \").concat(_this198.audioContext.sampleRate));\n      } else _this198.sampleRateHz = _this198.audioContext.sampleRate;\n\n      var t = _this198.audioContext.createMediaStreamSource(_this198.stream);\n\n      _this198.analyser = _this198.audioContext.createAnalyser(), _this198.analyser.fftSize = 2 * _this198.fftSize, _this198.analyser.smoothingTimeConstant = _this198.smoothingTimeConstant, t.connect(_this198.analyser), _this198.freqData = new Float32Array(_this198.fftSize), _this198.timeData = new Float32Array(_this198.fftSize);\n    })();\n  }\n\n  next() {\n    var _this199 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this199.isClosed) return {\n        value: null,\n        done: !0\n      };\n      var e, t;\n      var n = yield _this199.getAudioData();\n\n      if (_this199.includeSpectrogram) {\n        var _t545 = _this199.flattenQueue(n.freqDataQueue);\n\n        e = _this199.getTensorFromAudioDataArray(_t545, [_this199.numFrames, _this199.columnTruncateLength, 1]);\n      }\n\n      if (_this199.includeWaveform) {\n        var _e793 = _this199.flattenQueue(n.timeDataQueue);\n\n        t = _this199.getTensorFromAudioDataArray(_e793, [_this199.numFrames * _this199.fftSize, 1]);\n      }\n\n      return {\n        value: {\n          spectrogram: e,\n          waveform: t\n        },\n        done: !1\n      };\n    })();\n  }\n\n  capture() {\n    var _this200 = this;\n\n    return _asyncToGenerator(function* () {\n      return (yield _this200.next()).value;\n    })();\n  }\n\n  getAudioData() {\n    var _this201 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = [],\n          t = [];\n      var n = 0;\n      return new Promise(r => {\n        var a = setInterval(() => {\n          _this201.includeSpectrogram && (_this201.analyser.getFloatFrequencyData(_this201.freqData), -Infinity === _this201.freqData[0] && r({\n            freqDataQueue: e,\n            timeDataQueue: t\n          }), e.push(_this201.freqData.slice(0, _this201.columnTruncateLength))), _this201.includeWaveform && (_this201.analyser.getFloatTimeDomainData(_this201.timeData), t.push(_this201.timeData.slice())), ++n === _this201.numFrames && (clearInterval(a), r({\n            freqDataQueue: e,\n            timeDataQueue: t\n          }));\n        }, _this201.fftSize / _this201.sampleRateHz * 1e3);\n      });\n    })();\n  }\n\n  stop() {\n    this.isClosed || (this.isClosed = !0, this.analyser.disconnect(), this.audioContext.close(), null != this.stream && this.stream.getTracks().length > 0 && this.stream.getTracks()[0].stop());\n  }\n\n  toArray() {\n    throw new Error(\"Can not convert infinite audio stream to array.\");\n  }\n\n  getSampleRate() {\n    return this.sampleRateHz;\n  }\n\n  flattenQueue(e) {\n    var t = e[0].length,\n        n = new Float32Array(e.length * t);\n    return e.forEach((e, r) => n.set(e, r * t)), n;\n  }\n\n  getTensorFromAudioDataArray(e, t) {\n    var n = new Float32Array(sizeFromShape(t));\n    return n.set(e, n.length - e.length), tensor(n, t);\n  }\n\n}\n\nclass WebcamIterator extends LazyIterator {\n  constructor(e, t) {\n    if (super(), this.webcamVideoElement = e, this.webcamConfig = t, this.isClosed = !0, this.resize = !1, this.needToResize()) if (this.resize = !0, this.cropSize = [this.webcamConfig.resizeHeight, this.webcamConfig.resizeWidth], this.cropBoxInd = tensor1d([0], \"int32\"), this.webcamConfig.centerCrop) {\n      var _e794 = 1 * this.webcamConfig.resizeWidth / this.webcamVideoElement.width,\n          _t546 = 1 * this.webcamConfig.resizeHeight / this.webcamVideoElement.height,\n          n = (1 - _e794) / 2,\n          r = (1 - _t546) / 2;\n\n      this.cropBox = tensor2d([r, n, _t546 + r, n + _e794], [1, 4]);\n    } else this.cropBox = tensor2d([0, 0, 1, 1], [1, 4]);\n  }\n\n  summary() {\n    return \"webcam\";\n  }\n\n  static create(e) {\n    var _arguments9 = arguments;\n    return _asyncToGenerator(function* () {\n      var t = _arguments9.length > 1 && _arguments9[1] !== undefined ? _arguments9[1] : {};\n      if (env().get(\"IS_NODE\")) throw new Error(\"tf.data.webcam is only supported in browser environment.\");\n\n      if (!e) {\n        if (e = document.createElement(\"video\"), !t.resizeWidth || !t.resizeHeight) throw new Error(\"Please provide webcam video element, or resizeWidth and resizeHeight to create a hidden video element.\");\n        e.width = t.resizeWidth, e.height = t.resizeHeight;\n      }\n\n      var n = new WebcamIterator(e, t);\n      return yield n.start(), n;\n    })();\n  }\n\n  start() {\n    var _this202 = this;\n\n    return _asyncToGenerator(function* () {\n      _this202.webcamConfig.facingMode && assert$4(\"user\" === _this202.webcamConfig.facingMode || \"environment\" === _this202.webcamConfig.facingMode, () => \"Invalid webcam facing mode: \".concat(_this202.webcamConfig.facingMode, \". Please provide 'user' or 'environment'\"));\n\n      try {\n        _this202.stream = yield navigator.mediaDevices.getUserMedia({\n          video: {\n            deviceId: _this202.webcamConfig.deviceId,\n            facingMode: _this202.webcamConfig.facingMode ? _this202.webcamConfig.facingMode : \"user\",\n            width: _this202.webcamVideoElement.width,\n            height: _this202.webcamVideoElement.height\n          }\n        });\n      } catch (e) {\n        throw e.message = \"Error thrown while initializing video stream: \".concat(e.message), e;\n      }\n\n      if (!_this202.stream) throw new Error(\"Could not obtain video from webcam.\");\n\n      try {\n        _this202.webcamVideoElement.srcObject = _this202.stream;\n      } catch (e) {\n        console.log(e), _this202.webcamVideoElement.src = window.URL.createObjectURL(_this202.stream);\n      }\n\n      return _this202.webcamVideoElement.play(), _this202.isClosed = !1, new Promise(e => {\n        _this202.webcamVideoElement.onloadedmetadata = () => {\n          e();\n        };\n      });\n    })();\n  }\n\n  next() {\n    var _this203 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this203.isClosed) return {\n        value: null,\n        done: !0\n      };\n      var e;\n\n      try {\n        e = fromPixels$1(_this203.webcamVideoElement);\n      } catch (e) {\n        throw new Error(\"Error thrown converting video to pixels: \".concat(JSON.stringify(e)));\n      }\n\n      if (!_this203.resize) return {\n        value: e,\n        done: !1\n      };\n\n      try {\n        return {\n          value: _this203.cropAndResizeFrame(e),\n          done: !1\n        };\n      } catch (e) {\n        throw new Error(\"Error thrown cropping the video: \".concat(e.message));\n      } finally {\n        e.dispose();\n      }\n    })();\n  }\n\n  needToResize() {\n    return !(!this.webcamConfig.resizeWidth || !this.webcamConfig.resizeHeight || this.webcamVideoElement.width === this.webcamConfig.resizeWidth && this.webcamVideoElement.height === this.webcamConfig.resizeHeight);\n  }\n\n  cropAndResizeFrame(e) {\n    return tidy(() => {\n      var t = expandDims$3(cast$3(e, \"float32\"), 0);\n      var n;\n      return n = image$1.cropAndResize(t, this.cropBox, this.cropBoxInd, this.cropSize, \"bilinear\"), reshape$3(n, n.shape.slice(1));\n    });\n  }\n\n  capture() {\n    var _this204 = this;\n\n    return _asyncToGenerator(function* () {\n      return (yield _this204.next()).value;\n    })();\n  }\n\n  stop() {\n    this.stream.getTracks().forEach(e => e.stop());\n\n    try {\n      this.webcamVideoElement.srcObject = null;\n    } catch (e) {\n      console.log(e), this.webcamVideoElement.src = null;\n    }\n\n    this.isClosed = !0;\n  }\n\n  toArray() {\n    throw new Error(\"Can not convert infinite video stream to array.\");\n  }\n\n}\n\nclass DataSource {}\n\nclass StringIterator extends LazyIterator {\n  split(e) {\n    return new SplitIterator(this, e);\n  }\n\n}\n\nclass SplitIterator extends StringIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.impl = new SplitIteratorImpl(e, t);\n  }\n\n  summary() {\n    return this.impl.summary();\n  }\n\n  next() {\n    var _this205 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this205.impl.next();\n    })();\n  }\n\n}\n\nclass SplitIteratorImpl extends OneToManyIterator {\n  constructor(e, t) {\n    super(), this.upstream = e, this.separator = t, this.carryover = \"\";\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Split('\").concat(this.separator, \"')\");\n  }\n\n  pump() {\n    var _this206 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this206.upstream.next();\n      if (e.done) return \"\" !== _this206.carryover && (_this206.outputQueue.push(_this206.carryover), _this206.carryover = \"\", !0);\n      var t = e.value.split(_this206.separator);\n      t[0] = _this206.carryover + t[0];\n\n      for (var _e795 of t.slice(0, -1)) {\n        _this206.outputQueue.push(_e795);\n      }\n\n      return _this206.carryover = t[t.length - 1], !0;\n    })();\n  }\n\n}\n\nclass ByteChunkIterator extends LazyIterator {\n  decodeUTF8() {\n    return new Utf8Iterator(this);\n  }\n\n}\n\nclass Utf8Iterator extends StringIterator {\n  constructor(e) {\n    super(), this.upstream = e, this.impl = new Utf8IteratorImpl(e);\n  }\n\n  summary() {\n    return this.impl.summary();\n  }\n\n  next() {\n    var _this207 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this207.impl.next();\n    })();\n  }\n\n}\n\nclass Utf8IteratorImpl extends OneToManyIterator {\n  constructor(e) {\n    if (super(), this.upstream = e, env().get(\"IS_BROWSER\")) this.decoder = new TextDecoder(\"utf-8\");else {\n      var {\n        StringDecoder: _e796\n      } = require(\"string_decoder\");\n\n      this.decoder = new _e796(\"utf8\");\n    }\n  }\n\n  summary() {\n    return \"\".concat(this.upstream.summary(), \" -> Utf8\");\n  }\n\n  pump() {\n    var _this208 = this;\n\n    return _asyncToGenerator(function* () {\n      var e = yield _this208.upstream.next();\n      var t, n;\n      return !e.done && (t = e.value, n = env().get(\"IS_BROWSER\") ? _this208.decoder.decode(t, {\n        stream: !0\n      }) : _this208.decoder.write(Buffer.from(t.buffer)), _this208.outputQueue.push(n), !0);\n    })();\n  }\n\n}\n\nclass FileChunkIterator extends ByteChunkIterator {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    super(), this.file = e, this.options = t, assert$4(e instanceof Uint8Array || !!env().get(\"IS_BROWSER\") && (e instanceof File || e instanceof Blob), () => \"FileChunkIterator only supports File, Blob and Uint8Array right now.\"), this.offset = t.offset || 0, this.chunkSize = t.chunkSize || 1048576;\n  }\n\n  summary() {\n    return \"FileChunks \".concat(this.file);\n  }\n\n  next() {\n    var _this209 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this209.offset >= (_this209.file instanceof Uint8Array ? _this209.file.byteLength : _this209.file.size)) return {\n        value: null,\n        done: !0\n      };\n      var e = new Promise((e, t) => {\n        var n = _this209.offset + _this209.chunkSize;\n        if (_this209.file instanceof Uint8Array) e(new Uint8Array(_this209.file.slice(_this209.offset, n)));else {\n          var r = new FileReader();\n          r.onload = n => {\n            var a = r.result;\n            if (a instanceof ArrayBuffer && (a = new Uint8Array(a)), !(a instanceof Uint8Array)) return t(new TypeError(\"FileReader returned unknown type.\"));\n            e(a);\n          }, r.onabort = e => t(new Error(\"Aborted\")), r.onerror = e => t(new Error(e.type));\n\n          var a = _this209.file.slice(_this209.offset, n);\n\n          r.readAsArrayBuffer(a);\n        }\n        _this209.offset = n;\n      });\n      return {\n        value: yield e,\n        done: !1\n      };\n    })();\n  }\n\n}\n\nfunction urlChunkIterator(_x135) {\n  return _urlChunkIterator.apply(this, arguments);\n}\n\nfunction _urlChunkIterator() {\n  _urlChunkIterator = _asyncToGenerator(function* (e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    var n, r;\n    \"string\" == typeof e ? n = e : (n = e.url, r = getRequestInitFromRequest(e));\n    var a = yield fetch$1(n, r);\n\n    if (a.ok) {\n      var _e1164 = new Uint8Array(yield a.arrayBuffer());\n\n      return new FileChunkIterator(_e1164, t);\n    }\n\n    throw new Error(a.statusText);\n  });\n  return _urlChunkIterator.apply(this, arguments);\n}\n\nvar getRequestInitFromRequest = e => ({\n  method: e.method,\n  headers: e.headers,\n  body: e.body,\n  mode: e.mode,\n  credentials: e.credentials,\n  cache: e.cache,\n  redirect: e.redirect,\n  referrer: e.referrer,\n  integrity: e.integrity\n});\n\nfunction isLocalPath(e) {\n  return \"string\" == typeof e && \"file://\" === e.substr(0, 7);\n}\n\nclass FileDataSource extends DataSource {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    super(), this.input = e, this.options = t;\n  }\n\n  iterator() {\n    var _this210 = this;\n\n    return _asyncToGenerator(function* () {\n      if (isLocalPath(_this210.input) && env().get(\"IS_NODE\")) {\n        var _e797 = require(\"fs\");\n\n        _this210.input = _e797.readFileSync(_this210.input.substr(7));\n      }\n\n      return new FileChunkIterator(_this210.input, _this210.options);\n    })();\n  }\n\n}\n\nclass URLDataSource extends DataSource {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    super(), this.url = e, this.fileOptions = t;\n  }\n\n  iterator() {\n    var _this211 = this;\n\n    return _asyncToGenerator(function* () {\n      return isLocalPath(_this211.url) ? new FileDataSource(_this211.url, _this211.fileOptions).iterator() : urlChunkIterator(_this211.url, _this211.fileOptions);\n    })();\n  }\n\n}\n\nfunction csv(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return new CSVDataset(new URLDataSource(e), t);\n}\n\nfunction func(e) {\n  var t = iteratorFromFunction(e);\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n    return t;\n  }));\n}\n\nfunction generator(e) {\n  return datasetFromIteratorFn( /*#__PURE__*/_asyncToGenerator(function* () {\n    var t = yield e();\n    return iteratorFromFunction(() => t.next());\n  }));\n}\n\nfunction webcam(_x136, _x137) {\n  return _webcam.apply(this, arguments);\n}\n\nfunction _webcam() {\n  _webcam = _asyncToGenerator(function* (e, t) {\n    return WebcamIterator.create(e, t);\n  });\n  return _webcam.apply(this, arguments);\n}\n\nfunction microphone(_x138) {\n  return _microphone.apply(this, arguments);\n}\n\nfunction _microphone() {\n  _microphone = _asyncToGenerator(function* (e) {\n    return MicrophoneIterator.create(e);\n  });\n  return _microphone.apply(this, arguments);\n}\n\nvar version$4 = \"3.8.0\";\nvar index = {\n  __proto__: null,\n  array,\n  Dataset,\n  zip,\n  CSVDataset,\n  TextLineDataset,\n  csv,\n  func,\n  generator,\n  microphone,\n  webcam,\n  FileDataSource,\n  URLDataSource,\n  version_data: version$4\n};\n\nfunction assertNotComplex$1(e, t) {\n  Array.isArray(e) || (e = [e]), e.forEach(e => {\n    null != e && assert$4(\"complex64\" !== e.dtype, () => \"\".concat(t, \" does not support complex64 tensors in the CPU backend.\"));\n  });\n}\n\nvar whereImpl$1 = whereImpl$2;\n\nclass MathBackendCPU extends KernelBackend {\n  constructor() {\n    super(), this.blockSize = 48, this.firstUse = !0, this.data = new DataStorage(this, engine());\n  }\n\n  nextDataId() {\n    return MathBackendCPU.nextDataId++;\n  }\n\n  write(e, t, n) {\n    this.firstUse && (this.firstUse = !1, env().get(\"IS_NODE\") && warn(\"\\n============================\\nHi there 👋. Looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, which binds to TensorFlow C++, by running npm i @tensorflow/tfjs-node, or npm i @tensorflow/tfjs-node-gpu if you have CUDA. Then call require('@tensorflow/tfjs-node'); (-gpu suffix for CUDA) at the start of your program. Visit https://github.com/tensorflow/tfjs-node for more details.\\n============================\"));\n    var r = {\n      id: this.nextDataId()\n    };\n    return this.data.set(r, {\n      values: e,\n      dtype: n,\n      refCount: 1\n    }), r;\n  }\n\n  makeTensorInfo(e, t, n) {\n    var r;\n\n    if (\"string\" === t && null != n && n.length > 0 && isString(n[0])) {\n      var a = n.map(e => encodeString(e));\n      r = this.write(a, e, t);\n    } else r = this.write(n, e, t);\n\n    return {\n      dataId: r,\n      shape: e,\n      dtype: t\n    };\n  }\n\n  refCount(e) {\n    return this.data.has(e) ? this.data.get(e).refCount : 0;\n  }\n\n  incRef(e) {\n    this.data.get(e).refCount++;\n  }\n\n  decRef(e) {\n    this.data.has(e) && this.data.get(e).refCount--;\n  }\n\n  move(e, t, n, r, a) {\n    this.data.set(e, {\n      values: t,\n      dtype: r,\n      refCount: a\n    });\n  }\n\n  numDataIds() {\n    return this.data.numDataIds();\n  }\n\n  read(e) {\n    var _this212 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this212.readSync(e);\n    })();\n  }\n\n  readSync(e) {\n    var {\n      dtype: t,\n      complexTensorInfos: n\n    } = this.data.get(e);\n    return \"complex64\" === t ? mergeRealAndImagArrays(this.readSync(n.real.dataId), this.readSync(n.imag.dataId)) : this.data.get(e).values;\n  }\n\n  bufferSync(e) {\n    var t = this.readSync(e.dataId);\n    var n = t;\n    if (\"string\" === e.dtype) try {\n      n = t.map(e => decodeString(e));\n    } catch (e) {\n      throw new Error(\"Failed to decode encoded string bytes into utf-8\");\n    }\n    return buffer(e.shape, e.dtype, n);\n  }\n\n  makeOutput(e, t, n) {\n    var r = this.write(e, t, n);\n    return engine().makeTensorFromDataId(r, t, n, this);\n  }\n\n  disposeData(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n\n    if (this.data.has(e)) {\n      if (this.data.get(e).refCount--, !t && this.data.get(e).refCount > 0) return !1;\n      var {\n        complexTensorInfos: n\n      } = this.data.get(e);\n      null != n && (this.disposeData(n.real.dataId, !0), this.disposeData(n.imag.dataId, !0)), this.data.delete(e);\n    }\n\n    return !0;\n  }\n\n  disposeIntermediateTensorInfo(e) {\n    this.disposeData(e.dataId);\n  }\n\n  time(e) {\n    return _asyncToGenerator(function* () {\n      var t = now();\n      return e(), {\n        kernelMs: now() - t\n      };\n    })();\n  }\n\n  memory() {\n    return {\n      unreliable: !0,\n      reasons: [\"The reported memory is an upper bound. Due to automatic garbage collection, the true allocated memory may be less.\"]\n    };\n  }\n\n  where(e) {\n    assertNotComplex$1([e], \"where\");\n    var t = this.readSync(e.dataId);\n    return whereImpl$1(e.shape, t);\n  }\n\n  dispose() {}\n\n  floatPrecision() {\n    return 32;\n  }\n\n  epsilon() {\n    return super.epsilon();\n  }\n\n}\n\nfunction simpleAbsImpl(e) {\n  var t = new Float32Array(e.length);\n\n  for (var n = 0; n < e.length; ++n) {\n    t[n] = Math.abs(e[n]);\n  }\n\n  return t;\n}\n\nMathBackendCPU.nextDataId = 0;\n\nvar abs$1 = e => {\n  var {\n    x: t\n  } = e.inputs,\n      n = e.backend;\n  assertNotComplex$1(t, \"abs\");\n  var r = new Float32Array(sizeFromShape(t.shape));\n  return r = simpleAbsImpl(n.data.get(t.dataId).values), n.makeOutput(r, t.shape, \"float32\");\n},\n    absConfig$1 = {\n  kernelName: Abs,\n  backendName: \"cpu\",\n  kernelFunc: abs$1\n};\n\nfunction createSimpleBinaryKernelImpl(e) {\n  return (t, n, r, a, s) => {\n    var o = assertAndGetBroadcastShape(t, n),\n        i = o.length,\n        l = computeStrides(o),\n        u = getTypedArrayFromDType(s, sizeFromShape(o)),\n        c = t.length,\n        p = n.length,\n        d = computeStrides(t),\n        h = computeStrides(n),\n        m = getBroadcastDims$1(t, o),\n        f = getBroadcastDims$1(n, o);\n    if (m.length + f.length === 0) for (var _t547 = 0; _t547 < u.length; ++_t547) {\n      u[_t547] = e(r[_t547 % r.length], a[_t547 % a.length]);\n    } else {\n      var _loop51 = function _loop51(_t548) {\n        var n = indexToLoc(_t548, i, l),\n            s = n.slice(-c);\n        m.forEach(e => s[e] = 0);\n        var o = locToIndex(s, c, d),\n            g = n.slice(-p);\n        f.forEach(e => g[e] = 0);\n        var $ = locToIndex(g, p, h);\n        u[_t548] = e(r[o], a[$]);\n      };\n\n      for (var _t548 = 0; _t548 < u.length; ++_t548) {\n        _loop51(_t548);\n      }\n    }\n    return [u, o];\n  };\n}\n\nfunction complex$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    real: r,\n    imag: a\n  } = t,\n      s = n.data.get(r.dataId).values,\n      o = n.data.get(a.dataId).values,\n      i = n.makeTensorInfo(r.shape, \"complex64\");\n  return n.data.get(i.dataId).complexTensorInfos = {\n    real: n.makeTensorInfo(r.shape, \"float32\", s),\n    imag: n.makeTensorInfo(a.shape, \"float32\", o)\n  }, i;\n}\n\nvar complexConfig$1 = {\n  kernelName: Complex,\n  backendName: \"cpu\",\n  kernelFunc: complex$1\n};\n\nfunction zeros(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"float32\";\n  if (\"complex64\" === n) return complex$1({\n    inputs: {\n      real: zeros(e, t, \"float32\"),\n      imag: zeros(e, t, \"float32\")\n    },\n    backend: e\n  });\n  var r = makeZerosTypedArray(sizeFromShape(t), n);\n  return e.makeTensorInfo(t, n, r);\n}\n\nfunction identity$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  return n.incRef(r.dataId), {\n    dataId: r.dataId,\n    shape: r.shape,\n    dtype: r.dtype\n  };\n}\n\nvar identityConfig$1 = {\n  kernelName: Identity$1,\n  backendName: \"cpu\",\n  kernelFunc: identity$1\n};\n\nfunction real$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = n.data.get(r.dataId).complexTensorInfos.real,\n      s = n.data.get(a.dataId).values;\n  return n.makeTensorInfo(a.shape, a.dtype, s);\n}\n\nvar realConfig$1 = {\n  kernelName: Real,\n  backendName: \"cpu\",\n  kernelFunc: real$1\n};\n\nfunction cast$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dtype: s\n  } = r;\n\n  if (\"complex64\" === s) {\n    if (\"complex64\" === a.dtype) return identity$1({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    var _e798 = zeros(n, a.shape, a.dtype),\n        _t549 = cast$1({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        dtype: \"float32\"\n      }\n    }),\n        _r323 = complex$1({\n      inputs: {\n        real: _t549,\n        imag: _e798\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e798), n.disposeIntermediateTensorInfo(_t549), _r323;\n  }\n\n  if (\"complex64\" === a.dtype) {\n    var _e799 = real$1({\n      inputs: {\n        input: a\n      },\n      backend: n\n    }),\n        _t550 = cast$1({\n      inputs: {\n        x: _e799\n      },\n      backend: n,\n      attrs: {\n        dtype: s\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(_e799), _t550;\n  }\n\n  if (!hasEncodingLoss(a.dtype, s)) {\n    var _e800 = identity$1({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    return {\n      dataId: _e800.dataId,\n      shape: _e800.shape,\n      dtype: s\n    };\n  }\n\n  if (\"int32\" === s) {\n    var _e801 = n.data.get(a.dataId).values,\n        _t551 = Int32Array.from(_e801);\n\n    return n.makeTensorInfo(a.shape, \"int32\", _t551);\n  }\n\n  if (\"bool\" === s) {\n    var _e802 = n.data.get(a.dataId).values,\n        _t552 = toTypedArray([0], a.dtype),\n        [_r324, _s161] = createSimpleBinaryKernelImpl((e, t) => e !== t ? 1 : 0)(a.shape, [], _e802, _t552, \"bool\");\n\n    return n.makeTensorInfo(_s161, \"bool\", _r324);\n  }\n\n  throw new Error(\"Error in Cast: failed to cast \".concat(a.dtype, \" to \").concat(s));\n}\n\nvar castConfig$1 = {\n  kernelName: Cast,\n  backendName: \"cpu\",\n  kernelFunc: cast$1\n};\n\nfunction binaryKernelFunc$1(e, t, n, r) {\n  return null == n ? _ref50 => {\n    var {\n      inputs: n,\n      backend: a\n    } = _ref50;\n    var {\n      a: s,\n      b: o\n    } = n,\n        i = a;\n    assertNotComplex$1([s, o], e);\n    var l = i.data.get(s.dataId).values,\n        u = i.data.get(o.dataId).values,\n        c = \"string\" === s.dtype ? fromUint8ToStringArray(l) : l,\n        p = \"string\" === s.dtype ? fromUint8ToStringArray(u) : u,\n        d = r || s.dtype,\n        [h, m] = t(s.shape, o.shape, c, p, d);\n    return i.makeTensorInfo(m, d, h);\n  } : _ref51 => {\n    var {\n      inputs: e,\n      backend: a\n    } = _ref51;\n    var {\n      a: s,\n      b: o\n    } = e,\n        i = a;\n\n    if (\"complex64\" === s.dtype || \"complex64\" === o.dtype) {\n      var _e803 = cast$1({\n        inputs: {\n          x: s\n        },\n        backend: i,\n        attrs: {\n          dtype: \"complex64\"\n        }\n      }),\n          _t553 = i.data.get(_e803.dataId),\n          _r325 = _t553.complexTensorInfos.imag,\n          _a234 = i.data.get(_t553.complexTensorInfos.real.dataId).values,\n          l = i.data.get(_r325.dataId).values,\n          u = cast$1({\n        inputs: {\n          x: o\n        },\n        backend: i,\n        attrs: {\n          dtype: \"complex64\"\n        }\n      }),\n          c = i.data.get(u.dataId),\n          _p26 = c.complexTensorInfos.imag,\n          d = i.data.get(c.complexTensorInfos.real.dataId).values,\n          h = i.data.get(_p26.dataId).values,\n          [m, f, g] = n(s.shape, o.shape, _a234, l, d, h),\n          $ = i.makeTensorInfo(g, \"float32\", m),\n          y = i.makeTensorInfo(g, \"float32\", f),\n          b = complex$1({\n        inputs: {\n          real: $,\n          imag: y\n        },\n        backend: i\n      });\n\n      return i.disposeIntermediateTensorInfo(_e803), i.disposeIntermediateTensorInfo(u), i.disposeIntermediateTensorInfo($), i.disposeIntermediateTensorInfo(y), b;\n    }\n\n    {\n      var _e804 = i.data.get(s.dataId).values,\n          _n305 = i.data.get(o.dataId).values,\n          _a235 = r || s.dtype,\n          [_l42, _u36] = t(s.shape, o.shape, _e804, _n305, _a235);\n\n      return i.makeTensorInfo(_u36, _a235, _l42);\n    }\n  };\n}\n\nfunction createComplexBinaryKernelImpl(e) {\n  return (t, n, r, a, s, o) => {\n    var i = assertAndGetBroadcastShape(t, n),\n        l = sizeFromShape(i),\n        u = i.length,\n        c = computeStrides(i),\n        p = getTypedArrayFromDType(\"float32\", l),\n        d = getTypedArrayFromDType(\"float32\", l),\n        h = getBroadcastDims$1(t, i),\n        m = getBroadcastDims$1(n, i),\n        f = mergeRealAndImagArrays(r, a),\n        g = mergeRealAndImagArrays(s, o),\n        $ = t.length,\n        y = computeStrides(t),\n        b = n.length,\n        x = computeStrides(n);\n    if (h.length + m.length === 0) for (var _t554 = 0; _t554 < p.length; _t554++) {\n      var _n306 = _t554 % f.length,\n          _r326 = _t554 % g.length,\n          _a236 = e(f[2 * _n306], f[2 * _n306 + 1], g[2 * _r326], g[2 * _r326 + 1]);\n\n      p[_t554] = _a236.real, d[_t554] = _a236.imag;\n    } else {\n      var _loop52 = function _loop52(_t555) {\n        var n = indexToLoc(_t555, u, c),\n            r = n.slice(-$);\n        h.forEach(e => r[e] = 0);\n        var a = locToIndex(r, $, y),\n            s = n.slice(-b);\n        m.forEach(e => s[e] = 0);\n        var o = locToIndex(s, b, x),\n            i = e(f[2 * a], f[2 * a + 1], g[2 * o], g[2 * o + 1]);\n        p[_t555] = i.real, d[_t555] = i.imag;\n      };\n\n      for (var _t555 = 0; _t555 < p.length; _t555++) {\n        _loop52(_t555);\n      }\n    }\n    return [p, d, i];\n  };\n}\n\nvar addImpl = createSimpleBinaryKernelImpl((e, t) => e + t),\n    addComplexImpl = createComplexBinaryKernelImpl((e, t, n, r) => ({\n  real: e + n,\n  imag: t + r\n})),\n    add = binaryKernelFunc$1(Add$1, addImpl, addComplexImpl),\n    addConfig$1 = {\n  kernelName: Add$1,\n  backendName: \"cpu\",\n  kernelFunc: add\n};\n\nfunction bincountImpl(e, t, n, r, a) {\n  var s = sizeFromShape(r),\n      o = makeZerosTypedArray(a, n);\n\n  for (var _n307 = 0; _n307 < e.length; _n307++) {\n    var _r327 = e[_n307];\n    if (_r327 < 0) throw new Error(\"Input x must be non-negative!\");\n    _r327 >= a || (o[_r327] += s > 0 ? t[_n307] : 1);\n  }\n\n  return o;\n}\n\nfunction bincountReduceImpl(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n  var a = e.shape[0],\n      s = e.shape[1],\n      o = buffer([a, n], t.dtype);\n\n  for (var i = 0; i < a; i++) {\n    for (var _a237 = 0; _a237 < s; _a237++) {\n      var _s162 = e.get(i, _a237);\n\n      if (_s162 < 0) throw new Error(\"Input x must be non-negative!\");\n      _s162 >= n || o.set(r ? 1 : t.size > 0 ? o.get(i, _s162) + t.get(i, _a237) : o.get(i, _s162) + 1, i, _s162);\n    }\n  }\n\n  return o;\n}\n\nfunction createSimpleUnaryImpl(e) {\n  return (t, n, r) => {\n    var a = getTypedArrayFromDType(n, t.length);\n\n    for (var _n308 = 0; _n308 < t.length; ++_n308) {\n      a[_n308] = e(t[_n308], r);\n    }\n\n    return a;\n  };\n}\n\nfunction unaryKernelFunc$1(e, t, n) {\n  return _ref52 => {\n    var {\n      inputs: r,\n      attrs: a,\n      backend: s\n    } = _ref52;\n    var {\n      x: o\n    } = r;\n    if (assertNotComplex$1(o, e), \"string\" === o.dtype || \"string\" === n) throw new Error(\"unaryKernelFunc does not support string input/output\");\n    var i = s,\n        l = i.data.get(o.dataId).values,\n        u = sizeFromShape(o.shape),\n        c = n || o.dtype,\n        p = getArrayFromDType(c, u);\n\n    for (var _e805 = 0; _e805 < u; ++_e805) {\n      p[_e805] = t(l[_e805], a);\n    }\n\n    return i.makeTensorInfo(o.shape, c, p);\n  };\n}\n\nfunction unaryKernelFuncFromImpl(e, t, n) {\n  return _ref53 => {\n    var {\n      inputs: r,\n      attrs: a,\n      backend: s\n    } = _ref53;\n    var {\n      x: o\n    } = r;\n    if (assertNotComplex$1(o, e), \"string\" === o.dtype || \"string\" === n) throw new Error(\"unaryKernelFunc does not support string input/output\");\n    var i = s,\n        l = i.data.get(o.dataId).values,\n        u = n || o.dtype,\n        c = t(l, u, a);\n    return i.makeTensorInfo(o.shape, u, c);\n  };\n}\n\nvar ceilImpl = createSimpleUnaryImpl(e => Math.ceil(e)),\n    ceil$1 = unaryKernelFuncFromImpl(Ceil, ceilImpl),\n    ceilConfig$1 = {\n  kernelName: Ceil,\n  backendName: \"cpu\",\n  kernelFunc: ceil$1\n};\n\nfunction concatImpl$1(e, t, n, r) {\n  var a = getArrayFromDType(n, sizeFromShape(t));\n\n  if (r && \"string\" !== n) {\n    var _t556 = 0;\n    e.forEach(e => {\n      var n = sizeFromShape(e.shape);\n      a.set(e.vals, _t556), _t556 += n;\n    });\n  } else {\n    var _r328 = 0;\n    e.forEach(e => {\n      var s = \"string\" === n ? fromUint8ToStringArray(e.vals) : e.vals;\n      var o = 0;\n\n      for (var _n309 = 0; _n309 < e.shape[0]; ++_n309) {\n        var i = _n309 * t[1] + _r328;\n\n        for (var _t557 = 0; _t557 < e.shape[1]; ++_t557) {\n          a[i + _t557] = s[o++];\n        }\n      }\n\n      _r328 += e.shape[1];\n    });\n  }\n\n  return a;\n}\n\nvar equalImpl = createSimpleBinaryKernelImpl((e, t) => e === t ? 1 : 0),\n    equal$1 = binaryKernelFunc$1(Equal, equalImpl, null, \"bool\"),\n    equalConfig$1 = {\n  kernelName: Equal,\n  backendName: \"cpu\",\n  kernelFunc: equal$1\n},\n    expImpl = createSimpleUnaryImpl(e => Math.exp(e)),\n    exp$1 = unaryKernelFuncFromImpl(Exp, expImpl),\n    expConfig$1 = {\n  kernelName: Exp,\n  backendName: \"cpu\",\n  kernelFunc: exp$1\n},\n    expm1Impl = createSimpleUnaryImpl(e => Math.expm1(e)),\n    expm1$1 = unaryKernelFuncFromImpl(Expm1, expm1Impl),\n    expm1Config$1 = {\n  kernelName: Expm1,\n  backendName: \"cpu\",\n  kernelFunc: expm1$1\n},\n    floorImpl = createSimpleUnaryImpl(e => Math.floor(e)),\n    floor$1 = unaryKernelFuncFromImpl(Floor, floorImpl),\n    floorConfig$1 = {\n  kernelName: Floor,\n  backendName: \"cpu\",\n  kernelFunc: floor$1\n};\n\nfunction gatherNdImpl(e, t, n, r, a, s, o, i, l) {\n  var u = buffer([r, s], n);\n\n  for (var _n310 = 0; _n310 < r; _n310++) {\n    var _r329 = [];\n    var c = 0;\n\n    for (var _t558 = 0; _t558 < a; _t558++) {\n      var _s163 = e[_n310 * a + _t558];\n      c += _s163 * o[_t558], _r329.push(_s163);\n    }\n\n    if (c < 0 || c >= l / s) throw new Error(\"Invalid indices: \".concat(_r329, \" does not index into \").concat(i));\n\n    for (var _e806 = 0; _e806 < s; _e806++) {\n      u.values[_n310 * s + _e806] = t.get(...t.indexToLoc(c * s + _e806));\n    }\n  }\n\n  return u;\n}\n\nfunction gatherV2Impl(e, t, n) {\n  var r = buffer(n, e.dtype);\n\n  for (var _n311 = 0; _n311 < r.size; ++_n311) {\n    var a = r.indexToLoc(_n311).slice(),\n        s = t.locToIndex([a[0], a[2]]);\n    a[2] = t.values[s];\n    var o = e.locToIndex(a);\n    r.values[_n311] = e.values[o];\n  }\n\n  return r;\n}\n\nvar greaterImpl = createSimpleBinaryKernelImpl((e, t) => e > t ? 1 : 0),\n    greater$1 = binaryKernelFunc$1(Greater, greaterImpl, null, \"bool\"),\n    greaterConfig$1 = {\n  kernelName: Greater,\n  backendName: \"cpu\",\n  kernelFunc: greater$1\n},\n    greaterEqualImpl = createSimpleBinaryKernelImpl((e, t) => e >= t ? 1 : 0),\n    greaterEqual$1 = binaryKernelFunc$1(GreaterEqual, greaterEqualImpl, null, \"bool\"),\n    greaterEqualConfig$1 = {\n  kernelName: GreaterEqual,\n  backendName: \"cpu\",\n  kernelFunc: greaterEqual$1\n},\n    lessImpl = createSimpleBinaryKernelImpl((e, t) => e < t ? 1 : 0),\n    less$1 = binaryKernelFunc$1(Less, lessImpl, null, \"bool\"),\n    lessConfig$1 = {\n  kernelName: Less,\n  backendName: \"cpu\",\n  kernelFunc: less$1\n},\n    lessEqualImpl = createSimpleBinaryKernelImpl((e, t) => e <= t ? 1 : 0),\n    lessEqual$1 = binaryKernelFunc$1(LessEqual, lessEqualImpl, null, \"bool\"),\n    lessEqualConfig$1 = {\n  kernelName: LessEqual,\n  backendName: \"cpu\",\n  kernelFunc: lessEqual$1\n};\n\nfunction linSpaceImpl(e, t, n) {\n  var r = (t - e) / (n - 1),\n      a = makeZerosTypedArray(n, \"float32\");\n  a[0] = e;\n\n  for (var _e807 = 1; _e807 < a.length; _e807++) {\n    a[_e807] = a[_e807 - 1] + r;\n  }\n\n  return a;\n}\n\nvar logImpl = createSimpleUnaryImpl(e => Math.log(e)),\n    log$1 = unaryKernelFuncFromImpl(Log, logImpl),\n    logConfig$1 = {\n  kernelName: Log,\n  backendName: \"cpu\",\n  kernelFunc: log$1\n};\n\nfunction maxImpl$1(e, t, n, r) {\n  var a = getTypedArrayFromDType(r, sizeFromShape(n));\n\n  for (var _n312 = 0; _n312 < a.length; ++_n312) {\n    var _r330 = _n312 * t;\n\n    var s = e[_r330];\n\n    for (var _n313 = 0; _n313 < t; ++_n313) {\n      var _t559 = e[_r330 + _n313];\n      (Number.isNaN(_t559) || _t559 > s) && (s = _t559);\n    }\n\n    a[_n312] = s;\n  }\n\n  return a;\n}\n\nvar maximumImpl = createSimpleBinaryKernelImpl((e, t) => Math.max(e, t)),\n    maximum$1 = binaryKernelFunc$1(Maximum$1, maximumImpl),\n    maximumConfig$1 = {\n  kernelName: Maximum$1,\n  backendName: \"cpu\",\n  kernelFunc: maximum$1\n},\n    minimumImpl = createSimpleBinaryKernelImpl((e, t) => Math.min(e, t)),\n    minimum$1 = binaryKernelFunc$1(Minimum$1, minimumImpl),\n    minimumConfig$1 = {\n  kernelName: Minimum$1,\n  backendName: \"cpu\",\n  kernelFunc: minimum$1\n},\n    multiplyImpl = createSimpleBinaryKernelImpl((e, t) => e * t),\n    multiplyComplexImpl = createComplexBinaryKernelImpl((e, t, n, r) => ({\n  real: e * n - t * r,\n  imag: e * r + t * n\n})),\n    multiply$1 = binaryKernelFunc$1(Multiply$1, multiplyImpl, multiplyComplexImpl),\n    multiplyConfig$1 = {\n  kernelName: Multiply$1,\n  backendName: \"cpu\",\n  kernelFunc: multiply$1\n};\n\nfunction negImpl(e, t, n) {\n  var r = createScalarValue(-1, n);\n  return multiplyImpl([], t, r, e, n);\n}\n\nfunction neg$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  assertNotComplex$1(r, \"neg\");\n  var a = n.data.get(r.dataId).values,\n      [s, o] = negImpl(a, r.shape, r.dtype);\n  return n.makeTensorInfo(o, r.dtype, s);\n}\n\nvar negConfig$1 = {\n  kernelName: Neg,\n  backendName: \"cpu\",\n  kernelFunc: neg$1\n},\n    notEqualImpl = createSimpleBinaryKernelImpl((e, t) => e !== t ? 1 : 0),\n    notEqual$1 = binaryKernelFunc$1(NotEqual, notEqualImpl, null, \"bool\"),\n    notEqualConfig$1 = {\n  kernelName: NotEqual,\n  backendName: \"cpu\",\n  kernelFunc: notEqual$1\n};\n\nfunction transposeImpl$1(e, t, n, r, a) {\n  var s = t.length,\n      o = sizeFromShape(t),\n      i = computeStrides(t),\n      l = computeStrides(a),\n      u = getTypedArrayFromDType(n, sizeFromShape(a));\n\n  for (var _t560 = 0; _t560 < o; ++_t560) {\n    var _n314 = indexToLoc(_t560, s, i),\n        _a238 = new Array(_n314.length);\n\n    for (var _e808 = 0; _e808 < _a238.length; _e808++) {\n      _a238[_e808] = _n314[r[_e808]];\n    }\n\n    u[locToIndex(_a238, s, l)] = e[_t560];\n  }\n\n  return u;\n}\n\nfunction transpose$1(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    perm: s\n  } = n;\n  assertNotComplex$1(a, \"transpose\");\n  var o = new Array(a.shape.length);\n\n  for (var _e809 = 0; _e809 < o.length; _e809++) {\n    o[_e809] = a.shape[s[_e809]];\n  }\n\n  var i = transposeImpl$1(r.data.get(a.dataId).values, a.shape, a.dtype, s, o);\n  return {\n    dataId: r.write(i, o, a.dtype),\n    shape: o,\n    dtype: a.dtype\n  };\n}\n\nvar transposeConfig$1 = {\n  kernelName: Transpose,\n  backendName: \"cpu\",\n  kernelFunc: transpose$1\n};\n\nfunction prodImpl(e, t, n, r) {\n  var [a, s] = computeOutAndReduceShapes(e, r),\n      o = upcastType(t, \"int32\"),\n      i = makeZerosTypedArray(sizeFromShape(a), o),\n      l = sizeFromShape(s);\n\n  for (var _e810 = 0; _e810 < i.length; ++_e810) {\n    var _t561 = _e810 * l;\n\n    var _r331 = 1;\n\n    for (var _e811 = 0; _e811 < l; ++_e811) {\n      _r331 *= n[_t561 + _e811];\n    }\n\n    i[_e810] = _r331;\n  }\n\n  return {\n    outVals: i,\n    outShape: a,\n    outDtype: o\n  };\n}\n\nfunction prod$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$1(a, \"prod\");\n  var i = a.shape.length,\n      l = parseAxisParam(s, a.shape),\n      u = getAxesPermutation(l, i);\n  var c = l,\n      p = a;\n  var d = [];\n  null != u && (p = transpose$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), d.push(p), c = getInnerMostAxes(c.length, i));\n  var h = n.data.get(p.dataId).values,\n      {\n    outVals: m,\n    outShape: f,\n    outDtype: g\n  } = prodImpl(p.shape, p.dtype, h, c);\n  var $ = f;\n  return o && ($ = expandShapeToKeepDim(f, l)), d.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo($, g, m);\n}\n\nvar prodConfig$1 = {\n  kernelName: Prod,\n  backendName: \"cpu\",\n  kernelFunc: prod$1\n};\n\nfunction rangeImpl(e, t, n, r) {\n  if (e === t || e < t && n < 0 || t < e && n > 1) return makeZerosTypedArray(0, r);\n  var a = makeZerosTypedArray(Math.abs(Math.ceil((t - e) / n)), r);\n  t < e && 1 === n && (n = -1), a[0] = e;\n\n  for (var _e812 = 1; _e812 < a.length; _e812++) {\n    a[_e812] = a[_e812 - 1] + n;\n  }\n\n  return a;\n}\n\nvar rsqrtImpl = createSimpleUnaryImpl(e => 1 / Math.sqrt(e)),\n    rsqrt$1 = unaryKernelFuncFromImpl(Rsqrt, rsqrtImpl),\n    rsqrtConfig$1 = {\n  kernelName: Rsqrt,\n  backendName: \"cpu\",\n  kernelFunc: rsqrt$1\n};\n\nfunction sliceImpl(e, t, n, r, a) {\n  var s = isSliceContinous(r, t, n),\n      o = sizeFromShape(n),\n      i = computeStrides(r);\n\n  if (s) {\n    var _n315 = computeFlatOffset(t, i);\n\n    return \"string\" === a ? e.slice(_n315, _n315 + o) : e.subarray(_n315, _n315 + o);\n  }\n\n  var l = buffer(r, a, \"string\" === a ? fromUint8ToStringArray(e) : e),\n      u = buffer(n, a);\n\n  for (var _e813 = 0; _e813 < u.size; ++_e813) {\n    var _n316 = u.indexToLoc(_e813),\n        _r332 = _n316.map((e, n) => e + t[n]);\n\n    u.set(l.get(..._r332), ..._n316);\n  }\n\n  return \"string\" === a ? fromStringArrayToUint8(u.values) : u.values;\n}\n\nfunction slice$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    size: o\n  } = r;\n  assertNotComplex$1(a, \"slice\");\n  var [i, l] = parseSliceParams(a, s, o);\n  assertParamsValid(a, i, l);\n  var u = sliceImpl(n.data.get(a.dataId).values, i, l, a.shape, a.dtype);\n  return n.makeTensorInfo(l, a.dtype, u);\n}\n\nvar sliceConfig$1 = {\n  kernelName: Slice,\n  backendName: \"cpu\",\n  kernelFunc: slice$1\n};\n\nfunction sparseFillEmptyRowsImpl(e, t, n, r, a, s, o) {\n  var i = t[0],\n      l = s[0],\n      u = new Array(l),\n      c = new Array(i),\n      p = t[1];\n\n  if (0 === l) {\n    if (0 !== i) throw new Error(\"Received SparseTensor with denseShape[0] = 0 but\\n         indices.shape[0] = \".concat(i));\n    return [getArrayFromDType(n, 0), [0, p], getArrayFromDType(a, 0), u, c];\n  }\n\n  var d = !0,\n      h = 0;\n  var m = new Array(l).fill(0);\n\n  for (var _t562 = 0; _t562 < i; ++_t562) {\n    var _n317 = e[_t562 * p];\n    if (_n317 < 0) throw new Error(\"indices(\".concat(_t562, \", 0) is invalid: \").concat(_n317, \" < 0\"));\n    if (_n317 >= l) throw new Error(\"indices(\".concat(_t562, \", 0) is invalid: \").concat(_n317, \" >= \").concat(l));\n    ++m[_n317], d = d && _n317 >= h, h = _n317;\n  }\n\n  var f = !0;\n\n  for (var _e814 = 0; _e814 < l; ++_e814) {\n    var _t563 = 0 === m[_e814];\n\n    u[_e814] = _t563, f = f && !_t563, m[_e814] = Math.max(m[_e814], 1), _e814 > 0 && (m[_e814] += m[_e814 - 1]);\n  }\n\n  if (f && d) {\n    var _t564 = e,\n        _n318 = r;\n\n    for (var _e815 = 0; _e815 < i; ++_e815) {\n      c[_e815] = _e815;\n    }\n\n    return [_t564, [i, p], _n318, u, c];\n  }\n\n  {\n    var _t565 = m[l - 1],\n        _s164 = getArrayFromDType(n, _t565 * p),\n        _d13 = getArrayFromDType(a, _t565),\n        _h14 = new Array(l).fill(0);\n\n    for (var _t566 = 0; _t566 < i; ++_t566) {\n      var _n319 = e[_t566 * p],\n          _a239 = (0 === _n319 ? 0 : m[_n319 - 1]) + _h14[_n319];\n\n      _h14[_n319]++;\n\n      for (var _n320 = 0; _n320 < p; ++_n320) {\n        _s164[_a239 * p + _n320] = e[_t566 * p + _n320];\n      }\n\n      _d13[_a239] = r[_t566], c[_t566] = _a239;\n    }\n\n    for (var _e816 = 0; _e816 < l; ++_e816) {\n      if (0 === _h14[_e816]) {\n        var _t567 = 0 === _e816 ? 0 : m[_e816 - 1];\n\n        _s164[_t567 * p + 0] = _e816;\n\n        for (var _e817 = 1; _e817 < p; ++_e817) {\n          _s164[_t567 * p + _e817] = 0;\n        }\n\n        _d13[_t567] = o;\n      }\n    }\n\n    return [_s164, [_t565, p], _d13, u, c];\n  }\n}\n\nfunction sparseReshapeImpl(e, t, n, r, a) {\n  var s = sizeFromShape(r),\n      o = t[0],\n      i = a.length,\n      l = [];\n  var u = 1,\n      c = -1;\n\n  for (var _e818 = 0; _e818 < i; ++_e818) {\n    var _t568 = a[_e818];\n\n    if (-1 === _t568) {\n      if (-1 !== c) throw new Error(\"only one output dimension may be -1, not both \".concat(c, \" and \").concat(_e818));\n      c = _e818, l.push(1);\n    } else {\n      if (_t568 < 0) throw new Error(\"size \".concat(_e818, \" must be non-negative, not \").concat(_t568));\n      u *= _t568, l.push(_t568);\n    }\n  }\n\n  if (-1 !== c) {\n    if (u <= 0) throw new Error(\"reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\");\n\n    var _e819 = Math.trunc(s / u);\n\n    if (u * _e819 !== s) throw new Error(\"Input to reshape is a SparseTensor with \".concat(s, \"\\n          dense values, but the requested shape requires a multiple of \").concat(u, \". inputShape=\").concat(r, \" outputShape= \").concat(l));\n    l[c] = _e819;\n  }\n\n  var p = sizeFromShape(l);\n  if (p !== s) throw new Error(\"Input to reshape is a tensor with \".concat(s, \" dense values, but the requested shape has \").concat(p, \". inputShape=\").concat(r, \" outputShape=\").concat(l));\n  var d = r.length,\n      h = [];\n\n  if (d > 0) {\n    h[d - 1] = 1;\n\n    for (var _e820 = d - 2; _e820 >= 0; --_e820) {\n      h[_e820] = h[_e820 + 1] * r[_e820 + 1];\n    }\n  }\n\n  var m = [];\n\n  if (i > 0) {\n    m[i - 1] = 1;\n\n    for (var _e821 = i - 2; _e821 >= 0; --_e821) {\n      m[_e821] = m[_e821 + 1] * l[_e821 + 1];\n    }\n  }\n\n  var f = getArrayFromDType(n, o * i);\n\n  for (var _t569 = 0; _t569 < o; ++_t569) {\n    var _n321 = 0;\n\n    for (var _r333 = 0; _r333 < d; ++_r333) {\n      _n321 += e[_t569 * d + _r333] * h[_r333];\n    }\n\n    for (var _e822 = 0; _e822 < i; ++_e822) {\n      f[_t569 * i + _e822] = Math.trunc(_n321 / m[_e822]), _n321 %= m[_e822];\n    }\n  }\n\n  return [f, [o, i], l];\n}\n\nfunction sparseSegmentReductionImpl(e, t, n, r, a) {\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : 0;\n  var i = r.length;\n  if (i !== a.length) throw new Error(\"segmentIds and indices should have same size.\");\n  var l = [t[0], e.length / t[0]],\n      u = l[1],\n      c = i > 0 ? a[i - 1] + 1 : 0;\n  if (c < 0) throw new Error(\"segment ids must be >= 0\");\n  var p = t.slice();\n  p[0] = c;\n  var d = getArrayFromDType(n, p.reduce((e, t) => e * t, 1));\n  if (0 === i) return c > 0 && d.fill(o), [d, p];\n  if (c <= 0) throw new Error(\"segment ids must be >= 0\");\n  var h = 0,\n      m = 1,\n      f = 0,\n      g = a[h];\n\n  for (;;) {\n    var _t570 = 0;\n\n    if (m < i) {\n      if (_t570 = a[m], g === _t570) {\n        ++m;\n        continue;\n      }\n\n      if (g >= _t570) throw new Error(\"segment ids are not increasing\");\n    }\n\n    if (g < 0 || g >= c) throw new Error(\"Segment id \".concat(g, \" out of range [0, \").concat(c, \"), possibly because segmentIds input is not sorted.\"));\n    g > f && d.fill(o, f * u, g * u);\n\n    for (var _t571 = h; _t571 < m; ++_t571) {\n      var _n322 = r[_t571];\n      if (_n322 < 0 || _n322 >= l[0]) throw new Error(\"Bad: indices[\".concat(_t571, \"] == \").concat(r[_t571], \" out of range [0, \").concat(l[0], \")\"));\n\n      for (var _t572 = 0; _t572 < u; _t572++) {\n        d[g * u + _t572] += e[_n322 * u + _t572];\n      }\n    }\n\n    if (s) for (var _e823 = 0; _e823 < u; _e823++) {\n      d[g * u + _e823] /= m - h;\n    }\n    if (h = m, ++m, f = g + 1, g = _t570, m > i) break;\n  }\n\n  return f < c && d.fill(o, f * u, c * u), [d, p];\n}\n\nvar squaredDifferenceImpl = createSimpleBinaryKernelImpl((e, t) => {\n  var n = e - t;\n  return n * n;\n}),\n    squaredDifference$1 = binaryKernelFunc$1(SquaredDifference, squaredDifferenceImpl),\n    squaredDifferenceConfig$1 = {\n  kernelName: SquaredDifference,\n  backendName: \"cpu\",\n  kernelFunc: squaredDifference$1\n};\n\nfunction stridedSliceImpl(e, t, n, r) {\n  var a = buffer(e, t.dtype);\n\n  for (var _e824 = 0; _e824 < a.size; _e824++) {\n    var s = a.indexToLoc(_e824),\n        o = new Array(s.length);\n\n    for (var _e825 = 0; _e825 < o.length; _e825++) {\n      o[_e825] = s[_e825] * n[_e825] + r[_e825];\n    }\n\n    a.set(t.get(...o), ...s);\n  }\n\n  return a;\n}\n\nclass StringNGramsOp {\n  constructor(e, t, n, r, a, s) {\n    this.separator = encodeString(e), this.nGramWidths = t, this.leftPad = encodeString(n), this.rightPad = encodeString(r), this.padWidth = a, this.preserveShort = s;\n  }\n\n  getPadWidth(e) {\n    return Math.min(this.padWidth < 0 ? e - 1 : this.padWidth, e - 1);\n  }\n\n  getNumNGrams(e, t) {\n    var n = this.getPadWidth(t);\n    return Math.max(0, e + 2 * n - t + 1);\n  }\n\n  createNGrams(e, t, n, r, a, s) {\n    var _this213 = this;\n\n    var _loop53 = function _loop53(o) {\n      var i = _this213.getPadWidth(s),\n          l = Math.max(0, i - o),\n          u = Math.max(0, i - (a - (o + 1))),\n          c = s - (l + u),\n          p = t + (l > 0 ? 0 : o - i);\n\n      var d = 0;\n      d += l * _this213.leftPad.length;\n\n      for (var _t573 = 0; _t573 < c; ++_t573) {\n        d += e[p + _t573].length;\n      }\n\n      d += u * _this213.rightPad.length, d += (l + u + c - 1) * _this213.separator.length, n[r + o] = new Uint8Array(d);\n      var h = n[r + o];\n      var m = 0;\n\n      var f = e => e.forEach(e => h[m++] = e);\n\n      for (var _e826 = 0; _e826 < l; ++_e826) {\n        f(_this213.leftPad), f(_this213.separator);\n      }\n\n      for (var _t574 = 0; _t574 < c - 1; ++_t574) {\n        f(e[p + _t574]), f(_this213.separator);\n      }\n\n      if (c > 0) {\n        f(e[p + c - 1]);\n\n        for (var _e827 = 0; _e827 < u; ++_e827) {\n          f(_this213.separator), f(_this213.rightPad);\n        }\n      } else {\n        for (var _e828 = 0; _e828 < u - 1; ++_e828) {\n          f(_this213.rightPad), f(_this213.separator);\n        }\n\n        f(_this213.rightPad);\n      }\n    };\n\n    for (var o = 0; o < a; ++o) {\n      _loop53(o);\n    }\n  }\n\n  compute(e, t) {\n    var _this214 = this;\n\n    var n = e.length,\n        r = t.length;\n\n    if (r > 0) {\n      var _e829 = t[0];\n      if (0 !== _e829) throw new Error(\"First split value must be 0, got \".concat(_e829));\n\n      for (var _a240 = 1; _a240 < r; ++_a240) {\n        var _r334 = t[_a240] >= _e829;\n\n        if (_r334 = _r334 && t[_a240] <= n, !_r334) throw new Error(\"Invalid split value \".concat(t[_a240], \", must be in [\").concat(_e829, \", \").concat(n, \"]\"));\n        _e829 = t[_a240];\n      }\n\n      if (_e829 !== n) throw new Error(\"Last split value must be data size. Expected \".concat(n, \", got \").concat(_e829));\n    }\n\n    var a = r - 1,\n        s = getArrayFromDType(\"int32\", r);\n\n    if (0 === n || 0 === r) {\n      var _e830 = new Array(n);\n\n      for (var _e831 = 0; _e831 <= a; ++_e831) {\n        s[_e831] = 0;\n      }\n\n      return [_e830, s];\n    }\n\n    s[0] = 0;\n\n    var _loop54 = function _loop54(_e832) {\n      var n = t[_e832] - t[_e832 - 1];\n      var r = 0;\n      _this214.nGramWidths.forEach(e => {\n        r += _this214.getNumNGrams(n, e);\n      }), _this214.preserveShort && n > 0 && 0 === r && (r = 1), s[_e832] = s[_e832 - 1] + r;\n    };\n\n    for (var _e832 = 1; _e832 <= a; ++_e832) {\n      _loop54(_e832);\n    }\n\n    var o = new Array(s[a]);\n\n    var _loop55 = function _loop55(_n323) {\n      var r = t[_n323];\n      var a = s[_n323];\n\n      if (_this214.nGramWidths.forEach(s => {\n        var i = _this214.getNumNGrams(t[_n323 + 1] - t[_n323], s);\n\n        _this214.createNGrams(e, r, o, a, i, s), a += i;\n      }), _this214.preserveShort && a === s[_n323]) {\n        var _s165 = t[_n323 + 1] - t[_n323];\n\n        if (0 === _s165) return \"continue\";\n\n        _this214.createNGrams(e, r, o, a, 1, _s165 + 2 * _this214.padWidth);\n      }\n    };\n\n    for (var _n323 = 0; _n323 < a; ++_n323) {\n      var _ret3 = _loop55(_n323);\n\n      if (_ret3 === \"continue\") continue;\n    }\n\n    return [o, s];\n  }\n\n}\n\nfunction stringNGramsImpl(e, t, n, r, a, s, o, i) {\n  return new StringNGramsOp(n, r, a, s, o, i).compute(e, t);\n}\n\nfunction split(e, t, n) {\n  if (!e.length) return [];\n\n  if (0 === t.length) {\n    var _t575 = new Array(e.length);\n\n    for (var _n324 = 0; _n324 < e.length; ++_n324) {\n      _t575[_n324] = e.subarray(_n324, _n324 + 1);\n    }\n\n    return _t575;\n  }\n\n  if (1 === t.length) {\n    var _r335 = t[0],\n        _a241 = [];\n    var s = e.indexOf(_r335);\n\n    for (; -1 !== s;) {\n      var _t576 = e.subarray(0, s);\n\n      n && 0 === _t576.length || _a241.push(_t576), s = (e = e.subarray(s + 1)).indexOf(_r335);\n    }\n\n    return n && 0 === e.length || _a241.push(e), _a241;\n  }\n\n  var r = [];\n  var a = 0;\n\n  for (var _s166 = 0; _s166 < e.length + 1; _s166++) {\n    if (_s166 === e.length || -1 !== t.indexOf(e[_s166])) {\n      var _t577 = e.subarray(a, _s166);\n\n      n && 0 === _t577.length || r.push(_t577), a = _s166 + 1;\n    }\n  }\n\n  return r;\n}\n\nfunction stringSplitImpl(e, t, n) {\n  var r = e.length,\n      a = [];\n  var s = 0,\n      o = 0;\n  var i = new Array(r);\n\n  for (var _l43 = 0; _l43 < r; ++_l43) {\n    var _r336 = split(e[_l43], t, n),\n        _u37 = _r336.length;\n\n    i[_l43] = _u37, s += _u37, o = Math.max(o, _u37), a.push(..._r336);\n  }\n\n  var l = getArrayFromDType(\"int32\", 2 * s),\n      u = new Array(s),\n      c = [r, o];\n  var p = 0;\n\n  for (var _e833 = 0; _e833 < r; ++_e833) {\n    for (var _t578 = 0; _t578 < i[_e833]; ++_t578) {\n      l[2 * p] = _e833, l[2 * p + 1] = _t578, u[p] = a[p], ++p;\n    }\n  }\n\n  return [l, u, c];\n}\n\nfunction stringToHashBucketFastImpl(e, t) {\n  var n = getArrayFromDType(\"int32\", e.length);\n\n  for (var r = 0; r < e.length; ++r) {\n    n[r] = fingerPrint64(e[r]).modulo(t).getLowBitsUnsigned();\n  }\n\n  return n;\n}\n\nvar subImpl = createSimpleBinaryKernelImpl((e, t) => e - t),\n    subComplexImpl = createComplexBinaryKernelImpl((e, t, n, r) => ({\n  real: e - n,\n  imag: t - r\n})),\n    sub$1 = binaryKernelFunc$1(Sub, subImpl, subComplexImpl),\n    subConfig$1 = {\n  kernelName: Sub,\n  backendName: \"cpu\",\n  kernelFunc: sub$1\n};\n\nfunction tileImpl(e, t) {\n  var n = new Array(e.rank);\n\n  for (var _r337 = 0; _r337 < n.length; _r337++) {\n    n[_r337] = e.shape[_r337] * t[_r337];\n  }\n\n  var r = buffer(n, e.dtype);\n\n  for (var _t579 = 0; _t579 < r.values.length; ++_t579) {\n    var _n325 = r.indexToLoc(_t579),\n        a = new Array(e.rank);\n\n    for (var _t580 = 0; _t580 < a.length; _t580++) {\n      a[_t580] = _n325[_t580] % e.shape[_t580];\n    }\n\n    var s = e.locToIndex(a);\n    r.values[_t579] = e.values[s];\n  }\n\n  return r;\n}\n\nvar comparePair = (e, t) => {\n  var n = t.value - e.value;\n  return 0 === n ? e.index - t.index : n;\n};\n\nfunction select$2(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : e.length - 1;\n\n  for (; r > n;) {\n    if (r - n > 600) {\n      var _a242 = r - n + 1,\n          _s167 = t - n + 1,\n          _o104 = Math.log(_a242),\n          i = .5 * Math.exp(2 * _o104 / 3),\n          l = .5 * Math.sqrt(_o104 * i * (_a242 - i) / _a242) * Math.sign(_s167 - _a242 / 2);\n\n      select$2(e, t, Math.max(n, Math.floor(t - _s167 * i / _a242 + l)), Math.min(r, Math.floor(t + (_a242 - _s167) * i / _a242 + l)));\n    }\n\n    var a = e[t];\n    var s = n,\n        o = r;\n\n    for (swap(e, n, t), comparePair(e[r], a) > 0 && swap(e, n, r); s < o;) {\n      for (swap(e, s, o), s++, o--; comparePair(e[s], a) < 0;) {\n        s += 1;\n      }\n\n      for (; comparePair(e[o], a) > 0;) {\n        o -= 1;\n      }\n    }\n\n    0 === comparePair(e[n], a) ? swap(e, n, o) : (o += 1, swap(e, o, r)), o <= t && (n = o + 1), t <= o && (r = o - 1);\n  }\n}\n\nfunction topKImpl(e, t, n, r, a) {\n  var s = t[t.length - 1],\n      [o, i] = [e.length / s, s],\n      l = getTypedArrayFromDType(n, o * r),\n      u = getTypedArrayFromDType(\"int32\", o * r);\n\n  var _loop56 = function _loop56(_t581) {\n    var n = _t581 * i,\n        s = e.subarray(n, n + i);\n    var o = new Array(s.length);\n    s.forEach((e, t) => o[t] = {\n      value: e,\n      index: t\n    }), r < o.length && (select$2(o, r), o = o.slice(0, r)), a && o.sort(comparePair);\n    var c = _t581 * r,\n        p = l.subarray(c, c + r),\n        d = u.subarray(c, c + r);\n\n    for (var _e834 = 0; _e834 < r; _e834++) {\n      p[_e834] = o[_e834].value, d[_e834] = o[_e834].index;\n    }\n  };\n\n  for (var _t581 = 0; _t581 < o; _t581++) {\n    _loop56(_t581);\n  }\n\n  var c = t.slice();\n  return c[c.length - 1] = r, [buffer(c, n, l), buffer(c, \"int32\", u)];\n}\n\nfunction uniqueImpl(e, t, n, r) {\n  var a = parseAxisParam(t, n)[0],\n      s = [1, n[0], 1];\n\n  for (var _e835 = 0; _e835 < a; _e835++) {\n    s[0] *= n[_e835];\n  }\n\n  s[1] = n[a];\n\n  for (var _e836 = a + 1; _e836 < n.length; _e836++) {\n    s[2] *= n[_e836];\n  }\n\n  var o = {},\n      i = new Int32Array(n[a]),\n      l = new TensorBuffer(s, r, e),\n      u = [],\n      c = 1 === s[0] && 1 === s[2];\n\n  for (var _t582 = 0; _t582 < n[a]; _t582++) {\n    var _n326 = void 0;\n\n    if (c) _n326 = e[_t582].toString();else {\n      var _e837 = [];\n\n      for (var _n327 = 0; _n327 < s[0]; _n327++) {\n        for (var _r338 = 0; _r338 < s[2]; _r338++) {\n          _e837.push(l.get(_n327, _t582, _r338));\n        }\n      }\n\n      _n326 = _e837.join(\",\");\n    }\n    if (void 0 !== o[_n326]) i[_t582] = o[_n326];else {\n      var _e838 = Object.keys(o).length;\n      o[_n326] = _e838, i[_t582] = _e838, u.push(_t582);\n    }\n  }\n\n  var p = s.slice();\n  p[1] = Object.keys(o).length;\n  var d = new TensorBuffer(p, r);\n  u.forEach((e, t) => {\n    for (var _n328 = 0; _n328 < s[0]; _n328++) {\n      for (var _r339 = 0; _r339 < s[2]; _r339++) {\n        d.set(l.get(_n328, e, _r339), _n328, t, _r339);\n      }\n    }\n  });\n  var h = n.slice();\n  return h[a] = p[1], {\n    outputValues: d.values,\n    outputShape: h,\n    indices: i\n  };\n}\n\nvar shared = {\n  __proto__: null,\n  simpleAbsImpl,\n  addImpl,\n  bincountImpl,\n  bincountReduceImpl,\n  ceilImpl,\n  concatImpl: concatImpl$1,\n  equalImpl,\n  expImpl,\n  expm1Impl,\n  floorImpl,\n  gatherNdImpl,\n  gatherV2Impl,\n  greaterImpl,\n  greaterEqualImpl,\n  lessImpl,\n  lessEqualImpl,\n  linSpaceImpl,\n  logImpl,\n  maxImpl: maxImpl$1,\n  maximumImpl,\n  minimumImpl,\n  multiplyImpl,\n  negImpl,\n  notEqualImpl,\n  prodImpl,\n  rangeImpl,\n  rsqrtImpl,\n  sliceImpl,\n  sparseFillEmptyRowsImpl,\n  sparseReshapeImpl,\n  sparseSegmentReductionImpl,\n  squaredDifferenceImpl,\n  stridedSliceImpl,\n  stringNGramsImpl,\n  stringSplitImpl,\n  stringToHashBucketFastImpl,\n  subImpl,\n  tileImpl,\n  topKImpl,\n  transposeImpl: transposeImpl$1,\n  uniqueImpl\n};\nvar version$3 = \"3.8.0\";\nregisterBackend(\"cpu\", () => new MathBackendCPU(), 1);\nvar elu$1 = unaryKernelFunc$1(Elu$1, e => e >= 0 ? e : Math.exp(e) - 1),\n    eluConfig$1 = {\n  kernelName: Elu$1,\n  backendName: \"cpu\",\n  kernelFunc: elu$1\n};\n\nfunction leakyRelu$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    alpha: s\n  } = r;\n  assertNotComplex$1([a], \"leakyRelu\");\n  var o = sizeFromShape(a.shape),\n      i = n.data.get(a.dataId).values,\n      l = getTypedArrayFromDType(\"float32\", o);\n\n  for (var _e839 = 0; _e839 < i.length; _e839++) {\n    l[_e839] = i[_e839] < 0 ? s * i[_e839] : i[_e839];\n  }\n\n  return n.makeTensorInfo(a.shape, \"float32\", l);\n}\n\nvar leakyReluConfig$1 = {\n  kernelName: LeakyRelu,\n  backendName: \"cpu\",\n  kernelFunc: leakyRelu$1\n},\n    preluImpl = createSimpleBinaryKernelImpl((e, t) => e < 0 ? t * e : e);\n\nfunction prelu$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r,\n    alpha: a\n  } = t;\n  assertNotComplex$1([r, a], \"prelu\");\n  var s = n.data.get(r.dataId).values,\n      o = n.data.get(a.dataId).values,\n      [i, l] = preluImpl(r.shape, a.shape, s, o, r.dtype);\n  return n.makeTensorInfo(l, r.dtype, i);\n}\n\nvar preluConfig$1 = {\n  kernelName: Prelu,\n  backendName: \"cpu\",\n  kernelFunc: prelu$1\n},\n    relu$2 = unaryKernelFunc$1(Relu$1, e => Math.max(0, e)),\n    reluConfig$1 = {\n  kernelName: Relu$1,\n  backendName: \"cpu\",\n  kernelFunc: relu$2\n},\n    relu6$1 = unaryKernelFunc$1(Relu6$1, e => Math.min(Math.max(0, e), 6)),\n    relu6Config$1 = {\n  kernelName: Relu6$1,\n  backendName: \"cpu\",\n  kernelFunc: relu6$1\n},\n    sigmoid$1 = unaryKernelFunc$1(Sigmoid$1, e => 1 / (1 + Math.exp(-e))),\n    sigmoidConfig$1 = {\n  kernelName: Sigmoid$1,\n  backendName: \"cpu\",\n  kernelFunc: sigmoid$1\n};\n\nfunction applyActivation(e, t, n, r, a) {\n  if (\"linear\" === n) return identity$1({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"relu\" === n) return relu$2({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"elu\" === n) return elu$1({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"relu6\" === n) return relu6$1({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  if (\"prelu\" === n) return prelu$1({\n    inputs: {\n      x: t,\n      alpha: r\n    },\n    backend: e\n  });\n  if (\"leakyrelu\" === n) return leakyRelu$1({\n    inputs: {\n      x: t\n    },\n    backend: e,\n    attrs: {\n      alpha: a\n    }\n  });\n  if (\"sigmoid\" === n) return sigmoid$1({\n    inputs: {\n      x: t\n    },\n    backend: e\n  });\n  throw new Error(\"Activation \".concat(n, \" has not been implemented for the CPU backend.\"));\n}\n\nfunction reshape$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    shape: s\n  } = r,\n      o = sizeFromShape(a.shape),\n      i = inferFromImplicitShape(s, o),\n      l = sizeFromShape(i);\n  assert$4(o === l, () => \"The new shape (\".concat(i, \") has \").concat(l, \" elements and the old shape (\").concat(a.shape, \") has \").concat(o, \" elements. The new shape and old shape must have the same number of elements.\")), n.incRef(a.dataId);\n  var u = n.data.get(a.dataId);\n\n  if (null != u.complexTensorInfos) {\n    var _e840 = u.complexTensorInfos.imag;\n    u.complexTensorInfos.real.shape = i, _e840.shape = i;\n  }\n\n  return {\n    dataId: a.dataId,\n    shape: i,\n    dtype: a.dtype\n  };\n}\n\nvar reshapeConfig$1 = {\n  kernelName: Reshape$1,\n  backendName: \"cpu\",\n  kernelFunc: reshape$1\n};\n\nfunction batchMatMul$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s\n  } = t,\n      {\n    transposeA: o,\n    transposeB: i\n  } = r;\n  assertNotComplex$1([a, s], \"matMul\");\n  var l = a.shape.length,\n      u = s.shape.length,\n      c = o ? a.shape[l - 2] : a.shape[l - 1],\n      p = i ? s.shape[u - 1] : s.shape[u - 2],\n      d = o ? a.shape[l - 1] : a.shape[l - 2],\n      h = i ? s.shape[u - 2] : s.shape[u - 1],\n      m = a.shape.slice(0, -2),\n      f = s.shape.slice(0, -2),\n      g = sizeFromShape(m),\n      $ = sizeFromShape(f);\n  assert$4(l >= 2 && u >= 2 && (g === $ || 1 === g || 1 === $), () => \"Error in matMul: the input batch dimensions must either be the same or at least one input batch dimension must be 1. Got input batch dimensions of (\".concat(m, \") and (\").concat(f, \").\"));\n  var y = (g > $ ? a.shape.slice(0, -2) : s.shape.slice(0, -2)).concat([d, h]);\n  assert$4(c === p, () => \"Error in matMul: inner shapes (\".concat(c, \") and (\").concat(p, \") of Tensors with shapes \").concat(a.shape, \" and \").concat(s.shape, \" and transposeA=\").concat(o, \" and transposeB=\").concat(i, \" must match.\"));\n  var b = i ? [$, h, p] : [$, p, h],\n      x = reshape$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: o ? [g, c, d] : [g, d, c]\n    }\n  }),\n      v = reshape$1({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: b\n    }\n  }),\n      I = o ? x.shape[1] : x.shape[2],\n      C = o ? x.shape[2] : x.shape[1],\n      S = i ? v.shape[1] : v.shape[2],\n      k = Math.max(g, $),\n      T = n.data.get(x.dataId).values,\n      N = n.data.get(v.dataId).values,\n      w = computeStrides(x.shape),\n      E = computeStrides(v.shape),\n      [A, D, R] = o ? [w[0], 1, w[1]] : [w[0], w[1], 1],\n      [_, F, P] = i ? [1, E[1], E[0]] : [E[1], 1, E[0]],\n      O = C * S,\n      M = buffer([k, C, S], x.dtype),\n      L = M.values,\n      z = n.blockSize;\n\n  for (var _e841 = 0; _e841 < k; _e841++) {\n    for (var _t583 = 0; _t583 < C; _t583 += z) {\n      for (var _n329 = 0; _n329 < S; _n329 += z) {\n        for (var _r340 = 0; _r340 < I; _r340 += z) {\n          var _a243 = Math.min(_t583 + z, C),\n              _s168 = Math.min(_n329 + z, S),\n              _o105 = Math.min(_r340 + z, I);\n\n          for (var _i53 = _t583; _i53 < _a243; _i53++) {\n            for (var _t584 = _n329; _t584 < _s168; _t584++) {\n              var _n330 = 0;\n\n              for (var _a244 = _r340; _a244 < _o105; _a244++) {\n                var _r341 = Math.min(_e841, g - 1) * A,\n                    _s169 = Math.min(_e841, $ - 1) * P;\n\n                _n330 += T[_r341 + _i53 * D + _a244 * R] * N[_a244 * _ + _t584 * F + _s169];\n              }\n\n              L[_e841 * O + (_i53 * S + _t584)] += _n330;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.disposeIntermediateTensorInfo(x), n.disposeIntermediateTensorInfo(v), n.makeTensorInfo(y, M.dtype, M.values);\n}\n\nvar batchMatMulConfig$1 = {\n  kernelName: BatchMatMul,\n  backendName: \"cpu\",\n  kernelFunc: batchMatMul$1\n};\n\nfunction _fusedMatMul$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    transposeA: l,\n    transposeB: u,\n    activation: c,\n    leakyreluAlpha: p\n  } = r;\n  var d, h, m;\n  var f = [];\n  d = batchMatMul$1({\n    inputs: {\n      a,\n      b: s\n    },\n    attrs: {\n      transposeA: l,\n      transposeB: u\n    },\n    backend: n\n  }), o && (h = add({\n    inputs: {\n      a: d,\n      b: o\n    },\n    backend: n\n  }), f.push(d), d = h), c && (m = applyActivation(n, d, c, i, p), f.push(d), d = m);\n\n  for (var _e842 of f) {\n    n.disposeIntermediateTensorInfo(_e842);\n  }\n\n  return d;\n}\n\nvar _fusedMatMulConfig$1 = {\n  kernelName: _FusedMatMul,\n  backendName: \"cpu\",\n  kernelFunc: _fusedMatMul$1\n},\n    acos$1 = unaryKernelFunc$1(Acos, e => Math.acos(e)),\n    acosConfig$1 = {\n  kernelName: Acos,\n  backendName: \"cpu\",\n  kernelFunc: acos$1\n},\n    acosh$1 = unaryKernelFunc$1(Acosh, e => Math.acosh(e)),\n    acoshConfig$1 = {\n  kernelName: Acosh,\n  backendName: \"cpu\",\n  kernelFunc: acosh$1\n};\n\nfunction addN$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      r = t;\n  assertNotComplex$1(t, \"addN\");\n  var a = r.map(e => n.data.get(e.dataId).values),\n      s = buffer(r[0].shape, r[0].dtype),\n      o = s.values;\n\n  for (var _e843 = 0; _e843 < r.length; _e843++) {\n    var _t585 = a[_e843];\n\n    for (var _e844 = 0; _e844 < o.length; _e844++) {\n      o[_e844] += _t585[_e844];\n    }\n  }\n\n  return n.makeTensorInfo(s.shape, s.dtype, s.values);\n}\n\nvar addNConfig$1 = {\n  kernelName: AddN,\n  backendName: \"cpu\",\n  kernelFunc: addN$1\n};\n\nfunction all$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$1(a, \"all\");\n  var i = parseAxisParam(s, a.shape);\n  var l = i;\n  var u = getAxesPermutation(l, a.shape.length);\n  var c = a;\n  null != u && (c = transpose$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), l = getInnerMostAxes(l.length, a.shape.length)), assertAxesAreInnerMostDims(\"all\", l, c.shape.length);\n  var [p, d] = computeOutAndReduceShapes(c.shape, l),\n      h = sizeFromShape(d),\n      m = makeZerosTypedArray(sizeFromShape(p), c.dtype),\n      f = n.data.get(c.dataId).values;\n\n  for (var _e845 = 0; _e845 < m.length; ++_e845) {\n    var _t586 = _e845 * h;\n\n    var _n331 = f[_t586];\n\n    for (var _e846 = 0; _e846 < h; ++_e846) {\n      var _r342 = f[_t586 + _e846];\n      _n331 = _n331 && _r342;\n    }\n\n    m[_e845] = _n331;\n  }\n\n  null != u && n.disposeIntermediateTensorInfo(c);\n  var g = n.makeTensorInfo(p, c.dtype, m);\n\n  if (o) {\n    var _e847 = reshape$1({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim(p, i)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), _e847;\n  }\n\n  return g;\n}\n\nvar allConfig$1 = {\n  kernelName: All,\n  backendName: \"cpu\",\n  kernelFunc: all$1\n};\n\nfunction any$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$1(a, \"any\");\n  var i = parseAxisParam(s, a.shape);\n  var l = i;\n  var u = getAxesPermutation(l, a.shape.length);\n  var c = a;\n  null != u && (c = transpose$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), l = getInnerMostAxes(l.length, a.shape.length)), assertAxesAreInnerMostDims(\"any\", l, c.shape.length);\n  var [p, d] = computeOutAndReduceShapes(c.shape, l),\n      h = sizeFromShape(d),\n      m = makeZerosTypedArray(sizeFromShape(p), c.dtype),\n      f = n.data.get(c.dataId).values;\n\n  for (var _e848 = 0; _e848 < m.length; ++_e848) {\n    var _t587 = _e848 * h;\n\n    var _n332 = f[_t587];\n\n    for (var _e849 = 0; _e849 < h; ++_e849) {\n      var _r343 = f[_t587 + _e849];\n      _n332 = _n332 || _r343;\n    }\n\n    m[_e848] = _n332;\n  }\n\n  null != u && n.disposeIntermediateTensorInfo(c);\n  var g = n.makeTensorInfo(p, c.dtype, m);\n\n  if (o) {\n    var _e850 = reshape$1({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim(p, i)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), _e850;\n  }\n\n  return g;\n}\n\nvar anyConfig$1 = {\n  kernelName: Any,\n  backendName: \"cpu\",\n  kernelFunc: any$1\n};\n\nfunction argMax$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  assertNotComplex$1(a, \"argMax\");\n  var o = parseAxisParam(s, a.shape);\n  var i = getAxesPermutation(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes(o.length, l.shape.length)), o = [o[0]], assertAxesAreInnerMostDims(\"argMax\", o, l.shape.length);\n  var [c, p] = computeOutAndReduceShapes(l.shape, o),\n      d = makeZerosTypedArray(sizeFromShape(c), \"int32\"),\n      h = sizeFromShape(p),\n      m = n.data.get(l.dataId).values;\n\n  for (var _e851 = 0; _e851 < d.length; ++_e851) {\n    var _t588 = _e851 * h;\n\n    var _n333 = m[_t588],\n        _r344 = 0;\n\n    for (var _e852 = 0; _e852 < h; ++_e852) {\n      var _a245 = m[_t588 + _e852];\n      _a245 > _n333 && (_n333 = _a245, _r344 = _e852);\n    }\n\n    d[_e851] = _r344;\n  }\n\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo(c, \"int32\", d);\n}\n\nvar argMaxConfig$1 = {\n  kernelName: ArgMax,\n  backendName: \"cpu\",\n  kernelFunc: argMax$1\n};\n\nfunction argMin$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  assertNotComplex$1(a, \"argMin\");\n  var o = parseAxisParam(s, a.shape);\n  var i = getAxesPermutation(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes(o.length, l.shape.length)), o = [o[0]], assertAxesAreInnerMostDims(\"argMin\", o, l.shape.length);\n  var [c, p] = computeOutAndReduceShapes(l.shape, o),\n      d = makeZerosTypedArray(sizeFromShape(c), \"int32\"),\n      h = sizeFromShape(p),\n      m = n.data.get(l.dataId).values;\n\n  for (var _e853 = 0; _e853 < d.length; ++_e853) {\n    var _t589 = _e853 * h;\n\n    var _n334 = m[_t589],\n        _r345 = 0;\n\n    for (var _e854 = 0; _e854 < h; ++_e854) {\n      var _a246 = m[_t589 + _e854];\n      _a246 < _n334 && (_n334 = _a246, _r345 = _e854);\n    }\n\n    d[_e853] = _r345;\n  }\n\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo(c, \"int32\", d);\n}\n\nvar argMinConfig$1 = {\n  kernelName: ArgMin,\n  backendName: \"cpu\",\n  kernelFunc: argMin$1\n},\n    asin$1 = unaryKernelFunc$1(Asin, e => Math.asin(e)),\n    asinConfig$1 = {\n  kernelName: Asin,\n  backendName: \"cpu\",\n  kernelFunc: asin$1\n},\n    asinh$1 = unaryKernelFunc$1(Asinh, e => Math.asinh(e)),\n    asinhConfig$1 = {\n  kernelName: Asinh,\n  backendName: \"cpu\",\n  kernelFunc: asinh$1\n},\n    atan$1 = unaryKernelFunc$1(Atan, e => Math.atan(e)),\n    atanConfig$1 = {\n  kernelName: Atan,\n  backendName: \"cpu\",\n  kernelFunc: atan$1\n},\n    atan2Impl = createSimpleBinaryKernelImpl((e, t) => Math.atan2(e, t)),\n    atan2$1 = binaryKernelFunc$1(Atan2, atan2Impl),\n    atan2Config$1 = {\n  kernelName: Atan2,\n  backendName: \"cpu\",\n  kernelFunc: atan2$1\n},\n    atanh$1 = unaryKernelFunc$1(Atanh, e => Math.atanh(e)),\n    atanhConfig$1 = {\n  kernelName: Atanh,\n  backendName: \"cpu\",\n  kernelFunc: atanh$1\n};\n\nfunction pool(e, t, n, r, a, s) {\n  var o = a.strideHeight,\n      i = a.strideWidth,\n      l = a.dilationHeight,\n      u = a.dilationWidth,\n      c = a.effectiveFilterHeight,\n      p = a.effectiveFilterWidth,\n      d = a.padInfo.top,\n      h = a.padInfo.left,\n      m = \"max\" === s ? Number.NEGATIVE_INFINITY : Number.POSITIVE_INFINITY,\n      f = buffer(a.outShape, n),\n      g = f.values,\n      $ = a.outShape[1] * a.outShape[2] * a.outShape[3],\n      y = a.outShape[2] * a.outShape[3],\n      b = a.outShape[3];\n\n  for (var _t590 = 0; _t590 < a.batchSize; ++_t590) {\n    var _n335 = _t590 * $,\n        _f10 = _t590 * r[0];\n\n    for (var _t591 = 0; _t591 < a.inChannels; ++_t591) {\n      for (var _$7 = 0; _$7 < a.outHeight; ++_$7) {\n        var x = _$7 * o - d,\n            v = Math.max(0, x),\n            I = Math.min(a.inHeight, c + x),\n            C = _n335 + _$7 * y;\n\n        for (var _n336 = 0; _n336 < a.outWidth; ++_n336) {\n          var _o106 = _n336 * i - h,\n              _c24 = Math.max(0, _o106),\n              _d14 = Math.min(a.inWidth, p + _o106);\n\n          var _$8 = m,\n              _y3 = 0,\n              _x139 = 0;\n\n          for (var _n337 = v; _n337 < I; _n337 += l) {\n            var _a247 = _f10 + _n337 * r[1];\n\n            for (var _n338 = _c24; _n338 < _d14; _n338 += u) {\n              var _o107 = e[_a247 + _n338 * r[2] + _t591];\n              \"max\" === s && _o107 > _$8 ? _$8 = _o107 : \"avg\" === s && (_y3 += _o107, _x139++);\n            }\n\n            if (isNaN(_$8)) break;\n          }\n\n          g[C + _n336 * b + _t591] = \"avg\" === s ? _y3 / _x139 : _$8;\n        }\n      }\n    }\n  }\n\n  return f;\n}\n\nfunction maxPoolPositions(e, t, n, r) {\n  var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n  var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n  var o = buffer(r.outShape, \"int32\"),\n      i = r.strideHeight,\n      l = r.strideWidth,\n      u = r.dilationHeight,\n      c = r.dilationWidth,\n      p = r.effectiveFilterHeight,\n      d = r.effectiveFilterWidth,\n      h = r.padInfo.top,\n      m = r.padInfo.left,\n      f = buffer(t, n, e);\n\n  for (var _e855 = 0; _e855 < r.batchSize; ++_e855) {\n    for (var _t592 = 0; _t592 < r.inChannels; ++_t592) {\n      for (var _n339 = 0; _n339 < r.outHeight; ++_n339) {\n        var g = _n339 * i - h;\n        var $ = g;\n\n        for (; $ < 0;) {\n          $ += u;\n        }\n\n        var y = Math.min(r.inHeight, p + g);\n\n        for (var _i54 = 0; _i54 < r.outWidth; ++_i54) {\n          var _p27 = _i54 * l - m;\n\n          var _h15 = _p27;\n\n          for (; _h15 < 0;) {\n            _h15 += c;\n          }\n\n          var b = Math.min(r.inWidth, d + _p27);\n          var x = Number.NEGATIVE_INFINITY,\n              v = -1;\n\n          for (var _n340 = $; _n340 < y; _n340 += u) {\n            var _o108 = _n340 - g;\n\n            for (var _i55 = _h15; _i55 < b; _i55 += c) {\n              var _l44 = _i55 - _p27,\n                  _u38 = f.get(_e855, _n340, _i55, _t592);\n\n              _u38 > x && (x = _u38, v = a ? s ? ((_e855 * r.inHeight + _n340) * r.inWidth + _i55) * r.inChannels + _t592 : (_n340 * r.inWidth + _i55) * r.inChannels + _t592 : _o108 * d + _l44);\n            }\n          }\n\n          o.set(v, _e855, _n339, _i54, _t592);\n        }\n      }\n    }\n  }\n\n  return o;\n}\n\nfunction pool3d(e, t, n, r, a, s) {\n  var o = a.strideDepth,\n      i = a.strideHeight,\n      l = a.strideWidth,\n      u = a.dilationDepth,\n      c = a.dilationHeight,\n      p = a.dilationWidth,\n      d = a.effectiveFilterDepth,\n      h = a.effectiveFilterHeight,\n      m = a.effectiveFilterWidth,\n      f = a.padInfo.front,\n      g = a.padInfo.top,\n      $ = a.padInfo.left,\n      y = \"max\" === s ? Number.NEGATIVE_INFINITY : Number.POSITIVE_INFINITY,\n      b = buffer(a.outShape, n),\n      x = b.values,\n      v = a.outShape[1] * a.outShape[2] * a.outShape[3] * a.outShape[4],\n      I = a.outShape[2] * a.outShape[3] * a.outShape[4],\n      C = a.outShape[3] * a.outShape[4],\n      S = a.outShape[4];\n\n  for (var _t593 = 0; _t593 < a.batchSize; ++_t593) {\n    var _n341 = _t593 * v,\n        _b2 = _t593 * r[0];\n\n    for (var _t594 = 0; _t594 < a.inChannels; ++_t594) {\n      for (var _v3 = 0; _v3 < a.outDepth; ++_v3) {\n        var k = _v3 * o - f;\n        var T = k;\n\n        for (; T < 0;) {\n          T += u;\n        }\n\n        var N = Math.min(a.inDepth, d + k),\n            w = _n341 + _v3 * I;\n\n        for (var _n342 = 0; _n342 < a.outHeight; ++_n342) {\n          var _o109 = _n342 * i - g;\n\n          var _d15 = _o109;\n\n          for (; _d15 < 0;) {\n            _d15 += c;\n          }\n\n          var _f11 = Math.min(a.inHeight, h + _o109),\n              _v4 = w + _n342 * C;\n\n          for (var _n343 = 0; _n343 < a.outWidth; ++_n343) {\n            var _o110 = _n343 * l - $;\n\n            var _i56 = _o110;\n\n            for (; _i56 < 0;) {\n              _i56 += p;\n            }\n\n            var _h16 = Math.min(a.inWidth, m + _o110),\n                _g4 = _v4 + _n343 * S;\n\n            var _I2 = y,\n                _C2 = 0,\n                _k2 = 0;\n\n            for (var _n344 = T; _n344 < N; _n344 += u) {\n              var _a248 = _b2 + _n344 * r[1];\n\n              for (var _n345 = _d15; _n345 < _f11; _n345 += c) {\n                var _o111 = _a248 + _n345 * r[2];\n\n                for (var _n346 = _i56; _n346 < _h16; _n346 += p) {\n                  var _a249 = e[_o111 + _n346 * r[3] + _t594];\n                  if (\"max\" === s && _a249 > _I2 ? _I2 = _a249 : \"avg\" === s && (_C2 += _a249, _k2++), isNaN(_I2)) break;\n                }\n\n                if (isNaN(_I2)) break;\n              }\n\n              if (isNaN(_I2)) break;\n            }\n\n            x[_g4 + _t594] = \"avg\" === s ? _C2 / _k2 : _I2;\n          }\n        }\n      }\n    }\n  }\n\n  return b;\n}\n\nfunction maxPool3dPositions(e, t) {\n  var n = buffer(t.outShape, \"int32\"),\n      r = t.strideDepth,\n      a = t.strideHeight,\n      s = t.strideWidth,\n      o = t.dilationDepth,\n      i = t.dilationHeight,\n      l = t.dilationWidth,\n      u = t.effectiveFilterDepth,\n      c = t.effectiveFilterHeight,\n      p = t.effectiveFilterWidth,\n      d = t.padInfo.front,\n      h = t.padInfo.top,\n      m = t.padInfo.left;\n\n  for (var f = 0; f < t.batchSize; ++f) {\n    for (var g = 0; g < t.inChannels; ++g) {\n      for (var $ = 0; $ < t.outDepth; ++$) {\n        var y = $ * r - d;\n        var b = y;\n\n        for (; b < 0;) {\n          b += o;\n        }\n\n        var x = Math.min(t.inDepth, u + y);\n\n        for (var _r346 = 0; _r346 < t.outHeight; ++_r346) {\n          var _u39 = _r346 * a - h;\n\n          var _d16 = _u39;\n\n          for (; _d16 < 0;) {\n            _d16 += i;\n          }\n\n          var v = Math.min(t.inHeight, c + _u39);\n\n          for (var _a250 = 0; _a250 < t.outWidth; ++_a250) {\n            var _h17 = _a250 * s - m;\n\n            var I = _h17;\n\n            for (; I < 0;) {\n              I += l;\n            }\n\n            var C = Math.min(t.inWidth, p + _h17);\n            var S = Number.NEGATIVE_INFINITY,\n                k = -1;\n\n            for (var _t595 = b; _t595 < x; _t595 += o) {\n              var _n347 = _t595 - y;\n\n              for (var _r347 = _d16; _r347 < v; _r347 += i) {\n                var _a251 = _r347 - _u39;\n\n                for (var _s170 = I; _s170 < C; _s170 += l) {\n                  var _o112 = _s170 - _h17,\n                      _i57 = e.get(f, _t595, _r347, _s170, g);\n\n                  _i57 >= S && (S = _i57, k = _n347 * c * p + _a251 * c + _o112);\n                }\n              }\n            }\n\n            n.set(k, f, $, _r346, _a250, g);\n          }\n        }\n      }\n    }\n  }\n\n  return n;\n}\n\nfunction avgPool$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex$1(a, \"avgPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$4(eitherStridesOrDilationsAreOne(o, 1), () => \"Error in avgPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo(a.shape, s, o, 1, i, l);\n  var c;\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual(u.inShape, u.outShape)) c = identity$1({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });else {\n    var _e856 = n.data.get(a.dataId).values,\n        _t596 = computeStrides(a.shape),\n        _r348 = pool(_e856, a.shape, a.dtype, _t596, u, \"avg\");\n\n    c = n.makeTensorInfo(u.outShape, a.dtype, _r348.values);\n  }\n  return c;\n}\n\nvar avgPoolConfig$1 = {\n  kernelName: AvgPool,\n  backendName: \"cpu\",\n  kernelFunc: avgPool$1\n};\n\nfunction avgPool3D$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l,\n    dataFormat: u\n  } = r;\n  assertNotComplex$1(a, \"avgPool3d\");\n  var c = computePool3DInfo(a.shape, s, o, 1, i, l, u),\n      p = pool3d(n.data.get(a.dataId).values, a.shape, a.dtype, computeStrides(a.shape), c, \"avg\");\n  return n.makeTensorInfo(p.shape, \"float32\", p.values);\n}\n\nvar avgPool3DConfig$1 = {\n  kernelName: AvgPool3D,\n  backendName: \"cpu\",\n  kernelFunc: avgPool3D$1\n};\n\nfunction avgPool3DGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      {\n    filterSize: o,\n    strides: i,\n    pad: l,\n    dimRoundingMode: u\n  } = r;\n  assertNotComplex$1([a, s], \"avgPool3DGrad\");\n  var c = computePool3DInfo(s.shape, o, i, 1, l, u),\n      p = c.strideDepth,\n      d = c.strideHeight,\n      h = c.strideWidth,\n      m = c.filterDepth,\n      f = c.filterHeight,\n      g = c.filterWidth,\n      $ = c.dilationDepth,\n      y = c.dilationHeight,\n      b = c.dilationWidth,\n      x = c.effectiveFilterDepth,\n      v = c.effectiveFilterHeight,\n      I = c.effectiveFilterWidth,\n      C = x - 1 - c.padInfo.front,\n      S = I - 1 - c.padInfo.left,\n      k = v - 1 - c.padInfo.top,\n      T = buffer(s.shape, \"float32\"),\n      N = 1 / (m * f * g),\n      w = n.bufferSync(a);\n\n  for (var _e857 = 0; _e857 < c.batchSize; ++_e857) {\n    for (var _t597 = 0; _t597 < c.inChannels; ++_t597) {\n      for (var _n348 = 0; _n348 < c.inDepth; ++_n348) {\n        for (var _r349 = 0; _r349 < c.inHeight; ++_r349) {\n          for (var _a252 = 0; _a252 < c.inWidth; ++_a252) {\n            var _s171 = _n348 - C,\n                _o113 = _r349 - k,\n                _i58 = _a252 - S;\n\n            var _l45 = 0;\n\n            for (var _n349 = 0; _n349 < x; _n349 += $) {\n              var _r350 = (_s171 + _n349) / p;\n\n              if (!(_r350 < 0 || _r350 >= c.outDepth || Math.floor(_r350) !== _r350)) for (var _n350 = 0; _n350 < v; _n350 += y) {\n                var _a253 = (_o113 + _n350) / d;\n\n                if (!(_a253 < 0 || _a253 >= c.outHeight || Math.floor(_a253) !== _a253)) for (var _n351 = 0; _n351 < I; _n351 += b) {\n                  var _s172 = (_i58 + _n351) / h;\n\n                  _s172 < 0 || _s172 >= c.outWidth || Math.floor(_s172) !== _s172 || (_l45 += w.get(_e857, _r350, _a253, _s172, _t597));\n                }\n              }\n            }\n\n            T.set(_l45 * N, _e857, _n348, _r349, _a252, _t597);\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(T.shape, T.dtype, T.values);\n}\n\nvar avgPool3DGradConfig = {\n  kernelName: AvgPool3DGrad,\n  backendName: \"cpu\",\n  kernelFunc: avgPool3DGrad$1\n};\n\nfunction avgPoolGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s;\n  assertNotComplex$1([a, s], \"avgPoolGrad\");\n  var {\n    filterSize: i,\n    strides: l,\n    pad: u\n  } = r,\n      c = computePool2DInfo(o.shape, i, l, 1, u),\n      p = c.strideHeight,\n      d = c.strideWidth,\n      h = c.filterHeight,\n      m = c.filterWidth,\n      f = c.dilationHeight,\n      g = c.dilationWidth,\n      $ = c.effectiveFilterHeight,\n      y = c.effectiveFilterWidth,\n      b = y - 1 - c.padInfo.left,\n      x = $ - 1 - c.padInfo.top,\n      v = buffer(o.shape, \"float32\"),\n      I = 1 / (h * m),\n      C = n.data.get(a.dataId).values,\n      S = buffer(a.shape, \"float32\", C);\n\n  for (var _e858 = 0; _e858 < c.batchSize; ++_e858) {\n    for (var _t598 = 0; _t598 < c.inChannels; ++_t598) {\n      for (var _n352 = 0; _n352 < c.inHeight; ++_n352) {\n        for (var _r351 = 0; _r351 < c.inWidth; ++_r351) {\n          var _a254 = _n352 - x,\n              _s173 = _r351 - b;\n\n          var _o114 = 0;\n\n          for (var _n353 = 0; _n353 < $; _n353 += f) {\n            var _r352 = (_a254 + _n353) / p;\n\n            if (!(_r352 < 0 || _r352 >= c.outHeight || Math.floor(_r352) !== _r352)) for (var _n354 = 0; _n354 < y; _n354 += g) {\n              var _a255 = (_s173 + _n354) / d;\n\n              _a255 < 0 || _a255 >= c.outWidth || Math.floor(_a255) !== _a255 || (_o114 += S.get(_e858, _r352, _a255, _t598));\n            }\n          }\n\n          v.set(_o114 * I, _e858, _n352, _r351, _t598);\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(v.shape, v.dtype, v.values);\n}\n\nvar avgPoolGradConfig$1 = {\n  kernelName: AvgPoolGrad,\n  backendName: \"cpu\",\n  kernelFunc: avgPoolGrad$1\n};\n\nfunction batchNorm$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    scale: s,\n    offset: o,\n    mean: i,\n    variance: l\n  } = t;\n  assert$4(i.shape.length === l.shape.length, () => \"Batch normalization gradient requires mean and variance to have equal ranks.\"), assert$4(null == o || i.shape.length === o.shape.length, () => \"Batch normalization gradient requires mean and offset to have equal ranks.\"), assert$4(null == s || i.shape.length === s.shape.length, () => \"Batch normalization gradient requires mean and scale to have equal ranks.\"), assertNotComplex$1([a, i, l, s, o], \"batchNorm\");\n  var {\n    varianceEpsilon: u\n  } = r;\n  null == u && (u = .001);\n  var c = n.data.get(a.dataId).values,\n      p = n.data.get(i.dataId).values,\n      d = n.data.get(l.dataId).values,\n      h = s ? n.data.get(s.dataId).values : new Float32Array([1]),\n      m = o ? n.data.get(o.dataId).values : new Float32Array([0]),\n      f = new Float32Array(c.length),\n      g = m.length,\n      $ = h.length,\n      y = d.length,\n      b = p.length;\n  var x = 0,\n      v = 0,\n      I = 0,\n      C = 0;\n\n  for (var _e859 = 0; _e859 < c.length; ++_e859) {\n    f[_e859] = m[x++] + (c[_e859] - p[v++]) * h[I++] / Math.sqrt(d[C++] + u), x >= g && (x = 0), v >= b && (v = 0), I >= $ && (I = 0), C >= y && (C = 0);\n  }\n\n  return n.makeTensorInfo(a.shape, a.dtype, f);\n}\n\nvar batchNormConfig$1 = {\n  kernelName: FusedBatchNorm,\n  backendName: \"cpu\",\n  kernelFunc: batchNorm$1\n};\n\nfunction batchToSpaceND$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    crops: o\n  } = r;\n  assertNotComplex$1([a], \"batchToSpaceND\");\n  var i = s.reduce((e, t) => e * t),\n      l = getReshaped(a.shape, s, i),\n      u = getPermuted(l.length, s.length),\n      c = getReshapedPermuted(a.shape, s, i),\n      p = getSliceBeginCoords(o, s.length),\n      d = getSliceSize(c, o, s.length),\n      h = reshape$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      m = transpose$1({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }),\n      f = reshape$1({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      g = slice$1({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      begin: p,\n      size: d\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), g;\n}\n\nvar batchToSpaceNDConfig$1 = {\n  kernelName: BatchToSpaceND,\n  backendName: \"cpu\",\n  kernelFunc: batchToSpaceND$1\n};\n\nfunction bincount$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o\n  } = r,\n      i = bincountImpl(n.data.get(a.dataId).values, n.data.get(s.dataId).values, s.dtype, s.shape, o);\n  return n.makeTensorInfo([o], s.dtype, i);\n}\n\nvar bincountConfig$1 = {\n  kernelName: Bincount,\n  backendName: \"cpu\",\n  kernelFunc: bincount$1\n},\n    clip = unaryKernelFunc$1(ClipByValue, (e, t) => e > t.clipValueMax ? t.clipValueMax : e < t.clipValueMin ? t.clipValueMin : e),\n    clipConfig = {\n  kernelName: ClipByValue,\n  backendName: \"cpu\",\n  kernelFunc: clip\n},\n    complexAbs$1 = e => {\n  var {\n    x: t\n  } = e.inputs,\n      n = e.backend,\n      r = new Float32Array(sizeFromShape(t.shape)),\n      a = n.data.get(t.dataId),\n      s = a.complexTensorInfos.imag,\n      o = n.data.get(a.complexTensorInfos.real.dataId).values,\n      i = n.data.get(s.dataId).values;\n\n  for (var _e860 = 0; _e860 < o.length; _e860++) {\n    r[_e860] = Math.hypot(o[_e860], i[_e860]);\n  }\n\n  return n.makeOutput(r, t.shape, \"float32\");\n},\n    complexAbsConfig$1 = {\n  kernelName: ComplexAbs,\n  backendName: \"cpu\",\n  kernelFunc: complexAbs$1\n};\n\nfunction imag$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = n.data.get(r.dataId).complexTensorInfos.imag,\n      s = n.data.get(a.dataId).values;\n  return n.makeTensorInfo(a.shape, a.dtype, s);\n}\n\nvar imagConfig$1 = {\n  kernelName: Imag,\n  backendName: \"cpu\",\n  kernelFunc: imag$1\n};\n\nfunction concat$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r,\n      s = parseAxisParam(a, t[0].shape)[0];\n  var o = computeOutShape$1(t.map(e => e.shape), s);\n  if (0 === sizeFromShape(o)) return n.makeTensorInfo(o, t[0].dtype, []);\n  var i = t.filter(e => sizeFromShape(e.shape) > 0);\n  if (1 === i.length) return identity$1({\n    inputs: {\n      x: i[0]\n    },\n    backend: n\n  });\n\n  if (assertParamsConsistent(i.map(e => e.shape), s), \"complex64\" === i[0].dtype) {\n    var _e861 = i.map(e => real$1({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _t599 = i.map(e => imag$1({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _r353 = concat$1({\n      inputs: _e861,\n      backend: n,\n      attrs: {\n        axis: s\n      }\n    }),\n        _a256 = concat$1({\n      inputs: _t599,\n      backend: n,\n      attrs: {\n        axis: s\n      }\n    }),\n        _o115 = complex$1({\n      inputs: {\n        real: _r353,\n        imag: _a256\n      },\n      backend: n\n    });\n\n    return _e861.forEach(e => n.disposeIntermediateTensorInfo(e)), _t599.forEach(e => n.disposeIntermediateTensorInfo(e)), n.disposeIntermediateTensorInfo(_r353), n.disposeIntermediateTensorInfo(_a256), _o115;\n  }\n\n  var l = i.map(e => {\n    var t = sizeFromShape(e.shape.slice(s));\n    return reshape$1({\n      inputs: {\n        x: e\n      },\n      backend: n,\n      attrs: {\n        shape: [-1, t]\n      }\n    });\n  }),\n      u = l.map(e => ({\n    vals: n.data.get(e.dataId).values,\n    shape: e.shape\n  }));\n  o = computeOutShape$1(l.map(e => e.shape), 1);\n  var c = concatImpl$1(u, o, t[0].dtype, 1 === l[0].shape[0]),\n      p = computeOutShape$1(i.map(e => e.shape), s),\n      d = n.makeTensorInfo(p, t[0].dtype, c);\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), d;\n}\n\nvar concatConfig$1 = {\n  kernelName: Concat,\n  backendName: \"cpu\",\n  kernelFunc: concat$1\n};\n\nfunction conv2D(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dilations: u,\n    dimRoundingMode: c\n  } = r;\n  assertNotComplex$1([a, s], \"conv2d\");\n  var p = convertConv2DDataFormat(l),\n      d = computeConv2DInfo(a.shape, s.shape, o, u, i, c, !1, p),\n      h = d.filterHeight,\n      m = d.filterWidth,\n      f = d.dilationHeight,\n      g = d.dilationWidth,\n      $ = d.padInfo.left,\n      y = d.padInfo.top,\n      b = \"channelsLast\" === d.dataFormat,\n      x = new TensorBuffer(d.outShape, a.dtype),\n      v = computeStrides(a.shape),\n      I = computeStrides(s.shape),\n      C = v[0],\n      S = b ? v[1] : v[2],\n      k = b ? v[2] : 1,\n      T = b ? 1 : v[1],\n      N = x.strides[0],\n      w = b ? x.strides[1] : x.strides[2],\n      E = b ? x.strides[2] : 1,\n      A = b ? 1 : x.strides[1],\n      D = n.data.get(a.dataId).values,\n      R = n.data.get(s.dataId).values,\n      _ = x.values;\n\n  for (var _e862 = 0; _e862 < d.batchSize; ++_e862) {\n    var _t600 = _e862 * C,\n        _n355 = _e862 * N;\n\n    for (var _e863 = 0; _e863 < d.outHeight; ++_e863) {\n      var _r354 = _n355 + _e863 * w,\n          _a257 = _e863 * d.strideHeight - y;\n\n      for (var _e864 = 0; _e864 < h; ++_e864) {\n        var _n356 = _a257 + _e864 * f;\n\n        if (_n356 < 0 || _n356 >= d.inHeight) continue;\n\n        var _s174 = _e864 * I[0],\n            _o116 = _t600 + _n356 * S;\n\n        for (var _e865 = 0; _e865 < d.outWidth; ++_e865) {\n          var _t601 = _r354 + _e865 * E,\n              _n357 = _e865 * d.strideWidth - $;\n\n          for (var _e866 = 0; _e866 < m; ++_e866) {\n            var _r355 = _n357 + _e866 * g;\n\n            if (_r355 < 0 || _r355 >= d.inWidth) continue;\n\n            var _a258 = _o116 + _r355 * k;\n\n            var _i59 = _s174 + _e866 * I[1];\n\n            for (var _e867 = 0; _e867 < d.inChannels; ++_e867) {\n              var _n358 = D[_a258 + _e867 * T];\n\n              for (var _e868 = 0; _e868 < d.outChannels; ++_e868) {\n                _[_t601 + _e868 * A] += _n358 * R[_i59 + _e868];\n              }\n\n              _i59 += d.outChannels;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(x.shape, x.dtype, _);\n}\n\nvar conv2DConfig$1 = {\n  kernelName: Conv2D$1,\n  backendName: \"cpu\",\n  kernelFunc: conv2D\n};\n\nfunction conv2DBackpropFilter$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r;\n  assertNotComplex$1([a, s], \"conv2dBackpropFilter\");\n  var p = convertConv2DDataFormat(l),\n      d = computeConv2DInfo(a.shape, c, o, 1, i, u, !1, p),\n      {\n    strideHeight: h,\n    strideWidth: m,\n    filterHeight: f,\n    filterWidth: g\n  } = d,\n      $ = \"channelsLast\" === d.dataFormat,\n      y = new TensorBuffer(d.filterShape, \"float32\"),\n      b = d.padInfo.left,\n      x = d.padInfo.top,\n      v = n.data.get(a.dataId).values,\n      I = n.data.get(s.dataId).values,\n      C = new TensorBuffer(a.shape, a.dtype, v),\n      S = new TensorBuffer(s.shape, s.dtype, I);\n\n  for (var _e869 = 0; _e869 < f; ++_e869) {\n    var _t602 = Math.max(0, Math.ceil((x - _e869) / h)),\n        _n359 = Math.min(d.outHeight, (d.inHeight + x - _e869) / h);\n\n    for (var _r356 = 0; _r356 < g; ++_r356) {\n      var _a259 = Math.max(0, Math.ceil((b - _r356) / m)),\n          _s175 = Math.min(d.outWidth, (d.inWidth + b - _r356) / m);\n\n      for (var _o117 = 0; _o117 < d.inChannels; ++_o117) {\n        for (var _i60 = 0; _i60 < d.outChannels; ++_i60) {\n          var _l46 = 0;\n\n          for (var _u40 = 0; _u40 < d.batchSize; ++_u40) {\n            for (var _c25 = _t602; _c25 < _n359; ++_c25) {\n              var _t603 = _e869 + _c25 * h - x;\n\n              for (var _e870 = _a259; _e870 < _s175; ++_e870) {\n                var _n360 = _r356 + _e870 * m - b;\n\n                _l46 += $ ? C.get(_u40, _t603, _n360, _o117) * S.get(_u40, _c25, _e870, _i60) : C.get(_u40, _o117, _t603, _n360) * S.get(_u40, _i60, _c25, _e870);\n              }\n            }\n          }\n\n          y.set(_l46, _e869, _r356, _o117, _i60);\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(y.shape, y.dtype, y.values);\n}\n\nvar conv2DBackpropFilterConfig$1 = {\n  kernelName: Conv2DBackpropFilter,\n  backendName: \"cpu\",\n  kernelFunc: conv2DBackpropFilter$1\n};\n\nfunction conv2DBackpropInput$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    inputShape: o,\n    strides: i,\n    pad: l,\n    dataFormat: u,\n    dimRoundingMode: c\n  } = r;\n  assertNotComplex$1([a, s], \"conv2dBackpropInput\");\n  var p = computeStrides(s.shape),\n      d = computeStrides(a.shape);\n  var h = convertConv2DDataFormat(u);\n  var m = computeConv2DInfo(o, s.shape, i, 1, l, c, !1, h),\n      f = new TensorBuffer(m.inShape, \"float32\"),\n      g = f.values,\n      $ = n.data.get(a.dataId).values,\n      y = n.data.get(s.dataId).values,\n      [b, x, v] = p,\n      {\n    batchSize: I,\n    filterHeight: C,\n    filterWidth: S,\n    inChannels: k,\n    inHeight: T,\n    inWidth: N,\n    outChannels: w,\n    outHeight: E,\n    outWidth: A,\n    strideHeight: D,\n    strideWidth: R\n  } = m;\n  h = m.dataFormat;\n\n  var _ = C - 1 - m.padInfo.top,\n      F = S - 1 - m.padInfo.left,\n      P = \"channelsLast\" === h,\n      O = f.strides[0],\n      M = P ? f.strides[1] : f.strides[2],\n      L = P ? f.strides[2] : 1,\n      z = P ? 1 : f.strides[1],\n      B = d[0],\n      V = P ? d[1] : d[2],\n      G = P ? d[2] : 1,\n      U = P ? 1 : d[1];\n\n  for (var _e871 = 0; _e871 < I; ++_e871) {\n    for (var _t604 = 0; _t604 < k; ++_t604) {\n      for (var _n361 = 0; _n361 < T; ++_n361) {\n        var _r357 = _n361 - _,\n            _a260 = Math.max(0, Math.ceil(_r357 / D)),\n            _s176 = Math.min(E, (C + _r357) / D);\n\n        for (var _o118 = 0; _o118 < N; ++_o118) {\n          var _i61 = _o118 - F,\n              _l47 = Math.max(0, Math.ceil(_i61 / R)),\n              _u41 = Math.min(A, (S + _i61) / R);\n\n          var _c26 = 0;\n\n          for (var _n362 = _a260; _n362 < _s176; ++_n362) {\n            var _a261 = _n362 * D - _r357;\n\n            for (var _r358 = _l47; _r358 < _u41; ++_r358) {\n              var _s177 = B * _e871 + V * _n362 + G * _r358,\n                  _o119 = b * (C - 1 - _a261) + x * (S - 1 - (_r358 * R - _i61)) + v * _t604;\n\n              for (var _e872 = 0; _e872 < w; ++_e872) {\n                _c26 += $[_s177 + U * _e872] * y[_o119 + _e872];\n              }\n            }\n          }\n\n          g[O * _e871 + M * _n361 + L * _o118 + z * _t604] = _c26;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(f.shape, f.dtype, f.values);\n}\n\nvar conv2DBackpropInputConfig$1 = {\n  kernelName: Conv2DBackpropInput,\n  backendName: \"cpu\",\n  kernelFunc: conv2DBackpropInput$1\n};\n\nfunction conv3D$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l\n  } = r;\n  assertNotComplex$1([a, s], \"conv3d\");\n  var u = computeConv3DInfo(a.shape, s.shape, o, l, i),\n      {\n    filterDepth: c,\n    filterHeight: p,\n    filterWidth: d,\n    dilationDepth: h,\n    dilationHeight: m,\n    dilationWidth: f,\n    padInfo: g\n  } = u,\n      $ = g.front,\n      y = g.left,\n      b = g.top,\n      x = new TensorBuffer(u.outShape, a.dtype),\n      v = n.data.get(a.dataId).values,\n      I = n.data.get(s.dataId).values,\n      C = x.values,\n      S = computeStrides(a.shape),\n      k = computeStrides(s.shape);\n\n  for (var _e873 = 0; _e873 < u.batchSize; ++_e873) {\n    var _t605 = _e873 * S[0],\n        _n363 = _e873 * x.strides[0];\n\n    for (var _e874 = 0; _e874 < u.outDepth; ++_e874) {\n      var _r359 = _n363 + _e874 * x.strides[1],\n          _a262 = _e874 * u.strideDepth - $;\n\n      for (var _e875 = 0; _e875 < c; ++_e875) {\n        var _n364 = _a262 + _e875 * h;\n\n        if (_n364 < 0 || _n364 >= u.inDepth) continue;\n\n        var _s178 = _e875 * k[0],\n            _o120 = _t605 + _n364 * S[1];\n\n        for (var _e876 = 0; _e876 < u.outHeight; ++_e876) {\n          var _t606 = _r359 + _e876 * x.strides[2],\n              _n365 = _e876 * u.strideHeight - b;\n\n          for (var _e877 = 0; _e877 < p; ++_e877) {\n            var _r360 = _n365 + _e877 * m;\n\n            if (_r360 < 0 || _r360 >= u.inHeight) continue;\n\n            var _a263 = _s178 + _e877 * k[1],\n                _i62 = _o120 + _r360 * S[2];\n\n            for (var _e878 = 0; _e878 < u.outWidth; ++_e878) {\n              var _n366 = _t606 + _e878 * u.outChannels,\n                  _r361 = _e878 * u.strideWidth - y;\n\n              for (var _e879 = 0; _e879 < d; ++_e879) {\n                var _t607 = _r361 + _e879 * f;\n\n                if (_t607 < 0 || _t607 >= u.inWidth) continue;\n\n                var _s179 = _i62 + _t607 * u.inChannels;\n\n                var _o121 = _a263 + _e879 * k[2];\n\n                for (var _e880 = 0; _e880 < u.inChannels; ++_e880) {\n                  var _t608 = v[_s179 + _e880];\n\n                  for (var _e881 = 0; _e881 < u.outChannels; ++_e881) {\n                    C[_n366 + _e881] += _t608 * I[_o121 + _e881];\n                  }\n\n                  _o121 += u.outChannels;\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(x.shape, x.dtype, x.values);\n}\n\nvar conv3DConfig$1 = {\n  kernelName: Conv3D$1,\n  backendName: \"cpu\",\n  kernelFunc: conv3D$1\n};\n\nfunction conv3DBackpropFilterV2$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    filterShape: l\n  } = r;\n  assertNotComplex$1([a, s], \"conv3dBackpropFilterV2\");\n  var u = computeStrides(a.shape),\n      c = computeStrides(s.shape),\n      p = computeConv3DInfo(a.shape, l, o, 1, i),\n      d = p.strideDepth,\n      h = p.strideHeight,\n      m = p.strideWidth,\n      f = p.filterDepth,\n      g = p.filterHeight,\n      $ = p.filterWidth,\n      y = new TensorBuffer(p.filterShape, \"float32\"),\n      b = y.values,\n      [x, v, I, C] = y.strides,\n      S = n.data.get(s.dataId).values,\n      [k, T, N, w] = c,\n      E = n.data.get(a.dataId).values,\n      [A, D, R, _] = u,\n      F = p.padInfo.front,\n      P = p.padInfo.left,\n      O = p.padInfo.top;\n\n  for (var _e882 = 0; _e882 < f; ++_e882) {\n    var _t609 = Math.max(0, Math.ceil((F - _e882) / d)),\n        _n367 = Math.min(p.outDepth, (p.inDepth + F - _e882) / d),\n        _r362 = _e882 * x;\n\n    for (var _a264 = 0; _a264 < g; ++_a264) {\n      var _s180 = Math.max(0, Math.ceil((O - _a264) / h)),\n          _o122 = Math.min(p.outHeight, (p.inHeight + O - _a264) / h),\n          _i63 = _a264 * v + _r362;\n\n      for (var _r363 = 0; _r363 < $; ++_r363) {\n        var _l48 = Math.max(0, Math.ceil((P - _r363) / m)),\n            _u42 = Math.min(p.outWidth, (p.inWidth + P - _r363) / m),\n            _c27 = _r363 * I + _i63;\n\n        for (var _i64 = 0; _i64 < p.inChannels; ++_i64) {\n          var _f12 = _i64 * C + _c27;\n\n          for (var _c28 = 0; _c28 < p.outChannels; ++_c28) {\n            var _g5 = 0;\n\n            for (var _f13 = 0; _f13 < p.batchSize; ++_f13) {\n              var _p28 = _f13 * A,\n                  _$9 = _f13 * k;\n\n              for (var _f14 = _t609; _f14 < _n367; ++_f14) {\n                var _t610 = (_e882 + _f14 * d - F) * D + _p28,\n                    _n368 = _f14 * T + _$9;\n\n                for (var _e883 = _s180; _e883 < _o122; ++_e883) {\n                  var _s181 = (_a264 + _e883 * h - O) * R + _t610,\n                      _o123 = _e883 * N + _n368;\n\n                  for (var _e884 = _l48; _e884 < _u42; ++_e884) {\n                    _g5 += E[(_r363 + _e884 * m - P) * _ + _s181 + _i64] * S[_e884 * w + _o123 + _c28];\n                  }\n                }\n              }\n            }\n\n            b[_f12 + _c28] = _g5;\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(y.shape, y.dtype, y.values);\n}\n\nvar conv3DBackpropFilterV2Config$1 = {\n  kernelName: Conv3DBackpropFilterV2,\n  backendName: \"cpu\",\n  kernelFunc: conv3DBackpropFilterV2$1\n};\n\nfunction conv3DBackpropInputV2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    pad: o,\n    strides: i,\n    inputShape: l\n  } = r;\n  assertNotComplex$1([a], \"conv3dBackpropInputV2\");\n  var u = computeStrides(a.shape),\n      c = computeStrides(s.shape),\n      p = computeConv3DInfo(l, s.shape, i, 1, o),\n      d = new TensorBuffer(p.inShape, \"float32\"),\n      h = d.values,\n      [m, f, g, $] = d.strides,\n      y = n.data.get(a.dataId).values,\n      [b, x, v, I] = u,\n      C = n.data.get(s.dataId).values,\n      [S, k, T, N] = c,\n      {\n    batchSize: w,\n    filterDepth: E,\n    filterHeight: A,\n    filterWidth: D,\n    inChannels: R,\n    inDepth: _,\n    inHeight: F,\n    inWidth: P,\n    outChannels: O,\n    outDepth: M,\n    outHeight: L,\n    outWidth: z,\n    strideDepth: B,\n    strideHeight: V,\n    strideWidth: G\n  } = p,\n      U = E - 1 - p.padInfo.front,\n      W = A - 1 - p.padInfo.top,\n      q = D - 1 - p.padInfo.left;\n\n  for (var _e885 = 0; _e885 < w; ++_e885) {\n    for (var _t611 = 0; _t611 < R; ++_t611) {\n      for (var _n369 = 0; _n369 < _; ++_n369) {\n        var _r364 = _n369 - U,\n            _a265 = Math.max(0, Math.ceil(_r364 / B)),\n            _s182 = Math.min(M, (E + _r364) / B);\n\n        for (var _o124 = 0; _o124 < F; ++_o124) {\n          var _i65 = _o124 - W,\n              _l49 = Math.max(0, Math.ceil(_i65 / V)),\n              _u43 = Math.min(L, (A + _i65) / V);\n\n          for (var _c29 = 0; _c29 < P; ++_c29) {\n            var _p29 = _c29 - q,\n                _d17 = Math.max(0, Math.ceil(_p29 / G)),\n                _w2 = Math.min(z, (D + _p29) / G);\n\n            var _R2 = 0;\n\n            for (var _n370 = _a265; _n370 < _s182; ++_n370) {\n              var _a266 = _n370 * B - _r364;\n\n              for (var _r365 = _l49; _r365 < _u43; ++_r365) {\n                var _s183 = _r365 * V - _i65;\n\n                for (var _o125 = _d17; _o125 < _w2; ++_o125) {\n                  var _i66 = b * _e885 + x * _n370 + v * _r365 + I * _o125,\n                      _l50 = S * (E - 1 - _a266) + k * (A - 1 - _s183) + T * (D - 1 - (_o125 * G - _p29)) + N * _t611;\n\n                  for (var _e886 = 0; _e886 < O; ++_e886) {\n                    _R2 += y[_i66 + _e886] * C[_l50 + _e886];\n                  }\n                }\n              }\n            }\n\n            h[m * _e885 + f * _n369 + g * _o124 + $ * _c29 + _t611] = _R2;\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(d.shape, d.dtype, d.values);\n}\n\nvar conv3DBackpropInputV2Config = {\n  kernelName: Conv3DBackpropInputV2,\n  backendName: \"cpu\",\n  kernelFunc: conv3DBackpropInputV2\n},\n    cos$1 = unaryKernelFunc$1(Cos, e => Math.cos(e)),\n    cosConfig$1 = {\n  kernelName: Cos,\n  backendName: \"cpu\",\n  kernelFunc: cos$1\n},\n    cosh$1 = unaryKernelFunc$1(Cosh, e => Math.cosh(e)),\n    coshConfig$1 = {\n  kernelName: Cosh,\n  backendName: \"cpu\",\n  kernelFunc: cosh$1\n};\n\nfunction cropAndResize$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    image: a,\n    boxes: s,\n    boxInd: o\n  } = t,\n      {\n    cropSize: i,\n    method: l,\n    extrapolationValue: u\n  } = r,\n      [c, p, d, h] = a.shape,\n      m = s.shape[0],\n      [f, g] = i,\n      $ = buffer([m, f, g, h], \"float32\"),\n      y = n.data.get(s.dataId).values,\n      b = n.data.get(o.dataId).values,\n      x = n.data.get(a.dataId).values,\n      v = computeStrides(a.shape),\n      I = computeStrides($.shape);\n\n  for (var _e887 = 0; _e887 < m; _e887++) {\n    var _t612 = 4 * _e887,\n        _n371 = y[_t612],\n        _r366 = y[_t612 + 1],\n        _a267 = y[_t612 + 2],\n        _s184 = y[_t612 + 3],\n        _o126 = b[_e887];\n\n    if (_o126 >= c) continue;\n\n    var _i67 = f > 1 ? (_a267 - _n371) * (p - 1) / (f - 1) : 0,\n        _m6 = g > 1 ? (_s184 - _r366) * (d - 1) / (g - 1) : 0;\n\n    for (var _t613 = 0; _t613 < f; _t613++) {\n      var _c30 = f > 1 ? _n371 * (p - 1) + _t613 * _i67 : .5 * (_n371 + _a267) * (p - 1);\n\n      if (_c30 < 0 || _c30 > p - 1) for (var _n372 = 0; _n372 < g; _n372++) {\n        for (var _r367 = 0; _r367 < h; _r367++) {\n          $.values[_r367 + _n372 * I[2] + _t613 * I[1] + _e887 * I[0]] = u;\n        }\n      } else if (\"bilinear\" === l) {\n        var _n373 = Math.floor(_c30),\n            _a268 = Math.ceil(_c30),\n            _i68 = _c30 - _n373;\n\n        for (var _l51 = 0; _l51 < g; _l51++) {\n          var _c31 = g > 1 ? _r366 * (d - 1) + _l51 * _m6 : .5 * (_r366 + _s184) * (d - 1);\n\n          if (_c31 < 0 || _c31 > d - 1) {\n            for (var _n374 = 0; _n374 < h; _n374++) {\n              $.values[_n374 + _l51 * I[2] + _t613 * I[1] + _e887 * I[0]] = u;\n            }\n\n            continue;\n          }\n\n          var _p30 = Math.floor(_c31),\n              _f15 = Math.ceil(_c31),\n              _y4 = _c31 - _p30;\n\n          for (var _r368 = 0; _r368 < h; _r368++) {\n            var _s185 = _r368 + _p30 * v[2] + _n373 * v[1] + _o126 * v[0];\n\n            var _u44 = x[_s185];\n            _s185 = _r368 + _f15 * v[2] + _n373 * v[1] + _o126 * v[0];\n            var _c32 = x[_s185];\n            _s185 = _r368 + _p30 * v[2] + _a268 * v[1] + _o126 * v[0];\n            var _d18 = x[_s185];\n            _s185 = _r368 + _f15 * v[2] + _a268 * v[1] + _o126 * v[0];\n\n            var _h18 = x[_s185],\n                _m7 = _u44 + (_c32 - _u44) * _y4;\n\n            _s185 = _r368 + _l51 * I[2] + _t613 * I[1] + _e887 * I[0], $.values[_s185] = _m7 + (_d18 + (_h18 - _d18) * _y4 - _m7) * _i68;\n          }\n        }\n      } else for (var _n375 = 0; _n375 < g; ++_n375) {\n        var _a269 = g > 1 ? _r366 * (d - 1) + _n375 * _m6 : .5 * (_r366 + _s184) * (d - 1);\n\n        if (_a269 < 0 || _a269 > d - 1) {\n          for (var _r369 = 0; _r369 < h; _r369++) {\n            $.values[_r369 + _n375 * I[2] + _t613 * I[1] + _e887 * I[0]] = u;\n          }\n\n          continue;\n        }\n\n        var _i69 = Math.round(_a269),\n            _l52 = Math.round(_c30);\n\n        for (var _r370 = 0; _r370 < h; _r370++) {\n          $.values[_r370 + _n375 * I[2] + _t613 * I[1] + _e887 * I[0]] = x[_r370 + _i69 * v[2] + _l52 * v[1] + _o126 * v[0]];\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo($.shape, $.dtype, $.values);\n}\n\nvar cropAndResizeConfig$1 = {\n  kernelName: CropAndResize,\n  backendName: \"cpu\",\n  kernelFunc: cropAndResize$1\n};\n\nfunction cumsum$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    exclusive: o,\n    reverse: i\n  } = r;\n  assertNotComplex$1(a, \"cumsum\");\n  var l = getAxesPermutation([s], a.shape.length);\n  var u = a;\n  null != l && (u = transpose$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: l\n    }\n  }));\n  var c = getInnerMostAxes(1, a.shape.length)[0];\n  if (c !== u.shape.length - 1) throw new Error(\"backend.cumsum in CPU expects an inner-most axis=\".concat(u.shape.length - 1, \" but got axis=\").concat(c));\n  var p = upcastType(u.dtype, \"int32\"),\n      d = makeZerosTypedArray(sizeFromShape(u.shape), p),\n      h = n.data.get(u.dataId).values,\n      m = u.shape[u.shape.length - 1],\n      f = i ? (e, t) => e + m - t - 1 : (e, t) => e + t;\n\n  for (var _e888 = 0; _e888 < h.length; _e888 += m) {\n    for (var _t614 = 0; _t614 < m; _t614++) {\n      var _n376 = f(_e888, _t614);\n\n      if (0 === _t614) d[_n376] = o ? 0 : h[_n376];else {\n        var _r371 = f(_e888, _t614 - 1);\n\n        d[_n376] = o ? h[_r371] + d[_r371] : h[_n376] + d[_r371];\n      }\n    }\n  }\n\n  var g = n.makeTensorInfo(u.shape, p, d);\n\n  if (null != l) {\n    var _e889 = transpose$1({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        perm: getUndoAxesPermutation(l)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), n.disposeIntermediateTensorInfo(u), _e889;\n  }\n\n  return g;\n}\n\nvar cumsumConfig$1 = {\n  kernelName: Cumsum,\n  backendName: \"cpu\",\n  kernelFunc: cumsum$1\n};\n\nfunction denseBincount$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o,\n    binaryOutput: i\n  } = r;\n\n  if (1 === a.shape.length) {\n    var _e890 = bincountImpl(n.data.get(a.dataId).values, n.data.get(s.dataId).values, s.dtype, s.shape, o);\n\n    return n.makeTensorInfo([o], s.dtype, _e890);\n  }\n\n  if (2 === a.shape.length) {\n    var _e891 = bincountReduceImpl(n.bufferSync(a), n.bufferSync(s), o, i);\n\n    return n.makeTensorInfo(_e891.shape, s.dtype, _e891.values);\n  }\n\n  throw new Error(\"Error in denseBincount: input must be at most rank 2, but got rank\".concat(a.shape.length, \".\"));\n}\n\nvar denseBincountConfig$1 = {\n  kernelName: DenseBincount,\n  backendName: \"cpu\",\n  kernelFunc: denseBincount$1\n};\n\nfunction depthToSpace$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockSize: s,\n    dataFormat: o\n  } = r;\n  assert$4(\"NHWC\" === o, () => \"Only NHWC dataFormat supported on CPU for depthToSpace. Got \".concat(o)), assert$4(s > 1, () => \"blockSize should be > 1 for depthToSpace, but was: \".concat(s));\n  var i = a.shape[0],\n      l = a.shape[1],\n      u = a.shape[2],\n      c = a.shape[3],\n      p = l * s,\n      d = u * s,\n      h = c / (s * s),\n      m = n.data.get(a.dataId).values,\n      f = new Float32Array(i * p * d * h);\n  var g = 0;\n\n  for (var _e892 = 0; _e892 < i; ++_e892) {\n    for (var _t615 = 0; _t615 < p; ++_t615) {\n      var _n377 = Math.floor(_t615 / s),\n          _r372 = _t615 % s;\n\n      for (var _t616 = 0; _t616 < d; ++_t616) {\n        var _a270 = Math.floor(_t616 / s),\n            _o127 = (_r372 * s + _t616 % s) * h;\n\n        for (var _t617 = 0; _t617 < h; ++_t617) {\n          f[g++] = m[_t617 + _o127 + c * (_a270 + u * (_n377 + l * _e892))];\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([i, p, d, h], a.dtype, f);\n}\n\nvar depthToSpaceConfig$1 = {\n  kernelName: DepthToSpace,\n  backendName: \"cpu\",\n  kernelFunc: depthToSpace$1\n};\n\nfunction depthwiseConv2dNative$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l,\n    dimRoundingMode: u\n  } = r;\n  assertNotComplex$1([a, s], \"depthwiseConv2DNative\");\n  var c = computeStrides(a.shape),\n      p = computeStrides(s.shape);\n  var d = l;\n  null == d && (d = [1, 1]), assert$4(eitherStridesOrDilationsAreOne(o, d), () => \"Error in depthwiseConv2d: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '\").concat(d, \"'\"));\n  var h = computeConv2DInfo(a.shape, s.shape, o, d, i, u, !0),\n      {\n    filterHeight: m,\n    filterWidth: f,\n    dilationHeight: g,\n    dilationWidth: $,\n    padInfo: y\n  } = h,\n      b = y.left,\n      x = y.top,\n      v = h.outChannels / h.inChannels,\n      I = new TensorBuffer(h.outShape, a.dtype),\n      C = n.data.get(a.dataId).values,\n      S = n.data.get(s.dataId).values,\n      k = I.values;\n\n  for (var _e893 = 0; _e893 < h.batchSize; ++_e893) {\n    var _t618 = _e893 * c[0],\n        _n378 = _e893 * I.strides[0];\n\n    for (var _e894 = 0; _e894 < h.outHeight; ++_e894) {\n      var _r373 = _n378 + _e894 * I.strides[1],\n          _a271 = _e894 * h.strideHeight - x;\n\n      for (var _e895 = 0; _e895 < m; ++_e895) {\n        var _n379 = _a271 + _e895 * g;\n\n        if (_n379 < 0 || _n379 >= h.inHeight) continue;\n\n        var _s186 = _e895 * p[0],\n            _o128 = _t618 + _n379 * c[1];\n\n        for (var _e896 = 0; _e896 < h.outWidth; ++_e896) {\n          var _t619 = _r373 + _e896 * I.strides[2],\n              _n380 = _e896 * h.strideWidth - b;\n\n          for (var _e897 = 0; _e897 < f; ++_e897) {\n            var _r374 = _n380 + _e897 * $;\n\n            if (_r374 < 0 || _r374 >= h.inWidth) continue;\n\n            var _a272 = _o128 + _r374 * h.inChannels;\n\n            var _i70 = _t619,\n                _l53 = _s186 + _e897 * p[1];\n\n            for (var _e898 = 0; _e898 < h.inChannels; ++_e898) {\n              var _t620 = C[_a272 + _e898];\n\n              for (var _e899 = 0; _e899 < v; ++_e899) {\n                k[_i70 + _e899] += _t620 * S[_l53 + _e899];\n              }\n\n              _i70 += v, _l53 += v;\n            }\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(I.shape, I.dtype, I.values);\n}\n\nvar depthwiseConv2dNativeConfig$1 = {\n  kernelName: DepthwiseConv2dNative,\n  backendName: \"cpu\",\n  kernelFunc: depthwiseConv2dNative$1\n};\n\nfunction depthwiseConv2dNativeBackpropFilter$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r;\n  assertNotComplex$1([a, s], \"depthwiseConv2dNativeBackpropFilter\");\n  var p = computeConv2DInfo(a.shape, c, o, i, l, u, !0),\n      {\n    strideHeight: d,\n    strideWidth: h,\n    filterHeight: m,\n    filterWidth: f\n  } = p,\n      g = new TensorBuffer(p.filterShape, \"float32\"),\n      $ = p.padInfo.left,\n      y = p.padInfo.top,\n      b = p.outChannels / p.inChannels,\n      x = n.data.get(a.dataId).values,\n      v = new TensorBuffer(a.shape, a.dtype, x),\n      I = n.data.get(s.dataId).values,\n      C = new TensorBuffer(s.shape, s.dtype, I);\n\n  for (var _e900 = 0; _e900 < m; ++_e900) {\n    var _t621 = Math.max(0, Math.ceil((y - _e900) / d)),\n        _n381 = Math.min(p.outHeight, (p.inHeight + y - _e900) / d);\n\n    for (var _r375 = 0; _r375 < f; ++_r375) {\n      var _a273 = Math.max(0, Math.ceil(($ - _r375) / h)),\n          _s187 = Math.min(p.outWidth, (p.inWidth + $ - _r375) / h);\n\n      for (var _o129 = 0; _o129 < p.outChannels; ++_o129) {\n        var _i71 = Math.trunc(_o129 / b),\n            _l54 = _o129 % b;\n\n        var _u45 = 0;\n\n        for (var _l55 = 0; _l55 < p.batchSize; ++_l55) {\n          for (var _c33 = _t621; _c33 < _n381; ++_c33) {\n            var _t622 = _e900 + _c33 * d - y;\n\n            for (var _e901 = _a273; _e901 < _s187; ++_e901) {\n              _u45 += v.get(_l55, _t622, _r375 + _e901 * h - $, _i71) * C.get(_l55, _c33, _e901, _o129);\n            }\n          }\n        }\n\n        g.set(_u45, _e900, _r375, _i71, _l54);\n      }\n    }\n  }\n\n  return n.makeTensorInfo(g.shape, g.dtype, g.values);\n}\n\nvar depthwiseConv2dNativeBackpropFilterConfig$1 = {\n  kernelName: DepthwiseConv2dNativeBackpropFilter,\n  backendName: \"cpu\",\n  kernelFunc: depthwiseConv2dNativeBackpropFilter$1\n};\n\nfunction depthwiseConv2dNativeBackpropInput$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    inputShape: c\n  } = r;\n  assertNotComplex$1([a, s], \"depthwiseConv2DNativeBackpropInput\");\n  var p = computeStrides(a.shape),\n      d = computeStrides(s.shape),\n      h = computeConv2DInfo(c, s.shape, o, i, l, u, !0),\n      m = new TensorBuffer(h.inShape, \"float32\"),\n      f = m.values,\n      [g, $, y] = m.strides,\n      b = n.data.get(a.dataId).values,\n      [x, v, I] = p,\n      C = n.data.get(s.dataId).values,\n      [S, k, T] = d,\n      {\n    batchSize: N,\n    filterHeight: w,\n    filterWidth: E,\n    inChannels: A,\n    inHeight: D,\n    inWidth: R,\n    outChannels: _,\n    outHeight: F,\n    outWidth: P,\n    strideHeight: O,\n    strideWidth: M\n  } = h,\n      L = w - 1 - h.padInfo.top,\n      z = E - 1 - h.padInfo.left,\n      B = _ / A;\n\n  for (var _e902 = 0; _e902 < N; ++_e902) {\n    for (var _t623 = 0; _t623 < A; ++_t623) {\n      for (var _n382 = 0; _n382 < D; ++_n382) {\n        var _r376 = _n382 - L,\n            _a274 = Math.max(0, Math.ceil(_r376 / O)),\n            _s188 = Math.min(F, (w + _r376) / O);\n\n        for (var _o130 = 0; _o130 < R; ++_o130) {\n          var _i72 = _o130 - z,\n              _l56 = Math.max(0, Math.ceil(_i72 / M)),\n              _u46 = Math.min(P, (E + _i72) / M);\n\n          var _c34 = 0;\n\n          for (var _n383 = _a274; _n383 < _s188; ++_n383) {\n            var _a275 = _n383 * O - _r376;\n\n            for (var _r377 = _l56; _r377 < _u46; ++_r377) {\n              var _s189 = x * _e902 + v * _n383 + I * _r377,\n                  _o131 = S * (w - 1 - _a275) + k * (E - 1 - (_r377 * M - _i72)) + T * _t623;\n\n              for (var _e903 = 0; _e903 < B; ++_e903) {\n                _c34 += b[_s189 + (_t623 * B + _e903)] * C[_o131 + _e903];\n              }\n            }\n          }\n\n          f[g * _e902 + $ * _n382 + y * _o130 + _t623] = _c34;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(m.shape, m.dtype, m.values);\n}\n\nvar depthwiseConv2dNativeBackpropInputConfig$1 = {\n  kernelName: DepthwiseConv2dNativeBackpropInput,\n  backendName: \"cpu\",\n  kernelFunc: depthwiseConv2dNativeBackpropInput$1\n};\n\nfunction diag$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t,\n      a = sizeFromShape(r.shape),\n      s = n.data.get(r.dataId).values,\n      o = buffer([a, a], r.dtype),\n      i = o.values;\n\n  for (var _e904 = 0; _e904 < s.length; _e904++) {\n    i[_e904 * a + _e904] = s[_e904];\n  }\n\n  var l = [...r.shape, ...r.shape];\n  return n.makeTensorInfo(l, o.dtype, o.values);\n}\n\nvar diagConfig$1 = {\n  kernelName: Diag,\n  backendName: \"cpu\",\n  kernelFunc: diag$1\n},\n    dilation2dConfig = {\n  kernelName: Dilation2D,\n  backendName: \"cpu\",\n  kernelFunc: _ref54 => {\n    var {\n      inputs: e,\n      backend: t,\n      attrs: n\n    } = _ref54;\n    var {\n      x: r,\n      filter: a\n    } = e,\n        {\n      strides: s,\n      pad: o,\n      dilations: i\n    } = n,\n        l = t,\n        u = l.data.get(r.dataId).values,\n        c = r.shape.length,\n        p = l.data.get(a.dataId).values,\n        d = a.shape.length,\n        {\n      batchSize: h,\n      inHeight: m,\n      inWidth: f,\n      inChannels: g,\n      outHeight: $,\n      outWidth: y,\n      padInfo: b,\n      strideHeight: x,\n      strideWidth: v,\n      filterHeight: I,\n      filterWidth: C,\n      dilationHeight: S,\n      dilationWidth: k,\n      outShape: T\n    } = computeDilation2DInfo(r.shape, a.shape, s, o, \"NHWC\", i),\n        N = sizeFromShape(T),\n        w = T.length,\n        E = getArrayFromDType(r.dtype, N);\n\n    for (var _e905 = 0; _e905 < h; ++_e905) {\n      for (var _t624 = 0; _t624 < $; ++_t624) {\n        var _n384 = _t624 * x - b.top;\n\n        for (var _s190 = 0; _s190 < y; ++_s190) {\n          var _o132 = _s190 * v - b.left;\n\n          for (var _i73 = 0; _i73 < g; ++_i73) {\n            var _l57 = Number.MIN_SAFE_INTEGER;\n\n            for (var _t625 = 0; _t625 < I; ++_t625) {\n              var _s191 = _n384 + _t625 * S;\n\n              if (_s191 >= 0 && _s191 < m) for (var _n385 = 0; _n385 < C; ++_n385) {\n                var _h19 = _o132 + _n385 * k;\n\n                if (_h19 >= 0 && _h19 < f) {\n                  var _o133 = locToIndex([_e905, _s191, _h19, _i73], c, computeStrides(r.shape)),\n                      _m8 = locToIndex([_t625, _n385, _i73], d, computeStrides(a.shape)),\n                      _f16 = u[_o133] + p[_m8];\n\n                  _f16 > _l57 && (_l57 = _f16);\n                }\n              }\n            }\n\n            E[locToIndex([_e905, _t624, _s190, _i73], w, computeStrides(T))] = _l57;\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: l.write(toTypedArray(E, r.dtype), T, r.dtype),\n      shape: T,\n      dtype: r.dtype\n    };\n  }\n},\n    dilation2dBackpropFilterConfig = {\n  kernelName: Dilation2DBackpropFilter,\n  backendName: \"cpu\",\n  kernelFunc: _ref55 => {\n    var {\n      inputs: e,\n      backend: t,\n      attrs: n\n    } = _ref55;\n    var {\n      x: r,\n      filter: a,\n      dy: s\n    } = e,\n        {\n      strides: o,\n      pad: i,\n      dilations: l\n    } = n,\n        u = t,\n        c = toNestedArray(r.shape, u.data.get(r.dataId).values),\n        p = toNestedArray(a.shape, u.data.get(a.dataId).values),\n        {\n      batchSize: d,\n      inHeight: h,\n      inWidth: m,\n      inChannels: f,\n      outHeight: g,\n      outWidth: $,\n      padInfo: y,\n      strideHeight: b,\n      strideWidth: x,\n      filterHeight: v,\n      filterWidth: I,\n      dilationHeight: C,\n      dilationWidth: S,\n      outShape: k\n    } = computeDilation2DInfo(r.shape, a.shape, o, i, \"NHWC\", l);\n    assert$4(s.rank === k.length, () => \"Error in \".concat(Dilation2DBackpropFilter, \", dy must have the same rank as output \").concat(k.length, \", but got \").concat(s.rank));\n    var T = toNestedArray(k, u.data.get(s.dataId).values),\n        N = makeZerosNestedTypedArray(a.shape, a.dtype);\n\n    for (var _e906 = 0; _e906 < d; ++_e906) {\n      for (var _t626 = 0; _t626 < g; ++_t626) {\n        var _n386 = _t626 * b - y.top;\n\n        for (var _r378 = 0; _r378 < $; ++_r378) {\n          var _a276 = _r378 * x - y.left;\n\n          for (var _s192 = 0; _s192 < f; ++_s192) {\n            var _o134 = Number.MIN_SAFE_INTEGER,\n                _i74 = 0,\n                _l58 = 0;\n\n            for (var _t627 = 0; _t627 < v; ++_t627) {\n              var _r379 = _n386 + _t627 * C;\n\n              if (_r379 >= 0 && _r379 < h) for (var _n387 = 0; _n387 < I; ++_n387) {\n                var _u47 = _a276 + _n387 * S;\n\n                if (_u47 >= 0 && _u47 < m) {\n                  var _a277 = c[_e906][_r379][_u47][_s192] + p[_t627][_n387][_s192];\n\n                  _a277 > _o134 && (_o134 = _a277, _i74 = _t627, _l58 = _n387);\n                }\n              }\n            }\n\n            N[_i74][_l58][_s192] += T[_e906][_t626][_r378][_s192];\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: u.write(toTypedArray(N, r.dtype), a.shape, a.dtype),\n      shape: a.shape,\n      dtype: a.dtype\n    };\n  }\n},\n    dilation2dBackpropInputConfig = {\n  kernelName: Dilation2DBackpropInput,\n  backendName: \"cpu\",\n  kernelFunc: _ref56 => {\n    var {\n      inputs: e,\n      backend: t,\n      attrs: n\n    } = _ref56;\n    var {\n      x: r,\n      filter: a,\n      dy: s\n    } = e,\n        {\n      strides: o,\n      pad: i,\n      dilations: l\n    } = n,\n        u = t,\n        c = toNestedArray(r.shape, u.data.get(r.dataId).values),\n        p = toNestedArray(a.shape, u.data.get(a.dataId).values),\n        {\n      batchSize: d,\n      inHeight: h,\n      inWidth: m,\n      inChannels: f,\n      outHeight: g,\n      outWidth: $,\n      padInfo: y,\n      strideHeight: b,\n      strideWidth: x,\n      filterHeight: v,\n      filterWidth: I,\n      dilationHeight: C,\n      dilationWidth: S,\n      outShape: k\n    } = computeDilation2DInfo(r.shape, a.shape, o, i, \"NHWC\", l);\n    assert$4(s.rank === k.length, () => \"Error in \".concat(Dilation2DBackpropInput, \", dy must have the same rank as output \").concat(k.length, \", but got \").concat(s.rank));\n    var T = toNestedArray(k, u.data.get(s.dataId).values),\n        N = makeZerosNestedTypedArray(r.shape, r.dtype);\n\n    for (var _e907 = 0; _e907 < d; ++_e907) {\n      for (var _t628 = 0; _t628 < g; ++_t628) {\n        var _n388 = _t628 * b - y.top;\n\n        for (var _r380 = 0; _r380 < $; ++_r380) {\n          var _a278 = _r380 * x - y.left;\n\n          for (var _s193 = 0; _s193 < f; ++_s193) {\n            var _o135 = Number.MIN_SAFE_INTEGER,\n                _i75 = _n388 < 0 ? 0 : _n388,\n                _l59 = _a278 < 0 ? 0 : _a278;\n\n            for (var _t629 = 0; _t629 < v; ++_t629) {\n              var _r381 = _n388 + _t629 * C;\n\n              if (_r381 >= 0 && _r381 < h) for (var _n389 = 0; _n389 < I; ++_n389) {\n                var _u48 = _a278 + _n389 * S;\n\n                if (_u48 >= 0 && _u48 < m) {\n                  var _a279 = c[_e907][_r381][_u48][_s193] + p[_t629][_n389][_s193];\n\n                  _a279 > _o135 && (_o135 = _a279, _i75 = _r381, _l59 = _u48);\n                }\n              }\n            }\n\n            N[_e907][_i75][_l59][_s193] += T[_e907][_t628][_r380][_s193];\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: u.write(toTypedArray(N, r.dtype), r.shape, r.dtype),\n      shape: r.shape,\n      dtype: r.dtype\n    };\n  }\n};\n\nfunction sum$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  var i;\n  assertNotComplex$1(a, \"sum\"), i = \"bool\" === a.dtype ? cast$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      dtype: \"int32\"\n    }\n  }) : identity$1({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var l = i.shape.length,\n      u = parseAxisParam(s, i.shape),\n      c = getAxesPermutation(u, l);\n  var p = u,\n      d = i;\n  null != c && (d = transpose$1({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), p = getInnerMostAxes(p.length, l)), assertAxesAreInnerMostDims(\"sum\", p, d.shape.length);\n  var [h, m] = computeOutAndReduceShapes(d.shape, p);\n  var f = zeros(n, h, upcastType(d.dtype, \"int32\"));\n  var g = sizeFromShape(m),\n      $ = n.data.get(f.dataId).values,\n      y = n.data.get(d.dataId).values;\n\n  for (var _e908 = 0; _e908 < $.length; ++_e908) {\n    var _t630 = _e908 * g;\n\n    var _n390 = 0;\n\n    for (var _e909 = 0; _e909 < g; ++_e909) {\n      _n390 += y[_t630 + _e909];\n    }\n\n    $[_e908] = _n390;\n  }\n\n  if (o) {\n    var _e910 = f;\n    f = reshape$1({\n      inputs: {\n        x: f\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim(f.shape, u)\n      }\n    }), n.disposeIntermediateTensorInfo(_e910);\n  }\n\n  return n.disposeIntermediateTensorInfo(i), null != c && n.disposeIntermediateTensorInfo(d), f;\n}\n\nvar sumConfig$1 = {\n  kernelName: Sum,\n  backendName: \"cpu\",\n  kernelFunc: sum$1\n};\n\nfunction einsum$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    equation: a\n  } = r,\n      s = t,\n      {\n    allDims: o,\n    summedDims: i,\n    idDims: l\n  } = decodeEinsumEquation(a, s.length);\n  checkEinsumDimSizes(o.length, l, s);\n  var {\n    path: u,\n    steps: c\n  } = getEinsumComputePath(i, l),\n      p = c.length;\n  var d = null,\n      h = o.length;\n  var m = [];\n\n  for (var _e911 = 0; _e911 < p; ++_e911) {\n    for (var _t631 of c[_e911]) {\n      var {\n        permutationIndices: _e912,\n        expandDims: _r382\n      } = getEinsumPermutation(h, l[_t631]);\n\n      var _a280 = void 0;\n\n      isIdentityPermutation(_e912) ? _a280 = s[_t631] : (_a280 = transpose$1({\n        inputs: {\n          x: s[_t631]\n        },\n        backend: n,\n        attrs: {\n          perm: _e912\n        }\n      }), m.push(_a280));\n\n      var _o136 = _a280.shape.slice();\n\n      for (var _e913 = 0; _e913 < _r382.length; ++_e913) {\n        _o136.splice(_r382[_e913], 0, 1);\n      }\n\n      arraysEqual(_a280.shape, _o136) || (_a280 = reshape$1({\n        inputs: {\n          x: _a280\n        },\n        backend: n,\n        attrs: {\n          shape: _o136\n        }\n      }), m.push(_a280)), null === d ? d = _a280 : (d = multiply$1({\n        inputs: {\n          a: _a280,\n          b: d\n        },\n        backend: n\n      }), m.push(d));\n    }\n\n    _e911 < p - 1 && (u[_e911] >= 0 && (d = sum$1({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        axis: u[_e911] - (o.length - h),\n        keepDims: !1\n      }\n    }), m.push(d)), h--);\n  }\n\n  for (var _e914 of m) {\n    _e914 !== d && n.disposeIntermediateTensorInfo(_e914);\n  }\n\n  return d;\n}\n\nvar einsumConfig$1 = {\n  kernelName: Einsum,\n  backendName: \"cpu\",\n  kernelFunc: einsum$1\n};\n\nfunction eluGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    dy: r,\n    y: a\n  } = t;\n  assertNotComplex$1([r, a], \"eluGrad\");\n  var s = new Float32Array(sizeFromShape(a.shape)),\n      o = n.data.get(a.dataId).values,\n      i = n.data.get(r.dataId).values;\n\n  for (var _e915 = 0; _e915 < o.length; ++_e915) {\n    var _t632 = o[_e915];\n    s[_e915] = _t632 >= 1 ? i[_e915] : i[_e915] * (_t632 + 1);\n  }\n\n  return n.makeTensorInfo(a.shape, \"float32\", s);\n}\n\nvar eluGradConfig$1 = {\n  kernelName: EluGrad,\n  backendName: \"cpu\",\n  kernelFunc: eluGrad$1\n},\n    p = ERF_P,\n    a1 = ERF_A1,\n    a2 = ERF_A2,\n    a3 = ERF_A3,\n    a4 = ERF_A4,\n    a5 = ERF_A5,\n    erf$1 = unaryKernelFunc$1(Erf, e => {\n  var t = Math.sign(e),\n      n = Math.abs(e),\n      r = 1 / (1 + p * n);\n  return t * (1 - ((((a5 * r + a4) * r + a3) * r + a2) * r + a1) * r * Math.exp(-n * n));\n}),\n    erfConfig$1 = {\n  kernelName: Erf,\n  backendName: \"cpu\",\n  kernelFunc: erf$1\n};\n\nfunction expandDims$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    input: a\n  } = t,\n      {\n    dim: s\n  } = r,\n      o = a.shape.length,\n      i = a.shape.slice();\n  var l = s;\n  return s < 0 && (assert$4(-(o + 1) <= s, () => \"Axis must be in the interval [\".concat(-(o + 1), \", \").concat(o, \"]\")), l = o + s + 1), i.splice(l, 0, 1), reshape$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: i\n    }\n  });\n}\n\nvar expandDimsConfig$1 = {\n  kernelName: ExpandDims,\n  backendName: \"cpu\",\n  kernelFunc: expandDims$1\n},\n    realDivImpl = createSimpleBinaryKernelImpl((e, t) => e / t),\n    div = binaryKernelFunc$1(RealDiv, realDivImpl),\n    realDivConfig$1 = {\n  kernelName: RealDiv,\n  backendName: \"cpu\",\n  kernelFunc: div\n};\n\nfunction fftBatch(e, t, n) {\n  var r = e.shape,\n      a = r[0],\n      s = r[1],\n      o = n.data.get(e.dataId),\n      i = o.complexTensorInfos.real,\n      l = o.complexTensorInfos.imag,\n      u = [a, s],\n      c = sizeFromShape(u),\n      p = getTypedArrayFromDType(\"float32\", c),\n      d = getTypedArrayFromDType(\"float32\", c);\n\n  for (var _e916 = 0; _e916 < a; _e916++) {\n    var _r383 = slice$1({\n      inputs: {\n        x: i\n      },\n      backend: n,\n      attrs: {\n        begin: [_e916, 0],\n        size: [1, s]\n      }\n    }),\n        _a281 = slice$1({\n      inputs: {\n        x: l\n      },\n      backend: n,\n      attrs: {\n        begin: [_e916, 0],\n        size: [1, s]\n      }\n    }),\n        _o137 = complex$1({\n      inputs: {\n        real: _r383,\n        imag: _a281\n      },\n      backend: n\n    }),\n        {\n      real: _u49,\n      imag: _c35\n    } = fftImpl$1(_o137, t, n),\n        _h20 = mergeRealAndImagArrays(_u49, _c35);\n\n    for (var _t633 = 0; _t633 < s; _t633++) {\n      var _n391 = getComplexWithIndex(_h20, _t633);\n\n      p[_e916 * s + _t633] = _n391.real, d[_e916 * s + _t633] = _n391.imag;\n    }\n\n    n.disposeIntermediateTensorInfo(_r383), n.disposeIntermediateTensorInfo(_a281), n.disposeIntermediateTensorInfo(_o137);\n  }\n\n  var h = n.makeTensorInfo(u, \"float32\", p),\n      m = n.makeTensorInfo(u, \"float32\", d),\n      f = complex$1({\n    inputs: {\n      real: h,\n      imag: m\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), f;\n}\n\nfunction fftImpl$1(e, t, n) {\n  var r = sizeFromShape(e.shape),\n      a = n.data.get(e.dataId),\n      s = n.data.get(a.complexTensorInfos.real.dataId).values,\n      o = n.data.get(a.complexTensorInfos.imag.dataId).values;\n\n  if (isExponentOf2(r)) {\n    var _a282 = fftRadix2(s, o, r, t, n),\n        i = [e.shape[0], e.shape[1]];\n\n    if (t) {\n      var _e917 = n.makeTensorInfo(i, \"float32\", _a282.real),\n          _t634 = n.makeTensorInfo(i, \"float32\", _a282.imag),\n          _s194 = n.makeTensorInfo([], \"float32\", createScalarValue(r, \"float32\")),\n          _o138 = identity$1({\n        inputs: {\n          x: _s194\n        },\n        backend: n\n      }),\n          l = realDivConfig$1.kernelFunc({\n        inputs: {\n          a: _e917,\n          b: _s194\n        },\n        backend: n\n      }),\n          u = realDivConfig$1.kernelFunc({\n        inputs: {\n          a: _t634,\n          b: _o138\n        },\n        backend: n\n      }),\n          c = n.data.get(l.dataId).values,\n          _p31 = n.data.get(u.dataId).values;\n\n      return n.disposeIntermediateTensorInfo(_e917), n.disposeIntermediateTensorInfo(_t634), n.disposeIntermediateTensorInfo(_s194), n.disposeIntermediateTensorInfo(_o138), n.disposeIntermediateTensorInfo(l), n.disposeIntermediateTensorInfo(u), {\n        real: c,\n        imag: _p31\n      };\n    }\n\n    return _a282;\n  }\n\n  return splitRealAndImagArrays(fourierTransformByMatmul(mergeRealAndImagArrays(s, o), r, t));\n}\n\nfunction isExponentOf2(e) {\n  return 0 == (e & e - 1);\n}\n\nfunction fftRadix2(e, t, n, r, a) {\n  if (1 === n) return {\n    real: e,\n    imag: t\n  };\n\n  var s = mergeRealAndImagArrays(e, t),\n      o = n / 2,\n      i = complexWithEvenIndex(s),\n      l = i.real,\n      u = i.imag,\n      c = [l.length],\n      p = a.makeTensorInfo(c, \"float32\", l),\n      d = a.makeTensorInfo(c, \"float32\", u),\n      h = complex$1({\n    inputs: {\n      real: p,\n      imag: d\n    },\n    backend: a\n  }),\n      m = complexWithOddIndex(s),\n      f = m.real,\n      g = m.imag,\n      $ = [f.length],\n      y = a.makeTensorInfo($, \"float32\", f),\n      b = a.makeTensorInfo($, \"float32\", g),\n      x = complex$1({\n    inputs: {\n      real: y,\n      imag: b\n    },\n    backend: a\n  }),\n      v = fftRadix2(l, u, o, r, a),\n      I = v.real,\n      C = v.imag,\n      S = [I.length],\n      k = a.makeTensorInfo(S, \"float32\", I),\n      T = a.makeTensorInfo(S, \"float32\", C),\n      N = complex$1({\n    inputs: {\n      real: k,\n      imag: T\n    },\n    backend: a\n  }),\n      w = fftRadix2(f, g, o, r, a),\n      E = w.real,\n      A = w.imag,\n      D = [E.length],\n      R = a.makeTensorInfo(D, \"float32\", E),\n      _ = a.makeTensorInfo(D, \"float32\", A),\n      F = complex$1({\n    inputs: {\n      real: R,\n      imag: _\n    },\n    backend: a\n  }),\n      P = exponents(n, r),\n      O = [P.real.length],\n      M = a.makeTensorInfo(O, \"float32\", P.real),\n      L = a.makeTensorInfo(O, \"float32\", P.imag),\n      z = complex$1({\n    inputs: {\n      real: M,\n      imag: L\n    },\n    backend: a\n  }),\n      B = multiply$1({\n    inputs: {\n      a: z,\n      b: F\n    },\n    backend: a\n  }),\n      V = add({\n    inputs: {\n      a: N,\n      b: B\n    },\n    backend: a\n  }),\n      G = sub$1({\n    inputs: {\n      a: N,\n      b: B\n    },\n    backend: a\n  }),\n      U = real$1({\n    inputs: {\n      input: V\n    },\n    backend: a\n  }),\n      W = real$1({\n    inputs: {\n      input: G\n    },\n    backend: a\n  }),\n      q = imag$1({\n    inputs: {\n      input: V\n    },\n    backend: a\n  }),\n      H = imag$1({\n    inputs: {\n      input: G\n    },\n    backend: a\n  }),\n      K = concat$1({\n    inputs: [U, W],\n    backend: a,\n    attrs: {\n      axis: 0\n    }\n  }),\n      j = concat$1({\n    inputs: [q, H],\n    backend: a,\n    attrs: {\n      axis: 0\n    }\n  }),\n      X = a.data.get(K.dataId).values,\n      Y = a.data.get(j.dataId).values;\n\n  return a.disposeIntermediateTensorInfo(p), a.disposeIntermediateTensorInfo(d), a.disposeIntermediateTensorInfo(h), a.disposeIntermediateTensorInfo(y), a.disposeIntermediateTensorInfo(b), a.disposeIntermediateTensorInfo(x), a.disposeIntermediateTensorInfo(k), a.disposeIntermediateTensorInfo(T), a.disposeIntermediateTensorInfo(N), a.disposeIntermediateTensorInfo(R), a.disposeIntermediateTensorInfo(_), a.disposeIntermediateTensorInfo(F), a.disposeIntermediateTensorInfo(M), a.disposeIntermediateTensorInfo(L), a.disposeIntermediateTensorInfo(z), a.disposeIntermediateTensorInfo(B), a.disposeIntermediateTensorInfo(V), a.disposeIntermediateTensorInfo(G), a.disposeIntermediateTensorInfo(U), a.disposeIntermediateTensorInfo(q), a.disposeIntermediateTensorInfo(W), a.disposeIntermediateTensorInfo(H), a.disposeIntermediateTensorInfo(K), a.disposeIntermediateTensorInfo(j), {\n    real: X,\n    imag: Y\n  };\n}\n\nfunction fourierTransformByMatmul(e, t, n) {\n  var r = new Float32Array(2 * t);\n\n  for (var a = 0; a < t; a++) {\n    var s = 0,\n        o = 0;\n\n    for (var _r384 = 0; _r384 < t; _r384++) {\n      var i = exponent(a * _r384, t, n),\n          l = getComplexWithIndex(e, _r384);\n      s += l.real * i.real - l.imag * i.imag, o += l.real * i.imag + l.imag * i.real;\n    }\n\n    n && (s /= t, o /= t), assignToTypedArray(r, s, o, a);\n  }\n\n  return r;\n}\n\nfunction fft$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = sizeFromShape(r.shape),\n      s = r.shape[r.shape.length - 1],\n      o = reshape$1({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [a / s, s]\n    }\n  }),\n      i = fftBatch(o, !1, n),\n      l = reshape$1({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      shape: r.shape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(i), l;\n}\n\nvar fftConfig$1 = {\n  kernelName: FFT,\n  backendName: \"cpu\",\n  kernelFunc: fft$1\n};\n\nfunction fill$1(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    shape: r,\n    value: a,\n    dtype: s\n  } = n,\n      o = s || inferDtype(a),\n      i = getArrayFromDType(o, sizeFromShape(r));\n  return fillValues(i, a, o), t.makeTensorInfo(r, o, i);\n}\n\nvar fillConfig$1 = {\n  kernelName: Fill,\n  backendName: \"cpu\",\n  kernelFunc: fill$1\n};\n\nfunction fillValues(e, t, n) {\n  e.fill(t);\n}\n\nvar flipLeftRightConfig$1 = {\n  kernelName: FlipLeftRight,\n  backendName: \"cpu\",\n  kernelFunc: _ref57 => {\n    var {\n      inputs: e,\n      backend: t\n    } = _ref57;\n    var {\n      image: n\n    } = e,\n        r = t,\n        a = getTypedArrayFromDType(n.dtype, sizeFromShape(n.shape)),\n        [s, o, i, l] = n.shape,\n        u = r.data.get(n.dataId).values;\n\n    for (var _e918 = 0; _e918 < s; _e918++) {\n      var _t635 = _e918 * i * o * l;\n\n      for (var _e919 = 0; _e919 < o; _e919++) {\n        var _n392 = _e919 * (i * l);\n\n        for (var _e920 = 0; _e920 < i; _e920++) {\n          var _r385 = _e920 * l;\n\n          for (var _s195 = 0; _s195 < l; _s195++) {\n            var _o139 = Math.round(i - _e920 - 1),\n                c = _t635 + _n392 + _r385 + _s195;\n\n            var _p32 = u[c];\n            _o139 >= 0 && _o139 < i && (_p32 = u[_t635 + _n392 + _o139 * l + _s195]), a[c] = _p32;\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: r.write(a, n.shape, n.dtype),\n      shape: n.shape,\n      dtype: n.dtype\n    };\n  }\n},\n    floorDivImpl = createSimpleBinaryKernelImpl((e, t) => Math.floor(e / t)),\n    floorDiv$1 = binaryKernelFunc$1(FloorDiv, floorDivImpl, null, \"int32\"),\n    floorDivConfig$1 = {\n  kernelName: FloorDiv,\n  backendName: \"cpu\",\n  kernelFunc: floorDiv$1\n};\n\nfunction fusedConv2D(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dataFormat: c,\n    dilations: p,\n    dimRoundingMode: d,\n    activation: h,\n    leakyreluAlpha: m\n  } = r;\n  var f = conv2D({\n    inputs: {\n      x: a,\n      filter: s\n    },\n    backend: n,\n    attrs: {\n      strides: l,\n      pad: u,\n      dataFormat: c,\n      dilations: p,\n      dimRoundingMode: d\n    }\n  });\n\n  if (o) {\n    var _e921 = f;\n    f = add({\n      inputs: {\n        a: f,\n        b: o\n      },\n      backend: n\n    }), n.disposeIntermediateTensorInfo(_e921);\n  }\n\n  if (h) {\n    var _e922 = f;\n    f = applyActivation(n, f, h, i, m), n.disposeIntermediateTensorInfo(_e922);\n  }\n\n  return f;\n}\n\nvar fusedConv2DConfig$1 = {\n  kernelName: FusedConv2D,\n  backendName: \"cpu\",\n  kernelFunc: fusedConv2D\n};\n\nfunction fusedDepthwiseConv2D$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dataFormat: c,\n    dilations: p,\n    dimRoundingMode: d,\n    activation: h,\n    leakyreluAlpha: m\n  } = r;\n  var f = depthwiseConv2dNative$1({\n    inputs: {\n      x: a,\n      filter: s\n    },\n    backend: n,\n    attrs: {\n      strides: l,\n      pad: u,\n      dataFormat: c,\n      dilations: p,\n      dimRoundingMode: d\n    }\n  });\n\n  if (o) {\n    var _e923 = f;\n    f = add({\n      inputs: {\n        a: f,\n        b: o\n      },\n      backend: n\n    }), n.disposeIntermediateTensorInfo(_e923);\n  }\n\n  if (h) {\n    var _e924 = f;\n    f = applyActivation(n, f, h, i, m), n.disposeIntermediateTensorInfo(_e924);\n  }\n\n  return f;\n}\n\nvar fusedDepthwiseConv2DConfig$1 = {\n  kernelName: FusedDepthwiseConv2D,\n  backendName: \"cpu\",\n  kernelFunc: fusedDepthwiseConv2D$1\n};\n\nfunction gatherNd$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    params: r,\n    indices: a\n  } = t,\n      s = sizeFromShape(r.shape),\n      o = a.shape,\n      i = o[o.length - 1],\n      [l, u, c, p] = prepareAndValidate(r, a);\n  if (0 === u) return n.makeTensorInfo(l, r.dtype, []);\n  var d = gatherNdImpl(n.data.get(a.dataId).values, n.bufferSync(r), r.dtype, u, i, c, p, r.shape, s);\n  return n.makeTensorInfo(l, r.dtype, d.values);\n}\n\nvar gatherNdConfig$1 = {\n  kernelName: GatherNd,\n  backendName: \"cpu\",\n  kernelFunc: gatherNd$1\n};\n\nfunction gatherV2$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    indices: s\n  } = t,\n      {\n    axis: o,\n    batchDims: i\n  } = r;\n  assertNotComplex$1([a, s], \"gatherV2\");\n  var l = i;\n  null == i && (l = 0);\n  var u = sizeFromShape(s.shape),\n      c = collectGatherOpShapeInfo(a, s, parseAxisParam(o, a.shape)[0], l),\n      p = reshape$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [c.batchSize, c.outerSize, c.dimSize, c.sliceSize]\n    }\n  }),\n      d = reshape$1({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: [c.batchSize, u / c.batchSize]\n    }\n  }),\n      h = [c.batchSize, c.outerSize, u / c.batchSize, c.sliceSize],\n      m = n.bufferSync(d),\n      f = gatherV2Impl(n.bufferSync(p), m, h);\n  return n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d), n.makeTensorInfo(c.outputShape, f.dtype, f.values);\n}\n\nvar gatherV2Config$1 = {\n  kernelName: GatherV2,\n  backendName: \"cpu\",\n  kernelFunc: gatherV2$1\n};\n\nfunction ifft$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t,\n      a = sizeFromShape(r.shape),\n      s = r.shape[r.shape.length - 1],\n      o = reshape$1({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [a / s, s]\n    }\n  }),\n      i = fftBatch(o, !0, n),\n      l = reshape$1({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      shape: r.shape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(i), l;\n}\n\nvar ifftConfig$1 = {\n  kernelName: IFFT,\n  backendName: \"cpu\",\n  kernelFunc: ifft$1\n},\n    isFinite$2 = unaryKernelFunc$1(IsFinite, e => Number.isFinite(e) ? 1 : 0, \"bool\"),\n    isFiniteConfig$1 = {\n  kernelName: IsFinite,\n  backendName: \"cpu\",\n  kernelFunc: isFinite$2\n},\n    isInf$1 = unaryKernelFunc$1(IsInf, e => Infinity === Math.abs(e) ? 1 : 0, \"bool\"),\n    isInfConfig$1 = {\n  kernelName: IsInf,\n  backendName: \"cpu\",\n  kernelFunc: isInf$1\n},\n    isNaN$2 = unaryKernelFunc$1(IsNan, e => Number.isNaN(e) ? 1 : 0, \"bool\"),\n    isNaNConfig$1 = {\n  kernelName: IsNan,\n  backendName: \"cpu\",\n  kernelFunc: isNaN$2\n};\n\nfunction linSpace$1(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    num: s\n  } = n,\n      o = linSpaceImpl(r, a, s);\n  return t.makeTensorInfo([o.length], \"float32\", o);\n}\n\nvar linSpaceConfig$1 = {\n  kernelName: LinSpace,\n  backendName: \"cpu\",\n  kernelFunc: linSpace$1\n},\n    log1p$1 = unaryKernelFunc$1(Log1p, e => Math.log1p(e)),\n    log1pConfig$1 = {\n  kernelName: Log1p,\n  backendName: \"cpu\",\n  kernelFunc: log1p$1\n},\n    logicalAndImpl = createSimpleBinaryKernelImpl((e, t) => e && t),\n    logicalAnd$1 = binaryKernelFunc$1(LogicalAnd, logicalAndImpl, null, \"bool\"),\n    logicalAndConfig$1 = {\n  kernelName: LogicalAnd,\n  backendName: \"cpu\",\n  kernelFunc: logicalAnd$1\n},\n    logicalNot$1 = unaryKernelFunc$1(LogicalNot, e => e ? 0 : 1, \"bool\"),\n    logicalNotConfig$1 = {\n  kernelName: LogicalNot,\n  backendName: \"cpu\",\n  kernelFunc: logicalNot$1\n},\n    logicalOrImpl = createSimpleBinaryKernelImpl((e, t) => e || t),\n    logicalOr$1 = binaryKernelFunc$1(LogicalOr, logicalOrImpl, null, \"bool\"),\n    logicalOrConfig$1 = {\n  kernelName: LogicalOr,\n  backendName: \"cpu\",\n  kernelFunc: logicalOr$1\n};\n\nfunction lRN(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    depthRadius: s,\n    bias: o,\n    alpha: i,\n    beta: l\n  } = r;\n  assertNotComplex$1(a, \"LRN\");\n  var u = a.shape[3],\n      c = u - 1,\n      p = n.data.get(a.dataId).values,\n      d = sizeFromShape(a.shape),\n      h = new Float32Array(d);\n\n  function m(e) {\n    var t = e % u;\n    var n = e - t + Math.max(0, t - s);\n    var r = e - t + Math.min(t + s, c);\n    var a = 0;\n\n    for (; n <= r; n++) {\n      var _e925 = p[n];\n      a += _e925 * _e925;\n    }\n\n    return a;\n  }\n\n  for (var _e926 = 0; _e926 < d; _e926++) {\n    var _t636 = m(_e926),\n        _n393 = p[_e926] * Math.pow(o + i * _t636, -l);\n\n    h[_e926] = _n393;\n  }\n\n  return n.makeTensorInfo(a.shape, a.dtype, h);\n}\n\nvar lRNConfig = {\n  kernelName: LRN,\n  backendName: \"cpu\",\n  kernelFunc: lRN\n};\n\nfunction lRNGrad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    y: s,\n    dy: o\n  } = t,\n      {\n    depthRadius: i,\n    bias: l,\n    alpha: u,\n    beta: c\n  } = r;\n  assertNotComplex$1(o, \"LRNGrad\");\n  var p = sizeFromShape(o.shape),\n      d = o.shape[3],\n      h = n.data.get(o.dataId).values,\n      m = n.data.get(a.dataId).values,\n      f = n.data.get(s.dataId).values,\n      g = new Float32Array(p),\n      $ = p;\n\n  for (var _e927 = 0; _e927 < $; _e927++) {\n    var _t637 = _e927 % d,\n        _n394 = _e927 - _t637 + Math.max(0, _t637 - i),\n        _r386 = _e927 - _t637 + Math.min(d, _t637 + i + 1);\n\n    var _a283 = 0;\n\n    for (var _e928 = _n394; _e928 < _r386; _e928++) {\n      _a283 += Math.pow(m[_e928], 2);\n    }\n\n    _a283 = u * _a283 + l;\n\n    for (var _t638 = _n394; _t638 < _r386; _t638++) {\n      var _n395 = -2 * u * c * m[_t638] * f[_e927] / _a283;\n\n      _e927 === _t638 && (_n395 += Math.pow(_a283, -c)), _n395 *= h[_e927], g[_t638] += _n395;\n    }\n  }\n\n  return n.makeTensorInfo(o.shape, a.dtype, g);\n}\n\nvar lRNGradConfig = {\n  kernelName: LRNGrad,\n  backendName: \"cpu\",\n  kernelFunc: lRNGrad\n};\n\nfunction max$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reductionIndices: s,\n    keepDims: o\n  } = r,\n      i = n;\n  var l = a.shape;\n  var u = l.length,\n      c = parseAxisParam(s, l);\n  var p = c;\n  var d = getAxesPermutation(p, u);\n  var h = i.data.get(a.dataId).values;\n\n  if (null != d) {\n    var _e929 = new Array(u);\n\n    for (var _t639 = 0; _t639 < _e929.length; _t639++) {\n      _e929[_t639] = l[d[_t639]];\n    }\n\n    h = transposeImpl$1(h, l, a.dtype, d, _e929), p = getInnerMostAxes(p.length, u), l = _e929;\n  }\n\n  assertNotComplex$1(a, \"max\"), assertAxesAreInnerMostDims(\"max\", p, u);\n  var [m, f] = computeOutAndReduceShapes(l, p),\n      g = maxImpl$1(h, sizeFromShape(f), m, a.dtype),\n      $ = i.write(g, m, a.dtype);\n  var y = m;\n  return o && (y = expandShapeToKeepDim(m, c)), {\n    dataId: $,\n    shape: y,\n    dtype: a.dtype\n  };\n}\n\nvar maxConfig$1 = {\n  kernelName: Max,\n  backendName: \"cpu\",\n  kernelFunc: max$1\n};\n\nfunction maxPool$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex$1(a, \"maxPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$4(eitherStridesOrDilationsAreOne(o, 1), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo(a.shape, s, o, 1, i, l);\n  var c;\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual(u.inShape, u.outShape)) c = identity$1({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });else {\n    var _e930 = n.data.get(a.dataId).values,\n        _t640 = computeStrides(a.shape),\n        _r387 = pool(_e930, a.shape, a.dtype, _t640, u, \"max\");\n\n    c = n.makeTensorInfo(u.outShape, a.dtype, _r387.values);\n  }\n  return c;\n}\n\nvar maxPoolConfig$1 = {\n  kernelName: MaxPool,\n  backendName: \"cpu\",\n  kernelFunc: maxPool$1\n};\n\nfunction maxPool3D(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l,\n    dataFormat: u\n  } = r;\n  assertNotComplex$1(a, \"maxPool3d\");\n  var c = computePool3DInfo(a.shape, s, o, 1, i, l, u),\n      p = pool3d(n.data.get(a.dataId).values, a.shape, a.dtype, computeStrides(a.shape), c, \"max\");\n  return n.makeTensorInfo(p.shape, \"float32\", p.values);\n}\n\nvar maxPool3DConfig$1 = {\n  kernelName: MaxPool3D,\n  backendName: \"cpu\",\n  kernelFunc: maxPool3D\n};\n\nfunction maxPool3DGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      {\n    filterSize: o,\n    strides: i,\n    pad: l,\n    dimRoundingMode: u\n  } = r;\n  assertNotComplex$1([a, s], \"maxPool3DGrad\");\n  var c = computePool3DInfo(s.shape, o, i, 1, l, u),\n      p = maxPool3dPositions(n.bufferSync(s), c),\n      d = c.strideDepth,\n      h = c.strideHeight,\n      m = c.strideWidth,\n      f = c.dilationDepth,\n      g = c.dilationHeight,\n      $ = c.dilationWidth,\n      y = c.effectiveFilterDepth,\n      b = c.effectiveFilterHeight,\n      x = c.effectiveFilterWidth,\n      v = y - 1 - c.padInfo.front,\n      I = x - 1 - c.padInfo.left,\n      C = b - 1 - c.padInfo.top,\n      S = buffer(s.shape, \"float32\"),\n      k = n.bufferSync(a);\n\n  for (var _e931 = 0; _e931 < c.batchSize; ++_e931) {\n    for (var _t641 = 0; _t641 < c.inChannels; ++_t641) {\n      for (var _n396 = 0; _n396 < c.inDepth; ++_n396) {\n        for (var _r388 = 0; _r388 < c.inHeight; ++_r388) {\n          for (var _a284 = 0; _a284 < c.inWidth; ++_a284) {\n            var _s196 = _n396 - v,\n                _o140 = _r388 - C,\n                _i76 = _a284 - I;\n\n            var _l60 = 0;\n\n            for (var _n397 = 0; _n397 < y; _n397 += f) {\n              var _r389 = (_s196 + _n397) / d;\n\n              if (!(_r389 < 0 || _r389 >= c.outDepth || Math.floor(_r389) !== _r389)) for (var _a285 = 0; _a285 < b; _a285 += g) {\n                var _s197 = (_o140 + _a285) / h;\n\n                if (!(_s197 < 0 || _s197 >= c.outHeight || Math.floor(_s197) !== _s197)) for (var _o141 = 0; _o141 < x; _o141 += $) {\n                  var _u50 = (_i76 + _o141) / m;\n\n                  if (_u50 < 0 || _u50 >= c.outWidth || Math.floor(_u50) !== _u50) continue;\n\n                  var _d19 = y * b * x - 1 - p.get(_e931, _r389, _s197, _u50, _t641) === _n397 * b * x + _a285 * x + _o141 ? 1 : 0;\n\n                  0 !== _d19 && (_l60 += k.get(_e931, _r389, _s197, _u50, _t641) * _d19);\n                }\n              }\n            }\n\n            S.set(_l60, _e931, _n396, _r388, _a284, _t641);\n          }\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(S.shape, S.dtype, S.values);\n}\n\nvar maxPool3DGradConfig = {\n  kernelName: MaxPool3DGrad,\n  backendName: \"cpu\",\n  kernelFunc: maxPool3DGrad$1\n};\n\nfunction maxPoolGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s,\n    output: o\n  } = t,\n      i = s;\n  assertNotComplex$1([s, o], \"maxPoolGrad\");\n  var {\n    filterSize: l,\n    strides: u,\n    pad: c,\n    dimRoundingMode: p\n  } = r,\n      d = computePool2DInfo(i.shape, l, u, 1, c, p),\n      h = n.data.get(i.dataId).values,\n      m = buffer(d.outShape, i.dtype, maxPoolPositions(h, i.shape, i.dtype, d).values),\n      f = d.strideHeight,\n      g = d.strideWidth,\n      $ = d.dilationHeight,\n      y = d.dilationWidth,\n      b = d.effectiveFilterHeight,\n      x = d.effectiveFilterWidth,\n      v = x - 1 - d.padInfo.left,\n      I = b - 1 - d.padInfo.top,\n      C = buffer(i.shape, \"float32\"),\n      S = n.data.get(a.dataId).values,\n      k = buffer(a.shape, \"float32\", S);\n\n  for (var _e932 = 0; _e932 < d.batchSize; ++_e932) {\n    for (var _t642 = 0; _t642 < d.inChannels; ++_t642) {\n      for (var _n398 = 0; _n398 < d.inHeight; ++_n398) {\n        for (var _r390 = 0; _r390 < d.inWidth; ++_r390) {\n          var _a286 = _n398 - I,\n              _s198 = _r390 - v;\n\n          var _o142 = 0;\n\n          for (var _n399 = 0; _n399 < b; _n399 += $) {\n            var _r391 = (_a286 + _n399) / f;\n\n            if (!(_r391 < 0 || _r391 >= d.outHeight || Math.floor(_r391) !== _r391)) for (var _a287 = 0; _a287 < x; _a287 += y) {\n              var _i77 = (_s198 + _a287) / g;\n\n              if (_i77 < 0 || _i77 >= d.outWidth || Math.floor(_i77) !== _i77) continue;\n\n              var _l61 = b * x - 1 - m.get(_e932, _r391, _i77, _t642) === _n399 * x + _a287 ? 1 : 0;\n\n              0 !== _l61 && (_o142 += k.get(_e932, _r391, _i77, _t642) * _l61);\n            }\n          }\n\n          C.set(_o142, _e932, _n398, _r390, _t642);\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(C.shape, C.dtype, C.values);\n}\n\nvar maxPoolGradConfig$1 = {\n  kernelName: MaxPoolGrad,\n  backendName: \"cpu\",\n  kernelFunc: maxPoolGrad$1\n};\n\nfunction maxPoolWithArgmaxImpl$1(e, t, n, r, a) {\n  var s = pool(e, t, n, computeStrides(t), a, \"max\"),\n      o = maxPoolPositions(e, t, n, a, !0, r);\n  return [s.values, o.values];\n}\n\nvar maxPoolWithArgmaxConfig$1 = {\n  kernelName: MaxPoolWithArgmax,\n  backendName: \"cpu\",\n  kernelFunc: _ref58 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref58;\n    var {\n      x: r\n    } = e,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o,\n      includeBatchInIndex: i\n    } = t,\n        l = n;\n    assertNotComplex$1(r, \"MaxPoolWithArgmax\");\n    var u = l.data.get(r.dataId).values,\n        c = computePool2DInfo(r.shape, a, s, [1, 1], o),\n        [p, d] = maxPoolWithArgmaxImpl$1(u, r.shape, r.dtype, i, c),\n        h = l.write(p, c.outShape, r.dtype),\n        m = l.write(d, c.outShape, r.dtype);\n    return [{\n      dataId: h,\n      shape: c.outShape,\n      dtype: r.dtype\n    }, {\n      dataId: m,\n      shape: c.outShape,\n      dtype: \"int32\"\n    }];\n  }\n};\n\nfunction mean(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = parseAxisParam(s, a.shape),\n      l = sizeFromShape(computeOutAndReduceShapes(a.shape, i)[1]),\n      u = [],\n      c = n.makeTensorInfo([], \"float32\", new Float32Array([l]));\n  u.push(c);\n  var p = cast$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      dtype: \"float32\"\n    }\n  });\n  u.push(p);\n  var d = div({\n    inputs: {\n      a: p,\n      b: c\n    },\n    backend: n\n  });\n  u.push(d);\n  var h = sum$1({\n    inputs: {\n      x: d\n    },\n    backend: n,\n    attrs: {\n      axis: s,\n      keepDims: o\n    }\n  });\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), h;\n}\n\nvar meanConfig$1 = {\n  kernelName: Mean,\n  backendName: \"cpu\",\n  kernelFunc: mean\n};\n\nfunction min$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  assertNotComplex$1(a, \"min\");\n  var i = parseAxisParam(s, a.shape);\n  var l = i;\n  var u = getAxesPermutation(l, a.shape.length);\n  var c = a;\n  null != u && (c = transpose$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }), l = getInnerMostAxes(l.length, a.shape.length)), assertAxesAreInnerMostDims(\"min\", l, c.shape.length);\n  var [p, d] = computeOutAndReduceShapes(c.shape, l),\n      h = sizeFromShape(d),\n      m = makeZerosTypedArray(sizeFromShape(p), c.dtype),\n      f = n.data.get(c.dataId).values;\n\n  for (var _e933 = 0; _e933 < m.length; ++_e933) {\n    var _t643 = _e933 * h;\n\n    var _n400 = f[_t643];\n\n    for (var _e934 = 0; _e934 < h; ++_e934) {\n      var _r392 = f[_t643 + _e934];\n      (Number.isNaN(_r392) || _r392 < _n400) && (_n400 = _r392);\n    }\n\n    m[_e933] = _n400;\n  }\n\n  null != u && n.disposeIntermediateTensorInfo(c);\n  var g = n.makeTensorInfo(p, c.dtype, m);\n\n  if (o) {\n    var _e935 = reshape$1({\n      inputs: {\n        x: g\n      },\n      backend: n,\n      attrs: {\n        shape: expandShapeToKeepDim(p, i)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(g), _e935;\n  }\n\n  return g;\n}\n\nvar minConfig$1 = {\n  kernelName: Min,\n  backendName: \"cpu\",\n  kernelFunc: min$1\n};\n\nfunction mirrorPad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    paddings: s,\n    mode: o\n  } = r;\n  assertNotComplex$1(a, \"mirrorPad\");\n  var i = s.map((e, t) => e[0] + a.shape[t] + e[1]),\n      l = s.map(e => e[0]),\n      u = s.map((e, t) => e[0] + a.shape[t]),\n      c = \"reflect\" === o ? 0 : 1,\n      p = n.data.get(a.dataId).values,\n      d = a.shape.length,\n      h = computeStrides(a.shape),\n      m = sizeFromShape(i),\n      f = i.length,\n      g = computeStrides(i),\n      $ = getTypedArrayFromDType(a.dtype, m);\n\n  for (var _e936 = 0; _e936 < m; _e936++) {\n    var _t644 = indexToLoc(_e936, f, g);\n\n    for (var _e937 = 0; _e937 < f; _e937++) {\n      _t644[_e937] < l[_e937] ? _t644[_e937] = 2 * l[_e937] - _t644[_e937] - c : _t644[_e937] >= u[_e937] && (_t644[_e937] = 2 * (u[_e937] - 1) - _t644[_e937] + c);\n    }\n\n    _t644 = _t644.map((e, t) => e - l[t]);\n\n    var _n401 = locToIndex(_t644, d, h);\n\n    $[_e936] = p[_n401];\n  }\n\n  return {\n    dataId: n.write($, i, a.dtype),\n    shape: i,\n    dtype: a.dtype\n  };\n}\n\nvar mirrorPadConfig$1 = {\n  kernelName: MirrorPad,\n  backendName: \"cpu\",\n  kernelFunc: mirrorPad\n},\n    modImpl = createSimpleBinaryKernelImpl((e, t) => {\n  var n = e % t;\n  return e < 0 && t < 0 || e >= 0 && t >= 0 ? n : (n + t) % t;\n}),\n    mod$1 = binaryKernelFunc$1(Mod, modImpl),\n    modConfig$1 = {\n  kernelName: Mod,\n  backendName: \"cpu\",\n  kernelFunc: mod$1\n};\n\nfunction softmax$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    dim: s\n  } = r,\n      o = a.shape.length;\n  var i = s;\n  if (-1 === i && (i = o - 1), i !== o - 1) throw Error(\"Softmax along a non-last dimension is not yet supported. Logits was rank \".concat(o, \" and dim was \").concat(i));\n  var l = parseAxisParam([i], a.shape),\n      u = max$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      reductionIndices: l,\n      keepDims: !1\n    }\n  }),\n      c = expandShapeToKeepDim(u.shape, l),\n      p = reshape$1({\n    inputs: {\n      x: u\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      d = sub$1({\n    inputs: {\n      a,\n      b: p\n    },\n    backend: n\n  }),\n      h = exp$1({\n    inputs: {\n      x: d\n    },\n    backend: n\n  }),\n      m = sum$1({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      axis: l,\n      keepDims: !1\n    }\n  }),\n      f = reshape$1({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      g = div({\n    inputs: {\n      a: h,\n      b: f\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(u), n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d), n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), g;\n}\n\nvar softmaxConfig$1 = {\n  kernelName: Softmax$2,\n  backendName: \"cpu\",\n  kernelFunc: softmax$1\n};\n\nfunction multinomial$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    numSamples: s,\n    seed: o,\n    normalized: i\n  } = r;\n  assertNotComplex$1(a, \"multinomial\");\n  var l = i ? a : softmax$1({\n    inputs: {\n      logits: a\n    },\n    backend: n,\n    attrs: {\n      dim: -1\n    }\n  }),\n      u = l.shape[0],\n      c = l.shape[1],\n      p = n.data.get(l.dataId).values,\n      d = [u, s],\n      h = makeZerosTypedArray(sizeFromShape(d), \"int32\");\n\n  for (var _e938 = 0; _e938 < u; ++_e938) {\n    var _t645 = _e938 * c,\n        _n402 = new Float32Array(c - 1);\n\n    _n402[0] = p[_t645];\n\n    for (var _e939 = 1; _e939 < _n402.length; ++_e939) {\n      _n402[_e939] = _n402[_e939 - 1] + p[_t645 + _e939];\n    }\n\n    var _r393 = seedrandom.alea(o.toString()),\n        _a288 = _e938 * s;\n\n    for (var _e940 = 0; _e940 < s; ++_e940) {\n      var _t646 = _r393();\n\n      h[_a288 + _e940] = _n402.length;\n\n      for (var _r394 = 0; _r394 < _n402.length; _r394++) {\n        if (_t646 < _n402[_r394]) {\n          h[_a288 + _e940] = _r394;\n          break;\n        }\n      }\n    }\n  }\n\n  return i || n.disposeIntermediateTensorInfo(l), n.makeTensorInfo(d, \"int32\", h);\n}\n\nvar multinomialConfig$1 = {\n  kernelName: Multinomial,\n  backendName: \"cpu\",\n  kernelFunc: multinomial$1\n},\n    nonMaxSuppressionV3Impl$1 = nonMaxSuppressionV3Impl$2;\n\nfunction nonMaxSuppressionV3$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l\n  } = r;\n  assertNotComplex$1(a, \"NonMaxSuppression\");\n  var u = n.data.get(a.dataId).values,\n      c = n.data.get(s.dataId).values,\n      {\n    selectedIndices: p\n  } = nonMaxSuppressionV3Impl$1(u, c, o, i, l);\n  return n.makeTensorInfo([p.length], \"int32\", new Int32Array(p));\n}\n\nvar nonMaxSuppressionV3Config$1 = {\n  kernelName: NonMaxSuppressionV3,\n  backendName: \"cpu\",\n  kernelFunc: nonMaxSuppressionV3$1\n},\n    nonMaxSuppressionV4Impl$1 = nonMaxSuppressionV4Impl$2;\n\nfunction nonMaxSuppressionV4$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    padToMaxOutputSize: u\n  } = r;\n  assertNotComplex$1(a, \"NonMaxSuppressionPadded\");\n  var c = n.data.get(a.dataId).values,\n      p = n.data.get(s.dataId).values,\n      {\n    selectedIndices: d,\n    validOutputs: h\n  } = nonMaxSuppressionV4Impl$1(c, p, o, i, l, u);\n  return [n.makeTensorInfo([d.length], \"int32\", new Int32Array(d)), n.makeTensorInfo([], \"int32\", new Int32Array([h]))];\n}\n\nvar nonMaxSuppressionV4Config$1 = {\n  kernelName: NonMaxSuppressionV4,\n  backendName: \"cpu\",\n  kernelFunc: nonMaxSuppressionV4$1\n},\n    nonMaxSuppressionV5Impl$1 = nonMaxSuppressionV5Impl$2;\n\nfunction nonMaxSuppressionV5$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    softNmsSigma: u\n  } = r;\n  assertNotComplex$1(a, \"NonMaxSuppressionWithScore\");\n  var c = n.data.get(a.dataId).values,\n      p = n.data.get(s.dataId).values,\n      d = o,\n      h = i,\n      m = l,\n      f = u,\n      {\n    selectedIndices: g,\n    selectedScores: $\n  } = nonMaxSuppressionV5Impl$1(c, p, d, h, m, f);\n  return [n.makeTensorInfo([g.length], \"int32\", new Int32Array(g)), n.makeTensorInfo([$.length], \"float32\", new Float32Array($))];\n}\n\nvar nonMaxSuppressionV5Config$1 = {\n  kernelName: NonMaxSuppressionV5,\n  backendName: \"cpu\",\n  kernelFunc: nonMaxSuppressionV5$1\n};\n\nfunction oneHot$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a\n  } = t,\n      {\n    depth: s,\n    onValue: o,\n    offValue: i\n  } = r;\n  assertNotComplex$1(a, \"oneHot\");\n  var l = sizeFromShape(a.shape),\n      u = new Float32Array(l * s);\n  u.fill(i);\n  var c = n.data.get(a.dataId).values;\n\n  for (var _e941 = 0; _e941 < l; ++_e941) {\n    c[_e941] >= 0 && c[_e941] < s && (u[_e941 * s + c[_e941]] = o);\n  }\n\n  return n.makeTensorInfo([...a.shape, s], \"int32\", u);\n}\n\nvar oneHotConfig$1 = {\n  kernelName: OneHot,\n  backendName: \"cpu\",\n  kernelFunc: oneHot$1\n};\n\nfunction zerosLike$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  if (\"string\" === r.dtype) throw new Error(\"zerosLike is not supported for string tensors\");\n\n  if (\"complex64\" === r.dtype) {\n    var _e942 = real$1({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t647 = zerosLike$1({\n      inputs: {\n        x: _e942\n      },\n      backend: n\n    }),\n        a = imag$1({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike$1({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex$1({\n      inputs: {\n        real: _t647,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e942), n.disposeIntermediateTensorInfo(_t647), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill$1({\n    backend: n,\n    attrs: {\n      shape: r.shape,\n      value: 0,\n      dtype: r.dtype\n    }\n  });\n}\n\nvar zerosLikeConfig$1 = {\n  kernelName: ZerosLike,\n  backendName: \"cpu\",\n  kernelFunc: zerosLike$1\n};\n\nfunction onesLike$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  if (\"string\" === r.dtype) throw new Error(\"onesLike is not supported for string tensors\");\n\n  if (\"complex64\" === r.dtype) {\n    var _e943 = real$1({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t648 = onesLike$1({\n      inputs: {\n        x: _e943\n      },\n      backend: n\n    }),\n        a = imag$1({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike$1({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex$1({\n      inputs: {\n        real: _t648,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e943), n.disposeIntermediateTensorInfo(_t648), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill$1({\n    backend: n,\n    attrs: {\n      shape: r.shape,\n      value: 1,\n      dtype: r.dtype\n    }\n  });\n}\n\nvar onesLikeConfig$1 = {\n  kernelName: OnesLike,\n  backendName: \"cpu\",\n  kernelFunc: onesLike$1\n};\n\nfunction pack$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r;\n  if (1 === t.length) return expandDims$1({\n    inputs: {\n      input: t[0]\n    },\n    backend: n,\n    attrs: {\n      dim: a\n    }\n  });\n  var s = t[0].shape,\n      o = t[0].dtype;\n  t.forEach(e => {\n    assertShapesMatch(s, e.shape, \"All tensors passed to stack must have matching shapes\"), assert$4(o === e.dtype, () => \"All tensors passed to stack must have matching dtypes\");\n  });\n  var i = [],\n      l = concat$1({\n    inputs: t.map(e => {\n      var t = expandDims$1({\n        inputs: {\n          input: e\n        },\n        backend: n,\n        attrs: {\n          dim: a\n        }\n      });\n      return i.push(t), t;\n    }),\n    backend: n,\n    attrs: {\n      axis: a\n    }\n  });\n  return i.forEach(e => n.disposeIntermediateTensorInfo(e)), l;\n}\n\nvar packConfig$1 = {\n  kernelName: Pack,\n  backendName: \"cpu\",\n  kernelFunc: pack$1\n};\n\nfunction padV2$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    paddings: s,\n    constantValue: o\n  } = r;\n  assertNotComplex$1(a, \"pad\");\n  var i = s.map((e, t) => e[0] + a.shape[t] + e[1]),\n      l = s.map(e => e[0]),\n      u = n.data.get(a.dataId).values,\n      c = sizeFromShape(a.shape),\n      p = a.shape.length,\n      d = computeStrides(a.shape),\n      h = sizeFromShape(i),\n      m = i.length,\n      f = computeStrides(i),\n      g = getTypedArrayFromDType(a.dtype, h);\n  0 !== o && g.fill(o);\n\n  for (var _e944 = 0; _e944 < c; _e944++) {\n    g[locToIndex(indexToLoc(_e944, p, d).map((e, t) => e + l[t]), m, f)] = u[_e944];\n  }\n\n  return {\n    dataId: n.write(g, i, a.dtype),\n    shape: i,\n    dtype: a.dtype\n  };\n}\n\nvar padV2Config$1 = {\n  kernelName: PadV2,\n  backendName: \"cpu\",\n  kernelFunc: padV2$1\n},\n    powImpl = createSimpleBinaryKernelImpl((e, t) => Math.pow(e, t)),\n    pow$1 = binaryKernelFunc$1(Pow, powImpl),\n    powConfig$1 = {\n  kernelName: Pow,\n  backendName: \"cpu\",\n  kernelFunc: pow$1\n};\n\nfunction range$2(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    dtype: s,\n    step: o\n  } = n,\n      i = rangeImpl(r, a, o, s);\n  return t.makeTensorInfo([i.length], s, i);\n}\n\nvar rangeConfig$1 = {\n  kernelName: Range,\n  backendName: \"cpu\",\n  kernelFunc: range$2\n},\n    reciprocal$1 = unaryKernelFunc$1(Reciprocal, e => 1 / e),\n    reciprocalConfig$1 = {\n  kernelName: Reciprocal,\n  backendName: \"cpu\",\n  kernelFunc: reciprocal$1\n};\n\nfunction resizeBilinear$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r;\n  assertNotComplex$1(a, \"resizeBilinear\");\n  var l = computeStrides(a.shape),\n      [u, c] = i,\n      [p, d, h, m] = a.shape,\n      f = n.data.get(a.dataId).values,\n      g = new Float32Array(sizeFromShape([p, u, c, m])),\n      $ = [s && u > 1 ? d - 1 : d, s && c > 1 ? h - 1 : h],\n      y = [s && u > 1 ? u - 1 : u, s && c > 1 ? c - 1 : c];\n  var b = 0;\n  var x = $[0] / y[0],\n      v = $[1] / y[1];\n\n  for (var _e945 = 0; _e945 < p; _e945++) {\n    for (var _t649 = 0; _t649 < u; _t649++) {\n      var _n403 = void 0;\n\n      _n403 = o ? x * (_t649 + .5) - .5 : x * _t649;\n\n      var _r395 = Math.max(0, Math.floor(_n403)),\n          _a289 = _n403 - _r395,\n          _s199 = Math.min(d - 1, Math.ceil(_n403)),\n          _i78 = _e945 * l[0] + _r395 * l[1],\n          _u51 = _e945 * l[0] + _s199 * l[1];\n\n      for (var _e946 = 0; _e946 < c; _e946++) {\n        var _t650 = void 0;\n\n        _t650 = o ? v * (_e946 + .5) - .5 : v * _e946;\n\n        var _n404 = Math.max(0, Math.floor(_t650)),\n            _r396 = _t650 - _n404,\n            _s200 = Math.min(h - 1, Math.ceil(_t650)),\n            _c36 = _i78 + _n404 * l[2],\n            _p33 = _u51 + _n404 * l[2],\n            _d20 = _i78 + _s200 * l[2],\n            _$10 = _u51 + _s200 * l[2];\n\n        for (var _e947 = 0; _e947 < m; _e947++) {\n          var _t651 = f[_c36 + _e947],\n              _n405 = f[_p33 + _e947],\n              _s201 = _t651 + (f[_d20 + _e947] - _t651) * _r396;\n\n          g[b++] = _s201 + (_n405 + (f[_$10 + _e947] - _n405) * _r396 - _s201) * _a289;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([p, u, c, m], \"float32\", g);\n}\n\nvar resizeBilinearConfig$1 = {\n  kernelName: ResizeBilinear,\n  backendName: \"cpu\",\n  kernelFunc: resizeBilinear$1\n};\n\nfunction resizeBilinearGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r;\n  assertNotComplex$1([s, a], \"resizeBilinearGrad\");\n  var i = computeStrides(a.shape),\n      [l, u, c, p] = a.shape,\n      [, d, h] = s.shape,\n      m = new Float32Array(l * u * c * p),\n      f = [o && d > 1 ? u - 1 : u, o && h > 1 ? c - 1 : c],\n      g = [o && d > 1 ? d - 1 : d, o && h > 1 ? h - 1 : h],\n      $ = f[0] / g[0],\n      y = f[1] / g[1],\n      b = n.data.get(s.dataId).values;\n  var x = 0;\n\n  for (var _e948 = 0; _e948 < l; _e948++) {\n    var _t652 = _e948 * i[0];\n\n    for (var _e949 = 0; _e949 < d; _e949++) {\n      var _n406 = _e949 * $,\n          _r397 = Math.floor(_n406),\n          _a290 = Math.min(Math.ceil(_n406), u - 1),\n          _s202 = _t652 + _r397 * i[1],\n          _o143 = _t652 + _a290 * i[1],\n          _l62 = _n406 - _r397,\n          _d21 = 1 - _l62;\n\n      for (var _e950 = 0; _e950 < h; _e950++) {\n        var _t653 = _e950 * y,\n            _n407 = Math.floor(_t653),\n            _r398 = Math.min(Math.ceil(_t653), c - 1),\n            _a291 = _t653 - _n407,\n            _u52 = 1 - _a291,\n            _h21 = _s202 + _n407 * i[2],\n            _f17 = _s202 + _r398 * i[2],\n            _g6 = _o143 + _n407 * i[2],\n            _$11 = _o143 + _r398 * i[2],\n            v = _d21 * _u52,\n            I = _d21 * _a291,\n            C = _l62 * _u52,\n            S = _l62 * _a291;\n\n        for (var _e951 = 0; _e951 < p; _e951++) {\n          var _t654 = b[x++];\n          m[_h21 + _e951] += _t654 * v, m[_f17 + _e951] += _t654 * I, m[_g6 + _e951] += _t654 * C, m[_$11 + _e951] += _t654 * S;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([l, c, u, p], \"float32\", m);\n}\n\nvar resizeBilinearGradConfig$1 = {\n  kernelName: ResizeBilinearGrad,\n  backendName: \"cpu\",\n  kernelFunc: resizeBilinearGrad$1\n};\n\nfunction resizeNearestNeighbor$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r;\n  assertNotComplex$1(a, \"resizeNearestNeighbor\");\n  var l = computeStrides(a.shape),\n      [u, c] = i,\n      [p, d, h, m] = a.shape,\n      f = n.data.get(a.dataId).values,\n      g = new Float32Array(p * u * c * m),\n      $ = [s && u > 1 ? d - 1 : d, s && c > 1 ? h - 1 : h],\n      y = [s && u > 1 ? u - 1 : u, s && c > 1 ? c - 1 : c],\n      b = $[0] / y[0],\n      x = $[1] / y[1];\n  var v = 0;\n\n  for (var _e952 = 0; _e952 < p; _e952++) {\n    var _t655 = _e952 * l[0];\n\n    for (var _e953 = 0; _e953 < u; _e953++) {\n      var _n408 = o ? b * (_e953 + .5) : b * _e953;\n\n      var _r399 = Math.min(d - 1, s ? Math.round(_n408) : Math.floor(_n408));\n\n      o && (_r399 = Math.max(0, _r399));\n\n      var _a292 = _t655 + _r399 * l[1];\n\n      for (var _e954 = 0; _e954 < c; _e954++) {\n        var _t656 = o ? x * (_e954 + .5) : x * _e954;\n\n        var _n409 = Math.min(h - 1, s ? Math.round(_t656) : Math.floor(_t656));\n\n        o && (_n409 = Math.max(0, _n409));\n\n        var _r400 = _a292 + _n409 * l[2];\n\n        for (var _e955 = 0; _e955 < m; _e955++) {\n          g[v++] = f[_r400 + _e955];\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo([p, u, c, m], a.dtype, g);\n}\n\nvar resizeNearestNeighborConfig$1 = {\n  kernelName: ResizeNearestNeighbor,\n  backendName: \"cpu\",\n  kernelFunc: resizeNearestNeighbor$1\n};\n\nfunction resizeNearestNeighborGrad$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r;\n  assertNotComplex$1([s, a], \"resizeNearestNeighborGrad\");\n  var i = computeStrides(a.shape),\n      l = computeStrides(s.shape),\n      [u, c, p, d] = a.shape,\n      [, h, m] = s.shape,\n      f = new Float32Array(u * c * p * d),\n      g = n.data.get(s.dataId).values,\n      $ = [o && h > 1 ? c - 1 : c, o && m > 1 ? p - 1 : p],\n      y = [o && h > 1 ? h - 1 : h, o && m > 1 ? m - 1 : m],\n      b = $[0] / y[0],\n      x = $[1] / y[1],\n      v = 1 / b,\n      I = 1 / x,\n      C = 2 * Math.ceil(v) + 2,\n      S = 2 * Math.ceil(I) + 2;\n\n  for (var _e956 = 0; _e956 < u; _e956++) {\n    var _t657 = _e956 * i[0];\n\n    for (var _e957 = 0; _e957 < c; _e957++) {\n      var _n410 = _t657 + _e957 * i[1],\n          _r401 = Math.floor(_e957 * v),\n          _a293 = Math.floor(_r401 - C / 2);\n\n      for (var _r402 = 0; _r402 < p; _r402++) {\n        var _s203 = _n410 + _r402 * i[2],\n            _u53 = Math.floor(_r402 * I),\n            _$12 = Math.floor(_u53 - S / 2);\n\n        for (var _n411 = 0; _n411 < d; _n411++) {\n          var _i79 = 0;\n\n          for (var _s204 = 0; _s204 < C; _s204++) {\n            var _u54 = _s204 + _a293;\n\n            if (_u54 < 0 || _u54 >= h) continue;\n\n            var _d22 = _t657 + _u54 * l[1],\n                _f18 = _u54 * b;\n\n            if (_e957 === Math.min(c - 1, o ? Math.round(_f18) : Math.floor(_f18))) for (var _e958 = 0; _e958 < S; _e958++) {\n              var _t658 = _e958 + _$12;\n\n              if (_t658 < 0 || _t658 >= m) continue;\n\n              var _a294 = _d22 + _t658 * l[2],\n                  _s205 = _t658 * x;\n\n              _r402 === Math.min(p - 1, o ? Math.round(_s205) : Math.floor(_s205)) && (_i79 += g[_a294 + _n411]);\n            }\n          }\n\n          f[_s203 + _n411] = _i79;\n        }\n      }\n    }\n  }\n\n  return n.makeTensorInfo(a.shape, a.dtype, f);\n}\n\nvar resizeNearestNeighborGradConfig$1 = {\n  kernelName: ResizeNearestNeighborGrad,\n  backendName: \"cpu\",\n  kernelFunc: resizeNearestNeighborGrad$1\n};\n\nfunction reverse$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dims: s\n  } = r;\n  assertNotComplex$1(a, \"reverse\");\n  var o = a.shape.length,\n      i = parseAxisParam(s, a.shape);\n  if (0 === o) return identity$1({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var l = new TensorBuffer(a.shape, a.dtype),\n      u = n.bufferSync(a);\n\n  var _loop57 = function _loop57(_e959) {\n    var t = l.indexToLoc(_e959),\n        n = t.slice();\n    i.forEach(e => n[e] = a.shape[e] - 1 - n[e]), l.set(u.get(...n), ...t);\n  };\n\n  for (var _e959 = 0; _e959 < l.size; _e959++) {\n    _loop57(_e959);\n  }\n\n  return n.makeTensorInfo(l.shape, l.dtype, l.values);\n}\n\nvar reverseConfig$1 = {\n  kernelName: Reverse,\n  backendName: \"cpu\",\n  kernelFunc: reverse$1\n},\n    rotateWithOffsetConfig$1 = {\n  kernelName: RotateWithOffset,\n  backendName: \"cpu\",\n  kernelFunc: _ref59 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref59;\n    var {\n      image: r\n    } = e,\n        {\n      radians: a,\n      fillValue: s,\n      center: o\n    } = t,\n        i = n,\n        l = getTypedArrayFromDType(r.dtype, sizeFromShape(r.shape)),\n        [u, c, p, d] = r.shape,\n        [h, m] = getImageCenter(o, c, p),\n        f = Math.sin(a),\n        g = Math.cos(a),\n        $ = i.data.get(r.dataId).values;\n\n    for (var _e960 = 0; _e960 < u; _e960++) {\n      var _t659 = _e960 * p * c * d;\n\n      for (var _e961 = 0; _e961 < c; _e961++) {\n        var _n412 = _e961 * (p * d);\n\n        for (var _r403 = 0; _r403 < p; _r403++) {\n          var _a295 = _r403 * d;\n\n          for (var _o144 = 0; _o144 < d; _o144++) {\n            var _i80 = [u, _e961, _r403, _o144],\n                y = _i80[2],\n                b = _i80[1];\n            var x = (y - h) * g - (b - m) * f,\n                v = (y - h) * f + (b - m) * g;\n            x = Math.round(x + h), v = Math.round(v + m);\n            var I = s;\n            \"number\" != typeof s && (I = 3 === _o144 ? 255 : s[_o144]), x >= 0 && x < p && v >= 0 && v < c && (I = $[_t659 + v * (p * d) + x * d + _o144]), l[_t659 + _n412 + _a295 + _o144] = I;\n          }\n        }\n      }\n    }\n\n    return {\n      dataId: i.write(l, r.shape, r.dtype),\n      shape: r.shape,\n      dtype: r.dtype\n    };\n  }\n},\n    round$1 = unaryKernelFunc$1(Round, e => {\n  var t = Math.floor(e);\n  return e - t < .5 ? Math.floor(e) : e - t > .5 ? Math.ceil(e) : t % 2 == 0 ? t : t + 1;\n}),\n    roundConfig$1 = {\n  kernelName: Round,\n  backendName: \"cpu\",\n  kernelFunc: round$1\n};\n\nfunction scatterImpl(e, t, n, r, a, s, o, i, l, u) {\n  var c = [r / a, a],\n      p = e.values,\n      d = t.values;\n  if (0 === r) return buffer(n, t.dtype);\n  var h = buffer(c, t.dtype);\n  h.values.fill(l);\n\n  for (var _e962 = 0; _e962 < s; _e962++) {\n    var _s206 = [];\n    var _l63 = 0;\n\n    for (var _t660 = 0; _t660 < o; _t660++) {\n      var _n413 = p[_e962 * o + _t660];\n      _s206.push(_n413), _l63 += _n413 * i[_t660];\n    }\n\n    if (_l63 < 0 || _l63 >= r / a) throw new Error(\"Invalid indices: \".concat(_s206, \" does not index into \").concat(n));\n\n    for (var _n414 = 0; _n414 < a; _n414++) {\n      u ? h.values[_l63 * a + _n414] += d[_e962 * a + _n414] : h.values[_l63 * a + _n414] = 0 === t.rank ? d[0] : d[_e962 * a + _n414];\n    }\n  }\n\n  return h;\n}\n\nfunction scatterNd$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a,\n    updates: s\n  } = t,\n      {\n    shape: o\n  } = r,\n      {\n    sliceRank: i,\n    numUpdates: l,\n    sliceSize: u,\n    strides: c,\n    outputSize: p\n  } = calculateShapes(s, a, o),\n      d = scatterImpl(n.bufferSync(a), n.bufferSync(s), o, p, u, l, i, c, 0, !0);\n  return n.makeTensorInfo(o, d.dtype, d.values);\n}\n\nvar scatterNdConfig$1 = {\n  kernelName: ScatterNd,\n  backendName: \"cpu\",\n  kernelFunc: scatterNd$1\n};\n\nfunction select$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    condition: r,\n    t: a,\n    e: s\n  } = t;\n  assertNotComplex$1([r, a, s], \"select\");\n  var o = r.shape.length,\n      i = n.data.get(r.dataId).values,\n      l = n.data.get(a.dataId).values,\n      u = n.data.get(s.dataId).values,\n      c = upcastType(a.dtype, s.dtype),\n      p = makeZerosTypedArray(sizeFromShape(a.shape), c);\n  var d = 0;\n  var h = 0 === o || o > 1 || 1 === a.shape.length ? 1 : sizeFromShape(a.shape.slice(1));\n\n  for (var _e963 = 0; _e963 < i.length; _e963++) {\n    for (var _t661 = 0; _t661 < h; _t661++) {\n      p[d++] = 1 === i[_e963] ? l[_e963] : u[_e963];\n    }\n  }\n\n  return n.makeTensorInfo(a.shape, c, p);\n}\n\nvar selectConfig$1 = {\n  kernelName: Select,\n  backendName: \"cpu\",\n  kernelFunc: select$1\n},\n    scaleAlpha = SELU_SCALEALPHA,\n    scale = SELU_SCALE,\n    selu$1 = unaryKernelFunc$1(Selu$1, e => e >= 0 ? scale * e : scaleAlpha * (Math.exp(e) - 1)),\n    seluConfig$1 = {\n  kernelName: Selu$1,\n  backendName: \"cpu\",\n  kernelFunc: selu$1\n},\n    sign$1 = unaryKernelFunc$1(Sign, e => e < 0 ? -1 : e > 0 ? 1 : 0),\n    signConfig$1 = {\n  kernelName: Sign,\n  backendName: \"cpu\",\n  kernelFunc: sign$1\n},\n    sin$1 = unaryKernelFunc$1(Sin, e => Math.sin(e)),\n    sinConfig$1 = {\n  kernelName: Sin,\n  backendName: \"cpu\",\n  kernelFunc: sin$1\n},\n    sinh$1 = unaryKernelFunc$1(Sinh, e => Math.sinh(e)),\n    sinhConfig$1 = {\n  kernelName: Sinh,\n  backendName: \"cpu\",\n  kernelFunc: sinh$1\n},\n    epsilon = 1.1920928955078125e-7,\n    threshold = Math.log(epsilon) + 2,\n    softplus$1 = unaryKernelFunc$1(Softplus$1, e => {\n  var t = e > -threshold,\n      n = e < threshold,\n      r = Math.exp(e);\n  var a;\n  return a = n ? r : t ? e : Math.log(1 + r), a;\n}),\n    softplusConfig$1 = {\n  kernelName: Softplus$1,\n  backendName: \"cpu\",\n  kernelFunc: softplus$1\n};\n\nfunction spaceToBatchND$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    paddings: o\n  } = r;\n  assertNotComplex$1([a], \"spaceToBatchND\");\n  var i = sizeFromShape(s),\n      l = [[0, 0]];\n  l.push(...o);\n\n  for (var _e964 = 1 + s.length; _e964 < a.shape.length; ++_e964) {\n    l.push([0, 0]);\n  }\n\n  var u = padV2Config$1.kernelFunc({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      paddings: l,\n      constantValue: 0\n    }\n  }),\n      c = getReshaped(u.shape, s, i, !1),\n      p = getPermuted(c.length, s.length, !1),\n      d = getReshapedPermuted(u.shape, s, i, !1),\n      h = reshape$1({\n    inputs: {\n      x: u\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      m = transpose$1({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      perm: p\n    }\n  }),\n      f = reshape$1({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  });\n  return n.disposeIntermediateTensorInfo(u), n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), f;\n}\n\nvar spaceToBatchNDConfig$1 = {\n  kernelName: SpaceToBatchND,\n  backendName: \"cpu\",\n  kernelFunc: spaceToBatchND$1\n};\n\nfunction sparseFillEmptyRows$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    indices: r,\n    values: a,\n    denseShape: s,\n    defaultValue: o\n  } = t;\n  if (1 !== s.shape.length) throw new Error(\"Dense shape must be a vector, saw:\\n        \".concat(s.shape));\n  if (2 !== r.shape.length) throw new Error(\"Indices must be a matrix, saw:\\n        \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Values must be a vector, saw:\\n        \".concat(a.shape));\n  if (0 !== o.shape.length) throw new Error(\"Default value must be a scalar, saw:\\n        \".concat(o.shape));\n  var i = n.data.get(r.dataId).values,\n      l = n.data.get(a.dataId).values,\n      u = n.data.get(s.dataId).values,\n      c = n.data.get(o.dataId).values[0],\n      [p, d, h, m, f] = sparseFillEmptyRowsImpl(i, r.shape, r.dtype, l, a.dtype, u, c);\n  return [n.makeTensorInfo(d, r.dtype, p), n.makeTensorInfo([d[0]], a.dtype, h), n.makeTensorInfo([m.length], \"bool\", new Uint8Array(m.map(e => Number(e)))), n.makeTensorInfo([f.length], r.dtype, new Int32Array(f))];\n}\n\nvar sparseFillEmptyRowsConfig$1 = {\n  kernelName: SparseFillEmptyRows,\n  backendName: \"cpu\",\n  kernelFunc: sparseFillEmptyRows$1\n};\n\nfunction sparseReshape$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    inputIndices: r,\n    inputShape: a,\n    newShape: s\n  } = t;\n  if (2 !== r.shape.length) throw new Error(\"Input indices should be a matrix but received shape\\n        \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Input shape should be a vector but received shape\\n        \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Target shape should be a vector but received shape \".concat(s.shape));\n  var o = Array.from(n.data.get(a.dataId).values),\n      i = n.data.get(r.dataId).values,\n      l = Array.from(n.data.get(s.dataId).values),\n      [u, c, p] = sparseReshapeImpl(i, r.shape, r.dtype, o, l);\n  return [n.makeTensorInfo(c, r.dtype, u), n.makeTensorInfo([p.length], s.dtype, new Int32Array(p))];\n}\n\nvar sparseReshapeConfig$1 = {\n  kernelName: SparseReshape,\n  backendName: \"cpu\",\n  kernelFunc: sparseReshape$1\n};\n\nfunction sparseSegmentMean$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n          \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n          \".concat(s.shape));\n  var o = n.data.get(r.dataId).values,\n      i = n.data.get(a.dataId).values,\n      l = n.data.get(s.dataId).values,\n      [u, c] = sparseSegmentReductionImpl(o, r.shape, r.dtype, i, l, !0);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentMeanConfig$1 = {\n  kernelName: SparseSegmentMean,\n  backendName: \"cpu\",\n  kernelFunc: sparseSegmentMean$1\n};\n\nfunction sparseSegmentSum$1(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n         \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n         \".concat(s.shape));\n  var o = n.data.get(r.dataId).values,\n      i = n.data.get(a.dataId).values,\n      l = n.data.get(s.dataId).values,\n      [u, c] = sparseSegmentReductionImpl(o, r.shape, r.dtype, i, l);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentSumConfig$1 = {\n  kernelName: SparseSegmentSum,\n  backendName: \"cpu\",\n  kernelFunc: sparseSegmentSum$1\n};\n\nfunction sparseToDense$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    sparseIndices: a,\n    sparseValues: s,\n    defaultValue: o\n  } = t,\n      {\n    outputShape: i\n  } = r,\n      {\n    sliceRank: l,\n    numUpdates: u,\n    sliceSize: c,\n    strides: p,\n    outputSize: d\n  } = calculateShapes(s, a, i),\n      h = scatterImpl(n.bufferSync(a), n.bufferSync(s), i, d, c, u, l, p, n.data.get(o.dataId).values[0], !1);\n  return n.makeTensorInfo(i, h.dtype, h.values);\n}\n\nvar sparseToDenseConfig$1 = {\n  kernelName: SparseToDense,\n  backendName: \"cpu\",\n  kernelFunc: sparseToDense$1\n};\n\nfunction splitV$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    numOrSizeSplits: s,\n    axis: o\n  } = r,\n      i = parseAxisParam(o, a.shape)[0],\n      l = prepareSplitSize(a, s, i),\n      u = new Array(a.shape.length).fill(0),\n      c = a.shape.slice();\n  return l.map(e => {\n    var t = [...c];\n    t[i] = e;\n    var r = slice$1({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        begin: u,\n        size: t\n      }\n    });\n    return u[i] += e, r;\n  });\n}\n\nvar splitVConfig$1 = {\n  kernelName: SplitV,\n  backendName: \"cpu\",\n  kernelFunc: splitV$1\n},\n    sqrt$1 = unaryKernelFunc$1(Sqrt, e => Math.sqrt(e)),\n    sqrtConfig$1 = {\n  kernelName: Sqrt,\n  backendName: \"cpu\",\n  kernelFunc: sqrt$1\n},\n    squareConfig$1 = {\n  kernelName: Square,\n  backendName: \"cpu\",\n  kernelFunc: _ref60 => {\n    var {\n      inputs: e,\n      backend: t\n    } = _ref60;\n    var {\n      x: n\n    } = e,\n        r = t;\n    assertNotComplex$1(n, \"square\");\n    var a = r.data.get(n.dataId).values,\n        s = new Float32Array(a.length);\n\n    for (var _e965 = 0; _e965 < a.length; ++_e965) {\n      var _t662 = a[_e965];\n      s[_e965] = _t662 * _t662;\n    }\n\n    return {\n      dataId: r.write(s, n.shape, n.dtype),\n      shape: n.shape,\n      dtype: n.dtype\n    };\n  }\n},\n    step$1 = unaryKernelFunc$1(Step, (e, t) => {\n  var n = t;\n  return isNaN(e) ? NaN : e > 0 ? 1 : n.alpha;\n}),\n    stepConfig$1 = {\n  kernelName: Step,\n  backendName: \"cpu\",\n  kernelFunc: step$1\n};\n\nfunction stridedSlice$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    end: o,\n    strides: i,\n    beginMask: l,\n    endMask: u,\n    ellipsisMask: c,\n    newAxisMask: p,\n    shrinkAxisMask: d\n  } = r;\n  assertNotComplex$1(a, \"stridedSlice\");\n  var {\n    nonStrided: h,\n    $begin: m,\n    $strides: f,\n    size: g,\n    newShape: $,\n    outShape: y\n  } = sliceInfo(a.shape, s, o, i, l, u, c, p, d),\n      b = reshape$1({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: $\n    }\n  });\n  var x;\n\n  if (h) {\n    var _e966 = slice$1({\n      inputs: {\n        x: b\n      },\n      backend: n,\n      attrs: {\n        begin: m,\n        size: g\n      }\n    });\n\n    x = reshape$1({\n      inputs: {\n        x: _e966\n      },\n      backend: n,\n      attrs: {\n        shape: y\n      }\n    }), n.disposeIntermediateTensorInfo(_e966);\n  } else if (y.some(e => 0 === e)) x = n.makeTensorInfo(y, a.dtype, []);else {\n    var _e967 = stridedSliceImpl(y, n.bufferSync(b), f, m);\n\n    x = n.makeTensorInfo(_e967.shape, _e967.dtype, _e967.values);\n  }\n\n  var v = reshape$1({\n    inputs: {\n      x\n    },\n    backend: n,\n    attrs: {\n      shape: y\n    }\n  });\n  return n.disposeIntermediateTensorInfo(b), n.disposeIntermediateTensorInfo(x), v;\n}\n\nvar stridedSliceConfig$1 = {\n  kernelName: StridedSlice,\n  backendName: \"cpu\",\n  kernelFunc: stridedSlice$1\n};\n\nfunction stringNGrams$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    separator: a,\n    nGramWidths: s,\n    leftPad: o,\n    rightPad: i,\n    padWidth: l,\n    preserveShortSequences: u\n  } = r,\n      {\n    data: c,\n    dataSplits: p\n  } = t,\n      d = n.data.get(c.dataId).values,\n      h = n.data.get(p.dataId).values,\n      [m, f] = stringNGramsImpl(d, h, a, s, o, i, l, u);\n  return [n.makeTensorInfo([m.length], \"string\", m), n.makeTensorInfo(p.shape, \"int32\", f)];\n}\n\nvar stringNGramsConfig$1 = {\n  kernelName: StringNGrams,\n  backendName: \"cpu\",\n  kernelFunc: stringNGrams$1\n};\n\nfunction stringSplit$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    skipEmpty: a\n  } = r,\n      {\n    input: s,\n    delimiter: o\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (1 !== s.shape.length) throw new Error(\"Input must be a vector, got shape: \".concat(s.shape));\n  if (0 !== o.shape.length) throw new Error(\"Delimiter must be a scalar, got shape: \".concat(o.shape));\n  var i = n.data.get(s.dataId).values,\n      l = n.data.get(o.dataId).values[0],\n      [u, c, p] = stringSplitImpl(i, l, a),\n      d = c.length;\n  return [n.makeTensorInfo([d, 2], \"int32\", u), n.makeTensorInfo([d], \"string\", c), n.makeTensorInfo([2], \"int32\", new Int32Array(p))];\n}\n\nvar stringSplitConfig$1 = {\n  kernelName: StringSplit,\n  backendName: \"cpu\",\n  kernelFunc: stringSplit$1\n};\n\nfunction stringToHashBucketFast$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    numBuckets: a\n  } = r,\n      {\n    input: s\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (a <= 0) throw new Error(\"Number of buckets must be at least 1\");\n  var o = stringToHashBucketFastImpl(n.data.get(s.dataId).values, a);\n  return n.makeTensorInfo(s.shape, \"int32\", o);\n}\n\nvar stringToHashBucketFastConfig$1 = {\n  kernelName: StringToHashBucketFast,\n  backendName: \"cpu\",\n  kernelFunc: stringToHashBucketFast$1\n},\n    tan$1 = unaryKernelFunc$1(Tan, e => Math.tan(e)),\n    tanConfig$1 = {\n  kernelName: Tan,\n  backendName: \"cpu\",\n  kernelFunc: tan$1\n},\n    tanh$1 = unaryKernelFunc$1(Tanh$1, e => Math.tanh(e)),\n    tanhConfig$1 = {\n  kernelName: Tanh$1,\n  backendName: \"cpu\",\n  kernelFunc: tanh$1\n};\n\nfunction tile$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reps: s\n  } = r;\n  assertNotComplex$1(a, \"tile\");\n  var o = tileImpl(n.bufferSync(a), s);\n  return n.makeTensorInfo(o.shape, o.dtype, o.values);\n}\n\nvar tileConfig$1 = {\n  kernelName: Tile,\n  backendName: \"cpu\",\n  kernelFunc: tile$1\n};\n\nfunction topK$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    k: s,\n    sorted: o\n  } = r;\n  assertNotComplex$1(a, \"topk\");\n  var i = n.data.get(a.dataId).values,\n      [l, u] = topKImpl(i, a.shape, a.dtype, s, o);\n  return [n.makeTensorInfo(l.shape, l.dtype, l.values), n.makeTensorInfo(u.shape, u.dtype, u.values)];\n}\n\nvar topKConfig$1 = {\n  kernelName: TopK,\n  backendName: \"cpu\",\n  kernelFunc: topK$1\n};\n\nfunction transform$1(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    image: a,\n    transforms: s\n  } = t,\n      {\n    interpolation: o,\n    fillMode: i,\n    fillValue: l,\n    outputShape: u\n  } = n,\n      [c, p, d, h] = a.shape,\n      [m, f] = null != u ? u : [p, d],\n      g = [c, m, f, h],\n      $ = computeStrides(a.shape),\n      y = $[0],\n      b = $[1],\n      x = $[2],\n      v = getTypedArrayFromDType(a.dtype, sizeFromShape(g));\n  v.fill(l);\n  var I = r.data.get(a.dataId).values,\n      C = r.data.get(s.dataId).values;\n\n  for (var _e968 = 0; _e968 < c; ++_e968) {\n    var _t663 = 1 === s.shape[0] ? C : C.subarray(8 * _e968, 8 * _e968 + 8);\n\n    for (var _n415 = 0; _n415 < m; ++_n415) {\n      for (var _r404 = 0; _r404 < f; ++_r404) {\n        for (var _a296 = 0; _a296 < h; ++_a296) {\n          var _s207 = void 0;\n\n          var _u55 = _t663[6] * _r404 + _t663[7] * _n415 + 1;\n\n          if (0 === _u55) continue;\n\n          var _c37 = (_t663[3] * _r404 + _t663[4] * _n415 + _t663[5]) / _u55,\n              _h22 = mapCoord((_t663[0] * _r404 + _t663[1] * _n415 + _t663[2]) / _u55, d, i),\n              _m9 = mapCoord(_c37, p, i);\n\n          switch (o) {\n            case \"nearest\":\n              _s207 = nearestInterpolation(I, p, d, y, b, x, _e968, _m9, _h22, _a296, l);\n              break;\n\n            case \"bilinear\":\n              _s207 = bilinearInterpolation(I, p, d, y, b, x, _e968, _m9, _h22, _a296, l);\n              break;\n\n            default:\n              throw new Error(\"Error in Transform: Expect 'nearest' or 'bilinear', but got \".concat(o));\n          }\n\n          v[_e968 * y + _n415 * b + _r404 * x + _a296] = _s207;\n        }\n      }\n    }\n\n    return r.makeTensorInfo(g, a.dtype, v);\n  }\n\n  return {\n    dataId: r.write(v, g, a.dtype),\n    shape: a.shape,\n    dtype: a.dtype\n  };\n}\n\nvar transformConfig$1 = {\n  kernelName: Transform,\n  backendName: \"cpu\",\n  kernelFunc: transform$1\n};\n\nfunction mapCoord(e, t, n) {\n  switch (n) {\n    case \"reflect\":\n      return mapCoordReflect(e, t);\n\n    case \"wrap\":\n      return mapCoordWrap(e, t);\n\n    case \"nearest\":\n      return mapCoordNearest(e, t);\n\n    case \"constant\":\n    default:\n      return mapCoordConstant(e);\n  }\n}\n\nfunction mapCoordReflect(e, t) {\n  var n = e;\n  if (n < 0) {\n    if (t <= 1) n = 0;else {\n      var _e969 = 2 * t;\n\n      n < _e969 && (n = _e969 * Math.trunc(-n / _e969) + n), n = n < -t ? n + _e969 : -n - 1;\n    }\n  } else if (n > t - 1) if (t <= 1) n = 0;else {\n    var _e970 = 2 * t;\n\n    n -= _e970 * Math.trunc(n / _e970), n >= t && (n = _e970 - n - 1);\n  }\n  return clamp(0, n, t - 1);\n}\n\nfunction mapCoordWrap(e, t) {\n  var n = e;\n  return n < 0 ? t <= 1 ? n = 0 : n += t * (Math.trunc(-n / (t - 1)) + 1) : n > t - 1 && (t <= 1 ? n = 0 : n -= t * Math.trunc(n / (t - 1))), clamp(0, n, t - 1);\n}\n\nfunction mapCoordConstant(e, t) {\n  return e;\n}\n\nfunction mapCoordNearest(e, t) {\n  return clamp(0, e, t - 1);\n}\n\nfunction readWithFillValue(e, t, n, r, a, s, o, i, l, u, c) {\n  return 0 <= i && i < t && 0 <= l && l < n ? e[o * r + i * a + l * s + u] : c;\n}\n\nfunction nearestInterpolation(e, t, n, r, a, s, o, i, l, u, c) {\n  return readWithFillValue(e, t, n, r, a, s, o, Math.round(i), Math.round(l), u, c);\n}\n\nfunction bilinearInterpolation(e, t, n, r, a, s, o, i, l, u, c) {\n  var p = Math.floor(i),\n      d = Math.floor(l),\n      h = p + 1,\n      m = d + 1;\n  return (h - i) * ((m - l) * readWithFillValue(e, t, n, r, a, s, o, p, d, u, c) + (l - d) * readWithFillValue(e, t, n, r, a, s, o, p, m, u, c)) + (i - p) * ((m - l) * readWithFillValue(e, t, n, r, a, s, o, h, d, u, c) + (l - d) * readWithFillValue(e, t, n, r, a, s, o, h, m, u, c));\n}\n\nfunction unique$1(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    axis: a\n  } = n,\n      {\n    x: s\n  } = t;\n  assertNotComplex$1(s, \"unique\");\n  var o = r.data.get(s.dataId).values,\n      {\n    outputValues: i,\n    outputShape: l,\n    indices: u\n  } = uniqueImpl(o, a, s.shape, s.dtype);\n  return [r.makeTensorInfo(l, s.dtype, i), r.makeTensorInfo([u.length], \"int32\", u)];\n}\n\nvar uniqueConfig$1 = {\n  kernelName: Unique,\n  backendName: \"cpu\",\n  kernelFunc: unique$1\n};\n\nfunction unpack$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    value: a\n  } = t;\n  var {\n    axis: s\n  } = r;\n  s < 0 && (s += a.shape.length);\n  var o = a.shape.length,\n      i = a.shape[s],\n      l = new Array(o - 1);\n  var u = 0;\n\n  for (var _e971 = 0; _e971 < o; _e971++) {\n    _e971 !== s && (l[u++] = a.shape[_e971]);\n  }\n\n  var c = new Array(o).fill(0),\n      p = a.shape.slice();\n  p[s] = 1;\n  var d = new Array(i);\n\n  for (var _e972 = 0; _e972 < d.length; _e972++) {\n    c[s] = _e972;\n\n    var _t664 = slice$1({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        begin: c,\n        size: p\n      }\n    });\n\n    d[_e972] = reshape$1({\n      inputs: {\n        x: _t664\n      },\n      backend: n,\n      attrs: {\n        shape: l\n      }\n    }), n.disposeIntermediateTensorInfo(_t664);\n  }\n\n  return d;\n}\n\nvar unpackConfig$1 = {\n  kernelName: Unpack,\n  backendName: \"cpu\",\n  kernelFunc: unpack$1\n};\n\nfunction unsortedSegmentSum$1(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    segmentIds: s\n  } = t,\n      {\n    numSegments: o\n  } = r;\n  assertNotComplex$1(a, \"unsortedSegmentSum\");\n  var i = [],\n      l = [],\n      u = a.shape.length - s.shape.length;\n  var c = s;\n\n  for (var _e973 = 0; _e973 < u; ++_e973) {\n    var _t665 = expandDims$1({\n      inputs: {\n        input: c\n      },\n      backend: n,\n      attrs: {\n        dim: _e973 + 1\n      }\n    });\n\n    c = _t665, l.push(_t665);\n  }\n\n  for (var _e974 = 0; _e974 < o; ++_e974) {\n    var _t666 = createScalarValue(_e974, \"int32\"),\n        _r405 = n.makeTensorInfo([], \"int32\", _t666),\n        _s208 = equal$1({\n      inputs: {\n        a: _r405,\n        b: c\n      },\n      backend: n\n    }),\n        _o145 = cast$1({\n      inputs: {\n        x: _s208\n      },\n      backend: n,\n      attrs: {\n        dtype: \"float32\"\n      }\n    }),\n        _u56 = multiply$1({\n      inputs: {\n        a: _o145,\n        b: a\n      },\n      backend: n\n    }),\n        _p34 = sum$1({\n      inputs: {\n        x: _u56\n      },\n      backend: n,\n      attrs: {\n        axis: 0,\n        keepDims: !1\n      }\n    });\n\n    i.push(_p34), l.push(_r405), l.push(_s208), l.push(_o145), l.push(_u56), l.push(_p34);\n  }\n\n  var p = pack$1({\n    inputs: i,\n    backend: n,\n    attrs: {\n      axis: 0\n    }\n  });\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), p;\n}\n\nvar unsortedSegmentSumConfig$1 = {\n  kernelName: UnsortedSegmentSum,\n  backendName: \"cpu\",\n  kernelFunc: unsortedSegmentSum$1\n},\n    kernelConfigs$1 = [_fusedMatMulConfig$1, absConfig$1, acosConfig$1, acoshConfig$1, addConfig$1, addNConfig$1, allConfig$1, anyConfig$1, argMaxConfig$1, argMinConfig$1, asinConfig$1, asinhConfig$1, atanConfig$1, atan2Config$1, atanhConfig$1, avgPoolConfig$1, avgPool3DConfig$1, avgPool3DGradConfig, avgPoolGradConfig$1, batchMatMulConfig$1, batchNormConfig$1, batchToSpaceNDConfig$1, bincountConfig$1, castConfig$1, ceilConfig$1, clipConfig, complexConfig$1, complexAbsConfig$1, concatConfig$1, conv2DBackpropFilterConfig$1, conv2DBackpropInputConfig$1, conv2DConfig$1, conv3DBackpropFilterV2Config$1, conv3DBackpropInputV2Config, conv3DConfig$1, cosConfig$1, coshConfig$1, cropAndResizeConfig$1, cumsumConfig$1, denseBincountConfig$1, depthToSpaceConfig$1, depthwiseConv2dNativeConfig$1, depthwiseConv2dNativeBackpropFilterConfig$1, depthwiseConv2dNativeBackpropInputConfig$1, diagConfig$1, dilation2dConfig, dilation2dBackpropInputConfig, dilation2dBackpropFilterConfig, realDivConfig$1, einsumConfig$1, eluConfig$1, eluGradConfig$1, equalConfig$1, erfConfig$1, expConfig$1, expandDimsConfig$1, expm1Config$1, fftConfig$1, fillConfig$1, flipLeftRightConfig$1, floorConfig$1, floorDivConfig$1, fusedConv2DConfig$1, fusedDepthwiseConv2DConfig$1, gatherNdConfig$1, gatherV2Config$1, greaterConfig$1, greaterEqualConfig$1, identityConfig$1, ifftConfig$1, imagConfig$1, isFiniteConfig$1, isInfConfig$1, isNaNConfig$1, leakyReluConfig$1, lessConfig$1, lessEqualConfig$1, linSpaceConfig$1, logConfig$1, log1pConfig$1, logicalAndConfig$1, logicalNotConfig$1, logicalOrConfig$1, lRNConfig, lRNGradConfig, maximumConfig$1, maxPoolConfig$1, maxPool3DConfig$1, maxPool3DGradConfig, maxPoolGradConfig$1, maxPoolWithArgmaxConfig$1, maxConfig$1, meanConfig$1, minConfig$1, minimumConfig$1, mirrorPadConfig$1, modConfig$1, multinomialConfig$1, multiplyConfig$1, negConfig$1, nonMaxSuppressionV3Config$1, nonMaxSuppressionV4Config$1, nonMaxSuppressionV5Config$1, notEqualConfig$1, oneHotConfig$1, onesLikeConfig$1, packConfig$1, padV2Config$1, powConfig$1, preluConfig$1, prodConfig$1, rangeConfig$1, realConfig$1, reciprocalConfig$1, reluConfig$1, relu6Config$1, reshapeConfig$1, resizeBilinearConfig$1, resizeBilinearGradConfig$1, resizeNearestNeighborConfig$1, resizeNearestNeighborGradConfig$1, reverseConfig$1, rotateWithOffsetConfig$1, roundConfig$1, rsqrtConfig$1, scatterNdConfig$1, selectConfig$1, seluConfig$1, sigmoidConfig$1, signConfig$1, sinConfig$1, sinhConfig$1, sliceConfig$1, softmaxConfig$1, softplusConfig$1, spaceToBatchNDConfig$1, sparseFillEmptyRowsConfig$1, sparseReshapeConfig$1, sparseSegmentMeanConfig$1, sparseSegmentSumConfig$1, sparseToDenseConfig$1, splitVConfig$1, sqrtConfig$1, squareConfig$1, squaredDifferenceConfig$1, stepConfig$1, stridedSliceConfig$1, stringNGramsConfig$1, stringSplitConfig$1, stringToHashBucketFastConfig$1, subConfig$1, sumConfig$1, tanConfig$1, tanhConfig$1, tileConfig$1, topKConfig$1, transposeConfig$1, transformConfig$1, uniqueConfig$1, unpackConfig$1, unsortedSegmentSumConfig$1, zerosLikeConfig$1];\n\nfor (var _e975 of kernelConfigs$1) {\n  registerKernel(_e975);\n}\n\nvar contexts = {},\n    WEBGL_ATTRIBUTES = {\n  alpha: !1,\n  antialias: !1,\n  premultipliedAlpha: !1,\n  preserveDrawingBuffer: !1,\n  depth: !1,\n  stencil: !1,\n  failIfMajorPerformanceCaveat: !0\n};\n\nfunction setWebGLContext(e, t) {\n  contexts[e] = t;\n}\n\nfunction getWebGLContext(e) {\n  if (!(e in contexts)) {\n    var _t667 = getWebGLRenderingContext(e);\n\n    if (null === _t667) return console.log(\"Could not get context for WebGL version\", e), null;\n    contexts[e] = _t667;\n  }\n\n  var t = contexts[e];\n  return t.isContextLost() ? (delete contexts[e], getWebGLContext(e)) : (t.disable(t.DEPTH_TEST), t.disable(t.STENCIL_TEST), t.disable(t.BLEND), t.disable(t.DITHER), t.disable(t.POLYGON_OFFSET_FILL), t.disable(t.SAMPLE_COVERAGE), t.enable(t.SCISSOR_TEST), t.enable(t.CULL_FACE), t.cullFace(t.BACK), contexts[e]);\n}\n\nfunction createCanvas(e) {\n  if (\"undefined\" != typeof OffscreenCanvas && 2 === e) return new OffscreenCanvas(300, 150);\n  if (\"undefined\" != typeof document) return document.createElement(\"canvas\");\n  throw new Error(\"Cannot create a canvas in this context\");\n}\n\nfunction getWebGLRenderingContext(e) {\n  if (1 !== e && 2 !== e) throw new Error(\"Cannot get WebGL rendering context, WebGL is disabled.\");\n  var t = createCanvas(e);\n  return t.addEventListener(\"webglcontextlost\", t => {\n    t.preventDefault(), delete contexts[e];\n  }, !1), 1 === e ? t.getContext(\"webgl\", WEBGL_ATTRIBUTES) || t.getContext(\"experimental-webgl\", WEBGL_ATTRIBUTES) : t.getContext(\"webgl2\", WEBGL_ATTRIBUTES);\n}\n\nvar PackingScheme, TextureUsage, PhysicalTextureType;\n\nfunction getUnpackedMatrixTextureShapeWidthHeight(e, t) {\n  return [t, e];\n}\n\nfunction getUnpackedArraySizeFromMatrixSize(e, t) {\n  return e * t;\n}\n\nfunction getDenseTexShape(e) {\n  var t = sizeFromShape(e);\n  return sizeToSquarishShape(Math.ceil(t / 4));\n}\n\nfunction getPackedMatrixTextureShapeWidthHeight(e, t) {\n  return [Math.max(1, Math.ceil(t / 2)), Math.max(1, Math.ceil(e / 2))];\n}\n\nfunction getPackedRGBAArraySizeFromMatrixShape(e, t) {\n  var [n, r] = getPackedMatrixTextureShapeWidthHeight(e, t);\n  return n * r * 4;\n}\n\nfunction getTextureConfig(e, t) {\n  var n = e;\n  var r, a, s, o, i, l, u, c, p, d;\n  return 2 === env().getNumber(\"WEBGL_VERSION\") ? (r = n.R32F, a = n.R16F, s = n.RGBA16F, o = n.RGBA32F, i = n.RED, u = 4, c = 1, p = n.HALF_FLOAT, d = n.FLOAT) : (r = e.RGBA, a = e.RGBA, s = e.RGBA, o = n.RGBA, i = e.RGBA, u = 4, c = 4, p = null != t ? t.HALF_FLOAT_OES : null, d = e.FLOAT), l = e.RGBA, {\n    internalFormatFloat: r,\n    internalFormatHalfFloat: a,\n    internalFormatPackedHalfFloat: s,\n    internalFormatPackedFloat: o,\n    textureFormatFloat: i,\n    downloadTextureFormat: l,\n    downloadUnpackNumChannels: u,\n    defaultNumChannels: c,\n    textureTypeHalfFloat: p,\n    textureTypeFloat: d\n  };\n}\n\nfunction callAndCheck(e, t) {\n  var n = t();\n  return env().getBool(\"DEBUG\") && checkWebGLError(e), n;\n}\n\nfunction checkWebGLError(e) {\n  var t = e.getError();\n  if (t !== e.NO_ERROR) throw new Error(\"WebGL Error: \" + getWebGLErrorMessage(e, t));\n}\n\n!function (e) {\n  e[e.DENSE = 0] = \"DENSE\", e[e.SHARED_BATCH = 1] = \"SHARED_BATCH\";\n}(PackingScheme || (PackingScheme = {})), function (e) {\n  e[e.RENDER = 0] = \"RENDER\", e[e.UPLOAD = 1] = \"UPLOAD\", e[e.PIXELS = 2] = \"PIXELS\", e[e.DOWNLOAD = 3] = \"DOWNLOAD\";\n}(TextureUsage || (TextureUsage = {})), function (e) {\n  e[e.UNPACKED_FLOAT16 = 0] = \"UNPACKED_FLOAT16\", e[e.UNPACKED_FLOAT32 = 1] = \"UNPACKED_FLOAT32\", e[e.PACKED_4X1_UNSIGNED_BYTE = 2] = \"PACKED_4X1_UNSIGNED_BYTE\", e[e.PACKED_2X2_FLOAT32 = 3] = \"PACKED_2X2_FLOAT32\", e[e.PACKED_2X2_FLOAT16 = 4] = \"PACKED_2X2_FLOAT16\";\n}(PhysicalTextureType || (PhysicalTextureType = {}));\nvar MIN_FLOAT16 = 5.96e-8,\n    MAX_FLOAT16 = 65504;\n\nfunction canBeRepresented(e) {\n  return !!(env().getBool(\"WEBGL_RENDER_FLOAT32_ENABLED\") || 0 === e || MIN_FLOAT16 < Math.abs(e) && Math.abs(e) < MAX_FLOAT16);\n}\n\nfunction getWebGLErrorMessage(e, t) {\n  switch (t) {\n    case e.NO_ERROR:\n      return \"NO_ERROR\";\n\n    case e.INVALID_ENUM:\n      return \"INVALID_ENUM\";\n\n    case e.INVALID_VALUE:\n      return \"INVALID_VALUE\";\n\n    case e.INVALID_OPERATION:\n      return \"INVALID_OPERATION\";\n\n    case e.INVALID_FRAMEBUFFER_OPERATION:\n      return \"INVALID_FRAMEBUFFER_OPERATION\";\n\n    case e.OUT_OF_MEMORY:\n      return \"OUT_OF_MEMORY\";\n\n    case e.CONTEXT_LOST_WEBGL:\n      return \"CONTEXT_LOST_WEBGL\";\n\n    default:\n      return \"Unknown error code \".concat(t);\n  }\n}\n\nfunction getExtensionOrThrow(e, t) {\n  return throwIfNull(e, () => e.getExtension(t), 'Extension \"' + t + '\" not supported on this browser.');\n}\n\nfunction createVertexShader$1(e, t) {\n  var n = throwIfNull(e, () => e.createShader(e.VERTEX_SHADER), \"Unable to create vertex WebGLShader.\");\n  if (callAndCheck(e, () => e.shaderSource(n, t)), callAndCheck(e, () => e.compileShader(n)), !1 === e.getShaderParameter(n, e.COMPILE_STATUS)) throw console.log(e.getShaderInfoLog(n)), new Error(\"Failed to compile vertex shader.\");\n  return n;\n}\n\nfunction createFragmentShader(e, t) {\n  var n = throwIfNull(e, () => e.createShader(e.FRAGMENT_SHADER), \"Unable to create fragment WebGLShader.\");\n  if (callAndCheck(e, () => e.shaderSource(n, t)), callAndCheck(e, () => e.compileShader(n)), !1 === e.getShaderParameter(n, e.COMPILE_STATUS)) throw logShaderSourceAndInfoLog(t, e.getShaderInfoLog(n)), new Error(\"Failed to compile fragment shader.\");\n  return n;\n}\n\nvar lineNumberRegex = /ERROR: [0-9]+:([0-9]+):/g;\n\nfunction logShaderSourceAndInfoLog(e, t) {\n  var n = lineNumberRegex.exec(t);\n  if (null == n) return console.log(\"Couldn't parse line number in error: \".concat(t)), void console.log(e);\n  var r = +n[1],\n      a = e.split(\"\\n\"),\n      s = a.length.toString().length + 2,\n      o = a.map((e, t) => rightPad((t + 1).toString(), s) + e);\n  var i = 0;\n\n  for (var _e976 = 0; _e976 < o.length; _e976++) {\n    i = Math.max(o[_e976].length, i);\n  }\n\n  var l = o.slice(0, r - 1),\n      u = o.slice(r - 1, r),\n      c = o.slice(r);\n  console.log(l.join(\"\\n\")), console.log(t.split(\"\\n\")[0]), console.log(\"%c \".concat(rightPad(u[0], i)), \"border:1px solid red; background-color:#e3d2d2; color:#a61717\"), console.log(c.join(\"\\n\"));\n}\n\nfunction createProgram(e) {\n  return throwIfNull(e, () => e.createProgram(), \"Unable to create WebGLProgram.\");\n}\n\nfunction linkProgram(e, t) {\n  if (callAndCheck(e, () => e.linkProgram(t)), !1 === e.getProgramParameter(t, e.LINK_STATUS)) throw console.log(e.getProgramInfoLog(t)), new Error(\"Failed to link vertex and fragment shaders.\");\n}\n\nfunction validateProgram(e, t) {\n  if (callAndCheck(e, () => e.validateProgram(t)), !1 === e.getProgramParameter(t, e.VALIDATE_STATUS)) throw console.log(e.getProgramInfoLog(t)), new Error(\"Shader program validation failed.\");\n}\n\nfunction createStaticVertexBuffer(e, t) {\n  var n = throwIfNull(e, () => e.createBuffer(), \"Unable to create WebGLBuffer\");\n  return callAndCheck(e, () => e.bindBuffer(e.ARRAY_BUFFER, n)), callAndCheck(e, () => e.bufferData(e.ARRAY_BUFFER, t, e.STATIC_DRAW)), n;\n}\n\nfunction createStaticIndexBuffer(e, t) {\n  var n = throwIfNull(e, () => e.createBuffer(), \"Unable to create WebGLBuffer\");\n  return callAndCheck(e, () => e.bindBuffer(e.ELEMENT_ARRAY_BUFFER, n)), callAndCheck(e, () => e.bufferData(e.ELEMENT_ARRAY_BUFFER, t, e.STATIC_DRAW)), n;\n}\n\nfunction createTexture(e) {\n  return throwIfNull(e, () => e.createTexture(), \"Unable to create WebGLTexture.\");\n}\n\nfunction validateTextureSize(e, t) {\n  var n = env().getNumber(\"WEBGL_MAX_TEXTURE_SIZE\");\n  if (e <= 0 || t <= 0) throw new Error(\"Requested texture size [\".concat(e, \"x\").concat(t, \"] is invalid.\"));\n  if (e > n || t > n) throw new Error(\"Requested texture size [\".concat(e, \"x\").concat(t, \"] greater than WebGL maximum on this browser / GPU [\").concat(n, \"x\").concat(n, \"].\"));\n}\n\nfunction createFramebuffer(e) {\n  return throwIfNull(e, () => e.createFramebuffer(), \"Unable to create WebGLFramebuffer.\");\n}\n\nfunction bindVertexBufferToProgramAttribute(e, t, n, r, a, s, o) {\n  var i = e.getAttribLocation(t, n);\n  return -1 !== i && (callAndCheck(e, () => e.bindBuffer(e.ARRAY_BUFFER, r)), callAndCheck(e, () => e.vertexAttribPointer(i, a, e.FLOAT, !1, s, o)), callAndCheck(e, () => e.enableVertexAttribArray(i)), !0);\n}\n\nfunction bindTextureUnit(e, t, n) {\n  validateTextureUnit(e, n), callAndCheck(e, () => e.activeTexture(e.TEXTURE0 + n)), callAndCheck(e, () => e.bindTexture(e.TEXTURE_2D, t));\n}\n\nfunction getProgramUniformLocationOrThrow(e, t, n) {\n  return throwIfNull(e, () => e.getUniformLocation(t, n), 'uniform \"' + n + '\" not present in program.');\n}\n\nfunction getProgramUniformLocation(e, t, n) {\n  return e.getUniformLocation(t, n);\n}\n\nfunction bindTextureToProgramUniformSampler(e, t, n, r) {\n  callAndCheck(e, () => bindTextureUnit(e, t, r)), callAndCheck(e, () => e.uniform1i(n, r));\n}\n\nfunction bindColorTextureToFramebuffer(e, t, n) {\n  callAndCheck(e, () => e.bindFramebuffer(e.FRAMEBUFFER, n)), callAndCheck(e, () => e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, t, 0));\n}\n\nfunction unbindColorTextureFromFramebuffer(e, t) {\n  callAndCheck(e, () => e.bindFramebuffer(e.FRAMEBUFFER, t)), callAndCheck(e, () => e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, null, 0));\n}\n\nfunction validateFramebuffer(e) {\n  var t = e.checkFramebufferStatus(e.FRAMEBUFFER);\n  if (t !== e.FRAMEBUFFER_COMPLETE) throw new Error(\"Error binding framebuffer: \" + getFramebufferErrorMessage(e, t));\n}\n\nfunction getFramebufferErrorMessage(e, t) {\n  switch (t) {\n    case e.FRAMEBUFFER_INCOMPLETE_ATTACHMENT:\n      return \"FRAMEBUFFER_INCOMPLETE_ATTACHMENT\";\n\n    case e.FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT:\n      return \"FRAMEBUFFER_INCOMPLETE_MISSING_ATTACHMENT\";\n\n    case e.FRAMEBUFFER_INCOMPLETE_DIMENSIONS:\n      return \"FRAMEBUFFER_INCOMPLETE_DIMENSIONS\";\n\n    case e.FRAMEBUFFER_UNSUPPORTED:\n      return \"FRAMEBUFFER_UNSUPPORTED\";\n\n    default:\n      return \"unknown error \".concat(t);\n  }\n}\n\nfunction throwIfNull(e, t, n) {\n  var r = callAndCheck(e, () => t());\n  if (null == r) throw new Error(n);\n  return r;\n}\n\nfunction validateTextureUnit(e, t) {\n  var n = e.MAX_COMBINED_TEXTURE_IMAGE_UNITS - 1,\n      r = t + e.TEXTURE0;\n  if (r < e.TEXTURE0 || r > n) throw new Error(\"textureUnit must be in [gl.TEXTURE0, gl.TEXTURE\".concat(n, \"].\"));\n}\n\nfunction getBatchDim(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 2;\n  return sizeFromShape(e.slice(0, e.length - t));\n}\n\nfunction getRowsCols(e) {\n  if (0 === e.length) throw Error(\"Cannot get rows and columns of an empty shape array.\");\n  return [e.length > 1 ? e[e.length - 2] : 1, e[e.length - 1]];\n}\n\nfunction getShapeAs3D(e) {\n  var t = [1, 1, 1];\n  return 0 === e.length || 1 === e.length && 1 === e[0] || (t = [getBatchDim(e), ...getRowsCols(e)]), t;\n}\n\nfunction getTextureShapeFromLogicalShape(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  var n = env().getNumber(\"WEBGL_MAX_TEXTURE_SIZE\");\n\n  if (t && (n *= 2, 1 === (e = e.map((t, n) => n >= e.length - 2 ? nearestLargerEven(e[n]) : e[n])).length && (e = [2, e[0]])), 2 !== e.length) {\n    var _t668 = squeezeShape(e);\n\n    e = _t668.newShape;\n  }\n\n  var r = sizeFromShape(e);\n  if (e.length <= 1 && r <= n) return [1, r];\n  if (2 === e.length && e[0] <= n && e[1] <= n) return e;\n  if (3 === e.length && e[0] * e[1] <= n && e[2] <= n) return [e[0] * e[1], e[2]];\n  if (3 === e.length && e[0] <= n && e[1] * e[2] <= n) return [e[0], e[1] * e[2]];\n  if (4 === e.length && e[0] * e[1] * e[2] <= n && e[3] <= n) return [e[0] * e[1] * e[2], e[3]];\n  if (4 === e.length && e[0] <= n && e[1] * e[2] * e[3] <= n) return [e[0], e[1] * e[2] * e[3]];\n\n  if (t) {\n    var _t669 = getBatchDim(e);\n\n    var _n416 = 2,\n        a = 2;\n    return e.length && ([_n416, a] = getRowsCols(e)), r = _t669 * (_n416 / 2) * (a / 2), sizeToSquarishShape(r).map(e => 2 * e);\n  }\n\n  return sizeToSquarishShape(r);\n}\n\nfunction isEven(e) {\n  return e % 2 == 0;\n}\n\nfunction isReshapeFree(e, t) {\n  if (arraysEqual(e = e.slice(-2), t = t.slice(-2))) return !0;\n  if (!e.length || !t.length) return !0;\n  if (0 === e[0] || 0 === e[1] || 0 === t[0] || 0 === t[1]) return !0;\n\n  if (e.length !== t.length) {\n    var n = e.slice(-1)[0],\n        r = t.slice(-1)[0];\n    if (n === r) return !0;\n    if (isEven(n) && isEven(r) && (1 === e[0] || 1 === t[0])) return !0;\n  }\n\n  return e[1] === t[1] && isEven(e[0]) && isEven(t[0]);\n}\n\nvar MAX_TEXTURE_SIZE, MAX_TEXTURES_IN_SHADER;\n\nfunction getWebGLMaxTextureSize(e) {\n  if (null == MAX_TEXTURE_SIZE) {\n    var t = getWebGLContext(e);\n    MAX_TEXTURE_SIZE = t.getParameter(t.MAX_TEXTURE_SIZE);\n  }\n\n  return MAX_TEXTURE_SIZE;\n}\n\nfunction getMaxTexturesInShader(e) {\n  if (null == MAX_TEXTURES_IN_SHADER) {\n    var t = getWebGLContext(e);\n    MAX_TEXTURES_IN_SHADER = t.getParameter(t.MAX_TEXTURE_IMAGE_UNITS);\n  }\n\n  return Math.min(16, MAX_TEXTURES_IN_SHADER);\n}\n\nfunction getWebGLDisjointQueryTimerVersion(e) {\n  if (0 === e) return 0;\n  var t;\n  var n = getWebGLContext(e);\n  return t = hasExtension(n, \"EXT_disjoint_timer_query_webgl2\") && 2 === e ? 2 : hasExtension(n, \"EXT_disjoint_timer_query\") ? 1 : 0, t;\n}\n\nfunction hasExtension(e, t) {\n  return null != e.getExtension(t);\n}\n\nfunction isWebGLVersionEnabled(e) {\n  try {\n    if (null != getWebGLContext(e)) return !0;\n  } catch (e) {\n    return console.log(\"Error when getting WebGL context: \", e), !1;\n  }\n\n  return !1;\n}\n\nfunction isCapableOfRenderingToFloatTexture(e) {\n  if (0 === e) return !1;\n  var t = getWebGLContext(e);\n\n  if (1 === e) {\n    if (!hasExtension(t, \"OES_texture_float\")) return !1;\n  } else if (!hasExtension(t, \"EXT_color_buffer_float\")) return !1;\n\n  return createFloatTextureAndBindToFramebuffer(t);\n}\n\nfunction isDownloadFloatTextureEnabled(e) {\n  if (0 === e) return !1;\n  var t = getWebGLContext(e);\n\n  if (1 !== e) {\n    if (hasExtension(t, \"EXT_color_buffer_float\")) return createFloatTextureAndBindToFramebuffer(t);\n    var _e977 = \"EXT_color_buffer_half_float\";\n\n    if (hasExtension(t, _e977)) {\n      var n = t.getExtension(_e977);\n      return createHalfFloatTextureAndBindToFramebuffer(t, n);\n    }\n\n    return !1;\n  }\n\n  return !!hasExtension(t, \"OES_texture_float\") && !!hasExtension(t, \"WEBGL_color_buffer_float\") && createFloatTextureAndBindToFramebuffer(t);\n}\n\nfunction createFloatTextureAndBindToFramebuffer(e) {\n  var t = getTextureConfig(e),\n      n = e.createTexture();\n  e.bindTexture(e.TEXTURE_2D, n), e.texImage2D(e.TEXTURE_2D, 0, t.internalFormatFloat, 1, 1, 0, t.textureFormatFloat, t.textureTypeFloat, null);\n  var r = e.createFramebuffer();\n  e.bindFramebuffer(e.FRAMEBUFFER, r), e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, n, 0);\n  var a = e.checkFramebufferStatus(e.FRAMEBUFFER) === e.FRAMEBUFFER_COMPLETE;\n  return e.bindTexture(e.TEXTURE_2D, null), e.bindFramebuffer(e.FRAMEBUFFER, null), e.deleteTexture(n), e.deleteFramebuffer(r), a;\n}\n\nfunction createHalfFloatTextureAndBindToFramebuffer(e, t) {\n  var n = getTextureConfig(e, t),\n      r = e.createTexture();\n  e.bindTexture(e.TEXTURE_2D, r), e.texImage2D(e.TEXTURE_2D, 0, n.internalFormatHalfFloat, 1, 1, 0, n.textureFormatFloat, n.textureTypeHalfFloat, null);\n  var a = e.createFramebuffer();\n  e.bindFramebuffer(e.FRAMEBUFFER, a), e.framebufferTexture2D(e.FRAMEBUFFER, e.COLOR_ATTACHMENT0, e.TEXTURE_2D, r, 0);\n  var s = e.checkFramebufferStatus(e.FRAMEBUFFER) === e.FRAMEBUFFER_COMPLETE;\n  return e.bindTexture(e.TEXTURE_2D, null), e.bindFramebuffer(e.FRAMEBUFFER, null), e.deleteTexture(r), e.deleteFramebuffer(a), s;\n}\n\nfunction isWebGLFenceEnabled(e) {\n  return 2 === e && null != getWebGLContext(e).fenceSync;\n}\n\nfunction assertNotComplex(e, t) {\n  Array.isArray(e) || (e = [e]), e.forEach(e => {\n    null != e && assert$4(\"complex64\" !== e.dtype, () => \"\".concat(t, \" does not support complex64 tensors in the WebGL backend.\"));\n  });\n}\n\nvar ENV = env();\n\nfunction getGlslDifferences() {\n  var e, t, n, r, a, s, o, i, l, u;\n  return 2 === env().getNumber(\"WEBGL_VERSION\") ? (e = \"#version 300 es\", t = \"in\", n = \"out\", r = \"in\", a = \"texture\", s = \"outputColor\", o = \"out vec4 outputColor;\", i = \"\\n      bool isnan_custom(float val) {\\n        return (val > 0.0 || val < 0.0) ? false : val != 0.0;\\n      }\\n\\n      bvec4 isnan_custom(vec4 val) {\\n        return bvec4(isnan_custom(val.x),\\n          isnan_custom(val.y), isnan_custom(val.z), isnan_custom(val.w));\\n      }\\n\\n      #define isnan(value) isnan_custom(value)\\n    \", l = \"\", u = \"\\n      #define round(value) newRound(value)\\n      int newRound(float value) {\\n        return int(floor(value + 0.5));\\n      }\\n\\n      ivec4 newRound(vec4 value) {\\n        return ivec4(floor(value + vec4(0.5)));\\n      }\\n    \") : (e = \"\", t = \"attribute\", n = \"varying\", r = \"varying\", a = \"texture2D\", s = \"gl_FragColor\", o = \"\", i = \"\\n      #define isnan(value) isnan_custom(value)\\n      bool isnan_custom(float val) {\\n        return (val > 0. || val < 1. || val == 0.) ? false : true;\\n      }\\n      bvec4 isnan_custom(vec4 val) {\\n        return bvec4(isnan(val.x), isnan(val.y), isnan(val.z), isnan(val.w));\\n      }\\n    \", l = \"\\n      uniform float INFINITY;\\n\\n      bool isinf(float val) {\\n        return abs(val) == INFINITY;\\n      }\\n      bvec4 isinf(vec4 val) {\\n        return equal(abs(val), vec4(INFINITY));\\n      }\\n    \", u = \"\\n      int round(float value) {\\n        return int(floor(value + 0.5));\\n      }\\n\\n      ivec4 round(vec4 value) {\\n        return ivec4(floor(value + vec4(0.5)));\\n      }\\n    \"), {\n    version: e,\n    attribute: t,\n    varyingVs: n,\n    varyingFs: r,\n    texture2D: a,\n    output: s,\n    defineOutput: o,\n    defineSpecialNaN: i,\n    defineSpecialInf: l,\n    defineRound: u\n  };\n}\n\nfunction getLogicalCoordinatesFromFlatIndex(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"index\";\n  var r = computeStrides(t);\n  return r.map((t, a) => \"int \".concat(e[a], \" = \").concat(n, \" / \").concat(t, \"; \").concat(a === r.length - 1 ? \"int \".concat(e[a + 1], \" = \").concat(n, \" - \").concat(e[a], \" * \").concat(t) : \"index -= \".concat(e[a], \" * \").concat(t), \";\")).join(\"\");\n}\n\nfunction getLogicalCoordinatesFromFlatIndexByUniform(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : \"index\";\n  var r = computeStrides(t);\n  return r.map((t, a) => \"int \".concat(e[a], \" = \").concat(n, \" / outShapeStrides[\").concat(a, \"]; \").concat(a === r.length - 1 ? \"int \".concat(e[a + 1], \" = \").concat(n, \" - \").concat(e[a], \" * outShapeStrides[\").concat(a, \"]\") : \"index -= \".concat(e[a], \" * outShapeStrides[\").concat(a, \"]\"), \";\")).join(\"\");\n}\n\nfunction getFlatIndexFrom3D(e) {\n  var t = computeStrides(e).map(e => e.toString());\n  return \"\\n  int getFlatIndex(ivec3 coords) {\\n    return coords.x * \".concat(t[0], \" + coords.y * \").concat(t[1], \" + coords.z;\\n  }\\n\");\n}\n\nENV.registerFlag(\"HAS_WEBGL\", () => ENV.getNumber(\"WEBGL_VERSION\") > 0), ENV.registerFlag(\"WEBGL_VERSION\", () => isWebGLVersionEnabled(2) ? 2 : isWebGLVersionEnabled(1) ? 1 : 0), ENV.registerFlag(\"WEBGL_CHECK_NUMERICAL_PROBLEMS\", () => !1), ENV.registerFlag(\"WEBGL_BUFFER_SUPPORTED\", () => 2 === ENV.get(\"WEBGL_VERSION\")), ENV.registerFlag(\"WEBGL_CPU_FORWARD\", () => !0), ENV.registerFlag(\"WEBGL_FORCE_F16_TEXTURES\", () => !1), ENV.registerFlag(\"WEBGL_PACK\", () => ENV.getBool(\"HAS_WEBGL\")), ENV.registerFlag(\"WEBGL_PACK_NORMALIZATION\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_PACK_CLIP\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_PACK_DEPTHWISECONV\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_PACK_BINARY_OPERATIONS\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_PACK_UNARY_OPERATIONS\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_PACK_ARRAY_OPERATIONS\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_PACK_IMAGE_OPERATIONS\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_PACK_REDUCE\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_LAZILY_UNPACK\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_CONV_IM2COL\", () => ENV.getBool(\"WEBGL_PACK\")), ENV.registerFlag(\"WEBGL_MAX_TEXTURE_SIZE\", () => getWebGLMaxTextureSize(ENV.getNumber(\"WEBGL_VERSION\"))), ENV.registerFlag(\"WEBGL_MAX_TEXTURES_IN_SHADER\", () => getMaxTexturesInShader(ENV.getNumber(\"WEBGL_VERSION\"))), ENV.registerFlag(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\", () => {\n  var e = ENV.getNumber(\"WEBGL_VERSION\");\n  return 0 === e ? 0 : getWebGLDisjointQueryTimerVersion(e);\n}), ENV.registerFlag(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\", () => ENV.getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\") > 0 && !isMobile()), ENV.registerFlag(\"WEBGL_RENDER_FLOAT32_CAPABLE\", () => isCapableOfRenderingToFloatTexture(ENV.getNumber(\"WEBGL_VERSION\"))), ENV.registerFlag(\"WEBGL_RENDER_FLOAT32_ENABLED\", () => !ENV.getBool(\"WEBGL_FORCE_F16_TEXTURES\") && ENV.getBool(\"WEBGL_RENDER_FLOAT32_CAPABLE\")), ENV.registerFlag(\"WEBGL_DOWNLOAD_FLOAT_ENABLED\", () => isDownloadFloatTextureEnabled(ENV.getNumber(\"WEBGL_VERSION\"))), ENV.registerFlag(\"WEBGL_FENCE_API_ENABLED\", () => isWebGLFenceEnabled(ENV.getNumber(\"WEBGL_VERSION\"))), ENV.registerFlag(\"WEBGL_SIZE_UPLOAD_UNIFORM\", () => ENV.getBool(\"WEBGL_RENDER_FLOAT32_ENABLED\") ? 4 : 0), ENV.registerFlag(\"WEBGL_DELETE_TEXTURE_THRESHOLD\", () => -1, e => {\n  if (e < 0 && -1 !== e) throw new Error(\"WEBGL_DELETE_TEXTURE_THRESHOLD must be -1 (indicating never delete) or at least 0, but got \".concat(e, \".\"));\n}), ENV.registerFlag(\"WEBGL_FLUSH_THRESHOLD\", () => isMobile() && ENV.getBool(\"IS_CHROME\") ? 1 : -1, e => {\n  if (e < 0 && -1 !== e) throw new Error(\"WEBGL_FLUSH_THRESHOLD must be -1 (indicating never manual flush) or at least 0, but got \".concat(e, \".\"));\n}), ENV.registerFlag(\"CPU_HANDOFF_SIZE_THRESHOLD\", () => 128), ENV.registerFlag(\"WEBGL_USE_SHAPES_UNIFORMS\", () => !1), ENV.registerFlag(\"TOPK_LAST_DIM_CPU_HANDOFF_SIZE_THRESHOLD\", () => 1e5), ENV.registerFlag(\"TOPK_K_CPU_HANDOFF_THRESHOLD\", () => 128);\nvar ENCODE_FLOAT_SNIPPET = \"\\n  const float FLOAT_MAX = 1.70141184e38;\\n  const float FLOAT_MIN = 1.17549435e-38;\\n\\n  lowp vec4 encode_float(highp float v) {\\n    if (isnan(v)) {\\n      return vec4(255, 255, 255, 255);\\n    }\\n\\n    highp float av = abs(v);\\n\\n    if(av < FLOAT_MIN) {\\n      return vec4(0.0, 0.0, 0.0, 0.0);\\n    } else if(v > FLOAT_MAX) {\\n      return vec4(0.0, 0.0, 128.0, 127.0) / 255.0;\\n    } else if(v < -FLOAT_MAX) {\\n      return vec4(0.0, 0.0,  128.0, 255.0) / 255.0;\\n    }\\n\\n    highp vec4 c = vec4(0,0,0,0);\\n\\n    highp float e = floor(log2(av));\\n    highp float m = exp2(fract(log2(av))) - 1.0;\\n\\n    c[2] = floor(128.0 * m);\\n    m -= c[2] / 128.0;\\n    c[1] = floor(32768.0 * m);\\n    m -= c[1] / 32768.0;\\n    c[0] = floor(8388608.0 * m);\\n\\n    highp float ebias = e + 127.0;\\n    c[3] = floor(ebias / 2.0);\\n    ebias -= c[3] * 2.0;\\n    c[2] += floor(ebias) * 128.0;\\n\\n    c[3] += 128.0 * step(0.0, -v);\\n\\n    return c / 255.0;\\n  }\\n\";\n\nclass DecodeMatrixProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0, this.outPackingScheme = PackingScheme.DENSE;\n    var t = getDenseTexShape(e),\n        n = getGlslDifferences();\n    this.outputShape = e, this.userCode = \"\\n      ivec3 outCoordsFromFlatIndex(int index) {\\n        \".concat(getLogicalCoordinatesFromFlatIndex([\"r\", \"c\", \"d\"], e), \"\\n        return ivec3(r, c, d);\\n      }\\n\\n      void main() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n          vec2(\").concat(t[0], \", \").concat(t[1], \"));\\n        int index = 4 * (resTexRC.x * \").concat(t[1], \" + resTexRC.y);\\n\\n        vec4 result = vec4(0.);\\n\\n        for (int i=0; i<4; i++) {\\n          int flatIndex = index + i;\\n          ivec3 rc = outCoordsFromFlatIndex(flatIndex);\\n          result[i] = getA(rc.x, rc.y, rc.z);\\n        }\\n\\n        \").concat(n.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nclass DecodeMatrixPackedProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outPackingScheme = PackingScheme.DENSE;\n    var t = getDenseTexShape(e),\n        n = getGlslDifferences();\n    this.outputShape = e, this.userCode = \"\\n      ivec3 outCoordsFromFlatIndex(int index) {\\n        \".concat(getLogicalCoordinatesFromFlatIndex([\"r\", \"c\", \"d\"], e), \"\\n        return ivec3(r, c, d);\\n      }\\n\\n      void main() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n          vec2(\").concat(t[0], \", \").concat(t[1], \"));\\n        int index = 4 * (resTexRC.x * \").concat(t[1], \" + resTexRC.y);\\n\\n        vec4 result = vec4(0.);\\n\\n        for (int i=0; i<4; i++) {\\n          int flatIndex = index + i;\\n          ivec3 rc = outCoordsFromFlatIndex(flatIndex);\\n          result[i] = getChannel(getA(rc.x, rc.y, rc.z), vec2(rc.y, rc.z));\\n        }\\n\\n        \").concat(n.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeFloatProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.outTexUsage = TextureUsage.DOWNLOAD;\n    var t = getGlslDifferences();\n    this.outputShape = e, this.userCode = \"\\n      \".concat(ENCODE_FLOAT_SNIPPET, \"\\n\\n      void main() {\\n        float x = getAAtOutCoords();\\n        \").concat(t.output, \" = encode_float(x);\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeFloatPackedProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !1, this.outTexUsage = TextureUsage.DOWNLOAD;\n    var t = getGlslDifferences();\n    this.outputShape = e, this.userCode = \"\\n      \".concat(ENCODE_FLOAT_SNIPPET, \"\\n\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n        float x = getChannel(getAAtOutCoords(), vec2(coords.y, coords.z));\\n        \").concat(t.output, \" = encode_float(x);\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeMatrixProgram {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    this.variableNames = [\"A\"];\n    var r = getGlslDifferences(),\n        [a, s] = t;\n    this.outputShape = e;\n    var o = \"result\";\n    n && (o = \"floor(result * 255. + 0.5)\"), this.userCode = \"\\n      \".concat(getFlatIndexFrom3D(e), \"\\n\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n\\n        int flatIndex = getFlatIndex(coords);\\n        int offset = imod(flatIndex, 4);\\n\\n        flatIndex = idiv(flatIndex, 4, 1.);\\n\\n        int r = flatIndex / \").concat(s, \";\\n        int c = imod(flatIndex, \").concat(s, \");\\n        vec2 uv = (vec2(c, r) + halfCR) / vec2(\").concat(s, \".0, \").concat(a, \".0);\\n        vec4 values = \").concat(r.texture2D, \"(A, uv);\\n\\n        float result;\\n\\n        if(offset == 0) {\\n          result = values[0];\\n        } else if(offset == 1) {\\n          result = values[1];\\n        } else if(offset == 2) {\\n          result = values[2];\\n        } else {\\n          result = values[3];\\n        }\\n\\n        \").concat(r.output, \" = vec4(\").concat(o, \", 0., 0., 0.);\\n      }\\n    \");\n  }\n\n}\n\nclass EncodeMatrixPackedProgram {\n  constructor(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0;\n    var r = getGlslDifferences(),\n        [a, s] = t;\n    this.outputShape = e;\n    var o = \"\",\n        i = \"result\";\n    n && (i = \"floor(result * 255. + 0.5)\");\n\n    for (var _t670 = 0; _t670 <= 1; _t670++) {\n      for (var _n417 = 0; _n417 <= 1; _n417++) {\n        var _i81 = 2 * _t670 + _n417;\n\n        o += \"\\n          localCoords = coords;\\n          if(localCoords[2] + \".concat(_n417, \" < \").concat(e[2], \") {\\n            localCoords[2] += \").concat(_n417, \";\\n            if(localCoords[1] + \").concat(_t670, \" < \").concat(e[1], \") {\\n              localCoords[1] += \").concat(_t670, \";\\n\\n              flatIndex = getFlatIndex(localCoords);\\n              offset = imod(flatIndex, 4);\\n\\n              flatIndex = idiv(flatIndex, 4, 1.);\\n\\n              r = flatIndex / \").concat(s, \";\\n              c = imod(flatIndex, \").concat(s, \");\\n              uv = (vec2(c, r) + halfCR) / vec2(\").concat(s, \".0, \").concat(a, \".0);\\n              values = \").concat(r.texture2D, \"(A, uv);\\n\\n              if(offset == 0) {\\n                result[\").concat(_i81, \"] = values[0];\\n              } else if(offset == 1) {\\n                result[\").concat(_i81, \"] = values[1];\\n              } else if(offset == 2) {\\n                result[\").concat(_i81, \"] = values[2];\\n              } else {\\n                result[\").concat(_i81, \"] = values[3];\\n              }\\n            }\\n          }\\n        \");\n      }\n    }\n\n    this.userCode = \"\\n      \".concat(getFlatIndexFrom3D(e), \"\\n\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n\\n        vec4 result = vec4(0.);\\n        int flatIndex, r, c, offset;\\n        ivec3 localCoords;\\n        vec2 uv;\\n        vec4 values;\\n\\n        \").concat(o, \"\\n\\n        \").concat(r.output, \" = \").concat(i, \";\\n      }\\n    \");\n  }\n\n}\n\nfunction createVertexShader(e) {\n  var t = getGlslDifferences();\n  return createVertexShader$1(e, \"\".concat(t.version, \"\\n    precision highp float;\\n    \").concat(t.attribute, \" vec3 clipSpacePos;\\n    \").concat(t.attribute, \" vec2 uv;\\n    \").concat(t.varyingVs, \" vec2 resultUV;\\n\\n    void main() {\\n      gl_Position = vec4(clipSpacePos, 1);\\n      resultUV = uv;\\n    }\"));\n}\n\nfunction createVertexBuffer(e) {\n  return createStaticVertexBuffer(e, new Float32Array([-1, 1, 0, 0, 1, -1, -1, 0, 0, 0, 1, 1, 0, 1, 1, 1, -1, 0, 1, 0]));\n}\n\nfunction createIndexBuffer(e) {\n  return createStaticIndexBuffer(e, new Uint16Array([0, 1, 2, 2, 1, 3]));\n}\n\nfunction createAndConfigureTexture(e, t, n, r, a, s) {\n  validateTextureSize(t, n);\n  var o = createTexture(e),\n      i = e.TEXTURE_2D;\n  return callAndCheck(e, () => e.bindTexture(i, o)), callAndCheck(e, () => e.texParameteri(i, e.TEXTURE_WRAP_S, e.CLAMP_TO_EDGE)), callAndCheck(e, () => e.texParameteri(i, e.TEXTURE_WRAP_T, e.CLAMP_TO_EDGE)), callAndCheck(e, () => e.texParameteri(i, e.TEXTURE_MIN_FILTER, e.NEAREST)), callAndCheck(e, () => e.texParameteri(i, e.TEXTURE_MAG_FILTER, e.NEAREST)), callAndCheck(e, () => e.texImage2D(i, 0, r, t, n, 0, a, s, null)), callAndCheck(e, () => e.bindTexture(e.TEXTURE_2D, null)), o;\n}\n\nfunction getInternalFormatForFloat32MatrixTexture(e) {\n  return e.internalFormatFloat;\n}\n\nfunction createFloat32MatrixTexture(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight(t, n);\n  return createAndConfigureTexture(e, a, s, getInternalFormatForFloat32MatrixTexture(r), r.textureFormatFloat, e.FLOAT);\n}\n\nfunction getInternalFormatForFloat16MatrixTexture(e) {\n  return e.internalFormatHalfFloat;\n}\n\nfunction createFloat16MatrixTexture(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight(t, n);\n  return createAndConfigureTexture(e, a, s, getInternalFormatForFloat16MatrixTexture(r), r.textureFormatFloat, r.textureTypeHalfFloat);\n}\n\nfunction getInternalFormatForUnsignedBytesMatrixTexture(e) {\n  return e.downloadTextureFormat;\n}\n\nfunction createUnsignedBytesMatrixTexture(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight(t, n);\n  return createAndConfigureTexture(e, a, s, getInternalFormatForUnsignedBytesMatrixTexture(r), e.RGBA, e.UNSIGNED_BYTE);\n}\n\nfunction getInternalFormatForPackedMatrixTexture(e) {\n  return e.internalFormatPackedFloat;\n}\n\nfunction createPackedMatrixTexture(e, t, n, r) {\n  var [a, s] = getPackedMatrixTextureShapeWidthHeight(t, n);\n  return createAndConfigureTexture(e, a, s, getInternalFormatForPackedMatrixTexture(r), e.RGBA, e.FLOAT);\n}\n\nfunction getInternalFormatForFloat16PackedMatrixTexture(e) {\n  return e.internalFormatPackedHalfFloat;\n}\n\nfunction createFloat16PackedMatrixTexture(e, t, n, r) {\n  var [a, s] = getPackedMatrixTextureShapeWidthHeight(t, n);\n  return createAndConfigureTexture(e, a, s, getInternalFormatForFloat16PackedMatrixTexture(r), e.RGBA, r.textureTypeHalfFloat);\n}\n\nfunction bindVertexProgramAttributeStreams(e, t, n) {\n  return callAndCheck(e, () => e.bindBuffer(e.ARRAY_BUFFER, n)), bindVertexBufferToProgramAttribute(e, t, \"clipSpacePos\", n, 3, 20, 0) && bindVertexBufferToProgramAttribute(e, t, \"uv\", n, 2, 20, 12);\n}\n\nfunction uploadDenseMatrixToTexture(e, t, n, r, a, s) {\n  var o, i, l;\n  callAndCheck(e, () => e.bindTexture(e.TEXTURE_2D, t)), a instanceof Uint8Array ? (o = new Uint8Array(n * r * 4), i = e.UNSIGNED_BYTE, l = e.RGBA) : (o = new Float32Array(n * r * 4), i = e.FLOAT, l = s.internalFormatPackedFloat), o.set(a), callAndCheck(e, () => e.texImage2D(e.TEXTURE_2D, 0, l, n, r, 0, e.RGBA, i, o)), callAndCheck(e, () => e.bindTexture(e.TEXTURE_2D, null));\n}\n\nfunction uploadPixelDataToTexture(e, t, n) {\n  callAndCheck(e, () => e.bindTexture(e.TEXTURE_2D, t)), n.data instanceof Uint8Array ? callAndCheck(e, () => e.texImage2D(e.TEXTURE_2D, 0, e.RGBA, n.width, n.height, 0, e.RGBA, e.UNSIGNED_BYTE, n.data)) : callAndCheck(e, () => e.texImage2D(e.TEXTURE_2D, 0, e.RGBA, e.RGBA, e.UNSIGNED_BYTE, n)), callAndCheck(e, () => e.bindTexture(e.TEXTURE_2D, null));\n}\n\nfunction createBufferFromOutputTexture(e, t, n, r) {\n  var a = e.createBuffer();\n  callAndCheck(e, () => e.bindBuffer(e.PIXEL_PACK_BUFFER, a));\n  var s = 16 * t * n;\n  return callAndCheck(e, () => e.bufferData(e.PIXEL_PACK_BUFFER, s, e.STREAM_READ)), callAndCheck(e, () => e.readPixels(0, 0, n, t, e.RGBA, e.FLOAT, 0)), callAndCheck(e, () => e.bindBuffer(e.PIXEL_PACK_BUFFER, null)), a;\n}\n\nfunction downloadFloat32MatrixFromBuffer(e, t, n) {\n  var r = e,\n      a = new Float32Array(n);\n  return r.bindBuffer(r.PIXEL_PACK_BUFFER, t), r.getBufferSubData(r.PIXEL_PACK_BUFFER, 0, a), r.bindBuffer(r.PIXEL_PACK_BUFFER, null), a;\n}\n\nfunction downloadByteEncodedFloatMatrixFromOutputTexture(e, t, n, r) {\n  var [a, s] = getUnpackedMatrixTextureShapeWidthHeight(t, n),\n      o = new Uint8Array(getUnpackedArraySizeFromMatrixSize(t * n, 4));\n  return callAndCheck(e, () => e.readPixels(0, 0, a, s, r.downloadTextureFormat, e.UNSIGNED_BYTE, o)), new Float32Array(o.buffer);\n}\n\nfunction downloadPackedMatrixFromBuffer(e, t, n, r, a, s, o, i) {\n  var l = e,\n      u = new Float32Array(getPackedRGBAArraySizeFromMatrixShape(s, o));\n  return l.bindBuffer(l.PIXEL_PACK_BUFFER, t), l.getBufferSubData(l.PIXEL_PACK_BUFFER, 0, u), l.bindBuffer(l.PIXEL_PACK_BUFFER, null), u;\n}\n\nfunction downloadMatrixFromPackedOutputTexture(e, t, n) {\n  var r = new Float32Array(t * n * 4);\n  return callAndCheck(e, () => e.readPixels(0, 0, n, t, e.RGBA, e.FLOAT, r)), r;\n}\n\nclass GPGPUContext {\n  constructor(e) {\n    this.outputTexture = null, this.program = null, this.disposed = !1, this.vertexAttrsAreBound = !1, this.itemsToPoll = [];\n    var t = env().getNumber(\"WEBGL_VERSION\");\n    null != e ? (this.gl = e, setWebGLContext(t, e)) : this.gl = getWebGLContext(t);\n    var n = \"WEBGL_color_buffer_float\";\n    var r = \"EXT_color_buffer_half_float\";\n\n    if (1 === env().getNumber(\"WEBGL_VERSION\")) {\n      var _e978 = \"OES_texture_half_float\";\n      if (this.textureFloatExtension = getExtensionOrThrow(this.gl, \"OES_texture_float\"), hasExtension(this.gl, _e978)) this.textureHalfFloatExtension = getExtensionOrThrow(this.gl, _e978);else if (env().get(\"WEBGL_FORCE_F16_TEXTURES\")) throw new Error(\"GL context does not support half float textures, yet the environment flag WEBGL_FORCE_F16_TEXTURES is set to true.\");\n      if (this.colorBufferFloatExtension = this.gl.getExtension(n), hasExtension(this.gl, r)) this.colorBufferHalfFloatExtension = getExtensionOrThrow(this.gl, r);else if (env().get(\"WEBGL_FORCE_F16_TEXTURES\")) throw new Error(\"GL context does not support color renderable half floats, yet the environment flag WEBGL_FORCE_F16_TEXTURES is set to true.\");\n    } else if (n = \"EXT_color_buffer_float\", hasExtension(this.gl, n)) this.colorBufferFloatExtension = this.gl.getExtension(n);else {\n      if (!hasExtension(this.gl, r)) throw new Error(\"GL context does not support color renderable floats\");\n      this.colorBufferHalfFloatExtension = this.gl.getExtension(r);\n    }\n\n    this.vertexBuffer = createVertexBuffer(this.gl), this.indexBuffer = createIndexBuffer(this.gl), this.framebuffer = createFramebuffer(this.gl), this.textureConfig = getTextureConfig(this.gl, this.textureHalfFloatExtension);\n  }\n\n  get debug() {\n    return env().getBool(\"DEBUG\");\n  }\n\n  dispose() {\n    if (this.disposed) return;\n    null != this.program && console.warn(\"Disposing a GPGPUContext that still has a bound WebGLProgram. This is probably a resource leak, delete the program with GPGPUContext.deleteProgram before disposing.\"), null != this.outputTexture && console.warn(\"Disposing a GPGPUContext that still has a bound output matrix texture.  This is probably a resource leak, delete the output matrix texture with GPGPUContext.deleteMatrixTexture before disposing.\");\n    var e = this.gl;\n    callAndCheck(e, () => e.finish()), callAndCheck(e, () => e.bindFramebuffer(e.FRAMEBUFFER, null)), callAndCheck(e, () => e.deleteFramebuffer(this.framebuffer)), callAndCheck(e, () => e.bindBuffer(e.ARRAY_BUFFER, null)), callAndCheck(e, () => e.bindBuffer(e.ELEMENT_ARRAY_BUFFER, null)), callAndCheck(e, () => e.deleteBuffer(this.indexBuffer)), this.disposed = !0;\n  }\n\n  createFloat32MatrixTexture(e, t) {\n    return this.throwIfDisposed(), createFloat32MatrixTexture(this.gl, e, t, this.textureConfig);\n  }\n\n  createFloat16MatrixTexture(e, t) {\n    return this.throwIfDisposed(), createFloat16MatrixTexture(this.gl, e, t, this.textureConfig);\n  }\n\n  createUnsignedBytesMatrixTexture(e, t) {\n    return this.throwIfDisposed(), createUnsignedBytesMatrixTexture(this.gl, e, t, this.textureConfig);\n  }\n\n  uploadPixelDataToTexture(e, t) {\n    this.throwIfDisposed(), uploadPixelDataToTexture(this.gl, e, t);\n  }\n\n  uploadDenseMatrixToTexture(e, t, n, r) {\n    this.throwIfDisposed(), uploadDenseMatrixToTexture(this.gl, e, t, n, r, this.textureConfig);\n  }\n\n  createFloat16PackedMatrixTexture(e, t) {\n    return this.throwIfDisposed(), createFloat16PackedMatrixTexture(this.gl, e, t, this.textureConfig);\n  }\n\n  createPackedMatrixTexture(e, t) {\n    return this.throwIfDisposed(), createPackedMatrixTexture(this.gl, e, t, this.textureConfig);\n  }\n\n  deleteMatrixTexture(e) {\n    this.throwIfDisposed(), this.outputTexture === e && (unbindColorTextureFromFramebuffer(this.gl, this.framebuffer), this.outputTexture = null), callAndCheck(this.gl, () => this.gl.deleteTexture(e));\n  }\n\n  downloadByteEncodedFloatMatrixFromOutputTexture(e, t, n) {\n    return this.downloadMatrixDriver(e, () => downloadByteEncodedFloatMatrixFromOutputTexture(this.gl, t, n, this.textureConfig));\n  }\n\n  downloadPackedMatrixFromBuffer(e, t, n, r, a, s) {\n    return downloadPackedMatrixFromBuffer(this.gl, e, t, n, r, a, s);\n  }\n\n  downloadFloat32MatrixFromBuffer(e, t) {\n    return downloadFloat32MatrixFromBuffer(this.gl, e, t);\n  }\n\n  createBufferFromTexture(e, t, n) {\n    this.bindTextureToFrameBuffer(e);\n    var r = createBufferFromOutputTexture(this.gl, t, n);\n    return this.unbindTextureToFrameBuffer(), r;\n  }\n\n  createAndWaitForFence() {\n    var e = this.createFence(this.gl);\n    return this.pollFence(e);\n  }\n\n  createFence(e) {\n    var t, n;\n\n    if (env().getBool(\"WEBGL_FENCE_API_ENABLED\")) {\n      var r = e,\n          a = r.fenceSync(r.SYNC_GPU_COMMANDS_COMPLETE, 0);\n      e.flush(), n = () => {\n        var e = r.clientWaitSync(a, 0, 0);\n        return e === r.ALREADY_SIGNALED || e === r.CONDITION_SATISFIED;\n      }, t = a;\n    } else env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\") > 0 ? (t = this.beginQuery(), this.endQuery(), n = () => this.isQueryAvailable(t, env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\"))) : n = () => !0;\n\n    return {\n      query: t,\n      isFencePassed: n\n    };\n  }\n\n  downloadMatrixFromPackedTexture(e, t, n) {\n    return this.downloadMatrixDriver(e, () => downloadMatrixFromPackedOutputTexture(this.gl, t, n));\n  }\n\n  createProgram(e) {\n    this.throwIfDisposed();\n    var t = this.gl,\n        n = createFragmentShader(t, e);\n    null == this.vertexShader && (this.vertexShader = createVertexShader(t));\n    var r = createProgram(t);\n    return callAndCheck(t, () => t.attachShader(r, this.vertexShader)), callAndCheck(t, () => t.attachShader(r, n)), linkProgram(t, r), this.debug && validateProgram(t, r), this.vertexAttrsAreBound || (this.setProgram(r), this.vertexAttrsAreBound = bindVertexProgramAttributeStreams(t, this.program, this.vertexBuffer)), r;\n  }\n\n  deleteProgram(e) {\n    this.throwIfDisposed(), e === this.program && (this.program = null), null != e && callAndCheck(this.gl, () => this.gl.deleteProgram(e));\n  }\n\n  setProgram(e) {\n    this.throwIfDisposed(), this.program = e, null != this.program && this.debug && validateProgram(this.gl, this.program), callAndCheck(this.gl, () => this.gl.useProgram(e));\n  }\n\n  getUniformLocation(e, t) {\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !0;\n    return this.throwIfDisposed(), n ? getProgramUniformLocationOrThrow(this.gl, e, t) : getProgramUniformLocation(this.gl, e, t);\n  }\n\n  getAttributeLocation(e, t) {\n    return this.throwIfDisposed(), callAndCheck(this.gl, () => this.gl.getAttribLocation(e, t));\n  }\n\n  getUniformLocationNoThrow(e, t) {\n    return this.throwIfDisposed(), this.gl.getUniformLocation(e, t);\n  }\n\n  setInputMatrixTexture(e, t, n) {\n    this.throwIfDisposed(), this.throwIfNoProgram(), bindTextureToProgramUniformSampler(this.gl, e, t, n);\n  }\n\n  setOutputMatrixTexture(e, t, n) {\n    this.setOutputMatrixTextureDriver(e, n, t);\n  }\n\n  setOutputPackedMatrixTexture(e, t, n) {\n    this.throwIfDisposed();\n    var [r, a] = getPackedMatrixTextureShapeWidthHeight(t, n);\n    this.setOutputMatrixTextureDriver(e, r, a);\n  }\n\n  setOutputMatrixWriteRegion(e, t, n, r) {\n    this.setOutputMatrixWriteRegionDriver(n, e, r, t);\n  }\n\n  setOutputPackedMatrixWriteRegion(e, t, n, r) {\n    throw new Error(\"setOutputPackedMatrixWriteRegion not implemented.\");\n  }\n\n  debugValidate() {\n    null != this.program && validateProgram(this.gl, this.program), validateFramebuffer(this.gl);\n  }\n\n  executeProgram() {\n    this.throwIfDisposed(), this.throwIfNoProgram();\n    var e = this.gl;\n    this.debug && this.debugValidate(), callAndCheck(e, () => e.drawElements(e.TRIANGLES, 6, e.UNSIGNED_SHORT, 0));\n  }\n\n  blockUntilAllProgramsCompleted() {\n    this.throwIfDisposed(), callAndCheck(this.gl, () => this.gl.finish());\n  }\n\n  getQueryTimerExtension() {\n    return null == this.disjointQueryTimerExtension && (this.disjointQueryTimerExtension = getExtensionOrThrow(this.gl, 2 === env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\") ? \"EXT_disjoint_timer_query_webgl2\" : \"EXT_disjoint_timer_query\")), this.disjointQueryTimerExtension;\n  }\n\n  getQueryTimerExtensionWebGL2() {\n    return this.getQueryTimerExtension();\n  }\n\n  getQueryTimerExtensionWebGL1() {\n    return this.getQueryTimerExtension();\n  }\n\n  beginQuery() {\n    if (2 === env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\")) {\n      var _e979 = this.gl,\n          _t671 = this.getQueryTimerExtensionWebGL2(),\n          n = _e979.createQuery();\n\n      return _e979.beginQuery(_t671.TIME_ELAPSED_EXT, n), n;\n    }\n\n    var e = this.getQueryTimerExtensionWebGL1(),\n        t = e.createQueryEXT();\n    return e.beginQueryEXT(e.TIME_ELAPSED_EXT, t), t;\n  }\n\n  endQuery() {\n    if (2 === env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\")) {\n      var _e980 = this.gl,\n          t = this.getQueryTimerExtensionWebGL2();\n      return void _e980.endQuery(t.TIME_ELAPSED_EXT);\n    }\n\n    var e = this.getQueryTimerExtensionWebGL1();\n    e.endQueryEXT(e.TIME_ELAPSED_EXT);\n  }\n\n  waitForQueryAndGetTime(e) {\n    var _this215 = this;\n\n    return _asyncToGenerator(function* () {\n      return yield repeatedTry(() => _this215.disposed || _this215.isQueryAvailable(e, env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\"))), _this215.getQueryTime(e, env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_VERSION\"));\n    })();\n  }\n\n  getQueryTime(e, t) {\n    if (0 === t) return null;\n\n    if (2 === t) {\n      var _t672 = this.gl;\n      return _t672.getQueryParameter(e, _t672.QUERY_RESULT) / 1e6;\n    }\n\n    {\n      var _t673 = this.getQueryTimerExtensionWebGL1();\n\n      return _t673.getQueryObjectEXT(e, _t673.QUERY_RESULT_EXT) / 1e6;\n    }\n  }\n\n  isQueryAvailable(e, t) {\n    if (0 === t) return !0;\n\n    if (2 === t) {\n      var _t674 = this.gl,\n          n = this.getQueryTimerExtensionWebGL2(),\n          r = _t674.getQueryParameter(e, _t674.QUERY_RESULT_AVAILABLE);\n\n      return null == this.disjoint && (this.disjoint = this.gl.getParameter(n.GPU_DISJOINT_EXT)), r && !this.disjoint;\n    }\n\n    {\n      var _t675 = this.getQueryTimerExtensionWebGL1(),\n          _n418 = _t675.getQueryObjectEXT(e, _t675.QUERY_RESULT_AVAILABLE_EXT);\n\n      return null == this.disjoint && (this.disjoint = this.gl.getParameter(_t675.GPU_DISJOINT_EXT)), _n418 && !this.disjoint;\n    }\n  }\n\n  pollFence(e) {\n    return new Promise(t => {\n      this.addItemToPoll(() => e.isFencePassed(), () => t());\n    });\n  }\n\n  pollItems() {\n    var e = linearSearchLastTrue(this.itemsToPoll.map(e => e.isDoneFn));\n\n    for (var t = 0; t <= e; ++t) {\n      var {\n        resolveFn: _e981\n      } = this.itemsToPoll[t];\n\n      _e981();\n    }\n\n    this.itemsToPoll = this.itemsToPoll.slice(e + 1);\n  }\n\n  addItemToPoll(e, t) {\n    this.itemsToPoll.push({\n      isDoneFn: e,\n      resolveFn: t\n    }), this.itemsToPoll.length > 1 || repeatedTry(() => (this.pollItems(), 0 === this.itemsToPoll.length));\n  }\n\n  bindTextureToFrameBuffer(e) {\n    this.throwIfDisposed(), bindColorTextureToFramebuffer(this.gl, e, this.framebuffer), this.debug && validateFramebuffer(this.gl);\n  }\n\n  unbindTextureToFrameBuffer() {\n    null != this.outputTexture ? (bindColorTextureToFramebuffer(this.gl, this.outputTexture, this.framebuffer), this.debug && validateFramebuffer(this.gl)) : unbindColorTextureFromFramebuffer(this.gl, this.framebuffer);\n  }\n\n  downloadMatrixDriver(e, t) {\n    this.bindTextureToFrameBuffer(e);\n    var n = t();\n    return this.unbindTextureToFrameBuffer(), n;\n  }\n\n  setOutputMatrixTextureDriver(e, t, n) {\n    this.throwIfDisposed();\n    var r = this.gl;\n    bindColorTextureToFramebuffer(r, e, this.framebuffer), this.debug && validateFramebuffer(r), this.outputTexture = e, callAndCheck(r, () => r.viewport(0, 0, t, n)), callAndCheck(r, () => r.scissor(0, 0, t, n));\n  }\n\n  setOutputMatrixWriteRegionDriver(e, t, n, r) {\n    this.throwIfDisposed(), callAndCheck(this.gl, () => this.gl.scissor(e, t, n, r));\n  }\n\n  throwIfDisposed() {\n    if (this.disposed) throw new Error(\"Attempted to use disposed GPGPUContext.\");\n  }\n\n  throwIfNoProgram() {\n    if (null == this.program) throw new Error(\"No GPU program is currently set.\");\n  }\n\n}\n\nfunction linearSearchLastTrue(e) {\n  var t = 0;\n\n  for (; t < e.length && e[t](); ++t) {\n    ;\n  }\n\n  return t - 1;\n}\n\nvar {\n  getBroadcastDims\n} = backend_util;\n\nfunction makeShader(e, t, n) {\n  var r = [];\n\n  if (e.forEach(e => {\n    var t = sizeFromShape(e.shapeInfo.logicalShape);\n\n    if (e.shapeInfo.isUniform ? r.push(\"uniform float \".concat(e.name).concat(t > 1 ? \"[\".concat(t, \"]\") : \"\", \";\")) : (r.push(\"uniform sampler2D \".concat(e.name, \";\")), r.push(\"uniform int offset\".concat(e.name, \";\"))), n.enableShapeUniforms) {\n      var {\n        uniformShape: _t676\n      } = getUniformInfoFromShape(n.packedInputs, e.shapeInfo.logicalShape, e.shapeInfo.texShape);\n\n      switch (_t676.length) {\n        case 1:\n          r.push(\"uniform int \".concat(e.name, \"Shape;\"));\n          break;\n\n        case 2:\n          r.push(\"uniform ivec2 \".concat(e.name, \"Shape;\"));\n          break;\n\n        case 3:\n          r.push(\"uniform ivec3 \".concat(e.name, \"Shape;\"));\n          break;\n\n        case 4:\n          r.push(\"uniform ivec4 \".concat(e.name, \"Shape;\"));\n      }\n\n      r.push(\"uniform ivec2 \".concat(e.name, \"TexShape;\"));\n    }\n  }), n.enableShapeUniforms) {\n    switch (t.logicalShape.length) {\n      case 1:\n        r.push(\"uniform int outShape;\");\n        break;\n\n      case 2:\n        r.push(\"uniform ivec2 outShape;\"), r.push(\"uniform int outShapeStrides;\");\n        break;\n\n      case 3:\n        r.push(\"uniform ivec3 outShape;\"), r.push(\"uniform ivec2 outShapeStrides;\");\n        break;\n\n      case 4:\n        r.push(\"uniform ivec4 outShape;\"), r.push(\"uniform ivec3 outShapeStrides;\");\n    }\n\n    r.push(\"uniform ivec2 outTexShape;\");\n  }\n\n  n.customUniforms && n.customUniforms.forEach(e => {\n    r.push(\"uniform \".concat(e.type, \" \").concat(e.name).concat(e.arrayIndex ? \"[\".concat(e.arrayIndex, \"]\") : \"\", \";\"));\n  });\n  var a = r.join(\"\\n\"),\n      s = e.map(e => getInputSamplingSnippet(e, t, n.packedInputs, n.enableShapeUniforms)).join(\"\\n\"),\n      o = t.texShape,\n      i = getGlslDifferences(),\n      l = getFloatTextureSampleSnippet(i);\n  var u,\n      c,\n      p = getShaderPrefix(i);\n  return t.isPacked ? (u = getPackedOutputSamplingSnippet(t.logicalShape, o, n.enableShapeUniforms), c = getFloatTextureSetRGBASnippet(i)) : (u = getOutputSamplingSnippet(t.logicalShape, o, n.enableShapeUniforms), c = getFloatTextureSetRSnippet(i)), n.packedInputs && (p += SHADER_PACKED_PREFIX), [p, l, c, a, u, s, n.userCode].join(\"\\n\");\n}\n\nfunction getSamplerFromInInfo(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  var n = e.shapeInfo.logicalShape;\n\n  switch (n.length) {\n    case 0:\n      return getSamplerScalar(e, t);\n\n    case 1:\n      return getSampler1D(e, t);\n\n    case 2:\n      return getSampler2D(e, t);\n\n    case 3:\n      return getSampler3D(e, t);\n\n    case 4:\n      return getSampler4D(e, t);\n\n    case 5:\n      return getSampler5D(e);\n\n    case 6:\n      return getSampler6D(e);\n\n    default:\n      throw new Error(\"\".concat(n.length, \"-D input sampling is not yet supported\"));\n  }\n}\n\nfunction getPackedSamplerFromInInfo(e, t) {\n  switch (e.shapeInfo.logicalShape.length) {\n    case 0:\n      return getPackedSamplerScalar(e);\n\n    case 1:\n      return getPackedSampler1D(e, t);\n\n    case 2:\n      return getPackedSampler2D(e, t);\n\n    case 3:\n      return getPackedSampler3D(e, t);\n\n    default:\n      return getPackedSamplerND(e, t);\n  }\n}\n\nfunction getInputSamplingSnippet(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : !1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  var a = \"\";\n  return a += n ? getPackedSamplerFromInInfo(e, r) : getSamplerFromInInfo(e, r), e.shapeInfo.logicalShape.length <= t.logicalShape.length && (a += n ? getPackedSamplerAtOutputCoords(e, t) : getSamplerAtOutputCoords(e, t)), a;\n}\n\nfunction getPackedOutputSamplingSnippet(e, t, n) {\n  switch (e.length) {\n    case 0:\n      return getOutputScalarCoords();\n\n    case 1:\n      return getOutputPacked1DCoords(e, t, n);\n\n    case 2:\n      return getOutputPacked2DCoords(e, t, n);\n\n    case 3:\n      return getOutputPacked3DCoords(e, t, n);\n\n    default:\n      return getOutputPackedNDCoords(e, t, n);\n  }\n}\n\nfunction getOutputSamplingSnippet(e, t, n) {\n  switch (e.length) {\n    case 0:\n      return getOutputScalarCoords();\n\n    case 1:\n      return getOutput1DCoords(e, t, n);\n\n    case 2:\n      return getOutput2DCoords(e, t, n);\n\n    case 3:\n      return getOutput3DCoords(e, t, n);\n\n    case 4:\n      return getOutput4DCoords(e, t, n);\n\n    case 5:\n      return getOutput5DCoords(e, t);\n\n    case 6:\n      return getOutput6DCoords(e, t);\n\n    default:\n      throw new Error(\"\".concat(e.length, \"-D output sampling is not yet supported\"));\n  }\n}\n\nfunction getFloatTextureSampleSnippet(e) {\n  return \"\\n    float sampleTexture(sampler2D textureSampler, vec2 uv) {\\n      return \".concat(e.texture2D, \"(textureSampler, uv).r;\\n    }\\n  \");\n}\n\nfunction getFloatTextureSetRSnippet(e) {\n  return \"\\n    void setOutput(float val) {\\n      \".concat(e.output, \" = vec4(val, 0, 0, 0);\\n    }\\n  \");\n}\n\nfunction getFloatTextureSetRGBASnippet(e) {\n  return \"\\n    void setOutput(vec4 val) {\\n      \".concat(e.output, \" = val;\\n    }\\n  \");\n}\n\nfunction getShaderPrefix(e) {\n  return \"\".concat(e.version, \"\\n    precision highp float;\\n    precision highp int;\\n    precision highp sampler2D;\\n    \").concat(e.varyingFs, \" vec2 resultUV;\\n    \").concat(e.defineOutput, \"\\n    const vec2 halfCR = vec2(0.5, 0.5);\\n\\n    struct ivec5\\n    {\\n      int x;\\n      int y;\\n      int z;\\n      int w;\\n      int u;\\n    };\\n\\n    struct ivec6\\n    {\\n      int x;\\n      int y;\\n      int z;\\n      int w;\\n      int u;\\n      int v;\\n    };\\n\\n    uniform float NAN;\\n    \").concat(e.defineSpecialNaN, \"\\n    \").concat(e.defineSpecialInf, \"\\n    \").concat(e.defineRound, \"\\n\\n    int imod(int x, int y) {\\n      return x - y * (x / y);\\n    }\\n\\n    int idiv(int a, int b, float sign) {\\n      int res = a / b;\\n      int mod = imod(a, b);\\n      if (sign < 0. && mod != 0) {\\n        res -= 1;\\n      }\\n      return res;\\n    }\\n\\n    //Based on the work of Dave Hoskins\\n    //https://www.shadertoy.com/view/4djSRW\\n    #define HASHSCALE1 443.8975\\n    float random(float seed){\\n      vec2 p = resultUV * seed;\\n      vec3 p3  = fract(vec3(p.xyx) * HASHSCALE1);\\n      p3 += dot(p3, p3.yzx + 19.19);\\n      return fract((p3.x + p3.y) * p3.z);\\n    }\\n\\n    \").concat(SAMPLE_1D_SNIPPET, \"\\n    \").concat(SAMPLE_2D_SNIPPET, \"\\n    \").concat(SAMPLE_3D_SNIPPET, \"\\n  \");\n}\n\nvar SAMPLE_1D_SNIPPET = \"\\nvec2 uvFromFlat(int texNumR, int texNumC, int index) {\\n  int texR = index / texNumC;\\n  int texC = index - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\nvec2 packedUVfrom1D(int texNumR, int texNumC, int index) {\\n  int texelIndex = index / 2;\\n  int texR = texelIndex / texNumC;\\n  int texC = texelIndex - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\n\",\n    SAMPLE_2D_SNIPPET = \"\\nvec2 packedUVfrom2D(int texelsInLogicalRow, int texNumR,\\n  int texNumC, int row, int col) {\\n  int texelIndex = (row / 2) * texelsInLogicalRow + (col / 2);\\n  int texR = texelIndex / texNumC;\\n  int texC = texelIndex - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\n\",\n    SAMPLE_3D_SNIPPET = \"\\nvec2 packedUVfrom3D(int texNumR, int texNumC,\\n    int texelsInBatch, int texelsInLogicalRow, int b,\\n    int row, int col) {\\n  int index = b * texelsInBatch + (row / 2) * texelsInLogicalRow + (col / 2);\\n  int texR = index / texNumC;\\n  int texC = index - texR * texNumC;\\n  return (vec2(texC, texR) + halfCR) / vec2(texNumC, texNumR);\\n}\\n\",\n    SHADER_PACKED_PREFIX = \"\\n  float getChannel(vec4 frag, vec2 innerDims) {\\n    vec2 modCoord = mod(innerDims, 2.);\\n    return modCoord.x == 0. ?\\n      (modCoord.y == 0. ? frag.r : frag.g) :\\n      (modCoord.y == 0. ? frag.b : frag.a);\\n  }\\n  float getChannel(vec4 frag, int dim) {\\n    float modCoord = mod(float(dim), 2.);\\n    return modCoord == 0. ? frag.r : frag.g;\\n  }\\n\";\n\nfunction getOutputScalarCoords() {\n  return \"\\n    int getOutputCoords() {\\n      return 0;\\n    }\\n  \";\n}\n\nfunction getOutputPacked1DCoords(e, t, n) {\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)];\n  return 1 === r[0] ? n ? \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.x * ceil(float(outTexShape[1]) / 2.0));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.x * \".concat(r[1], \".0);\\n      }\\n    \") : 1 === r[1] ? n ? \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.y * ceil(float(outTexShape[0]) / 2.0));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return 2 * int(resultUV.y * \".concat(r[0], \".0);\\n      }\\n    \") : n ? \"\\n    int getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n      return 2 * (resTexRC.x * packedTexShape[1] + resTexRC.y);\\n    }\\n  \" : \"\\n    int getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n      return 2 * (resTexRC.x * \").concat(r[1], \" + resTexRC.y);\\n    }\\n  \");\n}\n\nfunction getOutput1DCoords(e, t, n) {\n  return 1 === t[0] ? n ? \"\\n      int getOutputCoords() {\\n        return int(resultUV.x * float(outTexShape[1]));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return int(resultUV.x * \".concat(t[1], \".0);\\n      }\\n    \") : 1 === t[1] ? n ? \"\\n      int getOutputCoords() {\\n        return int(resultUV.y * float(outTexShape[0]));\\n      }\\n    \" : \"\\n      int getOutputCoords() {\\n        return int(resultUV.y * \".concat(t[0], \".0);\\n      }\\n    \") : n ? \"\\n    int getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(outTexShape[0], outTexShape[1]));\\n      return resTexRC.x * outTexShape[1] + resTexRC.y;\\n    }\\n  \" : \"\\n    int getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      return resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n    }\\n  \");\n}\n\nfunction getOutputPacked3DCoords(e, t, n) {\n  if (n) return \"\\n    ivec3 getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      int texelsInLogicalRow = int(ceil(float(outShape[2]) / 2.0));\\n      int texelsInBatch = texelsInLogicalRow * int(ceil(float(outShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n      int index = resTexRC.x * packedTexShape[1] + resTexRC.y;\\n\\n      int b = index / texelsInBatch;\\n      index -= b * texelsInBatch;\\n\\n      int r = 2 * (index / texelsInLogicalRow);\\n      int c = imod(index, texelsInLogicalRow) * 2;\\n\\n      return ivec3(b, r, c);\\n    }\\n  \";\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)],\n      a = Math.ceil(e[2] / 2),\n      s = a * Math.ceil(e[1] / 2);\n  return \"\\n    ivec3 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n      int index = resTexRC.x * \").concat(r[1], \" + resTexRC.y;\\n\\n      int b = index / \").concat(s, \";\\n      index -= b * \").concat(s, \";\\n\\n      int r = 2 * (index / \").concat(a, \");\\n      int c = imod(index, \").concat(a, \") * 2;\\n\\n      return ivec3(b, r, c);\\n    }\\n  \");\n}\n\nfunction getOutput3DCoords(e, t, n) {\n  if (n) return \"\\n  ivec3 getOutputCoords() {\\n    ivec2 resTexRC = ivec2(resultUV.yx *\\n                           vec2(outTexShape[0], outTexShape[1]));\\n    int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n    \".concat(getLogicalCoordinatesFromFlatIndexByUniform([\"r\", \"c\", \"d\"], e), \"\\n    return ivec3(r, c, d);\\n  }\\n\");\n  var r = getLogicalCoordinatesFromFlatIndex([\"r\", \"c\", \"d\"], e);\n  return \"\\n    ivec3 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n      \").concat(r, \"\\n      return ivec3(r, c, d);\\n    }\\n  \");\n}\n\nfunction getOutputPackedNDCoords(e, t, n) {\n  if (n) return \"\\n    ivec4 getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n      int index = resTexRC.x * packedTexShape[1] + resTexRC.y;\\n\\n      int texelsInLogicalRow = int(ceil(float(outShape[3]) / 2.0));\\n      int texelsInBatch = texelsInLogicalRow * int(ceil(float(outShape[2]) / 2.0));\\n      int texelsInBatchN = texelsInBatch * outShape[1];\\n\\n      int b2 = index / texelsInBatchN;\\n      index -= b2 * texelsInBatchN;\\n\\n      int b = index / texelsInBatch;\\n      index -= b * texelsInBatch;\\n\\n      int r = 2 * (index / texelsInLogicalRow);\\n      int c = imod(index, texelsInLogicalRow) * 2;\\n\\n      return ivec4(b2, b, r, c);\\n    }\\n  \";\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)],\n      a = Math.ceil(e[e.length - 1] / 2),\n      s = a * Math.ceil(e[e.length - 2] / 2);\n  var o = s,\n      i = \"\",\n      l = \"b, r, c\";\n\n  for (var _t677 = 2; _t677 < e.length - 1; _t677++) {\n    o *= e[e.length - _t677 - 1], i = \"\\n      int b\".concat(_t677, \" = index / \").concat(o, \";\\n      index -= b\").concat(_t677, \" * \").concat(o, \";\\n    \") + i, l = \"b\".concat(_t677, \", \") + l;\n  }\n\n  return \"\\n    ivec\".concat(e.length, \" getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\").concat(r[0], \", \").concat(r[1], \"));\\n      int index = resTexRC.x * \").concat(r[1], \" + resTexRC.y;\\n\\n      \").concat(i, \"\\n\\n      int b = index / \").concat(s, \";\\n      index -= b * \").concat(s, \";\\n\\n      int r = 2 * (index / \").concat(a, \");\\n      int c = imod(index, \").concat(a, \") * 2;\\n\\n      return ivec\").concat(e.length, \"(\").concat(l, \");\\n    }\\n  \");\n}\n\nfunction getOutput4DCoords(e, t, n) {\n  if (n) return \"\\n    ivec4 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n        vec2(outTexShape[0], outTexShape[1]));\\n      int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n      \".concat(getLogicalCoordinatesFromFlatIndexByUniform([\"r\", \"c\", \"d\", \"d2\"], e), \"\\n      return ivec4(r, c, d, d2);\\n    }\\n  \");\n  var r = getLogicalCoordinatesFromFlatIndex([\"r\", \"c\", \"d\", \"d2\"], e);\n  return \"\\n    ivec4 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n        vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n      \").concat(r, \"\\n      return ivec4(r, c, d, d2);\\n    }\\n  \");\n}\n\nfunction getOutput5DCoords(e, t) {\n  var n = getLogicalCoordinatesFromFlatIndex([\"r\", \"c\", \"d\", \"d2\", \"d3\"], e);\n  return \"\\n    ivec5 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx * vec2(\".concat(t[0], \",\\n                             \").concat(t[1], \"));\\n\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n\\n      \").concat(n, \"\\n\\n      ivec5 outShape = ivec5(r, c, d, d2, d3);\\n      return outShape;\\n    }\\n  \");\n}\n\nfunction getOutput6DCoords(e, t) {\n  var n = getLogicalCoordinatesFromFlatIndex([\"r\", \"c\", \"d\", \"d2\", \"d3\", \"d4\"], e);\n  return \"\\n    ivec6 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n        vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n\\n      \").concat(n, \"\\n\\n      ivec6 result = ivec6(r, c, d, d2, d3, d4);\\n      return result;\\n    }\\n  \");\n}\n\nfunction getOutputPacked2DCoords(e, t, n) {\n  var r = [Math.ceil(t[0] / 2), Math.ceil(t[1] / 2)];\n  if (arraysEqual(e, t)) return n ? \"\\n      ivec2 getOutputCoords() {\\n        ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n        return 2 * ivec2(resultUV.yx * vec2(packedTexShape[0], packedTexShape[1]));\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        return 2 * ivec2(resultUV.yx * vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n      }\\n    \");\n  var a = Math.ceil(e[1] / 2);\n  return n ? \"\\n    ivec2 getOutputCoords() {\\n      ivec2 packedTexShape = ivec2(ceil(float(outTexShape[0]) / 2.0), ceil(float(outTexShape[1]) / 2.0));\\n      int texelsInLogicalRow = int(ceil(float(outShape[1]) / 2.0));\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(packedTexShape[0], packedTexShape[1]));\\n\\n      int index = resTexRC.x * packedTexShape[1] + resTexRC.y;\\n      int r = 2 * (index / texelsInLogicalRow);\\n      int c = imod(index, texelsInLogicalRow) * 2;\\n\\n      return ivec2(r, c);\\n    }\\n  \" : \"\\n    ivec2 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(r[0], \", \").concat(r[1], \"));\\n\\n      int index = resTexRC.x * \").concat(r[1], \" + resTexRC.y;\\n      int r = 2 * (index / \").concat(a, \");\\n      int c = imod(index, \").concat(a, \") * 2;\\n\\n      return ivec2(r, c);\\n    }\\n  \");\n}\n\nfunction getOutput2DCoords(e, t, n) {\n  return arraysEqual(e, t) ? n ? \"\\n      ivec2 getOutputCoords() {\\n        return ivec2(resultUV.yx * vec2(outTexShape[0], outTexShape[1]));\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        return ivec2(resultUV.yx * vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      }\\n    \") : 1 === e[1] ? n ? \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(outTexShape[0], outTexShape[1]));\\n        int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n        return ivec2(index, 0);\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n        int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n        return ivec2(index, 0);\\n      }\\n    \") : 1 === e[0] ? n ? \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(outTexShape[0], outTexShape[1]));\\n        int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n        return ivec2(0, index);\\n      }\\n    \" : \"\\n      ivec2 getOutputCoords() {\\n        ivec2 resTexRC = ivec2(resultUV.yx *\\n                               vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n        int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n        return ivec2(0, index);\\n      }\\n    \") : n ? \"\\n    ivec2 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(outTexShape[0], outTexShape[1]));\\n      int index = resTexRC.x * outTexShape[1] + resTexRC.y;\\n      int r = index / outShape[1];\\n      int c = index - r * outShape[1];\\n      return ivec2(r, c);\\n    }\\n  \" : \"\\n    ivec2 getOutputCoords() {\\n      ivec2 resTexRC = ivec2(resultUV.yx *\\n                             vec2(\".concat(t[0], \", \").concat(t[1], \"));\\n      int index = resTexRC.x * \").concat(t[1], \" + resTexRC.y;\\n      int r = index / \").concat(e[1], \";\\n      int c = index - r * \").concat(e[1], \";\\n      return ivec2(r, c);\\n    }\\n  \");\n}\n\nfunction getFlatOffsetUniformName(e) {\n  return \"offset\".concat(e);\n}\n\nfunction getPackedSamplerScalar(e) {\n  var t = e.name;\n  return \"\\n    vec4 \".concat(\"get\" + t.charAt(0).toUpperCase() + t.slice(1), \"() {\\n      return \").concat(getGlslDifferences().texture2D, \"(\").concat(t, \", halfCR);\\n    }\\n  \");\n}\n\nfunction getSamplerScalar(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1);\n  if (e.shapeInfo.isUniform) return \"float \".concat(r, \"() {return \").concat(n, \";}\");\n  var [a, s] = e.shapeInfo.texShape;\n  if (1 === a && 1 === s) return \"\\n      float \".concat(r, \"() {\\n        return sampleTexture(\").concat(n, \", halfCR);\\n      }\\n    \");\n  var o = getFlatOffsetUniformName(n);\n  if (t) return \"\\n    float \".concat(r, \"() {\\n      vec2 uv = uvFromFlat(\").concat(n, \"TexShape[0], \").concat(n, \"TexShape[1], \").concat(o, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n  var [i, l] = e.shapeInfo.texShape;\n  return \"\\n    float \".concat(r, \"() {\\n      vec2 uv = uvFromFlat(\").concat(i, \", \").concat(l, \", \").concat(o, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getPackedSampler1D(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      a = e.shapeInfo.texShape,\n      s = getGlslDifferences();\n  if (t) return \"\\n    vec4 \".concat(r, \"(int index) {\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(n, \"TexShape[0]) / 2.0), ceil(float(\").concat(n, \"TexShape[1]) / 2.0));\\n      vec2 uv = packedUVfrom1D(\\n        packedTexShape[0], packedTexShape[1], index);\\n      return \").concat(s.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n  var o = [Math.ceil(a[0] / 2), Math.ceil(a[1] / 2)];\n  return \"\\n    vec4 \".concat(r, \"(int index) {\\n      vec2 uv = packedUVfrom1D(\\n        \").concat(o[0], \", \").concat(o[1], \", index);\\n      return \").concat(s.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler1D(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1);\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(r, \"(int index) {\\n        \").concat(getUniformSampler(e), \"\\n      }\\n    \");\n  var a = e.shapeInfo.texShape,\n      s = a[0],\n      o = a[1];\n  if (1 === o && 1 === s) return \"\\n      float \".concat(r, \"(int index) {\\n        return sampleTexture(\").concat(n, \", halfCR);\\n      }\\n    \");\n  var i = getFlatOffsetUniformName(n);\n  return 1 === o ? t ? \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2(0.5, (float(index + \").concat(i, \") + 0.5) / float(\").concat(n, \"TexShape[0]));\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2(0.5, (float(index + \").concat(i, \") + 0.5) / \").concat(s, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : 1 === s ? t ? \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2((float(index + \").concat(i, \") + 0.5) / float(\").concat(n, \"TexShape[1]), 0.5);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(r, \"(int index) {\\n        vec2 uv = vec2((float(index + \").concat(i, \") + 0.5) / \").concat(o, \".0, 0.5);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : t ? \"\\n    float \".concat(r, \"(int index) {\\n      vec2 uv = uvFromFlat(\").concat(n, \"TexShape[0], \").concat(n, \"TexShape[1], index + \").concat(i, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \") : \"\\n    float \".concat(r, \"(int index) {\\n      vec2 uv = uvFromFlat(\").concat(s, \", \").concat(o, \", index + \").concat(i, \");\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getPackedSampler2D(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = e.shapeInfo.texShape,\n      o = s[0],\n      i = s[1],\n      l = getGlslDifferences();\n  if (null != s && arraysEqual(n, s)) return t ? \"\\n      vec4 \".concat(a, \"(int row, int col) {\\n        vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n\\n        return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n      vec4 \".concat(a, \"(int row, int col) {\\n        vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(i, \".0, \").concat(o, \".0);\\n\\n        return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n      }\\n    \");\n  if (t) return \"\\n    vec4 \".concat(a, \"(int row, int col) {\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(r, \"TexShape[0]) / 2.0), ceil(float(\").concat(r, \"TexShape[1]) / 2.0));\\n      int valuesPerRow = int(ceil(float(\").concat(r, \"Shape[1]) / 2.0));\\n      vec2 uv = packedUVfrom2D(valuesPerRow, packedTexShape[0], packedTexShape[1], row, col);\\n      return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n  var u = [Math.ceil(s[0] / 2), Math.ceil(s[1] / 2)];\n  return \"\\n    vec4 \".concat(a, \"(int row, int col) {\\n      vec2 uv = packedUVfrom2D(\").concat(Math.ceil(n[1] / 2), \", \").concat(u[0], \", \").concat(u[1], \", row, col);\\n      return \").concat(l.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler2D(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = e.shapeInfo.texShape;\n  if (null != s && arraysEqual(n, s)) return t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col) {\\n      vec2 uv = (vec2(col, row) + halfCR) / vec2(\").concat(s[1], \".0, \").concat(s[0], \".0);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \");\n  var {\n    newShape: o,\n    keptDims: i\n  } = squeezeShape(n);\n\n  if (o.length < n.length) {\n    var _n419 = [\"row\", \"col\"];\n    return \"\\n      \".concat(getSamplerFromInInfo(squeezeInputInfo(e, o), t), \"\\n      float \").concat(a, \"(int row, int col) {\\n        return \").concat(a, \"(\").concat(getSqueezedParams(_n419, i), \");\\n      }\\n    \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(a, \"(int row, int col) {\\n        int index = round(dot(vec2(row, col), vec2(\").concat(n[1], \", 1)));\\n        \").concat(getUniformSampler(e), \"\\n      }\\n    \");\n  var l = s[0],\n      u = s[1],\n      c = getFlatOffsetUniformName(r);\n  return 1 === u ? t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(r, \"Shape[1], 1, 1));\\n        vec2 uv = vec2(0.5, (index + 0.5) / float(\").concat(r, \"TexShape[0]));\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col) {\\n      float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(n[1], \", 1, 1));\\n      vec2 uv = vec2(0.5, (index + 0.5) / \").concat(l, \".0);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \") : 1 === l ? t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(r, \"Shape[1], 1, 1));\\n        vec2 uv = vec2((index + 0.5) / float(\").concat(r, \"TexShape[1]), 0.5);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col) {\\n      float index = dot(vec3(row, col, \").concat(c, \"), vec3(\").concat(n[1], \", 1, 1));\\n      vec2 uv = vec2((index + 0.5) / \").concat(u, \".0, 0.5);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \") : t ? \"\\n      float \".concat(a, \"(int row, int col) {\\n        // Explicitly use integer operations as dot() only works on floats.\\n        int index = row * \").concat(r, \"Shape[1] + col + \").concat(c, \";\\n        vec2 uv = uvFromFlat(\").concat(r, \"TexShape[0], \").concat(r, \"TexShape[1], index);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n  float \".concat(a, \"(int row, int col) {\\n    // Explicitly use integer operations as dot() only works on floats.\\n    int index = row * \").concat(n[1], \" + col + \").concat(c, \";\\n    vec2 uv = uvFromFlat(\").concat(l, \", \").concat(u, \", index);\\n    return sampleTexture(\").concat(r, \", uv);\\n  }\\n\");\n}\n\nfunction getPackedSampler3D(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = e.shapeInfo.texShape,\n      o = [Math.ceil(s[0] / 2), Math.ceil(s[1] / 2)];\n\n  if (1 === n[0]) {\n    var _r406 = [1, 2],\n        _s209 = [\"b\", \"row\", \"col\"];\n    return \"\\n        \".concat(getPackedSamplerFromInInfo(squeezeInputInfo(e, n.slice(1)), t), \"\\n        vec4 \").concat(a, \"(int b, int row, int col) {\\n          return \").concat(a, \"(\").concat(getSqueezedParams(_s209, _r406), \");\\n        }\\n      \");\n  }\n\n  var i = getGlslDifferences();\n  if (t) return \"\\n    vec4 \".concat(a, \"(int b, int row, int col) {\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(r, \"TexShape[0]) / 2.0), ceil(float(\").concat(r, \"TexShape[1]) / 2.0));\\n      int valuesPerRow = int(ceil(float(\").concat(r, \"Shape[2]) / 2.0));\\n      int texelsInBatch = valuesPerRow * int(ceil(float(\").concat(r, \"Shape[1]) / 2.0));\\n      vec2 uv = packedUVfrom3D(\\n        packedTexShape[0], packedTexShape[1], texelsInBatch, valuesPerRow, b, row, col);\\n      return \").concat(i.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n  var l = o[0],\n      u = o[1],\n      c = Math.ceil(n[2] / 2);\n  return \"\\n    vec4 \".concat(a, \"(int b, int row, int col) {\\n      vec2 uv = packedUVfrom3D(\\n        \").concat(l, \", \").concat(u, \", \").concat(c * Math.ceil(n[1] / 2), \", \").concat(c, \", b, row, col);\\n      return \").concat(i.texture2D, \"(\").concat(r, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler3D(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = n[1] * n[2],\n      o = n[2],\n      {\n    newShape: i,\n    keptDims: l\n  } = squeezeShape(n);\n\n  if (i.length < n.length) {\n    var _n420 = [\"row\", \"col\", \"depth\"];\n    return \"\\n        \".concat(getSamplerFromInInfo(squeezeInputInfo(e, i), t), \"\\n        float \").concat(a, \"(int row, int col, int depth) {\\n          return \").concat(a, \"(\").concat(getSqueezedParams(_n420, l), \");\\n        }\\n      \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        int index = round(dot(vec3(row, col, depth),\\n                          vec3(\").concat(s, \", \").concat(o, \", 1)));\\n        \").concat(getUniformSampler(e), \"\\n      }\\n    \");\n  var u = e.shapeInfo.texShape,\n      c = u[0],\n      p = u[1],\n      d = e.shapeInfo.flatOffset;\n  if (p === s && null == d) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        int stride1 = \").concat(r, \"Shape[2];\\n        float texR = float(row);\\n        float texC = dot(vec2(col, depth), vec2(stride1, 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n        float \".concat(a, \"(int row, int col, int depth) {\\n          float texR = float(row);\\n          float texC = dot(vec2(col, depth), vec2(\").concat(o, \", 1));\\n          vec2 uv = (vec2(texC, texR) + halfCR) /\\n                     vec2(\").concat(p, \".0, \").concat(c, \".0);\\n          return sampleTexture(\").concat(r, \", uv);\\n        }\\n      \");\n  if (p === o && null == d) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        float texR = dot(vec2(row, col), vec2(\").concat(r, \"Shape[1], 1));\\n        float texC = float(depth);\\n        vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(a, \"(int row, int col, int depth) {\\n      float texR = dot(vec2(row, col), vec2(\").concat(n[1], \", 1));\\n      float texC = float(depth);\\n      vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\").concat(p, \".0, \").concat(c, \".0);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \");\n  var h = getFlatOffsetUniformName(r);\n  return t ? \"\\n    float \".concat(a, \"(int row, int col, int depth) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int stride0 = \").concat(r, \"Shape[1] * \").concat(r, \"Shape[2];\\n      int stride1 = \").concat(r, \"Shape[2];\\n      int index = row * \").concat(s, \" + col * \").concat(o, \" + depth + \").concat(h, \";\\n      vec2 uv = uvFromFlat(\").concat(r, \"TexShape[0], \").concat(r, \"TexShape[1], index);\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n    \") : \"\\n      float \".concat(a, \"(int row, int col, int depth) {\\n        // Explicitly use integer operations as dot() only works on floats.\\n        int index = row * \").concat(s, \" + col * \").concat(o, \" + depth + \").concat(h, \";\\n        vec2 uv = uvFromFlat(\").concat(c, \", \").concat(p, \", index);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n  \");\n}\n\nfunction getPackedSamplerND(e, t) {\n  var n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      a = getGlslDifferences();\n  if (t) return \"\\n    vec4 \".concat(r, \"(int b2, int b, int row, int col) {\\n      int valuesPerRow = int(ceil(float(\").concat(n, \"Shape[3]) / 2.0));\\n      int texelsInBatch = valuesPerRow * int(ceil(float(\").concat(n, \"Shape[2]) / 2.0));\\n      int index = b * texelsInBatch + (row / 2) * valuesPerRow + (col / 2);\\n      texelsInBatch *= \").concat(n, \"Shape[1];\\n      index = b2 * texelsInBatch + index;\\n      ivec2 packedTexShape = ivec2(ceil(float(\").concat(n, \"TexShape[0]) / 2.0), ceil(float(\").concat(n, \"TexShape[1]) / 2.0));\\n      int texR = index / packedTexShape[1];\\n      int texC = index - texR * packedTexShape[1];\\n      vec2 uv = (vec2(texC, texR) + halfCR) / vec2(packedTexShape[1], packedTexShape[0]); return \").concat(a.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n  var s = e.shapeInfo.logicalShape,\n      o = s.length,\n      i = e.shapeInfo.texShape,\n      l = [Math.ceil(i[0] / 2), Math.ceil(i[1] / 2)],\n      u = l[0],\n      c = l[1],\n      p = Math.ceil(s[o - 1] / 2);\n  var d = p * Math.ceil(s[o - 2] / 2),\n      h = \"int b, int row, int col\",\n      m = \"b * \".concat(d, \" + (row / 2) * \").concat(p, \" + (col / 2)\");\n\n  for (var _e982 = 2; _e982 < o - 1; _e982++) {\n    h = \"int b\".concat(_e982, \", \") + h, d *= s[o - _e982 - 1], m = \"b\".concat(_e982, \" * \").concat(d, \" + \") + m;\n  }\n\n  return \"\\n    vec4 \".concat(r, \"(\").concat(h, \") {\\n      int index = \").concat(m, \";\\n      int texR = index / \").concat(c, \";\\n      int texC = index - texR * \").concat(c, \";\\n      vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\").concat(c, \", \").concat(u, \");\\n      return \").concat(a.texture2D, \"(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler4D(e, t) {\n  var n = e.shapeInfo.logicalShape,\n      r = e.name,\n      a = \"get\" + r.charAt(0).toUpperCase() + r.slice(1),\n      s = n[3],\n      o = n[2] * s,\n      i = n[1] * o,\n      {\n    newShape: l,\n    keptDims: u\n  } = squeezeShape(n);\n\n  if (l.length < n.length) {\n    var _n421 = [\"row\", \"col\", \"depth\", \"depth2\"];\n    return \"\\n      \".concat(getSamplerFromInInfo(squeezeInputInfo(e, l), t), \"\\n      float \").concat(a, \"(int row, int col, int depth, int depth2) {\\n        return \").concat(a, \"(\").concat(getSqueezedParams(_n421, u), \");\\n      }\\n    \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        int index = round(dot(vec4(row, col, depth, depth2),\\n                          vec4(\").concat(i, \", \").concat(o, \", \").concat(s, \", 1)));\\n        \").concat(getUniformSampler(e), \"\\n      }\\n    \");\n  var c = e.shapeInfo.flatOffset,\n      p = e.shapeInfo.texShape,\n      d = p[0],\n      h = p[1],\n      m = \"int stride2 = \".concat(r, \"Shape[3];\"),\n      f = \"int stride1 = \".concat(r, \"Shape[2] * stride2;\"),\n      g = \"int stride0 = \".concat(r, \"Shape[1] * stride1;\");\n  if (h === i && null == c) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        \").concat(m, \"\\n        \").concat(f, \"\\n        float texR = float(row);\\n        float texC =\\n            dot(vec3(col, depth, depth2),\\n                vec3(stride1, stride2, 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        float texR = float(row);\\n        float texC =\\n            dot(vec3(col, depth, depth2),\\n                vec3(\").concat(o, \", \").concat(s, \", 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \");\n  if (h === s && null == c) return t ? \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        float texR = dot(vec3(row, col, depth),\\n                         vec3(\").concat(r, \"Shape[1] * \").concat(r, \"Shape[2], \").concat(r, \"Shape[2], 1));\\n        float texC = float(depth2);\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(r, \"TexShape[1], \").concat(r, \"TexShape[0]);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \") : \"\\n      float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n        float texR = dot(vec3(row, col, depth),\\n                         vec3(\").concat(n[1] * n[2], \", \").concat(n[2], \", 1));\\n        float texC = float(depth2);\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(r, \", uv);\\n      }\\n    \");\n  var $ = getFlatOffsetUniformName(r);\n  return t ? \"\\n    float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      \").concat(m, \"\\n      \").concat(f, \"\\n      \").concat(g, \"\\n      int index = row * stride0 + col * stride1 +\\n          depth * stride2 + depth2;\\n      vec2 uv = uvFromFlat(\").concat(r, \"TexShape[0], \").concat(r, \"TexShape[1], index + \").concat($, \");\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \") : \"\\n    float \".concat(a, \"(int row, int col, int depth, int depth2) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int index = row * \").concat(i, \" + col * \").concat(o, \" +\\n          depth * \").concat(s, \" + depth2;\\n      vec2 uv = uvFromFlat(\").concat(d, \", \").concat(h, \", index + \").concat($, \");\\n      return sampleTexture(\").concat(r, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler5D(e) {\n  var t = e.shapeInfo.logicalShape,\n      n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      a = t[4],\n      s = t[3] * a,\n      o = t[2] * s,\n      i = t[1] * o,\n      {\n    newShape: l,\n    keptDims: u\n  } = squeezeShape(t);\n\n  if (l.length < t.length) {\n    var _t678 = [\"row\", \"col\", \"depth\", \"depth2\", \"depth3\"];\n    return \"\\n      \".concat(getSamplerFromInInfo(squeezeInputInfo(e, l)), \"\\n      float \").concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        return \").concat(r, \"(\").concat(getSqueezedParams(_t678, u), \");\\n      }\\n    \");\n  }\n\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        float index = dot(\\n          vec4(row, col, depth, depth2),\\n          vec4(\").concat(i, \", \").concat(o, \", \").concat(s, \", \").concat(a, \")) +\\n          depth3;\\n        \").concat(getUniformSampler(e), \"\\n      }\\n    \");\n  var c = e.shapeInfo.flatOffset,\n      p = e.shapeInfo.texShape,\n      d = p[0],\n      h = p[1];\n  return h === i && null == c ? \"\\n      float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        int texR = row;\\n        float texC = dot(vec4(col, depth, depth2, depth3),\\n                         vec4(\").concat(o, \", \").concat(s, \", \").concat(a, \", 1));\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : h === a && null == c ? \"\\n      float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n        float texR = dot(\\n          vec4(row, col, depth, depth2),\\n          vec4(\").concat(t[1] * t[2] * t[3], \",\\n               \").concat(t[2] * t[3], \", \").concat(t[3], \", 1));\\n        int texC = depth3;\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(h, \".0, \").concat(d, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(r, \"(int row, int col, int depth, int depth2, int depth3) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int index = row * \").concat(i, \" + col * \").concat(o, \" + depth * \").concat(s, \" +\\n          depth2 * \").concat(a, \" + depth3 + \").concat(getFlatOffsetUniformName(n), \";\\n      vec2 uv = uvFromFlat(\").concat(d, \", \").concat(h, \", index);\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getSampler6D(e) {\n  var t = e.shapeInfo.logicalShape,\n      n = e.name,\n      r = \"get\" + n.charAt(0).toUpperCase() + n.slice(1),\n      {\n    newShape: a,\n    keptDims: s\n  } = squeezeShape(t);\n\n  if (a.length < t.length) {\n    var _t679 = [\"row\", \"col\", \"depth\", \"depth2\", \"depth3\", \"depth4\"];\n    return \"\\n      \".concat(getSamplerFromInInfo(squeezeInputInfo(e, a)), \"\\n      float \").concat(r, \"(int row, int col, int depth,\\n                    int depth2, int depth3, int depth4) {\\n        return \").concat(r, \"(\").concat(getSqueezedParams(_t679, s), \");\\n      }\\n    \");\n  }\n\n  var o = t[5],\n      i = t[4] * o,\n      l = t[3] * i,\n      u = t[2] * l,\n      c = t[1] * u;\n  if (e.shapeInfo.isUniform) return \"\\n      float \".concat(r, \"(int row, int col, int depth,\\n                  int depth2, int depth3, int depth4) {\\n        int index = round(dot(\\n          vec4(row, col, depth, depth2),\\n          vec4(\").concat(c, \", \").concat(u, \", \").concat(l, \", \").concat(i, \")) +\\n          dot(\\n            vec2(depth3, depth4),\\n            vec2(\").concat(o, \", 1)));\\n        \").concat(getUniformSampler(e), \"\\n      }\\n    \");\n  var p = e.shapeInfo.flatOffset,\n      d = e.shapeInfo.texShape,\n      h = d[0],\n      m = d[1];\n  return m === c && null == p ? \"\\n      float \".concat(r, \"(int row, int col, int depth,\\n                    int depth2, int depth3, int depth4) {\\n        int texR = row;\\n        float texC = dot(vec4(col, depth, depth2, depth3),\\n          vec4(\").concat(u, \", \").concat(l, \", \").concat(i, \", \").concat(o, \")) +\\n               float(depth4);\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                   vec2(\").concat(m, \".0, \").concat(h, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : m === o && null == p ? \"\\n      float \".concat(r, \"(int row, int col, int depth,\\n                    int depth2, int depth3, int depth4) {\\n        float texR = dot(vec4(row, col, depth, depth2),\\n          vec4(\").concat(t[1] * t[2] * t[3] * t[4], \",\\n               \").concat(t[2] * t[3] * t[4], \",\\n               \").concat(t[3] * t[4], \",\\n               \").concat(t[4], \")) + float(depth3);\\n        int texC = depth4;\\n        vec2 uv = (vec2(texC, texR) + halfCR) /\\n                  vec2(\").concat(m, \".0, \").concat(h, \".0);\\n        return sampleTexture(\").concat(n, \", uv);\\n      }\\n    \") : \"\\n    float \".concat(r, \"(int row, int col, int depth,\\n                  int depth2, int depth3, int depth4) {\\n      // Explicitly use integer operations as dot() only works on floats.\\n      int index = row * \").concat(c, \" + col * \").concat(u, \" + depth * \").concat(l, \" +\\n          depth2 * \").concat(i, \" + depth3 * \").concat(o, \" + depth4 + \").concat(getFlatOffsetUniformName(n), \";\\n      vec2 uv = uvFromFlat(\").concat(h, \", \").concat(m, \", index);\\n      return sampleTexture(\").concat(n, \", uv);\\n    }\\n  \");\n}\n\nfunction getUniformSampler(e) {\n  var t = e.name,\n      n = sizeFromShape(e.shapeInfo.logicalShape);\n  return n < 2 ? \"return \".concat(t, \";\") : \"\\n    for (int i = 0; i < \".concat(n, \"; i++) {\\n      if (i == index) {\\n        return \").concat(t, \"[i];\\n      }\\n    }\\n  \");\n}\n\nfunction getPackedSamplerAtOutputCoords(e, t) {\n  var n = e.name,\n      r = n.charAt(0).toUpperCase() + n.slice(1),\n      a = \"get\" + r + \"AtOutCoords\",\n      s = e.shapeInfo.logicalShape.length,\n      o = t.logicalShape.length,\n      i = getBroadcastDims(e.shapeInfo.logicalShape, t.logicalShape),\n      l = getCoordsDataType(o),\n      u = o - s;\n  var c;\n  var p = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"];\n  c = 0 === s ? \"\" : o < 2 && i.length >= 1 ? \"coords = 0;\" : i.map(e => \"coords.\".concat(p[e + u], \" = 0;\")).join(\"\\n\");\n  var d = \"\";\n  d = o < 2 && s > 0 ? \"coords\" : e.shapeInfo.logicalShape.map((e, t) => \"coords.\".concat(p[t + u])).join(\", \");\n  var h = \"return outputValue;\";\n  var m = 1 === sizeFromShape(e.shapeInfo.logicalShape),\n      f = 1 === sizeFromShape(t.logicalShape);\n\n  if (1 !== s || m || f) {\n    if (m && !f) h = 1 === o ? \"\\n        return vec4(outputValue.x, outputValue.x, 0., 0.);\\n      \" : \"\\n        return vec4(outputValue.x);\\n      \";else if (i.length) {\n      var _e983 = s - 2,\n          _t680 = s - 1;\n\n      i.indexOf(_e983) > -1 && i.indexOf(_t680) > -1 ? h = \"return vec4(outputValue.x);\" : i.indexOf(_e983) > -1 ? h = \"return vec4(outputValue.x, outputValue.y, outputValue.x, outputValue.y);\" : i.indexOf(_t680) > -1 && (h = \"return vec4(outputValue.xx, outputValue.zz);\");\n    }\n  } else h = \"\\n      return vec4(outputValue.xy, outputValue.xy);\\n    \";\n\n  return \"\\n    vec4 \".concat(a, \"() {\\n      \").concat(l, \" coords = getOutputCoords();\\n      \").concat(c, \"\\n      vec4 outputValue = get\").concat(r, \"(\").concat(d, \");\\n      \").concat(h, \"\\n    }\\n  \");\n}\n\nfunction getSamplerAtOutputCoords(e, t) {\n  var n = e.name,\n      r = n.charAt(0).toUpperCase() + n.slice(1),\n      a = \"get\" + r + \"AtOutCoords\",\n      s = e.shapeInfo.logicalShape.length,\n      o = t.logicalShape.length;\n  if (!e.shapeInfo.isUniform && s === o && null == e.shapeInfo.flatOffset && arraysEqual(e.shapeInfo.texShape, t.texShape)) return \"\\n      float \".concat(a, \"() {\\n        return sampleTexture(\").concat(n, \", resultUV);\\n      }\\n    \");\n  var i = getCoordsDataType(o),\n      l = getBroadcastDims(e.shapeInfo.logicalShape, t.logicalShape),\n      u = o - s;\n  var c;\n  var p = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"];\n  c = 0 === s ? \"\" : o < 2 && l.length >= 1 ? \"coords = 0;\" : l.map(e => \"coords.\".concat(p[e + u], \" = 0;\")).join(\"\\n\");\n  var d = \"\";\n  return d = o < 2 && s > 0 ? \"coords\" : e.shapeInfo.logicalShape.map((e, t) => \"coords.\".concat(p[t + u])).join(\", \"), \"\\n    float \".concat(a, \"() {\\n      \").concat(i, \" coords = getOutputCoords();\\n      \").concat(c, \"\\n      return get\").concat(r, \"(\").concat(d, \");\\n    }\\n  \");\n}\n\nfunction getCoordsDataType(e) {\n  if (e <= 1) return \"int\";\n  if (2 === e) return \"ivec2\";\n  if (3 === e) return \"ivec3\";\n  if (4 === e) return \"ivec4\";\n  if (5 === e) return \"ivec5\";\n  if (6 === e) return \"ivec6\";\n  throw Error(\"GPU for rank \".concat(e, \" is not yet supported\"));\n}\n\nfunction getUniformInfoFromShape(e, t, n) {\n  var {\n    newShape: r\n  } = squeezeShape(t),\n      a = t.length,\n      s = e && 3 === a && 1 === t[0],\n      o = s ? t.slice(1) : r,\n      i = !e && a > 1 && !arraysEqual(t, n) && r.length < a || s;\n  return {\n    useSqueezeShape: i,\n    uniformShape: i ? o : t\n  };\n}\n\nfunction squeezeInputInfo(e, t) {\n  var n = JSON.parse(JSON.stringify(e));\n  return n.shapeInfo.logicalShape = t, n;\n}\n\nfunction getSqueezedParams(e, t) {\n  return t.map(t => e[t]).join(\", \");\n}\n\nfunction compileProgram(e, t, n, r) {\n  var a = n.map((e, n) => {\n    var r = {\n      logicalShape: e.shape,\n      texShape: e.isUniform ? null : e.texData.texShape,\n      isUniform: e.isUniform,\n      isPacked: !e.isUniform && e.texData.isPacked,\n      flatOffset: null\n    };\n    return null != e.texData && null != e.texData.slice && e.texData.slice.flatOffset > 0 && (r.flatOffset = e.texData.slice.flatOffset), {\n      name: t.variableNames[n],\n      shapeInfo: r\n    };\n  }),\n      s = a.map(e => e.shapeInfo),\n      o = {\n    logicalShape: r.shape,\n    texShape: r.texData.texShape,\n    isUniform: !1,\n    isPacked: r.texData.isPacked,\n    flatOffset: null\n  },\n      i = makeShader(a, o, t),\n      l = e.createProgram(i);\n  var u = null;\n  var c = e.getUniformLocation(l, \"NAN\", !1);\n  1 === env().getNumber(\"WEBGL_VERSION\") && (u = e.getUniformLocation(l, \"INFINITY\", !1));\n  var p = !1,\n      d = {},\n      h = {},\n      m = {};\n\n  for (var _n422 = 0; _n422 < t.variableNames.length; _n422++) {\n    var _r407 = t.variableNames[_n422];\n    d[_r407] = e.getUniformLocation(l, _r407, p), d[\"offset\".concat(_r407)] = e.getUniformLocation(l, \"offset\".concat(_r407), p), t.enableShapeUniforms && (h[\"\".concat(_r407, \"Shape\")] = e.getUniformLocation(l, \"\".concat(_r407, \"Shape\"), p), m[\"\".concat(_r407, \"TexShape\")] = e.getUniformLocation(l, \"\".concat(_r407, \"TexShape\"), p));\n  }\n\n  var f, g, $;\n  t.enableShapeUniforms && (f = e.getUniformLocation(l, \"outShape\", p), $ = e.getUniformLocation(l, \"outShapeStrides\", p), g = e.getUniformLocation(l, \"outTexShape\", p));\n  var y = [];\n  return t.customUniforms && t.customUniforms.forEach((t, n) => {\n    y[n] = e.getUniformLocation(l, t.name, p);\n  }), {\n    program: t,\n    source: i,\n    webGLProgram: l,\n    uniformLocations: d,\n    customUniformLocations: y,\n    inShapeInfos: s,\n    outShapeInfo: o,\n    infLoc: u,\n    nanLoc: c,\n    inShapesLocations: h,\n    inTexShapesLocations: m,\n    outShapeLocation: f,\n    outShapeStridesLocation: $,\n    outTexShapeLocation: g\n  };\n}\n\nfunction validateBinaryAndProgram(e, t) {\n  if (e.length !== t.length) throw Error(\"Binary was compiled with \".concat(e.length, \" inputs, but was executed with \").concat(t.length, \" inputs\"));\n  e.forEach((e, n) => {\n    var r = e.logicalShape,\n        a = t[n],\n        s = a.shape;\n    if (!arraysEqual(r, s)) throw Error(\"Binary was compiled with different shapes than the current args. Shapes \".concat(r, \" and \").concat(s, \" must match\"));\n    if (e.isUniform && a.isUniform) return;\n    var o = e.texShape,\n        i = a.isUniform ? null : a.texData.texShape;\n    if (!arraysEqual(o, i)) throw Error(\"Binary was compiled with different texture shapes than the current args. Shape \".concat(o, \" and \").concat(i, \" must match\"));\n  });\n}\n\nfunction runProgram(e, t, n, r, a) {\n  t.program.enableShapeUniforms || (validateBinaryAndProgram(t.inShapeInfos, n), validateBinaryAndProgram([t.outShapeInfo], [r]));\n  var s = r.texData.texture,\n      o = r.texData.texShape;\n  r.texData.isPacked ? e.setOutputPackedMatrixTexture(s, o[0], o[1]) : e.setOutputMatrixTexture(s, o[0], o[1]), e.setProgram(t.webGLProgram), 1 === env().getNumber(\"WEBGL_VERSION\") && null !== t.infLoc && e.gl.uniform1f(t.infLoc, Infinity), null !== t.nanLoc && e.gl.uniform1f(t.nanLoc, NaN), n.forEach((n, r) => {\n    var a = t.program.variableNames[r],\n        s = t.uniformLocations[a],\n        o = t.uniformLocations[\"offset\".concat(a)],\n        i = t.inShapesLocations[\"\".concat(a, \"Shape\")],\n        l = t.inTexShapesLocations[\"\".concat(a, \"TexShape\")];\n\n    if (i) {\n      var {\n        uniformShape: _r408\n      } = getUniformInfoFromShape(t.program.packedInputs, n.shape, n.texData.texShape);\n\n      switch (_r408.length) {\n        case 1:\n          e.gl.uniform1iv(i, new Int32Array(_r408));\n          break;\n\n        case 2:\n          e.gl.uniform2iv(i, new Int32Array(_r408));\n          break;\n\n        case 3:\n          e.gl.uniform3iv(i, new Int32Array(_r408));\n          break;\n\n        case 4:\n          e.gl.uniform4iv(i, new Int32Array(_r408));\n      }\n    }\n\n    if (l && e.gl.uniform2i(l, n.texData.texShape[0], n.texData.texShape[1]), null != s) if (n.isUniform) {\n      if (sizeFromShape(n.shape) < 2) e.gl.uniform1f(s, n.uniformValues[0]);else {\n        var _t681 = n.uniformValues;\n        _t681 instanceof Float32Array || (_t681 = new Float32Array(_t681)), e.gl.uniform1fv(s, _t681);\n      }\n    } else null != n.texData.slice && null != o && e.gl.uniform1i(o, n.texData.slice.flatOffset), e.setInputMatrixTexture(n.texData.texture, s, r);\n  });\n  var i = t.outShapeLocation;\n  if (i) switch (r.shape.length) {\n    case 1:\n      e.gl.uniform1iv(i, new Int32Array(r.shape));\n      break;\n\n    case 2:\n      e.gl.uniform2iv(i, new Int32Array(r.shape));\n      break;\n\n    case 3:\n      e.gl.uniform3iv(i, new Int32Array(r.shape));\n      break;\n\n    case 4:\n      e.gl.uniform4iv(i, new Int32Array(r.shape));\n  }\n\n  if (t.outShapeStridesLocation) {\n    var _n423 = computeStrides(r.shape);\n\n    switch (r.shape.length) {\n      case 2:\n        e.gl.uniform1iv(t.outShapeStridesLocation, new Int32Array(_n423));\n        break;\n\n      case 3:\n        e.gl.uniform2iv(t.outShapeStridesLocation, new Int32Array(_n423));\n        break;\n\n      case 4:\n        e.gl.uniform3iv(t.outShapeStridesLocation, new Int32Array(_n423));\n    }\n  }\n\n  t.outTexShapeLocation && e.gl.uniform2i(t.outTexShapeLocation, r.texData.texShape[0], r.texData.texShape[1]), t.program.customUniforms && a && t.program.customUniforms.forEach((n, r) => {\n    var s = t.customUniformLocations[r],\n        o = a[r];\n    if (\"float\" === n.type) e.gl.uniform1fv(s, o);else if (\"vec2\" === n.type) e.gl.uniform2fv(s, o);else if (\"vec3\" === n.type) e.gl.uniform3fv(s, o);else if (\"vec4\" === n.type) e.gl.uniform4fv(s, o);else if (\"int\" === n.type) e.gl.uniform1iv(s, o);else if (\"ivec2\" === n.type) e.gl.uniform2iv(s, o);else if (\"ivec3\" === n.type) e.gl.uniform3iv(s, o);else {\n      if (\"ivec4\" !== n.type) throw Error(\"uniform type \".concat(n.type, \" is not supported yet.\"));\n      e.gl.uniform4iv(s, o);\n    }\n  }), e.executeProgram();\n}\n\nfunction makeShaderKey(e, t, n) {\n  var r = \"\";\n  t.concat(n).forEach(t => {\n    var a = null != t.texData && null != t.texData.slice && t.texData.slice.flatOffset > 0;\n\n    if (e.enableShapeUniforms && !t.isUniform) {\n      var s = t.texData.texShape,\n          {\n        useSqueezeShape: o,\n        uniformShape: i\n      } = getUniformInfoFromShape(e.packedInputs, t.shape, s);\n      var l = \"\",\n          u = \"\",\n          c = \"\";\n\n      if (1 === i.length && e.packedInputs) {\n        var _e984 = [Math.ceil(s[0] / 2), Math.ceil(s[1] / 2)];\n        l = \"\".concat(_e984[0] > 1, \"_\").concat(_e984[1] > 1);\n      } else if (2 !== i.length || e.packedInputs) {\n        if (i.length > 2 && !e.packedInputs) {\n          var _e985 = computeStrides(i);\n\n          c = \"\".concat(_e985[0] === s[1], \"_\").concat(_e985[_e985.length - 1] === s[1]);\n        }\n      } else u = \"\".concat(i[0] > 1, \"_\").concat(i[1] > 1);\n\n      var _p35 = t.shape.length,\n          d = 2 === _p35 && arraysEqual(t.shape, s),\n          h = 1 === sizeFromShape(t.shape),\n          m = getBroadcastDims$1(t.shape, n.shape),\n          f = !e.packedInputs && _p35 === n.shape.length && arraysEqual(s, n.texData.texShape);\n      r += \"\".concat(_p35, \"_\").concat(f, \"_\").concat(o, \"_\").concat(i.length, \"_\").concat(h, \"_\").concat(m, \"_\").concat(d, \"_\").concat(l, \"_\").concat(u, \"_\").concat(c, \"_\").concat(e.packedInputs || _p35 > 2 ? \"\" : \"\".concat(s[0] > 1, \"_\").concat(s[1] > 1), \"_\").concat(a);\n    } else r += \"\".concat(t.shape, \"_\").concat(t.isUniform ? \"uniform\" : t.texData.texShape, \"_\").concat(a);\n  });\n  var a = e.constructor.name;\n  return a += \"_\" + r + \"_\" + e.userCode + \"\".concat(env().getNumber(\"WEBGL_VERSION\")), a;\n}\n\nfunction useShapeUniforms(e) {\n  return env().getBool(\"WEBGL_USE_SHAPES_UNIFORMS\") && e <= 4;\n}\n\nvar {\n  addImpl: addImplCPU,\n  bincountImpl: bincountImplCPU,\n  bincountReduceImpl: bincountReduceImplCPU,\n  ceilImpl: ceilImplCPU,\n  concatImpl: concatImplCPU,\n  equalImpl: equalImplCPU,\n  expImpl: expImplCPU,\n  expm1Impl: expm1ImplCPU,\n  floorImpl: floorImplCPU,\n  gatherNdImpl: gatherNdImplCPU,\n  gatherV2Impl: gatherV2ImplCPU,\n  greaterImpl: greaterImplCPU,\n  greaterEqualImpl: greaterEqualImplCPU,\n  lessImpl: lessImplCPU,\n  lessEqualImpl: lessEqualImplCPU,\n  linSpaceImpl: linSpaceImplCPU,\n  logImpl: logImplCPU,\n  maxImpl: maxImplCPU,\n  maximumImpl: maximumImplCPU,\n  minimumImpl: minimumImplCPU,\n  multiplyImpl: multiplyImplCPU,\n  negImpl: negImplCPU,\n  notEqualImpl: notEqualImplCPU,\n  prodImpl: prodImplCPU,\n  rangeImpl: rangeImplCPU,\n  rsqrtImpl: rsqrtImplCPU,\n  simpleAbsImpl: simpleAbsImplCPU,\n  sliceImpl: sliceImplCPU,\n  sparseFillEmptyRowsImpl: sparseFillEmptyRowsImplCPU,\n  sparseReshapeImpl: sparseReshapeImplCPU,\n  sparseSegmentReductionImpl: sparseSegmentReductionImplCPU,\n  stridedSliceImpl: stridedSliceImplCPU,\n  stringNGramsImpl: stringNGramsImplCPU,\n  stringSplitImpl: stringSplitImplCPU,\n  stringToHashBucketFastImpl: stringToHashBucketFastImplCPU,\n  subImpl: subImplCPU,\n  tileImpl: tileImplCPU,\n  topKImpl: topKImplCPU,\n  transposeImpl: transposeImplCPU,\n  uniqueImpl: uniqueImplCPU\n} = shared;\n\nfunction getVecChannels(e, t) {\n  return [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"].slice(0, t).map(t => \"\".concat(e, \".\").concat(t));\n}\n\nfunction getChannels(e, t) {\n  return 1 === t ? [e] : getVecChannels(e, t);\n}\n\nfunction getSourceCoords$2(e, t) {\n  if (1 === e) return \"rc\";\n  var n = \"\";\n\n  for (var r = 0; r < e; r++) {\n    n += t[r], r < e - 1 && (n += \",\");\n  }\n\n  return n;\n}\n\nclass PackProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0, this.outputShape = e;\n    var t = e.length;\n    if (0 === t) this.userCode = \"\\n        void main() {\\n          setOutput(vec4(getA(), 0., 0., 0.));\\n        }\\n      \";else {\n      var n = getChannels(\"rc\", t),\n          r = getCoordsDataType(t),\n          a = getOutOfBoundsCondition(t, e, n),\n          s = getSetup(t, e[e.length - 1], e[e.length - 2], n),\n          o = getOutput(e, n);\n      this.userCode = \"\\n        void main() {\\n          \".concat(r, \" rc = getOutputCoords();\\n\\n          if(\").concat(a, \") {\\n            setOutput(vec4(0));\\n          } else {\\n            \").concat(s, \"\\n\\n            setOutput(vec4(\").concat(o, \"));\\n          }\\n        }\\n      \");\n    }\n  }\n\n}\n\nfunction getSourceCoordsArr(e, t) {\n  var n = [];\n\n  for (var r = 0; r <= 1; r++) {\n    for (var a = 0; a <= 1; a++) {\n      var s = \"\".concat(0 === r ? \"r\" : \"rp1\", \", \").concat(0 === a ? \"c\" : \"cp1\");\n\n      for (var _n424 = 2; _n424 < e; _n424++) {\n        s = \"\".concat(t[t.length - 1 - _n424], \",\") + s;\n      }\n\n      n.push(s);\n    }\n  }\n\n  return n;\n}\n\nfunction getOutOfBoundsCondition(e, t, n) {\n  if (1 === e) return \"rc > \".concat(t[0]);\n  var r = \"\";\n\n  for (var a = e - 2; a < e; a++) {\n    r += \"\".concat(n[a], \" >= \").concat(t[a]), a < e - 1 && (r += \"||\");\n  }\n\n  return r;\n}\n\nfunction getSetup(e, t, n, r) {\n  if (1 === e) return \"\";\n  var a = r.slice(-2);\n  return \"\\n    int r = \".concat(a[0], \";\\n    int c = \").concat(a[1], \";\\n    int rp1 = r + 1;\\n    int cp1 = c + 1;\\n\\n    bool cEdge = cp1 >= \").concat(t, \";\\n    bool rEdge = rp1 >= \").concat(n, \";\\n  \");\n}\n\nfunction getOutput(e, t) {\n  var n = e.length,\n      r = getSourceCoordsArr(n, t);\n  return 1 === n ? \"getA(rc),\\n            rc + 1 >= \".concat(e[0], \" ? 0. : getA(rc + 1),\\n            0, 0\") : \"getA(\".concat(r[0], \"),\\n          cEdge ? 0. : getA(\").concat(r[1], \"),\\n          rEdge ? 0. : getA(\").concat(r[2], \"),\\n          rEdge || cEdge ? 0. : getA(\").concat(r[3], \")\");\n}\n\nclass ReshapePackedProgram {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e;\n    var n = \"\";\n\n    for (var _e986 = 0; _e986 < 4; _e986++) {\n      var _t682 = \"thisRC = rc;\";\n      _e986 % 2 == 1 && (_t682 += \"thisRC.z += 1;\"), _e986 > 1 && (_t682 += \"thisRC.y += 1;\"), n += \"\\n        \".concat(_t682, \"\\n        \").concat(_e986 > 0 ? \"if(thisRC.y < rows && thisRC.z < cols){\" : \"\", \"\\n          int flatIndex = getFlatIndex(thisRC);\\n\\n          ivec3 inputRC = inputCoordsFromReshapedOutCoords(flatIndex);\\n          vec2 inputRCInnerDims = vec2(float(inputRC.y),float(inputRC.z));\\n\\n          result[\").concat(_e986, \"] =\\n            getChannel(getA(inputRC.x, inputRC.y, inputRC.z), inputRCInnerDims);\\n        \").concat(_e986 > 0 ? \"}\" : \"\", \"\\n      \");\n    }\n\n    this.userCode = \"\\n      \".concat(getReshapedInputCoords(t), \"\\n      \").concat(getFlatIndexFrom3D(e), \"\\n\\n      void main() {\\n        ivec3 rc = getOutputCoords();\\n\\n        vec4 result = vec4(0.);\\n\\n        ivec3 thisRC;\\n        int rows = \").concat(e[1], \";\\n        int cols = \").concat(e[2], \";\\n\\n        \").concat(n, \"\\n\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction getReshapedInputCoords(e) {\n  return \"\\n    ivec3 inputCoordsFromReshapedOutCoords(int index) {\\n      \".concat(getLogicalCoordinatesFromFlatIndex([\"r\", \"c\", \"d\"], e), \"\\n      return ivec3(r, c, d);\\n    }\\n  \");\n}\n\nclass TextureManager {\n  constructor(e) {\n    this.gpgpu = e, this.numUsedTextures = 0, this.numFreeTextures = 0, this._numBytesAllocated = 0, this._numBytesFree = 0, this.freeTextures = {}, this.logEnabled = !1, this.usedTextures = {};\n  }\n\n  acquireTexture(e, t, n) {\n    var r = getPhysicalFromLogicalTextureType(t, n),\n        a = getKeyFromTextureShape(e, r, n);\n    a in this.freeTextures || (this.freeTextures[a] = []), a in this.usedTextures || (this.usedTextures[a] = []);\n    var s = computeBytes(e, r, this.gpgpu.gl, this.gpgpu.textureConfig, n);\n\n    if (this.freeTextures[a].length > 0) {\n      this.numFreeTextures--, this.numUsedTextures++, this._numBytesFree -= s, this.log();\n\n      var _e987 = this.freeTextures[a].shift();\n\n      return this.usedTextures[a].push(_e987), _e987;\n    }\n\n    var o;\n    return r === PhysicalTextureType.PACKED_2X2_FLOAT32 ? o = this.gpgpu.createPackedMatrixTexture(e[0], e[1]) : r === PhysicalTextureType.PACKED_2X2_FLOAT16 ? o = this.gpgpu.createFloat16PackedMatrixTexture(e[0], e[1]) : r === PhysicalTextureType.UNPACKED_FLOAT32 ? o = this.gpgpu.createFloat32MatrixTexture(e[0], e[1]) : r === PhysicalTextureType.UNPACKED_FLOAT16 ? o = this.gpgpu.createFloat16MatrixTexture(e[0], e[1]) : r === PhysicalTextureType.PACKED_4X1_UNSIGNED_BYTE && (o = this.gpgpu.createUnsignedBytesMatrixTexture(e[0], e[1])), this.usedTextures[a].push(o), this.numUsedTextures++, this._numBytesAllocated += s, this.log(), o;\n  }\n\n  releaseTexture(e, t, n, r) {\n    if (null == this.freeTextures) return;\n    var a = getPhysicalFromLogicalTextureType(n, r),\n        s = getKeyFromTextureShape(t, a, r);\n    s in this.freeTextures || (this.freeTextures[s] = []);\n    var o = computeBytes(t, a, this.gpgpu.gl, this.gpgpu.textureConfig, r),\n        i = env().get(\"WEBGL_DELETE_TEXTURE_THRESHOLD\");\n    -1 !== i && this._numBytesAllocated > i ? (this.gpgpu.deleteMatrixTexture(e), this._numBytesAllocated -= o) : (this.freeTextures[s].push(e), this.numFreeTextures++, this._numBytesFree += o), this.numUsedTextures--;\n    var l = this.usedTextures[s],\n        u = l.indexOf(e);\n    if (u < 0) throw new Error(\"Cannot release a texture that was never provided by this texture manager\");\n    l.splice(u, 1), this.log();\n  }\n\n  log() {\n    if (!this.logEnabled) return;\n    console.log(\"Free/Used\", \"\".concat(this.numFreeTextures, \" / \").concat(this.numUsedTextures), \"(\".concat(this.numFreeTextures + this.numUsedTextures, \")\"));\n    var e = this._numBytesFree / this._numBytesAllocated;\n    console.log(\"Bytes allocated: \".concat(this._numBytesAllocated)), console.log(\"Bytes unused: \".concat(this._numBytesFree, \" (\").concat(Math.round(100 * e), \"%)\"));\n  }\n\n  get numBytesAllocated() {\n    return this._numBytesAllocated;\n  }\n\n  get numBytesFree() {\n    return this._numBytesFree;\n  }\n\n  getNumUsedTextures() {\n    return this.numUsedTextures;\n  }\n\n  getNumFreeTextures() {\n    return this.numFreeTextures;\n  }\n\n  dispose() {\n    if (null != this.freeTextures) {\n      for (var _e988 in this.freeTextures) {\n        this.freeTextures[_e988].forEach(e => {\n          this.gpgpu.deleteMatrixTexture(e);\n        });\n      }\n\n      for (var _e989 in this.usedTextures) {\n        this.usedTextures[_e989].forEach(e => {\n          this.gpgpu.deleteMatrixTexture(e);\n        });\n      }\n\n      this.freeTextures = null, this.usedTextures = null, this.numUsedTextures = 0, this.numFreeTextures = 0, this._numBytesAllocated = 0, this._numBytesFree = 0;\n    }\n  }\n\n}\n\nfunction numBytesForInternalFormat(e, t) {\n  if (t === e.R32F) return 4;\n  if (t === e.R16F) return 2;\n  if (t === e.RGBA32F) return 16;\n  if (t === e.RGBA) return 16;\n  if (t === e.RGBA16F) return 8;\n  throw new Error(\"Unknown internal format \".concat(t));\n}\n\nfunction computeBytes(e, t, n, r, a) {\n  var s = internalFormatForPhysicalTexType(t, r);\n  var o;\n\n  if (a) {\n    var [_t683, _n425] = getPackedMatrixTextureShapeWidthHeight(e[0], e[1]);\n    o = _t683 * _n425;\n  } else {\n    var [_t684, _n426] = getUnpackedMatrixTextureShapeWidthHeight(e[0], e[1]);\n    o = _t684 * _n426;\n  }\n\n  return o * numBytesForInternalFormat(n, s);\n}\n\nfunction internalFormatForPhysicalTexType(e, t) {\n  switch (e) {\n    case PhysicalTextureType.PACKED_2X2_FLOAT32:\n      return getInternalFormatForPackedMatrixTexture(t);\n\n    case PhysicalTextureType.PACKED_2X2_FLOAT16:\n      return getInternalFormatForFloat16PackedMatrixTexture(t);\n\n    case PhysicalTextureType.UNPACKED_FLOAT32:\n      return getInternalFormatForFloat32MatrixTexture(t);\n\n    case PhysicalTextureType.UNPACKED_FLOAT16:\n      return getInternalFormatForFloat16MatrixTexture(t);\n\n    case PhysicalTextureType.PACKED_4X1_UNSIGNED_BYTE:\n      return getInternalFormatForUnsignedBytesMatrixTexture(t);\n\n    default:\n      throw new Error(\"Unknown physical texture type \".concat(e));\n  }\n}\n\nfunction getPhysicalTextureForRendering(e) {\n  return env().getBool(\"WEBGL_RENDER_FLOAT32_ENABLED\") ? e ? PhysicalTextureType.PACKED_2X2_FLOAT32 : PhysicalTextureType.UNPACKED_FLOAT32 : e ? PhysicalTextureType.PACKED_2X2_FLOAT16 : PhysicalTextureType.UNPACKED_FLOAT16;\n}\n\nfunction getPhysicalFromLogicalTextureType(e, t) {\n  if (e === TextureUsage.UPLOAD) return PhysicalTextureType.PACKED_2X2_FLOAT32;\n  if (e === TextureUsage.RENDER || null == e) return getPhysicalTextureForRendering(t);\n  if (e === TextureUsage.DOWNLOAD || e === TextureUsage.PIXELS) return PhysicalTextureType.PACKED_4X1_UNSIGNED_BYTE;\n  throw new Error(\"Unknown logical texture type \".concat(e));\n}\n\nfunction getKeyFromTextureShape(e, t, n) {\n  return \"\".concat(e[0], \"_\").concat(e[1], \"_\").concat(t, \"_\").concat(n);\n}\n\nclass UnaryOpProgram {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.outputShape = e, this.enableShapeUniforms = useShapeUniforms(this.outputShape.length), this.userCode = \"\\n      float unaryOperation(float x) {\\n        \".concat(t, \"\\n      }\\n\\n      void main() {\\n        float x = getAAtOutCoords();\\n        float y = unaryOperation(x);\\n\\n        setOutput(y);\\n      }\\n    \");\n  }\n\n}\n\nvar CHECK_NAN_SNIPPET$2 = \"if (isnan(x)) return x;\",\n    LINEAR$1 = \"return x;\",\n    ABS$1 = \"return abs(x);\",\n    ELU$2 = \"return (x >= 0.0) ? x : (exp(x) - 1.0);\",\n    RELU$2 = CHECK_NAN_SNIPPET$2 + \"\\n  return (x < 0.0) ? 0.0 : x;\\n\",\n    RELU6$2 = CHECK_NAN_SNIPPET$2 + \"\\n  return (x < 0.0) ? 0.0 : min(6.0, x);\\n\",\n    CLONE = \"return x;\",\n    SIGMOID$2 = \"return 1.0 / (1.0 + exp(-1.0 * x));\",\n    LINEAR = \"return x;\",\n    ELU$1 = \"\\n  vec4 result;\\n\\n  result.r = (x.r >= 0.0) ? x.r : (exp(x.r) - 1.0);\\n  result.g = (x.g >= 0.0) ? x.g : (exp(x.g) - 1.0);\\n  result.b = (x.b >= 0.0) ? x.b : (exp(x.b) - 1.0);\\n  result.a = (x.a >= 0.0) ? x.a : (exp(x.a) - 1.0);\\n\\n  return result;\\n\",\n    RELU$1 = \"\\n  vec4 result = x * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    RELU6$1 = \"\\n  vec4 result = min(x, vec4(6.)) * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    SIGMOID$1 = \"return 1.0 / (1.0 + exp(-1.0 * x));\";\n\nclass UnaryOpPackedProgram {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e, this.enableShapeUniforms = useShapeUniforms(this.outputShape.length), this.userCode = \"\\n      vec4 unaryOperation(vec4 x) {\\n        \".concat(t, \"\\n      }\\n\\n      void main() {\\n        vec4 x = getAAtOutCoords();\\n        vec4 y = unaryOperation(x);\\n\\n        setOutput(y);\\n      }\\n    \");\n  }\n\n}\n\nclass UnpackProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !1, this.outputShape = e;\n    var t = e.length,\n        n = getChannels(\"rc\", t),\n        r = getCoordsDataType(t),\n        a = getSourceCoords$2(t, n),\n        s = n.slice(-2),\n        o = t <= 1 ? \"rc\" : \"vec2(\".concat(s.join(\",\"), \")\");\n    this.userCode = \"\\n      void main() {\\n        \".concat(r, \" rc = getOutputCoords();\\n        vec4 packedInput = getA(\").concat(a, \");\\n\\n        setOutput(getChannel(packedInput, \").concat(o, \"));\\n      }\\n    \");\n  }\n\n}\n\nvar whereImpl = whereImpl$2,\n    EPSILON_FLOAT32 = 1e-7,\n    EPSILON_FLOAT16 = 1e-4,\n    binaryCaches = {};\n\nfunction getBinaryCache(e) {\n  return e in binaryCaches || (binaryCaches[e] = {}), binaryCaches[e];\n}\n\nvar CPU_HANDOFF_SIZE_THRESHOLD = env().getNumber(\"CPU_HANDOFF_SIZE_THRESHOLD\"),\n    BEFORE_PAGING_CONSTANT = 600;\n\nfunction numMBBeforeWarning() {\n  return null == env().global.screen ? 1024 : env().global.screen.height * env().global.screen.width * window.devicePixelRatio * BEFORE_PAGING_CONSTANT / 1024 / 1024;\n}\n\nclass MathBackendWebGL extends KernelBackend {\n  constructor(e) {\n    if (super(), this.pendingRead = new WeakMap(), this.pendingDisposal = new WeakSet(), this.dataRefCount = new WeakMap(), this.numBytesInGPU = 0, this.uploadWaitMs = 0, this.downloadWaitMs = 0, this.lastGlFlushTime = 0, this.warnedAboutMemory = !1, this.pendingDeletes = 0, this.disposed = !1, !env().getBool(\"HAS_WEBGL\")) throw new Error(\"WebGL is not supported on this device\");\n\n    if (null == e) {\n      var _e990 = getWebGLContext(env().getNumber(\"WEBGL_VERSION\"));\n\n      this.binaryCache = getBinaryCache(env().getNumber(\"WEBGL_VERSION\")), this.gpgpu = new GPGPUContext(_e990), this.canvas = _e990.canvas, this.gpgpuCreatedLocally = !0;\n    } else this.gpgpu = e, this.binaryCache = {}, this.gpgpuCreatedLocally = !1, this.canvas = e.gl.canvas;\n\n    this.textureManager = new TextureManager(this.gpgpu), this.numMBBeforeWarning = numMBBeforeWarning(), this.texData = new DataStorage(this, engine());\n  }\n\n  nextDataId() {\n    return MathBackendWebGL.nextDataId++;\n  }\n\n  numDataIds() {\n    return this.texData.numDataIds() - this.pendingDeletes;\n  }\n\n  write(e, t, n) {\n    if ((env().getBool(\"WEBGL_CHECK_NUMERICAL_PROBLEMS\") || env().getBool(\"DEBUG\")) && this.checkNumericalProblems(e), \"complex64\" === n && null != e) throw new Error(\"Cannot write to a complex64 dtype. Please use tf.complex(real, imag).\");\n    var r = {\n      id: this.nextDataId()\n    };\n    return this.texData.set(r, {\n      shape: t,\n      dtype: n,\n      values: e,\n      usage: TextureUsage.UPLOAD,\n      refCount: 1\n    }), r;\n  }\n\n  refCount(e) {\n    return this.texData.has(e) ? this.texData.get(e).refCount : 0;\n  }\n\n  incRef(e) {\n    this.texData.get(e).refCount++;\n  }\n\n  decRef(e) {\n    this.texData.has(e) && this.texData.get(e).refCount--;\n  }\n\n  move(e, t, n, r, a) {\n    if (env().getBool(\"DEBUG\") && this.checkNumericalProblems(t), \"complex64\" === r) throw new Error(\"Cannot write to a complex64 dtype. Please use tf.complex(real, imag).\");\n    this.texData.set(e, {\n      shape: n,\n      dtype: r,\n      values: t,\n      usage: TextureUsage.UPLOAD,\n      refCount: a\n    });\n  }\n\n  disposeIntermediateTensorInfo(e) {\n    this.disposeData(e.dataId);\n  }\n\n  readSync(e) {\n    var t = this.texData.get(e),\n        {\n      values: n,\n      dtype: r,\n      complexTensorInfos: a,\n      slice: s,\n      shape: o,\n      isPacked: i\n    } = t;\n\n    if (null != s) {\n      var _t685;\n\n      _t685 = i ? new UnaryOpPackedProgram(o, CLONE) : new UnaryOpProgram(o, CLONE);\n\n      var _n427 = this.runWebGLProgram(_t685, [{\n        dataId: e,\n        shape: o,\n        dtype: r\n      }], r),\n          _a297 = this.readSync(_n427.dataId);\n\n      return this.disposeIntermediateTensorInfo(_n427), _a297;\n    }\n\n    if (null != n) return this.convertAndCacheOnCPU(e);\n    if (\"string\" === r) return n;\n    var l = null != this.activeTimers;\n    var u, c;\n    return l && (u = now()), c = \"complex64\" === r ? mergeRealAndImagArrays(this.readSync(a.real.dataId), this.readSync(a.imag.dataId)) : this.getValuesFromTexture(e), l && (this.downloadWaitMs += now() - u), this.convertAndCacheOnCPU(e, c);\n  }\n\n  read(e) {\n    var _this216 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this216.pendingRead.has(e)) {\n        var _t686 = _this216.pendingRead.get(e);\n\n        return new Promise(e => _t686.push(e));\n      }\n\n      var t = _this216.texData.get(e),\n          {\n        values: n,\n        shape: r,\n        slice: a,\n        dtype: s,\n        complexTensorInfos: o,\n        isPacked: i\n      } = t;\n\n      if (null != a) {\n        var _t687;\n\n        _t687 = i ? new UnaryOpPackedProgram(r, CLONE) : new UnaryOpProgram(r, CLONE);\n\n        var _n428 = _this216.runWebGLProgram(_t687, [{\n          dataId: e,\n          shape: r,\n          dtype: s\n        }], s),\n            _a298 = _this216.read(_n428.dataId);\n\n        return _this216.disposeIntermediateTensorInfo(_n428), _a298;\n      }\n\n      if (null != n) return _this216.convertAndCacheOnCPU(e);\n      if (!env().getBool(\"WEBGL_DOWNLOAD_FLOAT_ENABLED\") && 2 === env().getNumber(\"WEBGL_VERSION\")) throw new Error(\"tensor.data() with WEBGL_DOWNLOAD_FLOAT_ENABLED=false and WEBGL_VERSION=2 not yet supported.\");\n      var l,\n          u,\n          c = null;\n\n      if (\"complex64\" !== s && env().get(\"WEBGL_BUFFER_SUPPORTED\")) {\n        l = _this216.decode(e);\n\n        var _t688 = _this216.texData.get(l.dataId);\n\n        c = _this216.gpgpu.createBufferFromTexture(_t688.texture, ...getDenseTexShape(r));\n      }\n\n      if (_this216.pendingRead.set(e, []), \"complex64\" !== s && (yield _this216.gpgpu.createAndWaitForFence()), \"complex64\" === s) {\n        var _e991 = yield Promise.all([_this216.read(o.real.dataId), _this216.read(o.imag.dataId)]);\n\n        u = mergeRealAndImagArrays(_e991[0], _e991[1]);\n      } else if (null == c) u = _this216.getValuesFromTexture(e);else {\n        var _e992 = sizeFromShape(r);\n\n        u = _this216.gpgpu.downloadFloat32MatrixFromBuffer(c, _e992);\n      }\n\n      if (null != l && _this216.disposeIntermediateTensorInfo(l), null != c) {\n        var _e993 = _this216.gpgpu.gl;\n        callAndCheck(_e993, () => _e993.deleteBuffer(c));\n      }\n\n      var p = _this216.convertAndCacheOnCPU(e, u),\n          d = _this216.pendingRead.get(e);\n\n      return _this216.pendingRead.delete(e), d.forEach(e => e(p)), _this216.pendingDisposal.has(e) && (_this216.pendingDisposal.delete(e), _this216.disposeData(e) && engine().removeDataId(e, _this216), _this216.pendingDeletes--), p;\n    })();\n  }\n\n  bufferSync(e) {\n    var t = this.readSync(e.dataId);\n    var n = t;\n    if (\"string\" === e.dtype) try {\n      n = t.map(e => decodeString(e));\n    } catch (e) {\n      throw new Error(\"Failed to decode encoded string bytes into utf-8\");\n    }\n    return buffer(e.shape, e.dtype, n);\n  }\n\n  checkNumericalProblems(e) {\n    if (null != e) for (var t = 0; t < e.length; t++) {\n      var n = e[t];\n\n      if (!canBeRepresented(n)) {\n        if (env().getBool(\"WEBGL_RENDER_FLOAT32_CAPABLE\")) throw Error(\"The value \".concat(n, \" cannot be represented with your current settings. Consider enabling float32 rendering: 'tf.env().set('WEBGL_RENDER_FLOAT32_ENABLED', true);'\"));\n        throw Error(\"The value \".concat(n, \" cannot be represented on this device.\"));\n      }\n    }\n  }\n\n  getValuesFromTexture(e) {\n    var {\n      shape: t,\n      dtype: n,\n      isPacked: r\n    } = this.texData.get(e),\n        a = sizeFromShape(t);\n\n    if (env().getBool(\"WEBGL_DOWNLOAD_FLOAT_ENABLED\")) {\n      var _n429 = this.decode(e),\n          _r409 = this.texData.get(_n429.dataId),\n          _s210 = this.gpgpu.downloadMatrixFromPackedTexture(_r409.texture, ...getDenseTexShape(t)).subarray(0, a);\n\n      return this.disposeIntermediateTensorInfo(_n429), _s210;\n    }\n\n    var s = env().getBool(\"WEBGL_PACK\") && !0 === r,\n        o = s ? getShapeAs3D(t) : t,\n        i = s ? new EncodeFloatPackedProgram(o) : new EncodeFloatProgram(o),\n        l = this.runWebGLProgram(i, [{\n      shape: o,\n      dtype: n,\n      dataId: e\n    }], \"float32\"),\n        u = this.texData.get(l.dataId),\n        c = this.gpgpu.downloadByteEncodedFloatMatrixFromOutputTexture(u.texture, u.texShape[0], u.texShape[1]).subarray(0, a);\n    return this.disposeIntermediateTensorInfo(l), c;\n  }\n\n  timerAvailable() {\n    return env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0;\n  }\n\n  time(e) {\n    var _this217 = this;\n\n    return _asyncToGenerator(function* () {\n      var t = _this217.activeTimers,\n          n = [];\n      var r = !1;\n      null == _this217.programTimersStack ? (_this217.programTimersStack = n, r = !0) : _this217.activeTimers.push(n), _this217.activeTimers = n, e();\n      var a = flatten$3(_this217.activeTimers.map(e => e.query)).filter(e => null != e),\n          s = flatten$3(_this217.activeTimers.map(e => e.name)).filter(e => null != e);\n      _this217.activeTimers = t, r && (_this217.programTimersStack = null);\n      var o = {\n        uploadWaitMs: _this217.uploadWaitMs,\n        downloadWaitMs: _this217.downloadWaitMs,\n        kernelMs: null,\n        wallMs: null\n      };\n\n      if (env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0) {\n        var _e994 = yield Promise.all(a);\n\n        o.kernelMs = sum$3(_e994), o.getExtraProfileInfo = () => _e994.map((e, t) => ({\n          name: s[t],\n          ms: e\n        })).map(e => \"\".concat(e.name, \": \").concat(e.ms)).join(\", \");\n      } else o.kernelMs = {\n        error: \"WebGL query timers are not supported in this environment.\"\n      };\n\n      return _this217.uploadWaitMs = 0, _this217.downloadWaitMs = 0, o;\n    })();\n  }\n\n  memory() {\n    return {\n      unreliable: !1,\n      numBytesInGPU: this.numBytesInGPU,\n      numBytesInGPUAllocated: this.textureManager.numBytesAllocated,\n      numBytesInGPUFree: this.textureManager.numBytesFree\n    };\n  }\n\n  startTimer() {\n    return env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0 ? this.gpgpu.beginQuery() : {\n      startMs: now(),\n      endMs: null\n    };\n  }\n\n  endTimer(e) {\n    return env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0 ? (this.gpgpu.endQuery(), e) : (e.endMs = now(), e);\n  }\n\n  getQueryTime(e) {\n    var _this218 = this;\n\n    return _asyncToGenerator(function* () {\n      return env().getNumber(\"WEBGL_DISJOINT_QUERY_TIMER_EXTENSION_RELIABLE\") > 0 ? _this218.gpgpu.waitForQueryAndGetTime(e) : e.endMs - e.startMs;\n    })();\n  }\n\n  disposeData(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    if (this.pendingDisposal.has(e)) return !1;\n    if (!this.texData.has(e)) return !0;\n    if (t ? this.texData.get(e).refCount = 0 : this.texData.get(e).refCount--, !t && this.texData.get(e).refCount > 0) return !1;\n    if (this.pendingRead.has(e)) return this.pendingDisposal.add(e), this.pendingDeletes++, !1;\n    this.releaseGPUData(e);\n    var {\n      complexTensorInfos: n\n    } = this.texData.get(e);\n    return null != n && (this.disposeData(n.real.dataId, t), this.disposeData(n.imag.dataId, t)), this.texData.delete(e), !0;\n  }\n\n  releaseGPUData(e) {\n    var {\n      texture: t,\n      dtype: n,\n      texShape: r,\n      usage: a,\n      isPacked: s,\n      slice: o\n    } = this.texData.get(e),\n        i = o && o.origDataId || e,\n        l = this.dataRefCount.get(i);\n    l > 1 ? this.dataRefCount.set(i, l - 1) : (this.dataRefCount.delete(i), null != t && (this.numBytesInGPU -= this.computeBytes(r, n), this.textureManager.releaseTexture(t, r, a, s)));\n    var u = this.texData.get(e);\n    u.texture = null, u.texShape = null, u.isPacked = !1, u.slice = null;\n  }\n\n  getTexture(e) {\n    return this.uploadToGPU(e), this.texData.get(e).texture;\n  }\n\n  getDataInfo(e) {\n    return this.texData.get(e);\n  }\n\n  shouldExecuteOnCPU(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : CPU_HANDOFF_SIZE_THRESHOLD;\n    return env().getBool(\"WEBGL_CPU_FORWARD\") && e.every(e => null == this.texData.get(e.dataId).texture && sizeFromShape(e.shape) < t);\n  }\n\n  getGPGPUContext() {\n    return this.gpgpu;\n  }\n\n  where(e) {\n    warn(\"tf.where() in webgl locks the UI thread. Call tf.whereAsync() instead\");\n    var t = e.dataSync();\n    return whereImpl(e.shape, t);\n  }\n\n  packedUnaryOp(e, t, n) {\n    var r = new UnaryOpPackedProgram(e.shape, t),\n        a = this.compileAndRun(r, [e], n);\n    return engine().makeTensorFromDataId(a.dataId, a.shape, a.dtype);\n  }\n\n  abs(e) {\n    if (this.shouldExecuteOnCPU([e]) && \"complex64\" !== e.dtype) {\n      var _t689 = simpleAbsImplCPU(this.texData.get(e.dataId).values);\n\n      return this.makeOutput(e.shape, e.dtype, _t689);\n    }\n\n    if (env().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\")) return this.packedUnaryOp(e, ABS$1, e.dtype);\n    var t = new UnaryOpProgram(e.shape, ABS$1),\n        n = this.compileAndRun(t, [e]);\n    return engine().makeTensorFromDataId(n.dataId, n.shape, n.dtype);\n  }\n\n  makeTensorInfo(e, t, n) {\n    var r;\n\n    if (\"string\" === t && null != n && n.length > 0 && isString(n[0])) {\n      var a = n.map(e => encodeString(e));\n      r = this.write(a, e, t);\n    } else r = this.write(n, e, t);\n\n    return this.texData.get(r).usage = null, {\n      dataId: r,\n      shape: e,\n      dtype: t\n    };\n  }\n\n  makeOutput(e, t, n) {\n    var {\n      dataId: r\n    } = this.makeTensorInfo(e, t, n);\n    return engine().makeTensorFromDataId(r, e, t, this);\n  }\n\n  unpackTensor(e) {\n    var t = new UnpackProgram(e.shape);\n    return this.runWebGLProgram(t, [e], e.dtype);\n  }\n\n  packTensor(e) {\n    var t = new PackProgram(e.shape);\n    return this.runWebGLProgram(t, [e], e.dtype, null, !0);\n  }\n\n  packedReshape(e, t) {\n    var n = [getBatchDim(e.shape), ...getRowsCols(e.shape)],\n        r = {\n      dtype: e.dtype,\n      shape: n,\n      dataId: e.dataId\n    },\n        a = [getBatchDim(t), ...getRowsCols(t)],\n        s = new ReshapePackedProgram(a, n),\n        o = this.runWebGLProgram(s, [r], e.dtype, null, !0);\n    return {\n      dataId: o.dataId,\n      shape: t,\n      dtype: o.dtype\n    };\n  }\n\n  decode(e) {\n    var t = this.texData.get(e),\n        {\n      isPacked: n,\n      shape: r,\n      dtype: a\n    } = t,\n        s = getShapeAs3D(r);\n    var o;\n    return o = n ? new DecodeMatrixPackedProgram(s) : new DecodeMatrixProgram(s), {\n      dtype: a,\n      shape: r,\n      dataId: this.runWebGLProgram(o, [{\n        shape: s,\n        dtype: a,\n        dataId: e\n      }], a, null, !0).dataId\n    };\n  }\n\n  runWebGLProgram(e, t, n, r) {\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    var s = this.makeTensorInfo(e.outputShape, n),\n        o = this.texData.get(s.dataId);\n\n    if (e.packedOutput && (o.isPacked = !0), e.outPackingScheme === PackingScheme.DENSE) {\n      var _t690 = getDenseTexShape(e.outputShape);\n\n      o.texShape = _t690.map(e => 2 * e);\n    }\n\n    if (null != e.outTexUsage && (o.usage = e.outTexUsage), 0 === sizeFromShape(s.shape)) return o.values = getTypedArrayFromDType(s.dtype, 0), s;\n    var i = [],\n        l = t.map(t => {\n      if (\"complex64\" === t.dtype) throw new Error(\"GPGPUProgram does not support complex64 input. For complex64 dtypes, please separate the program into real and imaginary parts.\");\n      var n = this.texData.get(t.dataId);\n\n      if (null == n.texture) {\n        if (!e.packedInputs && sizeFromShape(t.shape) <= env().getNumber(\"WEBGL_SIZE_UPLOAD_UNIFORM\")) return {\n          shape: t.shape,\n          texData: null,\n          isUniform: !0,\n          uniformValues: n.values\n        };\n        e.packedInputs && (n.isPacked = !0, n.shape = t.shape);\n      } else if (!!n.isPacked != !!e.packedInputs) t = n.isPacked ? this.unpackTensor(t) : this.packTensor(t), i.push(t), n = this.texData.get(t.dataId);else if (n.isPacked && !isReshapeFree(n.shape, t.shape)) {\n        var _e995 = t,\n            _r410 = t.shape;\n        t.shape = n.shape, t = this.packedReshape(t, _r410), i.push(t), n = this.texData.get(t.dataId), _e995.shape = _r410;\n      }\n\n      return this.uploadToGPU(t.dataId), {\n        shape: t.shape,\n        texData: n,\n        isUniform: !1\n      };\n    });\n    this.uploadToGPU(s.dataId);\n    var u = {\n      shape: s.shape,\n      texData: o,\n      isUniform: !1\n    },\n        c = makeShaderKey(e, l, u),\n        p = this.getAndSaveBinary(c, () => compileProgram(this.gpgpu, e, l, u)),\n        d = null != this.activeTimers;\n    var h;\n    d && (h = this.startTimer()), runProgram(this.gpgpu, p, l, u, r), i.forEach(e => this.disposeIntermediateTensorInfo(e)), d && (h = this.endTimer(h), this.activeTimers.push({\n      name: e.constructor.name,\n      query: this.getQueryTime(h)\n    }));\n    var m = env().get(\"WEBGL_FLUSH_THRESHOLD\");\n\n    if (m > 0) {\n      var _e996 = now();\n\n      _e996 - this.lastGlFlushTime > m && (this.gpgpu.gl.flush(), this.lastGlFlushTime = _e996);\n    }\n\n    if (!env().getBool(\"WEBGL_LAZILY_UNPACK\") && o.isPacked && !1 === a) {\n      var _e997 = this.unpackTensor(s);\n\n      return this.disposeIntermediateTensorInfo(s), _e997;\n    }\n\n    return s;\n  }\n\n  compileAndRun(e, t, n, r) {\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    return this.runWebGLProgram(e, t, n = n || t[0].dtype, r, a);\n  }\n\n  getAndSaveBinary(e, t) {\n    return e in this.binaryCache || (this.binaryCache[e] = t()), this.binaryCache[e];\n  }\n\n  getTextureManager() {\n    return this.textureManager;\n  }\n\n  dispose() {\n    this.disposed || (env().getBool(\"IS_TEST\") || Object.keys(this.binaryCache).forEach(e => {\n      this.gpgpu.deleteProgram(this.binaryCache[e].webGLProgram), delete this.binaryCache[e];\n    }), this.textureManager.dispose(), null != this.canvas && \"undefined\" != typeof HTMLCanvasElement && this.canvas instanceof HTMLCanvasElement ? this.canvas.remove() : this.canvas = null, this.gpgpuCreatedLocally && (this.gpgpu.program = null, this.gpgpu.dispose()), this.disposed = !0);\n  }\n\n  floatPrecision() {\n    return null == this.floatPrecisionValue && (this.floatPrecisionValue = tidy(() => {\n      if (!env().get(\"WEBGL_RENDER_FLOAT32_ENABLED\")) {\n        var _e998 = env().getBool(\"DEBUG\");\n\n        env().set(\"DEBUG\", !1);\n        var t = this.abs(scalar(1e-8)).dataSync()[0];\n        if (env().set(\"DEBUG\", _e998), t > 0) return 32;\n      }\n\n      return 16;\n    })), this.floatPrecisionValue;\n  }\n\n  epsilon() {\n    return 32 === this.floatPrecision() ? EPSILON_FLOAT32 : EPSILON_FLOAT16;\n  }\n\n  uploadToGPU(e) {\n    var t = this.texData.get(e),\n        {\n      shape: n,\n      dtype: r,\n      values: a,\n      texture: s,\n      usage: o,\n      isPacked: i\n    } = t;\n    if (null != s) return;\n    var l = null != this.activeTimers;\n    var u;\n    l && (u = now());\n    var c = t.texShape;\n\n    if (null == c && (c = getTextureShapeFromLogicalShape(n, i), t.texShape = c), null != a) {\n      var _e999 = getShapeAs3D(n);\n\n      var _s211,\n          _o146 = c[1],\n          _p36 = c[0];\n\n      var d = a instanceof Uint8Array;\n      i ? ([_o146, _p36] = getPackedMatrixTextureShapeWidthHeight(c[0], c[1]), _s211 = new EncodeMatrixPackedProgram(_e999, [_p36, _o146], d)) : _s211 = new EncodeMatrixProgram(_e999, [_p36, _o146], d);\n      var h = this.makeTensorInfo([_p36, _o146], r);\n      this.texData.get(h.dataId).usage = d ? TextureUsage.PIXELS : TextureUsage.UPLOAD, this.gpgpu.uploadDenseMatrixToTexture(this.getTexture(h.dataId), _o146, _p36, a);\n      var m = this.runWebGLProgram(_s211, [h], r, null, !0),\n          f = this.texData.get(m.dataId);\n      t.texture = f.texture, t.texShape = f.texShape, t.isPacked = f.isPacked, t.usage = f.usage, this.disposeIntermediateTensorInfo(h), this.texData.delete(m.dataId), t.values = null, l && (this.uploadWaitMs += now() - u);\n    } else {\n      var _e1000 = this.acquireTexture(c, o, r, i);\n\n      t.texture = _e1000;\n    }\n  }\n\n  convertAndCacheOnCPU(e, t) {\n    var n = this.texData.get(e),\n        {\n      dtype: r\n    } = n;\n    return this.releaseGPUData(e), null != t && (n.values = float32ToTypedArray(t, r)), n.values;\n  }\n\n  acquireTexture(e, t, n, r) {\n    if (this.numBytesInGPU += this.computeBytes(e, n), !this.warnedAboutMemory && this.numBytesInGPU > 1024 * this.numMBBeforeWarning * 1024) {\n      var _e1001 = (this.numBytesInGPU / 1024 / 1024).toFixed(2);\n\n      this.warnedAboutMemory = !0, console.warn(\"High memory usage in GPU: \".concat(_e1001, \" MB, most likely due to a memory leak\"));\n    }\n\n    return this.textureManager.acquireTexture(e, t, r);\n  }\n\n  computeBytes(e, t) {\n    return e[0] * e[1] * bytesPerElement(t);\n  }\n\n}\n\nfunction float32ToTypedArray(e, t) {\n  if (\"float32\" === t || \"complex64\" === t) return e;\n\n  if (\"int32\" === t || \"bool\" === t) {\n    var n = \"int32\" === t ? new Int32Array(e.length) : new Uint8Array(e.length);\n\n    for (var _t691 = 0; _t691 < n.length; ++_t691) {\n      n[_t691] = Math.round(e[_t691]);\n    }\n\n    return n;\n  }\n\n  throw new Error(\"Unknown dtype \".concat(t));\n}\n\nMathBackendWebGL.nextDataId = 0;\nvar version$2 = \"3.8.0\";\nisBrowser() && registerBackend(\"webgl\", () => new MathBackendWebGL(), 2);\nvar CHECK_NAN_SNIPPET$1 = \"\\n  if (isnan(a)) return a;\\n  if (isnan(b)) return b;\\n\";\n\nclass BinaryOpProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"A\", \"B\"], this.outputShape = assertAndGetBroadcastShape(t, n), this.enableShapeUniforms = useShapeUniforms(this.outputShape.length), this.userCode = \"\\n      float binaryOperation(float a, float b) {\\n        \".concat(e, \"\\n      }\\n\\n      void main() {\\n        float a = getAAtOutCoords();\\n        float b = getBAtOutCoords();\\n        setOutput(binaryOperation(a, b));\\n      }\\n    \");\n  }\n\n}\n\nvar CHECK_NAN_SNIPPET = \"\\n  result.r = isNaN.r > 0. ? NAN : result.r;\\n  result.g = isNaN.g > 0. ? NAN : result.g;\\n  result.b = isNaN.b > 0. ? NAN : result.b;\\n  result.a = isNaN.a > 0. ? NAN : result.a;\\n\";\n\nclass BinaryOpPackedProgram {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    this.variableNames = [\"A\", \"B\"], this.supportsBroadcasting = !0, this.packedInputs = !0, this.packedOutput = !0, this.outputShape = assertAndGetBroadcastShape(t, n);\n    var a = this.outputShape.length;\n    this.enableShapeUniforms = useShapeUniforms(a);\n    var s = \"\";\n    if (r) if (0 === a || 1 === sizeFromShape(this.outputShape)) s = \"\\n          result.y = 0.;\\n          result.z = 0.;\\n          result.w = 0.;\\n        \";else if (s = \"\\n          \".concat(getCoordsDataType(a), \" coords = getOutputCoords();\\n        \"), 1 === a) s += this.enableShapeUniforms ? \"\\n            result.y = (coords + 1) >= outShape ? 0. : result.y;\\n            result.z = 0.;\\n            result.w = 0.;\\n          \" : \"\\n            result.y = (coords + 1) >= \".concat(this.outputShape[0], \" ? 0. : result.y;\\n            result.z = 0.;\\n            result.w = 0.;\\n          \");else {\n      var _e1002 = getChannels(\"coords\", a);\n\n      s += this.enableShapeUniforms ? \"\\n            bool nextRowOutOfBounds =\\n              (\".concat(_e1002[a - 2], \" + 1) >= outShape[\").concat(a, \" - 2];\\n            bool nextColOutOfBounds =\\n              (\").concat(_e1002[a - 1], \" + 1) >= outShape[\").concat(a, \" - 1];\\n            result.y = nextColOutOfBounds ? 0. : result.y;\\n            result.z = nextRowOutOfBounds ? 0. : result.z;\\n            result.w = nextColOutOfBounds || nextRowOutOfBounds ? 0. : result.w;\\n          \") : \"\\n            bool nextRowOutOfBounds =\\n              (\".concat(_e1002[a - 2], \" + 1) >= \").concat(this.outputShape[a - 2], \";\\n            bool nextColOutOfBounds =\\n              (\").concat(_e1002[a - 1], \" + 1) >= \").concat(this.outputShape[a - 1], \";\\n            result.y = nextColOutOfBounds ? 0. : result.y;\\n            result.z = nextRowOutOfBounds ? 0. : result.z;\\n            result.w = nextColOutOfBounds || nextRowOutOfBounds ? 0. : result.w;\\n          \");\n    }\n    this.userCode = \"\\n      vec4 binaryOperation(vec4 a, vec4 b) {\\n        \".concat(e, \"\\n      }\\n\\n      void main() {\\n        vec4 a = getAAtOutCoords();\\n        vec4 b = getBAtOutCoords();\\n\\n        vec4 result = binaryOperation(a, b);\\n        \").concat(s, \"\\n\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction identity(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  return n.incRef(r.dataId), {\n    dataId: r.dataId,\n    shape: r.shape,\n    dtype: r.dtype\n  };\n}\n\nvar identityConfig = {\n  kernelName: Identity$1,\n  backendName: \"webgl\",\n  kernelFunc: identity\n};\n\nfunction complex(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    real: r,\n    imag: a\n  } = t,\n      s = n.makeTensorInfo(r.shape, \"complex64\"),\n      o = n.texData.get(s.dataId),\n      i = identity({\n    inputs: {\n      x: r\n    },\n    backend: n\n  }),\n      l = identity({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  return o.complexTensorInfos = {\n    real: i,\n    imag: l\n  }, s;\n}\n\nvar complexConfig = {\n  kernelName: Complex,\n  backendName: \"webgl\",\n  kernelFunc: complex\n},\n    LEAKYRELU = \"return (a < 0.) ? b * a : a;\",\n    LEAKYRELU_PACKED = \"\\n  vec4 aLessThanZero = vec4(lessThan(a, vec4(0.)));\\n  return (aLessThanZero * (b * a)) + ((vec4(1.0) - aLessThanZero) * a);\\n\";\n\nfunction leakyRelu(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    alpha: s\n  } = r,\n      o = n.makeTensorInfo([], \"float32\", createScalarValue(s, \"float32\")),\n      i = env().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram(LEAKYRELU_PACKED, a.shape, o.shape) : new BinaryOpProgram(LEAKYRELU, a.shape, o.shape),\n      l = n.runWebGLProgram(i, [a, o], a.dtype);\n  return n.disposeIntermediateTensorInfo(o), l;\n}\n\nvar leakyReluConfig = {\n  kernelName: LeakyRelu,\n  backendName: \"webgl\",\n  kernelFunc: leakyRelu\n},\n    PRELU = \"return (a < 0.) ? b * a : a;\",\n    PRELU_PACKED = \"\\n  vec4 aLessThanZero = vec4(lessThan(a, vec4(0.)));\\n  return (aLessThanZero * (b * a)) + ((vec4(1.0) - aLessThanZero) * a);\\n\";\n\nfunction prelu(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r,\n    alpha: a\n  } = t,\n      s = env().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram(PRELU_PACKED, r.shape, a.shape) : new BinaryOpProgram(PRELU, r.shape, a.shape);\n  return n.runWebGLProgram(s, [r, a], r.dtype);\n}\n\nvar preluConfig = {\n  kernelName: Prelu,\n  backendName: \"webgl\",\n  kernelFunc: prelu\n},\n    CHECK_NAN_SNIPPET_UNARY = \"if (isnan(x)) return x;\",\n    CHECK_NAN_SNIPPET_BINARY = \"\\n  if (isnan(a)) return a;\\n  if (isnan(b)) return b;\\n\",\n    CHECK_NAN_SNIPPET_BINARY_PACKED = \"\\n  result.r = isNaN.r > 0. ? NAN : result.r;\\n  result.g = isNaN.g > 0. ? NAN : result.g;\\n  result.b = isNaN.b > 0. ? NAN : result.b;\\n  result.a = isNaN.a > 0. ? NAN : result.a;\\n\";\n\nfunction unaryKernelFunc(_ref61) {\n  var {\n    opSnippet: e,\n    packedOpSnippet: t,\n    cpuKernelImpl: n,\n    dtype: r\n  } = _ref61;\n  return _ref62 => {\n    var {\n      inputs: a,\n      backend: s\n    } = _ref62;\n    var {\n      x: o\n    } = a,\n        i = s,\n        l = r || o.dtype;\n\n    if (i.shouldExecuteOnCPU([o]) && null != n) {\n      var _e1003 = i.texData.get(o.dataId),\n          _t692 = n(_e1003.values, l);\n\n      return i.makeTensorInfo(o.shape, l, _t692);\n    }\n\n    var u;\n    return u = env().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\") && null != t ? new UnaryOpPackedProgram(o.shape, t) : new UnaryOpProgram(o.shape, e), i.runWebGLProgram(u, [o], l);\n  };\n}\n\nfunction binaryKernelFunc(_ref63) {\n  var {\n    opSnippet: e,\n    packedOpSnippet: t,\n    checkOutOfBounds: n = !1,\n    supportsComplex: r = !1,\n    cpuKernelImpl: a,\n    dtype: s\n  } = _ref63;\n  return _ref64 => {\n    var {\n      inputs: o,\n      backend: i\n    } = _ref64;\n    var {\n      a: l,\n      b: u\n    } = o,\n        c = i;\n\n    if (r && \"complex64\" === l.dtype) {\n      var _t693 = c.texData.get(l.dataId),\n          _n430 = c.texData.get(u.dataId),\n          [_r411, _a299] = [[_t693.complexTensorInfos.real, _n430.complexTensorInfos.real], [_t693.complexTensorInfos.imag, _n430.complexTensorInfos.imag]].map(t => {\n        var [n, r] = t,\n            a = {\n          dataId: n.dataId,\n          dtype: n.dtype,\n          shape: l.shape\n        },\n            s = {\n          dataId: r.dataId,\n          dtype: r.dtype,\n          shape: u.shape\n        },\n            o = new BinaryOpProgram(e, l.shape, u.shape);\n        return c.runWebGLProgram(o, [a, s], upcastType(n.dtype, r.dtype));\n      }),\n          _s212 = complex({\n        inputs: {\n          real: _r411,\n          imag: _a299\n        },\n        backend: c\n      });\n\n      return c.disposeIntermediateTensorInfo(_r411), c.disposeIntermediateTensorInfo(_a299), _s212;\n    }\n\n    var p = s || upcastType(l.dtype, u.dtype);\n\n    if ((\"string\" === l.dtype || \"string\" === u.dtype || c.shouldExecuteOnCPU([l, u])) && null != a) {\n      var _e1004 = c.texData.get(l.dataId).values,\n          _t694 = c.texData.get(u.dataId).values,\n          _n431 = \"string\" === l.dtype ? fromUint8ToStringArray(_e1004) : _e1004,\n          _r412 = \"string\" === l.dtype ? fromUint8ToStringArray(_t694) : _t694,\n          [_s213, _o147] = a(l.shape, u.shape, _n431, _r412, p),\n          _i82 = c.makeTensorInfo(_o147, p);\n\n      return c.texData.get(_i82.dataId).values = _s213, _i82;\n    }\n\n    var d;\n    return d = env().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") && null != t ? new BinaryOpPackedProgram(t, l.shape, u.shape, n) : new BinaryOpProgram(e, l.shape, u.shape), c.runWebGLProgram(d, [l, u], p);\n  };\n}\n\nfunction mapActivationToShaderProgram(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n  if (\"linear\" === e) return t ? LINEAR : LINEAR$1;\n  if (\"relu\" === e) return t ? RELU$1 : RELU$2;\n  if (\"elu\" === e) return t ? ELU$1 : ELU$2;\n  if (\"relu6\" === e) return t ? RELU6$1 : RELU6$2;\n  if (\"prelu\" === e) return t ? PRELU_PACKED : PRELU;\n  if (\"leakyrelu\" === e) return t ? LEAKYRELU_PACKED : LEAKYRELU;\n  if (\"sigmoid\" === e) return t ? SIGMOID$1 : SIGMOID$2;\n  throw new Error(\"Activation \".concat(e, \" has not been implemented for the WebGL backend.\"));\n}\n\nclass MatMulPackedProgram {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    var s = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : !1;\n    var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : null;\n    var i = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : !1;\n    var l = arguments.length > 8 && arguments[8] !== undefined ? arguments[8] : !1;\n    this.variableNames = [\"matrixA\", \"matrixB\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = n;\n    var u = Math.ceil((r ? e[1] : e[2]) / 2),\n        c = r ? \"i * 2, rc.y\" : \"rc.y, i * 2\",\n        p = a ? \"rc.z, i * 2\" : \"i * 2, rc.z\",\n        d = r ? [\"a.xxyy\", \"a.zzww\"] : [\"a.xxzz\", \"a.yyww\"],\n        h = a ? [\"b.xzxz\", \"b.ywyw\"] : [\"b.xyxy\", \"b.zwzw\"];\n    var m = \"\",\n        f = \"\";\n    o && (m = i ? \"vec4 activation(vec4 a) {\\n          vec4 b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(o, \"\\n        }\") : l ? \"vec4 activation(vec4 a) {\\n          vec4 b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(o, \"\\n        }\") : \"vec4 activation(vec4 x) {\\n          \".concat(o, \"\\n        }\"), f = \"result = activation(result);\");\n    var g = s ? \"result += getBiasAtOutCoords();\" : \"\";\n    s && this.variableNames.push(\"bias\"), i && this.variableNames.push(\"preluActivationWeights\"), l && this.variableNames.push(\"leakyreluAlpha\");\n    var $ = \"rc.x\",\n        y = \"rc.x\";\n    e[0] < t[0] ? $ = \"int(min(float(rc.x), \".concat(e[0] - 1, \".))\") : t[0] < e[0] && (y = \"int(min(float(rc.x), \".concat(t[0] - 1, \".))\")), this.userCode = \"\\n      \".concat(m, \"\\n\\n      const float sharedDimension = \").concat(u, \".0;\\n\\n      vec4 dot2x2ARowBCol(ivec3 rc) {\\n        vec4 result = vec4(0);\\n        for (int i = 0; i < \").concat(u, \"; i++) {\\n          int batchA = \").concat($, \";\\n          int batchB = \").concat(y, \";\\n          vec4 a = getMatrixA(batchA, \").concat(c, \");\\n          vec4 b = getMatrixB(batchB, \").concat(p, \");\\n\\n          // These swizzled products need to be separately added.\\n          // See: https://github.com/tensorflow/tfjs/issues/1735\\n          result += (\").concat(d[0], \" * \").concat(h[0], \");\\n          result += (\").concat(d[1], \" * \").concat(h[1], \");\\n        }\\n        return result;\\n      }\\n\\n      void main() {\\n        ivec3 rc = getOutputCoords();\\n        vec4 result = dot2x2ARowBCol(rc);\\n\\n        \").concat(g, \"\\n\\n        \").concat(f, \"\\n\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar COMPLEX_MULTIPLY = {\n  REAL: \"return areal * breal - aimag * bimag;\",\n  IMAG: \"return areal * bimag + aimag * breal;\"\n};\n\nclass BinaryOpComplexProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"AReal\", \"AImag\", \"BReal\", \"BImag\"], this.outputShape = assertAndGetBroadcastShape(t, n), this.userCode = \"\\n      float binaryOpComplex(\\n          float areal, float aimag, float breal, float bimag) {\\n        \".concat(e, \"\\n      }\\n\\n      void main() {\\n        float areal = getARealAtOutCoords();\\n        float aimag = getAImagAtOutCoords();\\n        float breal = getBRealAtOutCoords();\\n        float bimag = getBImagAtOutCoords();\\n        setOutput(binaryOpComplex(areal, aimag, breal, bimag));\\n      }\\n    \");\n  }\n\n}\n\nvar MUL = \"return a * b;\";\n\nfunction multiply(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    a: r,\n    b: a\n  } = t,\n      s = upcastType(r.dtype, a.dtype);\n\n  if (\"complex64\" === r.dtype) {\n    var _e1005 = n.texData.get(r.dataId),\n        _t695 = n.texData.get(a.dataId),\n        _s214 = new BinaryOpComplexProgram(COMPLEX_MULTIPLY.REAL, r.shape, a.shape),\n        _o148 = new BinaryOpComplexProgram(COMPLEX_MULTIPLY.IMAG, r.shape, a.shape),\n        i = [{\n      dataId: _e1005.complexTensorInfos.real.dataId,\n      dtype: _e1005.complexTensorInfos.real.dtype,\n      shape: r.shape\n    }, {\n      dataId: _e1005.complexTensorInfos.imag.dataId,\n      dtype: _e1005.complexTensorInfos.imag.dtype,\n      shape: r.shape\n    }, {\n      dataId: _t695.complexTensorInfos.real.dataId,\n      dtype: _t695.complexTensorInfos.real.dtype,\n      shape: a.shape\n    }, {\n      dataId: _t695.complexTensorInfos.imag.dataId,\n      dtype: _t695.complexTensorInfos.imag.dtype,\n      shape: a.shape\n    }],\n        l = n.runWebGLProgram(_s214, i, \"float32\"),\n        u = n.runWebGLProgram(_o148, i, \"float32\"),\n        c = complex({\n      inputs: {\n        real: l,\n        imag: u\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(l), n.disposeIntermediateTensorInfo(u), c;\n  }\n\n  if (n.shouldExecuteOnCPU([r, a])) {\n    var _e1006 = n.texData.get(r.dataId),\n        _t696 = n.texData.get(a.dataId),\n        [_o149, _i83] = multiplyImplCPU(r.shape, a.shape, _e1006.values, _t696.values, s),\n        _l64 = n.makeTensorInfo(_i83, s);\n\n    return n.texData.get(_l64.dataId).values = _o149, _l64;\n  }\n\n  var o;\n  return o = env().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram(MUL, r.shape, a.shape) : new BinaryOpProgram(MUL, r.shape, a.shape), n.runWebGLProgram(o, [r, a], s);\n}\n\nvar multiplyConfig = {\n  kernelName: Multiply$1,\n  backendName: \"webgl\",\n  kernelFunc: multiply\n};\n\nfunction packedReshape(e, t, n) {\n  var r = [getBatchDim(e.shape), ...getRowsCols(e.shape)],\n      a = {\n    dtype: e.dtype,\n    shape: r,\n    dataId: e.dataId\n  },\n      s = [getBatchDim(t), ...getRowsCols(t)],\n      o = new ReshapePackedProgram(s, r),\n      i = n.runWebGLProgram(o, [a], e.dtype, null, !0);\n  return {\n    dataId: i.dataId,\n    shape: t,\n    dtype: i.dtype\n  };\n}\n\nfunction reshape(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    shape: s\n  } = r,\n      o = n,\n      i = sizeFromShape(a.shape),\n      l = inferFromImplicitShape(s, i),\n      u = sizeFromShape(l);\n  assert$4(i === u, () => \"The new shape (\".concat(l, \") has \").concat(u, \" elements and the old shape (\").concat(a.shape, \") has \").concat(i, \" elements. The new shape and old shape must have the same number of elements.\"));\n  var c = o.texData.get(a.dataId);\n  return !c.isPacked || isReshapeFree(a.shape, l) || null !== c.texture && isReshapeFree(c.shape, l) ? (o.incRef(a.dataId), {\n    dataId: a.dataId,\n    shape: l,\n    dtype: a.dtype\n  }) : packedReshape(a, l, o);\n}\n\nvar reshapeConfig = {\n  kernelName: Reshape$1,\n  backendName: \"webgl\",\n  kernelFunc: reshape\n};\n\nclass MeanProgram {\n  constructor(e, t) {\n    this.variableNames = [\"x\"];\n    var {\n      windowSize: n,\n      batchSize: r,\n      inSize: a,\n      outSize: s\n    } = e;\n    this.outputShape = [r, s];\n    var o = 4 * Math.floor(n / 4),\n        i = n % 4;\n    var l = \"sumValue += dot(values, ones);\";\n\n    if (null != t) {\n      var _e1007 = 1 / t;\n\n      l = \"sumValue += dot(values * \".concat(isInt(_e1007) ? _e1007.toPrecision(2) : _e1007, \", ones);\");\n    }\n\n    var u = \"\";\n    a % n > 0 && (u = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return 0.0;\\n        }\\n      \")), this.userCode = \"\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float getValue(int batch, int inIdx) {\\n        \".concat(u, \"\\n        return getX(batch, inIdx);\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = outIdx * \").concat(n, \";\\n\\n        float sumValue = 0.0;\\n\\n        for (int i = 0; i < \").concat(o, \"; i += 4) {\\n          int inIdx = inOffset + i;\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            getValue(batch, inIdx + 3)\\n          );\\n\\n          \").concat(l, \"\\n        }\\n\\n        int inIdx = inOffset + \").concat(o, \";\\n        if (\").concat(1 === i, \") {\\n          vec4 values = vec4(getValue(batch, inIdx), 0.0, 0.0, 0.0);\\n\\n          \").concat(l, \"\\n        } else if (\").concat(2 === i, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1), 0.0, 0.0);\\n\\n          \").concat(l, \"\\n        } else if (\").concat(3 === i, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2), 0.0);\\n\\n          \").concat(l, \"\\n        }\\n        setOutput(sumValue);\\n      }\\n    \");\n  }\n\n}\n\nclass ReduceProgram {\n  constructor(e, t) {\n    this.variableNames = [\"x\"];\n    var {\n      windowSize: n,\n      batchSize: r,\n      inSize: a,\n      outSize: s\n    } = e;\n    this.outputShape = [r, s];\n    var o = \"0.0\",\n        i = \"\";\n    \"prod\" === t ? o = \"1.0\" : \"min\" === t ? (o = \"1.0 / 1e-20\", i = \"min\") : \"max\" === t && (o = \"-1.0 / 1e-20\", i = \"max\");\n    var l = \"\".concat(t, \"(\").concat(t, \"(\").concat(t, \"(minMaxValue[0], minMaxValue[1]), minMaxValue[2]), minMaxValue[3])\");\n    \"sum\" === t ? l = \"sumValue\" : \"prod\" === t ? l = \"prodValue\" : \"all\" === t ? l = \"allValue\" : \"any\" === t && (l = \"anyValue\");\n    var u = 4 * Math.floor(n / 4),\n        c = n % 4;\n    var p = \"\\n      if (\".concat(\"sum\" === t, \") {\\n        sumValue += dot(values, ones);\\n      } else if (\").concat(\"prod\" === t, \") {\\n        vec2 tmp = vec2(values[0], values[1]) * vec2(values[2], values[3]);\\n        prodValue *= tmp[0] * tmp[1];\\n      } else {\\n        minMaxValue = \").concat(i, \"(values, minMaxValue);\\n        if (\").concat(\"min\" === t, \" || \").concat(\"max\" === t, \") {\\n          minMaxValue = \").concat(i, \"(values, minMaxValue);\\n          bvec4 isNaN = isnan(values);\\n          if (isNaN.r || isNaN.g || isNaN.b || isNaN.a) {\\n            minMaxValue = vec4(NAN);\\n          }\\n        }\\n      }\\n    \"),\n        d = \"vec4\";\n    \"all\" === t ? (o = \"1.0\", p = \"\\n        bool reducedAllValue = all(values);\\n        float floatedReducedAllValue = float(reducedAllValue);\\n        allValue = float(allValue >= 1.0 && floatedReducedAllValue >= 1.0);\\n      \", d = \"bvec4\") : \"any\" === t && (o = \"0.0\", p = \"\\n        bool reducedAnyValue = any(values);\\n        float floatedReducedAnyValue = float(reducedAnyValue);\\n        anyValue = float(anyValue >= 1.0 || floatedReducedAnyValue >= 1.0);\\n      \", d = \"bvec4\");\n    var h = \"\";\n    a % n > 0 && (h = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return initializationValue;\\n        }\\n      \")), this.userCode = \"\\n      const float initializationValue = \".concat(o, \";\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float getValue(int batch, int inIdx) {\\n        \").concat(h, \"\\n        return getX(batch, inIdx);\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = outIdx * \").concat(n, \";\\n\\n        vec4 minMaxValue = vec4(\").concat(o, \");\\n        float prodValue = 1.0;\\n        float sumValue = 0.0;\\n        float allValue = 1.0;\\n        float anyValue = 0.0;\\n\\n        for (int i = 0; i < \").concat(u, \"; i += 4) {\\n          int inIdx = inOffset + i;\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            getValue(batch, inIdx + 3)\\n          );\\n\\n          \").concat(p, \"\\n        }\\n\\n        int inIdx = inOffset + \").concat(u, \";\\n        if (\").concat(1 === c, \") {\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            initializationValue,\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          \").concat(p, \"\\n        } else if (\").concat(2 === c, \") {\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          \").concat(p, \"\\n        } else if (\").concat(3 === c, \") {\\n          \").concat(d, \" values = \").concat(d, \"(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            initializationValue\\n          );\\n\\n          \").concat(p, \"\\n        }\\n        setOutput(\").concat(l, \");\\n      }\\n    \");\n  }\n\n}\n\nfunction getReductionStages(e) {\n  var t = [];\n\n  for (; 0 === t.length || 1 !== t[t.length - 1].outSize;) {\n    var n = t.length ? t[t.length - 1].outSize : e[1],\n        r = computeOptimalWindowSize(n);\n    t.push({\n      inSize: n,\n      windowSize: r,\n      outSize: Math.ceil(n / r)\n    });\n  }\n\n  return t;\n}\n\nfunction reduce(e, t, n, r) {\n  var a = getReductionStages(e.shape);\n  var s = e;\n\n  for (var o = 0; o < a.length; o++) {\n    var {\n      inSize: i,\n      windowSize: l,\n      outSize: u\n    } = a[o];\n\n    var c = void 0,\n        _p37 = void 0;\n\n    c = \"mean\" === n ? 0 === o ? new MeanProgram({\n      windowSize: l,\n      inSize: i,\n      batchSize: e.shape[0],\n      outSize: u\n    }, i) : new MeanProgram({\n      windowSize: l,\n      inSize: i,\n      batchSize: e.shape[0],\n      outSize: u\n    }) : new ReduceProgram({\n      windowSize: l,\n      inSize: i,\n      batchSize: e.shape[0],\n      outSize: u\n    }, n), _p37 = s, s = r.runWebGLProgram(c, [s], t), _p37.dataId !== e.dataId && r.disposeIntermediateTensorInfo(_p37);\n  }\n\n  return s;\n}\n\nclass TransposeProgram {\n  constructor(e, t) {\n    this.variableNames = [\"A\"];\n    var n = new Array(e.length);\n\n    for (var _r413 = 0; _r413 < n.length; _r413++) {\n      n[_r413] = e[t[_r413]];\n    }\n\n    this.outputShape = n, this.rank = n.length;\n    var r = getCoordsDataType(this.rank),\n        a = getSwitchedCoords(t);\n    this.userCode = \"\\n    void main() {\\n      \".concat(r, \" resRC = getOutputCoords();\\n      setOutput(getA(\").concat(a, \"));\\n    }\\n    \");\n  }\n\n}\n\nfunction getSwitchedCoords(e) {\n  var t = e.length;\n  if (t > 6) throw Error(\"Transpose for rank \".concat(t, \" is not yet supported\"));\n  var n = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\", \"resRC.u\", \"resRC.v\"],\n      r = new Array(t);\n\n  for (var _t697 = 0; _t697 < e.length; _t697++) {\n    r[e[_t697]] = n[_t697];\n  }\n\n  return r.join();\n}\n\nclass TransposePackedProgram {\n  constructor(e, t) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0;\n    var n = new Array(e.length);\n\n    for (var _r414 = 0; _r414 < n.length; _r414++) {\n      n[_r414] = e[t[_r414]];\n    }\n\n    if (this.outputShape = n, this.rank = n.length, this.rank > 6) throw Error(\"Packed transpose for rank \".concat(this.rank, \" is not yet supported.\"));\n    var r = getCoordsDataType(this.rank),\n        a = getVecChannels(\"rc\", this.rank),\n        s = new Array(this.rank);\n\n    for (var _e1008 = 0; _e1008 < t.length; _e1008++) {\n      s[t[_e1008]] = a[_e1008];\n    }\n\n    var o = \"vec2(\".concat(s.slice(-2).join(), \")\"),\n        i = \"++\".concat(a[this.rank - 1], \" < \").concat(n[this.rank - 1]),\n        l = \"getChannel(getA(\".concat(s.join(), \"), \").concat(o, \")\");\n    this.userCode = \"\\n    void main() {\\n      \".concat(r, \" rc = getOutputCoords();\\n      vec4 result = vec4(0.);\\n      result[0] = \").concat(l, \";\\n      if(\").concat(i, \") {\\n        result[1] = \").concat(l, \";\\n      }\\n      --\").concat(a[this.rank - 1], \";\\n      if(++\").concat(a[this.rank - 2], \" < \").concat(n[this.rank - 2], \") {\\n        result[2] = \").concat(l, \";\\n        if(\").concat(i, \") {\\n          result[3] = \").concat(l, \";\\n        }\\n      }\\n      setOutput(result);\\n    }\\n    \");\n  }\n\n}\n\nfunction transposeImpl(e, t, n) {\n  var r = env().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new TransposePackedProgram(e.shape, t) : new TransposeProgram(e.shape, t);\n  return n.runWebGLProgram(r, [e], e.dtype);\n}\n\nfunction sumImpl(e, t, n, r) {\n  var a = e.shape.length,\n      s = parseAxisParam(t, e.shape);\n  var o = s;\n  var i = getAxesPermutation(o, a),\n      l = null != i;\n  var u = e;\n  l && (u = transposeImpl(e, i, r), o = getInnerMostAxes(o.length, a)), assertAxesAreInnerMostDims(\"sum\", o, a);\n  var [c, p] = computeOutAndReduceShapes(u.shape, o);\n  var d = c;\n  n && (d = expandShapeToKeepDim(c, s));\n  var h = sizeFromShape(p),\n      m = reshape({\n    inputs: {\n      x: u\n    },\n    attrs: {\n      shape: [sizeFromShape(e.shape) / h, h]\n    },\n    backend: r\n  }),\n      f = reduce(m, sumOutType(e.dtype), \"sum\", r),\n      g = reshape({\n    inputs: {\n      x: f\n    },\n    attrs: {\n      shape: d\n    },\n    backend: r\n  });\n  return r.disposeIntermediateTensorInfo(m), r.disposeIntermediateTensorInfo(f), l && r.disposeIntermediateTensorInfo(u), g;\n}\n\nfunction sum(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r;\n  return sumImpl(a, s, o, n);\n}\n\nvar sumConfig = {\n  kernelName: Sum,\n  backendName: \"webgl\",\n  kernelFunc: sum\n};\n\nfunction transpose(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    perm: s\n  } = r,\n      o = n,\n      i = new Array(a.shape.length);\n\n  for (var _e1009 = 0; _e1009 < i.length; _e1009++) {\n    i[_e1009] = a.shape[s[_e1009]];\n  }\n\n  var l;\n\n  if (o.shouldExecuteOnCPU([a])) {\n    var _e1010 = o.texData.get(a.dataId),\n        _t698 = transposeImplCPU(_e1010.values, a.shape, a.dtype, s, i);\n\n    l = o.makeTensorInfo(i, a.dtype), o.texData.get(l.dataId).values = _t698;\n  } else l = transposeImpl(a, s, o);\n\n  return l;\n}\n\nvar transposeConfig = {\n  kernelName: Transpose,\n  backendName: \"webgl\",\n  kernelFunc: transpose\n},\n    MATMUL_SHARED_DIM_THRESHOLD = 1e3;\n\nfunction batchMatMulImpl(_ref65) {\n  var {\n    a: e,\n    b: t,\n    transposeA: n,\n    transposeB: r,\n    backend: a,\n    bias: s = null,\n    preluActivationWeights: o = null,\n    leakyreluAlpha: i = 0,\n    activation: l = null\n  } = _ref65;\n  var u = e.shape.length,\n      c = t.shape.length,\n      p = n ? e.shape[u - 2] : e.shape[u - 1],\n      d = r ? t.shape[c - 1] : t.shape[c - 2],\n      h = n ? e.shape[u - 1] : e.shape[u - 2],\n      m = r ? t.shape[c - 2] : t.shape[c - 1],\n      f = e.shape.slice(0, -2),\n      g = t.shape.slice(0, -2),\n      $ = sizeFromShape(f),\n      y = sizeFromShape(g);\n  assert$4(u >= 2 && c >= 2 && ($ === y || 1 === $ || 1 === y), () => \"Error in matMul: the input batch dimensions must either be the same or at least one input batch dimension must be 1. Got input batch dimensions of (\".concat(f, \") and (\").concat(g, \").\"));\n  var b = ($ > y ? e.shape.slice(0, -2) : t.shape.slice(0, -2)).concat([h, m]);\n  assert$4(p === d, () => \"Error in matMul: inner shapes (\".concat(p, \") and (\").concat(d, \") of Tensors with shapes \").concat(e.shape, \" and \").concat(t.shape, \" and transposeA=\").concat(n, \" and transposeB=\").concat(r, \" must match.\"));\n  var x = n ? [$, p, h] : [$, h, p],\n      v = r ? [y, m, d] : [y, d, m],\n      I = reshape({\n    inputs: {\n      x: e\n    },\n    backend: a,\n    attrs: {\n      shape: x\n    }\n  }),\n      C = reshape({\n    inputs: {\n      x: t\n    },\n    backend: a,\n    attrs: {\n      shape: v\n    }\n  }),\n      S = [I, C],\n      k = Math.max($, y),\n      T = n ? I.shape[1] : I.shape[2],\n      N = null != s,\n      w = null != o,\n      E = \"leakyrelu\" === l,\n      A = null != l ? mapActivationToShaderProgram(l, !0) : null;\n  var D;\n\n  if ((1 === h || 1 === m) && T > MATMUL_SHARED_DIM_THRESHOLD && !1 === (N || w || E || null != A)) {\n    var _e1011 = I,\n        _t699 = C;\n    n && (_e1011 = transpose({\n      inputs: {\n        x: I\n      },\n      backend: a,\n      attrs: {\n        perm: [0, 2, 1]\n      }\n    }), S.push(_e1011)), r && (_t699 = transpose({\n      inputs: {\n        x: C\n      },\n      backend: a,\n      attrs: {\n        perm: [0, 2, 1]\n      }\n    }), S.push(_t699));\n\n    var _s215 = 1 === m;\n\n    var _o150 = _e1011;\n    1 !== m && (_o150 = reshape({\n      inputs: {\n        x: _e1011\n      },\n      backend: a,\n      attrs: {\n        shape: [k, T, 1]\n      }\n    }), S.push(_o150));\n\n    var _i84 = 1 === m ? 2 : 1;\n\n    var _l65 = _t699;\n    _s215 && (_l65 = reshape({\n      inputs: {\n        x: _t699\n      },\n      backend: a,\n      attrs: {\n        shape: [k, 1, T]\n      }\n    }), S.push(_l65));\n\n    var _u57 = multiply({\n      inputs: {\n        a: _o150,\n        b: _l65\n      },\n      backend: a\n    });\n\n    D = sum({\n      inputs: {\n        x: _u57\n      },\n      backend: a,\n      attrs: {\n        axis: _i84,\n        keepDims: !0\n      }\n    }), S.push(_u57);\n  } else {\n    var _l66 = upcastType(e.dtype, t.dtype),\n        _u58 = new MatMulPackedProgram(x, v, [k, h, m], n, r, N, A, w, E),\n        _c38 = [I, C];\n\n    if (null != s && _c38.push(s), w && _c38.push(o), E) {\n      var _e1012 = a.makeTensorInfo([], \"float32\", createScalarValue(i, \"float32\"));\n\n      _c38.push(_e1012), S.push(_e1012);\n    }\n\n    D = a.runWebGLProgram(_u58, _c38, _l66);\n  }\n\n  var R = reshape({\n    inputs: {\n      x: D\n    },\n    backend: a,\n    attrs: {\n      shape: b\n    }\n  });\n  S.push(D);\n\n  for (var _e1013 of S) {\n    a.disposeIntermediateTensorInfo(_e1013);\n  }\n\n  return R;\n}\n\nfunction _fusedMatMul(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    transposeA: l,\n    transposeB: u,\n    activation: c,\n    leakyreluAlpha: p\n  } = r;\n  return batchMatMulImpl({\n    a,\n    b: s,\n    transposeA: l,\n    transposeB: u,\n    backend: n,\n    bias: o,\n    preluActivationWeights: i,\n    leakyreluAlpha: p,\n    activation: c\n  });\n}\n\nvar _fusedMatMulConfig = {\n  kernelName: _FusedMatMul,\n  backendName: \"webgl\",\n  kernelFunc: _fusedMatMul\n},\n    ABS = \"return abs(x);\";\n\nfunction abs(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n\n  if (n.shouldExecuteOnCPU([r]) && \"complex64\" !== r.dtype) {\n    var _e1014 = n.texData.get(r.dataId),\n        _t700 = simpleAbsImplCPU(_e1014.values);\n\n    return n.makeTensorInfo(r.shape, r.dtype, _t700);\n  }\n\n  var a;\n  return a = env().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\") ? new UnaryOpPackedProgram(r.shape, ABS) : new UnaryOpProgram(r.shape, ABS), n.runWebGLProgram(a, [r], r.dtype);\n}\n\nvar absConfig = {\n  kernelName: Abs,\n  backendName: \"webgl\",\n  kernelFunc: abs\n},\n    ACOS = CHECK_NAN_SNIPPET$2 + \"\\n  if (abs(x) > 1.) {\\n    return NAN;\\n  }\\n  return acos(x);\\n\",\n    acos = unaryKernelFunc({\n  opSnippet: ACOS\n}),\n    acosConfig = {\n  kernelName: Acos,\n  backendName: \"webgl\",\n  kernelFunc: acos\n},\n    ACOSH = CHECK_NAN_SNIPPET$2 + \"\\n  if (x < 1.0) return NAN;\\nreturn log(x + sqrt(x * x - 1.0));\",\n    acosh = unaryKernelFunc({\n  opSnippet: ACOSH\n}),\n    acoshConfig = {\n  kernelName: Acosh,\n  backendName: \"webgl\",\n  kernelFunc: acosh\n},\n    ADD = \"return a + b;\",\n    addKernelFunc = binaryKernelFunc({\n  opSnippet: ADD,\n  packedOpSnippet: ADD,\n  supportsComplex: !0,\n  cpuKernelImpl: addImplCPU\n}),\n    addConfig = {\n  kernelName: Add$1,\n  backendName: \"webgl\",\n  kernelFunc: addKernelFunc\n};\n\nclass AddNProgram {\n  constructor(e, t) {\n    this.outputShape = [], this.outputShape = e, this.variableNames = t.map((e, t) => \"T\".concat(t));\n    var n = [];\n    this.variableNames.forEach(e => {\n      n.push(\"float v\".concat(e, \" = get\").concat(e, \"AtOutCoords();\"));\n    });\n    var r = this.variableNames.map(e => \"v\".concat(e)).join(\" + \");\n    this.userCode = \"\\n      void main() {\\n        \".concat(n.join(\"\\n        \"), \"\\n\\n        float result = \").concat(r, \";\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nclass AddNPackedProgram {\n  constructor(e, t) {\n    this.outputShape = [], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e, this.variableNames = t.map((e, t) => \"T\".concat(t));\n    var n = [];\n    this.variableNames.forEach(e => {\n      n.push(\"vec4 v\".concat(e, \" = get\").concat(e, \"AtOutCoords();\"));\n    });\n    var r = this.variableNames.map(e => \"v\".concat(e)).join(\" + \");\n    this.userCode = \"\\n      void main() {\\n        \".concat(n.join(\"\\n        \"), \"\\n\\n        vec4 result = \").concat(r, \";\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction addN(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      r = t;\n  if (1 === r.length) return identity({\n    inputs: {\n      x: r[0]\n    },\n    backend: n\n  });\n\n  if (r.length > env().get(\"WEBGL_MAX_TEXTURES_IN_SHADER\")) {\n    var _e1015 = Math.floor(r.length / 2),\n        _t701 = addN({\n      inputs: r.slice(0, _e1015),\n      backend: n\n    }),\n        _a300 = addN({\n      inputs: r.slice(_e1015),\n      backend: n\n    });\n\n    return addN({\n      inputs: [_t701, _a300],\n      backend: n\n    });\n  }\n\n  var a = r.map(e => e.dtype).reduce((e, t) => upcastType(e, t)),\n      s = r.map(e => e.shape),\n      o = env().getBool(\"WEBGL_PACK\") ? new AddNPackedProgram(r[0].shape, s) : new AddNProgram(r[0].shape, s);\n  return n.runWebGLProgram(o, r, a);\n}\n\nvar addNConfig = {\n  kernelName: AddN,\n  backendName: \"webgl\",\n  kernelFunc: addN\n};\n\nfunction all(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation(u, i);\n  var p = a;\n  null != c && (p = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), u = getInnerMostAxes(u.length, i)), assertAxesAreInnerMostDims(\"all\", u, i);\n  var [d, h] = computeOutAndReduceShapes(p.shape, u),\n      m = reshape({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, sizeFromShape(h)]\n    }\n  }),\n      f = reduce(m, m.dtype, \"all\", n);\n  var g;\n  return g = reshape(o ? {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: expandShapeToKeepDim(d, l)\n    }\n  } : {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  }), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), null != c && n.disposeIntermediateTensorInfo(p), g;\n}\n\nvar allConfig = {\n  kernelName: All,\n  backendName: \"webgl\",\n  kernelFunc: all\n};\n\nfunction any(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation(u, i);\n  var p = a;\n  null != c && (p = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), u = getInnerMostAxes(u.length, i)), assertAxesAreInnerMostDims(\"any\", u, i);\n  var [d, h] = computeOutAndReduceShapes(p.shape, u),\n      m = reshape({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, sizeFromShape(h)]\n    }\n  }),\n      f = reduce(m, m.dtype, \"any\", n);\n  var g;\n  return g = reshape(o ? {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: expandShapeToKeepDim(d, l)\n    }\n  } : {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  }), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), null != c && n.disposeIntermediateTensorInfo(p), g;\n}\n\nvar anyConfig = {\n  kernelName: Any,\n  backendName: \"webgl\",\n  kernelFunc: any\n};\n\nclass ArgMinMaxProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"A\"];\n    var {\n      windowSize: r,\n      batchSize: a,\n      outSize: s\n    } = e;\n    n || this.variableNames.push(\"bestIndicesA\"), this.outputShape = [a, s], this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = outIdx * \".concat(r, \";\\n\\n        int bestIndex = inOffset;\\n        float bestValue = getA(batch, bestIndex);\\n\\n        for (int i = 0; i < \").concat(r, \"; i++) {\\n          int inIdx = \").concat(n ? \"inOffset + i;\" : \"round(getBestIndicesA(batch, inOffset + i));\", \";\\n          float candidate = getA(batch, inIdx);\\n          if (candidate \").concat(\"max\" === t ? \">\" : \"<\", \" bestValue) {\\n            bestValue = candidate;\\n            bestIndex = inIdx;\\n          }\\n        }\\n        setOutput(float(bestIndex));\\n      }\\n    \");\n  }\n\n}\n\nclass ArgMinMaxPackedProgram {\n  constructor(e, t, n, r) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, assert$4(e.length > 2, () => \"Packed arg\".concat(n.charAt(0).toUpperCase() + n.slice(1), \" supports only inputs with rank above 2.\"));\n    var a = Math.ceil(e[e.length - 1] / t);\n    this.outputShape = e.slice(0, -1), a > 1 && this.outputShape.push(a), r || this.variableNames.push(\"bestIndicesA\");\n    var s = this.outputShape,\n        o = s.length,\n        i = getCoordsDataType(o),\n        l = getChannels(\"coords\", o);\n    var u, c;\n\n    if (1 === a) {\n      c = o + 1;\n\n      var _e1016 = getCoordsDataType(c);\n\n      u = \"\\n        \".concat(_e1016, \" sourceLocR = \").concat(_e1016, \"(\").concat(l.join(), \", 0);\\n        ++\").concat(l[o - 1], \";\\n        \").concat(_e1016, \" sourceLocG = \").concat(_e1016, \"(\").concat(l.join(), \", 0);\\n        ++\").concat(l[o - 2], \";\\n        \").concat(_e1016, \" sourceLocA = \").concat(_e1016, \"(\").concat(l.join(), \", 0);\\n        --\").concat(l[o - 1], \";\\n        \").concat(_e1016, \" sourceLocB = \").concat(_e1016, \"(\").concat(l.join(), \", 0);\\n        --\").concat(l[o - 2], \";\");\n    } else c = o, u = \"\\n        \".concat(i, \" sourceLocR = coords;\\n        ++\").concat(l[o - 1], \";\\n        \").concat(i, \" sourceLocG = coords;\\n        ++\").concat(l[o - 2], \";\\n        \").concat(i, \" sourceLocA = coords;\\n        --\").concat(l[o - 1], \";\\n        \").concat(i, \" sourceLocB = coords;\\n        --\").concat(l[o - 2], \";\");\n\n    var p = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"].slice(0, c),\n        d = \".\" + p[c - 1],\n        h = p.map(e => \"int \" + e),\n        m = getChannels(\"sourceLocR\", c - 1).concat(\"inIdx.r\"),\n        f = getChannels(\"sourceLocG\", c - 1).concat(\"inIdx.g\"),\n        g = getChannels(\"sourceLocB\", c - 1).concat(\"inIdx.b\"),\n        $ = getChannels(\"sourceLocA\", c - 1).concat(\"inIdx.a\"),\n        y = \"max\" === n ? \"greaterThan\" : \"lessThan\",\n        b = r ? \"\" : \"\\n          inIdx = round(vec4(getBestIndicesAChannel(\".concat(m.join(), \"),\\n                             getBestIndicesAChannel(\").concat(f.join(), \"),\\n                             getBestIndicesAChannel(\").concat(g.join(), \"),\\n                             getBestIndicesAChannel(\").concat($.join(), \")));\"),\n        x = \"vec4(\\n            getAChannel(\".concat(m.join(), \"),\\n            hasNextCol ? getAChannel(\").concat(f.join(), \") : 0.,\\n            hasNextRow ? getAChannel(\").concat(g.join(), \") : 0.,\\n            hasNextRow && hasNextCol ? getAChannel(\").concat($.join(), \") : 0.)\"),\n        v = r ? \"\" : \"\\n      float getBestIndicesAChannel(\".concat(h.join(), \") {\\n        return getChannel(getBestIndicesA(\").concat(p.join(), \"),\\n                                          vec2(\").concat(p.slice(-2).join(), \"));\\n      }\");\n    this.userCode = \"\\n      float getAChannel(\".concat(h.join(), \") {\\n        return getChannel(getA(\").concat(p.join(), \"),\\n                               vec2(\").concat(p.slice(-2).join(), \"));\\n      }\\n      \").concat(v, \"\\n      void main() {\\n        \").concat(i, \" coords = getOutputCoords();\\n        bool hasNextCol = \").concat(l[o - 1], \" < \").concat(s[o - 1] - 1, \";\\n        bool hasNextRow = \").concat(l[o - 2], \" < \").concat(s[o - 2] - 1, \";\\n        \").concat(u, \"\\n        ivec4 srcIdx = ivec4(sourceLocR\").concat(d, \", sourceLocG\").concat(d, \",\\n          sourceLocB\").concat(d, \", sourceLocA\").concat(d, \") * \").concat(t, \";\\n        ivec4 inIdx = srcIdx;\\n        vec4 bestIndex = vec4(inIdx);\\n        vec4 bestValue = \").concat(x, \";\\n\\n        for (int i = 0; i < \").concat(t, \"; i++) {\\n          inIdx = srcIdx;\\n          \").concat(b, \"\\n          vec4 candidate = \").concat(x, \";\\n          bvec4 nan = isnan(candidate);\\n          bvec4 replace = bvec4(\\n            vec4(\").concat(y, \"(candidate, bestValue)) * (vec4(1.0) - vec4(nan)));\\n\\n          bestValue = vec4(replace.x  ? candidate.x : bestValue.x,\\n                           replace.y  ? candidate.y : bestValue.y,\\n                           replace.z  ? candidate.z : bestValue.z,\\n                           replace.w  ? candidate.w : bestValue.w);\\n          bestIndex = mix(bestIndex, vec4(inIdx), vec4(replace));\\n          srcIdx++;\\n        }\\n        setOutput(bestIndex);\\n      }\\n    \");\n  }\n\n}\n\nfunction argReduce(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n  var a = t.shape[0],\n      s = t.shape[1];\n  null != r && (a = r.shape[0], s = r.shape[1]);\n  var o = computeOptimalWindowSize(s),\n      i = {\n    windowSize: o,\n    inSize: s,\n    batchSize: a,\n    outSize: Math.ceil(s / o)\n  },\n      l = new ArgMinMaxProgram(i, n, null == r),\n      u = [t];\n  null != r && u.push(r);\n  var c = e.runWebGLProgram(l, u, \"int32\");\n  if (1 === c.shape[1]) return c;\n  var p = argReduce(e, t, n, c);\n  return e.disposeIntermediateTensorInfo(c), p;\n}\n\nfunction argReducePacked(e, t, n) {\n  var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n  var a = null != r ? r.shape : t.shape,\n      s = computeOptimalWindowSize(a[a.length - 1]),\n      o = new ArgMinMaxPackedProgram(a, s, n, null == r),\n      i = e.runWebGLProgram(o, null == r ? [t] : [t, r], \"int32\");\n\n  if (i.shape.length === t.shape.length) {\n    var _r415 = argReducePacked(e, t, n, i);\n\n    return e.disposeIntermediateTensorInfo(i), _r415;\n  }\n\n  return i;\n}\n\nfunction argMinMaxReduce(e, t, n, r) {\n  var a = [n];\n\n  if (assertAxesAreInnerMostDims(\"arg\" + r.charAt(0).toUpperCase() + r.slice(1), a, t.shape.length), !env().getBool(\"WEBGL_PACK_REDUCE\") || t.shape.length <= 2) {\n    var _n432 = [],\n        [s, o] = computeOutAndReduceShapes(t.shape, a),\n        i = sizeFromShape(o),\n        l = reshape({\n      inputs: {\n        x: t\n      },\n      backend: e,\n      attrs: {\n        shape: [-1, i]\n      }\n    });\n\n    _n432.push(l);\n\n    var u = argReduce(e, l, r);\n\n    _n432.push(u);\n\n    var c = reshape({\n      inputs: {\n        x: u\n      },\n      backend: e,\n      attrs: {\n        shape: s\n      }\n    });\n    return _n432.forEach(t => e.disposeIntermediateTensorInfo(t)), c;\n  }\n\n  return argReducePacked(e, t, r);\n}\n\nfunction argMax(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  var o = parseAxisParam(s, a.shape);\n  var i = getAxesPermutation(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes(o.length, l.shape.length)), assertAxesAreInnerMostDims(\"argMax\", [o[0]], l.shape.length);\n  var c = argMinMaxReduce(n, l, o[0], \"max\");\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), c;\n}\n\nvar argMaxConfig = {\n  kernelName: ArgMax,\n  backendName: \"webgl\",\n  kernelFunc: argMax\n};\n\nfunction argMin(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s\n  } = r;\n  var o = parseAxisParam(s, a.shape);\n  var i = getAxesPermutation(o, a.shape.length);\n  var l = a;\n  var u = [];\n  null != i && (l = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: i\n    }\n  }), u.push(l), o = getInnerMostAxes(o.length, l.shape.length)), assertAxesAreInnerMostDims(\"argMin\", [o[0]], l.shape.length);\n  var c = argMinMaxReduce(n, l, o[0], \"min\");\n  return u.forEach(e => n.disposeIntermediateTensorInfo(e)), c;\n}\n\nvar argMinConfig = {\n  kernelName: ArgMin,\n  backendName: \"webgl\",\n  kernelFunc: argMin\n},\n    ASIN = CHECK_NAN_SNIPPET$2 + \"\\n  if (abs(x) > 1.) {\\n    return NAN;\\n  }\\n  return asin(x);\\n\",\n    asin = unaryKernelFunc({\n  opSnippet: ASIN\n}),\n    asinConfig = {\n  kernelName: Asin,\n  backendName: \"webgl\",\n  kernelFunc: asin\n},\n    ASINH = CHECK_NAN_SNIPPET$2 + \"return log(x + sqrt(x * x + 1.0));\",\n    asinh = unaryKernelFunc({\n  opSnippet: ASINH\n}),\n    asinhConfig = {\n  kernelName: Asinh,\n  backendName: \"webgl\",\n  kernelFunc: asinh\n},\n    ATAN = CHECK_NAN_SNIPPET$2 + \"\\n  return atan(x);\\n\",\n    atan = unaryKernelFunc({\n  opSnippet: ATAN\n}),\n    atanConfig = {\n  kernelName: Atan,\n  backendName: \"webgl\",\n  kernelFunc: atan\n},\n    ATAN2 = CHECK_NAN_SNIPPET_BINARY + \"\\n  return atan(a, b);\\n\",\n    ATAN2_PACKED = \"\\n  vec4 result = atan(a, b);\\n  vec4 isNaN = min(vec4(isnan(a)) + vec4(isnan(b)), vec4(1.0));\\n  \" + CHECK_NAN_SNIPPET_BINARY_PACKED + \"\\n  return result;\\n\",\n    atan2 = binaryKernelFunc({\n  opSnippet: ATAN2,\n  packedOpSnippet: ATAN2_PACKED\n}),\n    atan2Config = {\n  kernelName: Atan2,\n  backendName: \"webgl\",\n  kernelFunc: atan2\n},\n    ATANH = CHECK_NAN_SNIPPET$2 + \"\\n  if ((x < -1.0) || (x > 1.0)) return NAN;\\nreturn (log(1.0 + x) - log(1.0 - x)) / 2.0;\",\n    atanh = unaryKernelFunc({\n  opSnippet: ATANH\n}),\n    atanhConfig = {\n  kernelName: Atanh,\n  backendName: \"webgl\",\n  kernelFunc: atanh\n};\n\nclass Pool2DProgram {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    if (this.variableNames = [\"x\"], \"avg\" === t && n) throw new Error(\"Cannot compute positions for average pool.\");\n    var s = e.filterWidth,\n        o = e.strideHeight,\n        i = e.strideWidth,\n        l = e.dilationHeight,\n        u = e.dilationWidth,\n        c = e.effectiveFilterHeight,\n        p = e.effectiveFilterWidth,\n        d = e.padInfo.top,\n        h = e.padInfo.left;\n    this.outputShape = e.outShape;\n    var m = \"avg\" === t;\n    var f = \"0.0\";\n    if (m || (f = \"-1.0 / 1e-20\"), n) return void (this.userCode = \"\\n        const ivec2 strides = ivec2(\".concat(o, \", \").concat(i, \");\\n        const ivec2 pads = ivec2(\").concat(d, \", \").concat(h, \");\\n\\n        void main() {\\n          ivec4 coords = getOutputCoords();\\n          int batch = coords[0];\\n          int d = coords[3];\\n\\n          ivec2 xRCCorner = coords.yz * strides - pads;\\n          int xRCorner = xRCCorner.x;\\n          int xCCorner = xRCCorner.y;\\n\\n          // max/min x(?, ?, d) to get y(yR, yC, d).\\n          // ? = to be determined\\n          float minMaxValue = 0.0;\\n          float minMaxValueFound = 0.0;\\n          int minMaxPosition = 0;\\n          float avgValue = 0.0;\\n\\n          for (int wR = 0; wR < \").concat(c, \";\\n              wR += \").concat(l, \") {\\n            int xR = xRCorner + wR;\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int wC = 0; wC < \").concat(p, \";\\n                wC += \").concat(u, \") {\\n              int xC = xCCorner + wC;\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              float value = getX(batch, xR, xC, d);\\n\\n              // If a min / max value has already been found, use it. If not,\\n              // use the current value.\\n              float currMinMaxValue = mix(\\n                  value, minMaxValue, minMaxValueFound);\\n              if (value >= currMinMaxValue) {\\n                minMaxValue = value;\\n                minMaxValueFound = 1.0;\\n                minMaxPosition = \").concat(r ? a ? \"((batch  * \".concat(e.inHeight, \" + xR) * \").concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + d\") : \"(xR * \".concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + d\") : \"wR * \".concat(p, \" + wC\"), \";\\n              }\\n            }\\n          }\\n          setOutput(float(minMaxPosition));\\n        }\\n      \"));\n    var g = \"\".concat(t, \"(\").concat(t, \"(\").concat(t, \"(minMaxValue[0], minMaxValue[1]), minMaxValue[2]), minMaxValue[3])\");\n    \"avg\" === t && (g = \"avgValue / count\");\n    var $ = 4 * Math.floor(s / 4),\n        y = s % 4,\n        b = \"\\n      if (\".concat(m, \") {\\n        avgValue += dot(values, ones);\\n      } else {\\n        minMaxValue = max(values, minMaxValue);\\n      }\\n    \");\n    this.userCode = \"\\n      const ivec2 strides = ivec2(\".concat(o, \", \").concat(i, \");\\n      const ivec2 pads = ivec2(\").concat(d, \", \").concat(h, \");\\n      const float initializationValue = \").concat(f, \";\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float count = 0.0;\\n\\n      float getValue(int batch, int xR, int xC, int d) {\\n        if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n          return initializationValue;\\n        }\\n        count += 1.0;\\n        return getX(batch, xR, xC, d);\\n      }\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d = coords[3];\\n\\n        ivec2 xRCCorner = coords.yz * strides - pads;\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        // max/min x(?, ?, d) to get y(yR, yC, d).\\n        // ? = to be determined\\n        vec4 minMaxValue = vec4(\").concat(f, \");\\n        float avgValue = 0.0;\\n        count = 0.0;\\n\\n        for (int wR = 0; wR < \").concat(c, \";\\n            wR += \").concat(l, \") {\\n          int xR = xRCorner + wR;\\n\\n          if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n            continue;\\n          }\\n\\n          for (int wC = 0; wC < \").concat($, \"; wC += 4) {\\n            int xC = xCCorner + wC * \").concat(u, \";\\n\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              getValue(batch, xR, xC + \").concat(u, \", d),\\n              getValue(batch, xR, xC + 2 * \").concat(u, \", d),\\n              getValue(batch, xR, xC + 3 * \").concat(u, \", d)\\n            );\\n\\n            \").concat(b, \"\\n          }\\n\\n          int xC = xCCorner + \").concat($, \";\\n          if (\").concat(1 === y, \") {\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              initializationValue,\\n              initializationValue,\\n              initializationValue\\n            );\\n\\n            \").concat(b, \"\\n          } else if (\").concat(2 === y, \") {\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              getValue(batch, xR, xC + \").concat(u, \", d),\\n              initializationValue,\\n              initializationValue\\n            );\\n\\n            \").concat(b, \"\\n          } else if (\").concat(3 === y, \") {\\n            vec4 values = vec4(\\n              getValue(batch, xR, xC, d),\\n              getValue(batch, xR, xC + \").concat(u, \", d),\\n              getValue(batch, xR, xC + 2 * \").concat(u, \", d),\\n              initializationValue\\n            );\\n\\n            \").concat(b, \"\\n          }\\n        }\\n        setOutput(\").concat(g, \");\\n      }\\n    \");\n  }\n\n}\n\nclass Pool3DProgram {\n  constructor(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    if (this.variableNames = [\"x\"], \"avg\" === t && n) throw new Error(\"Cannot compute positions for average pool.\");\n    var s = e.filterWidth,\n        o = e.strideDepth,\n        i = e.strideHeight,\n        l = e.strideWidth,\n        u = e.dilationDepth,\n        c = e.dilationHeight,\n        p = e.dilationWidth,\n        d = e.effectiveFilterDepth,\n        h = e.effectiveFilterHeight,\n        m = e.effectiveFilterWidth,\n        f = e.padInfo.front,\n        g = e.padInfo.top,\n        $ = e.padInfo.left;\n    this.outputShape = e.outShape;\n    var y = \"avg\" === t;\n    var b = \"0.0\";\n    if (y || (b = \"-1.0 / 1e-20\"), n) return void (this.userCode = \"\\n        const ivec3 strides =\\n            ivec3(\".concat(o, \", \").concat(i, \", \").concat(l, \");\\n        const ivec3 pads = ivec3(\").concat(f, \", \").concat(g, \", \").concat($, \");\\n\\n        void main() {\\n          ivec5 coords = getOutputCoords();\\n          int batch = coords.x;\\n          int ch = coords.u;\\n\\n          ivec3 xCorner = ivec3(coords.y, coords.z, coords.w) * strides - pads;\\n          int xDCorner = xCorner.x;\\n          int xRCorner = xCorner.y;\\n          int xCCorner = xCorner.z;\\n\\n          // max/min x(?, ?, ?, ch) to get y(yD, yR, yC, ch).\\n          // ? = to be determined\\n          float minMaxValue = 0.0;\\n          float minMaxValueFound = 0.0;\\n          int minMaxPosition = 0;\\n\\n          for (int wD = 0; wD < \").concat(d, \";\\n              wD += \").concat(u, \") {\\n            int xD = xDCorner + wD;\\n\\n            if (xD < 0 || xD >= \").concat(e.inDepth, \") {\\n              continue;\\n            }\\n\\n            for (int wR = 0; wR < \").concat(h, \";\\n                wR += \").concat(c, \") {\\n              int xR = xRCorner + wR;\\n\\n              if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n                continue;\\n              }\\n\\n              for (int wC = 0; wC < \").concat(m, \";\\n                  wC += \").concat(p, \") {\\n                int xC = xCCorner + wC;\\n\\n                if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                  continue;\\n                }\\n\\n                float value = getX(batch, xD, xR, xC, ch);\\n\\n                // If a min / max value has already been found, use it. If not,\\n                // use the current value.\\n                float currMinMaxValue = mix(\\n                    value, minMaxValue, minMaxValueFound);\\n                if (value >= currMinMaxValue) {\\n                  minMaxValue = value;\\n                  minMaxValueFound = 1.0;\\n                  minMaxPosition = \").concat(r ? a ? \"(((batch * \".concat(e.inDepth, \" + xD) * \").concat(e.inHeight, \" + xR) * \").concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + ch\") : \"((xD * \".concat(e.inHeight, \" + xR) * \").concat(e.inWidth, \" + xC) * \").concat(e.inChannels, \" + ch\") : \"wD * \".concat(h, \" * \").concat(m, \" +\\n                      wR * \").concat(m, \" + wC\"), \";\\n                }\\n              }\\n            }\\n          }\\n          setOutput(float(minMaxPosition));\\n        }\\n      \"));\n    var x = \"\".concat(t, \"(\").concat(t, \"(\").concat(t, \"(minMaxValue[0], minMaxValue[1]), minMaxValue[2]), minMaxValue[3])\");\n    \"avg\" === t && (x = \"avgValue / count\");\n    var v = 4 * Math.floor(s / 4),\n        I = s % 4,\n        C = \"\\n      if (\".concat(y, \") {\\n        avgValue += dot(values, ones);\\n      } else {\\n        minMaxValue = max(values, minMaxValue);\\n      }\\n    \");\n    this.userCode = \"\\n      const ivec3 strides =\\n        ivec3(\".concat(o, \", \").concat(i, \", \").concat(l, \");\\n      const ivec3 pads = ivec3(\").concat(f, \", \").concat(g, \", \").concat($, \");\\n      const float initializationValue = \").concat(b, \";\\n      const vec4 ones = vec4(1.0, 1.0, 1.0, 1.0);\\n\\n      float count = 0.0;\\n\\n      float getValue(int batch, int xD, int xR, int xC, int ch) {\\n        if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n          return initializationValue;\\n        }\\n        count += 1.0;\\n        return getX(batch, xD, xR, xC, ch);\\n      }\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int ch = coords.u;\\n\\n        ivec3 xCorner = ivec3(coords.y, coords.z, coords.w) * strides - pads;\\n        int xDCorner = xCorner.x;\\n        int xRCorner = xCorner.y;\\n        int xCCorner = xCorner.z;\\n\\n        // max/min x(?, ?, ?, d) to get y(yD, yR, yC, ch).\\n        // ? = to be determined\\n        vec4 minMaxValue = vec4(\").concat(b, \");\\n        float avgValue = 0.0;\\n        count = 0.0;\\n\\n        for (int wD = 0; wD < \").concat(d, \";\\n            wD += \").concat(u, \") {\\n          int xD = xDCorner + wD;\\n\\n          if (xD < 0 || xD >= \").concat(e.inDepth, \") {\\n            continue;\\n          }\\n\\n          for (int wR = 0; wR < \").concat(h, \";\\n            wR += \").concat(c, \") {\\n            int xR = xRCorner + wR;\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int wC = 0; wC < \").concat(v, \"; wC += 4) {\\n              int xC = xCCorner + wC * \").concat(p, \";\\n\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                getValue(batch, xD, xR, xC + \").concat(p, \", ch),\\n                getValue(batch, xD, xR, xC + 2 * \").concat(p, \", ch),\\n                getValue(batch, xD, xR, xC + 3 * \").concat(p, \", ch)\\n              );\\n\\n              \").concat(C, \"\\n            }\\n\\n            int xC = xCCorner + \").concat(v, \";\\n            if (\").concat(1 === I, \") {\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                initializationValue,\\n                initializationValue,\\n                initializationValue\\n              );\\n\\n              \").concat(C, \"\\n            } else if (\").concat(2 === I, \") {\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                getValue(batch, xD, xR, xC + \").concat(p, \", ch),\\n                initializationValue,\\n                initializationValue\\n              );\\n\\n              \").concat(C, \"\\n            } else if (\").concat(3 === I, \") {\\n              vec4 values = vec4(\\n                getValue(batch, xD, xR, xC, ch),\\n                getValue(batch, xD, xR, xC + \").concat(p, \", ch),\\n                getValue(batch, xD, xR, xC + 2 * \").concat(p, \", ch),\\n                initializationValue\\n              );\\n\\n              \").concat(C, \"\\n            }\\n          }\\n          setOutput(\").concat(x, \");\\n        }\\n      }\\n    \");\n  }\n\n}\n\nfunction avgPool(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex(a, \"avgPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$4(eitherStridesOrDilationsAreOne(o, 1), () => \"Error in avgPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo(a.shape, s, o, 1, i, l);\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual(u.inShape, u.outShape)) return identity({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var c = new Pool2DProgram(u, \"avg\", !1);\n  return n.runWebGLProgram(c, [a], \"float32\");\n}\n\nvar avgPoolConfig = {\n  kernelName: AvgPool,\n  backendName: \"webgl\",\n  kernelFunc: avgPool\n};\n\nfunction avgPool3D(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l,\n    dataFormat: u\n  } = r,\n      c = computePool3DInfo(a.shape, s, o, [1, 1, 1], i, l, u),\n      p = new Pool3DProgram(c, \"avg\", !1);\n  return n.runWebGLProgram(p, [a], \"float32\");\n}\n\nvar avgPool3DConfig = {\n  kernelName: AvgPool3D,\n  backendName: \"webgl\",\n  kernelFunc: avgPool3D\n};\n\nclass AvgPool2DBackpropProgram {\n  constructor(e) {\n    this.variableNames = [\"dy\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterHeight,\n        n = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n      const float avgMultiplier = float(\").concat(1 / (e.filterHeight * e.filterWidth), \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n\\n        ivec2 dyRCCorner = coords.yz - pads;\\n        int dyRCorner = dyRCCorner.x;\\n        int dyCCorner = dyRCCorner.y;\\n\\n        // Convolve dy(?, ?, d) with pos mask(:, :, d) to get dx(xR, xC, d).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(t, \";\\n            wR += \").concat(e.dilationHeight, \") {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          for (int wC = 0; wC < \").concat(n, \";\\n            wC+= \").concat(e.dilationWidth, \") {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            float dyValue = getDy(b, idyR, idyC, d);\\n\\n            dotProd += dyValue * avgMultiplier;\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass AvgPool3DBackpropProgram {\n  constructor(e) {\n    this.variableNames = [\"dy\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterDepth,\n        n = e.effectiveFilterHeight,\n        r = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec3 pads = ivec3(\".concat(t - 1 - e.padInfo.front, \", \").concat(n - 1 - e.padInfo.top, \", \").concat(r - 1 - e.padInfo.left, \");\\n      const float avgMultiplier = float(\").concat(1 / (e.filterDepth * e.filterHeight * e.filterWidth), \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int ch = coords.u;\\n\\n        ivec3 dyCorner = ivec3(coords.y, coords.z, coords.w) - pads;\\n        int dyDCorner = dyCorner.x;\\n        int dyRCorner = dyCorner.y;\\n        int dyCCorner = dyCorner.z;\\n\\n        // Convolve dy(?, ?, ?, d) with pos mask(:, :, :, ch) to get\\n        // dx(xD, xR, xC, ch).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n\\n        for (int wD = 0; wD < \").concat(t, \";\\n            wD += \").concat(e.dilationDepth, \") {\\n          float dyD = float(dyDCorner + wD) / \").concat(e.strideDepth, \".0;\\n\\n          if (dyD < 0.0 || dyD >= \").concat(e.outDepth, \".0 || fract(dyD) > 0.0) {\\n            continue;\\n          }\\n          int idyD = int(dyD);\\n\\n          for (int wR = 0; wR < \").concat(n, \";\\n              wR += \").concat(e.dilationHeight, \") {\\n            float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n            if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 ||\\n                fract(dyR) > 0.0) {\\n              continue;\\n            }\\n            int idyR = int(dyR);\\n\\n            for (int wC = 0; wC < \").concat(r, \";\\n                wC += \").concat(e.dilationWidth, \") {\\n              float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n              if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                  fract(dyC) > 0.0) {\\n                continue;\\n              }\\n              int idyC = int(dyC);\\n\\n              float dyValue = getDy(batch, idyD, idyR, idyC, ch);\\n\\n              dotProd += dyValue * avgMultiplier;\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction avgPool3DGrad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s,\n      {\n    filterSize: i,\n    strides: l,\n    pad: u,\n    dimRoundingMode: c\n  } = r,\n      p = computePool3DInfo(o.shape, i, l, [1, 1, 1], u, c),\n      d = new AvgPool3DBackpropProgram(p);\n  return n.runWebGLProgram(d, [a], o.dtype);\n}\n\nvar avgPoolGrad3DConfig = {\n  kernelName: AvgPool3DGrad,\n  backendName: \"webgl\",\n  kernelFunc: avgPool3DGrad\n};\n\nfunction avgPoolGrad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s;\n  assertNotComplex([a, s], \"avgPoolGrad\");\n  var {\n    filterSize: i,\n    strides: l,\n    pad: u\n  } = r,\n      c = computePool2DInfo(o.shape, i, l, 1, u),\n      p = new AvgPool2DBackpropProgram(c);\n  return n.runWebGLProgram(p, [a], o.dtype);\n}\n\nvar avgPoolGradConfig = {\n  kernelName: AvgPoolGrad,\n  backendName: \"webgl\",\n  kernelFunc: avgPoolGrad\n};\n\nfunction batchMatMul(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    a,\n    b: s\n  } = t,\n      {\n    transposeA: o,\n    transposeB: i\n  } = r;\n  return batchMatMulImpl({\n    a,\n    b: s,\n    transposeA: o,\n    transposeB: i,\n    backend: n\n  });\n}\n\nvar batchMatMulConfig = {\n  kernelName: BatchMatMul,\n  backendName: \"webgl\",\n  kernelFunc: batchMatMul\n};\n\nclass BatchNormProgram {\n  constructor(e, t, n, r, a, s) {\n    this.outputShape = [], this.variableNames = [\"x\", \"mean\", \"variance\"], assertAndGetBroadcastShape(e, t), assertAndGetBroadcastShape(e, n);\n    var o = \"0.0\";\n    null != r && (assertAndGetBroadcastShape(e, r), this.variableNames.push(\"offset\"), o = \"getOffsetAtOutCoords()\");\n    var i = \"1.0\";\n    null != a && (assertAndGetBroadcastShape(e, a), this.variableNames.push(\"scale\"), i = \"getScaleAtOutCoords()\"), this.outputShape = e, this.userCode = \"\\n      void main() {\\n        float x = getXAtOutCoords();\\n        float mean = getMeanAtOutCoords();\\n        float variance = getVarianceAtOutCoords();\\n        float offset = \".concat(o, \";\\n        float scale = \").concat(i, \";\\n        float inv = scale * inversesqrt(variance + float(\").concat(s, \"));\\n        setOutput(dot(vec3(x, -mean, offset), vec3(inv, inv, 1)));\\n      }\\n    \");\n  }\n\n}\n\nclass BatchNormPackedProgram {\n  constructor(e, t, n, r, a, s) {\n    this.packedInputs = !0, this.packedOutput = !0, this.variableNames = [\"x\", \"mean\", \"variance\"], assertAndGetBroadcastShape(e, t), assertAndGetBroadcastShape(e, n);\n    var o = \"vec4(0.0)\";\n    null != r && (assertAndGetBroadcastShape(e, r), this.variableNames.push(\"offset\"), o = \"getOffsetAtOutCoords()\");\n    var i = \"vec4(1.0)\";\n    null != a && (assertAndGetBroadcastShape(e, a), this.variableNames.push(\"scale\"), i = \"getScaleAtOutCoords()\"), this.outputShape = e, this.userCode = \"\\n      void main() {\\n        vec4 offset = \".concat(o, \";\\n        vec4 scale = \").concat(i, \";\\n\\n        vec4 x = getXAtOutCoords();\\n        vec4 mean = getMeanAtOutCoords();\\n        vec4 variance = getVarianceAtOutCoords();\\n\\n        vec4 inv = scale * inversesqrt(variance + vec4(\").concat(s, \"));\\n\\n        setOutput((x - mean) * inv + offset);\\n      }\\n    \");\n  }\n\n}\n\nvar batchNorm = _ref66 => {\n  var {\n    inputs: e,\n    backend: t,\n    attrs: n\n  } = _ref66;\n  var {\n    x: r,\n    mean: a,\n    variance: s,\n    offset: o,\n    scale: i\n  } = e;\n  assert$4(a.shape.length === s.shape.length, () => \"Batch normalization gradient requires mean and variance to have equal ranks.\"), assert$4(null == o || a.shape.length === o.shape.length, () => \"Batch normalization gradient requires mean and offset to have equal ranks.\"), assert$4(null == i || a.shape.length === i.shape.length, () => \"Batch normalization gradient requires mean and scale to have equal ranks.\");\n  var {\n    varianceEpsilon: l\n  } = n;\n  null == l && (l = .001);\n  var u = [r, a, s];\n  var c = null;\n  null != o && (c = o.shape, u.push(o));\n  var p = null;\n  null != i && (p = i.shape, u.push(i));\n  var d = env().getBool(\"WEBGL_PACK_NORMALIZATION\") ? new BatchNormPackedProgram(r.shape, a.shape, s.shape, c, p, l) : new BatchNormProgram(r.shape, a.shape, s.shape, c, p, l);\n  return t.runWebGLProgram(d, u, u[0].dtype);\n},\n    batchNormConfig = {\n  kernelName: FusedBatchNorm,\n  backendName: \"webgl\",\n  kernelFunc: batchNorm\n};\n\nclass SliceProgram {\n  constructor(e) {\n    this.variableNames = [\"source\"], this.outputShape = e, this.rank = e.length;\n    var t = getCoordsDataType(this.rank);\n    this.customUniforms = [{\n      name: \"start\",\n      arrayIndex: this.rank,\n      type: \"int\"\n    }];\n    var n = getCoords$1(this.rank);\n    var r;\n    r = \"\\n        \".concat(t, \" sourceLoc;\\n        \").concat(t, \" coords = getOutputCoords();\\n        \").concat(e.map((e, t) => \"sourceLoc.\".concat(coords[t], \" = start[\").concat(t, \"] + coords.\").concat(coords[t], \";\")).join(\"\\n\"), \"\\n      \"), this.userCode = \"\\n      void main() {\\n        \".concat(r, \"\\n        setOutput(getSource(\").concat(n, \"));\\n      }\\n    \");\n  }\n\n}\n\nvar coords = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"];\n\nfunction getCoords$1(e) {\n  if (1 === e) return \"sourceLoc\";\n  if (e <= 6) return coords.slice(0, e).map(e => \"sourceLoc.\" + e).join(\",\");\n  throw Error(\"Slicing for rank \".concat(e, \" is not yet supported\"));\n}\n\nclass SlicePackedProgram {\n  constructor(e) {\n    this.variableNames = [\"source\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e, this.rank = e.length, this.customUniforms = [{\n      name: \"start\",\n      arrayIndex: this.rank,\n      type: \"int\"\n    }];\n    var t = getCoordsDataType(this.rank),\n        n = getChannels(\"coords\", this.rank),\n        r = getChannels(\"sourceLoc\", this.rank),\n        a = 1 === this.rank ? \"sourceLoc\" : \"vec2(\".concat(r.slice(-2).join(), \")\"),\n        s = \"getChannel(getSource(\".concat(r.join(), \"), \").concat(a, \")\"),\n        o = \"\\n      result.x = \".concat(s, \";\\n      if (++\").concat(n[this.rank - 1], \" < \").concat(e[this.rank - 1], \") {\\n        ++\").concat(r[this.rank - 1], \";\\n        result.y = \").concat(s, \";\\n        --\").concat(r[this.rank - 1], \";\\n      }\\n    \"),\n        i = 1 === this.rank ? \"\" : \"\\n      --\".concat(n[this.rank - 1], \";\\n      if (++\").concat(n[this.rank - 2], \" < \").concat(e[this.rank - 2], \") {\\n        ++\").concat(r[this.rank - 2], \";\\n        result.z = \").concat(s, \";\\n        if (++\").concat(n[this.rank - 1], \" < \").concat(e[this.rank - 1], \") {\\n          ++\").concat(r[this.rank - 1], \";\\n          result.w = \").concat(s, \";\\n        }\\n      }\\n    \"),\n        l = this.rank <= 4 ? \"sourceLoc = coords +\\n            \".concat(t, \"(\").concat(e.map((e, t) => \"start[\".concat(t, \"]\")).join(), \");\") : e.map((e, t) => \"\".concat(r[t], \" = \").concat(n[t], \" + start[\").concat(t, \"];\")).join(\"\\n\");\n    this.userCode = \"\\n      void main() {\\n        \".concat(t, \" coords = getOutputCoords();\\n        \").concat(t, \" sourceLoc;\\n        \").concat(l, \"\\n        vec4 result = vec4(0.);\\n        \").concat(o, \"\\n        \").concat(i, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction shallowSlice(e, t, n, r) {\n  var a = r.texData.get(e.dataId),\n      s = r.makeTensorInfo(n, e.dtype),\n      o = r.texData.get(s.dataId);\n  Object.assign(o, a), o.refCount = 1, o.shape = n, o.dtype = e.dtype;\n  var i = computeFlatOffset(t, computeStrides(e.shape));\n  a.slice && (i += a.slice.flatOffset), o.slice = {\n    flatOffset: i,\n    origDataId: a.slice && a.slice.origDataId || e.dataId\n  };\n  var l = r.dataRefCount.get(o.slice.origDataId) || 1;\n  return r.dataRefCount.set(o.slice.origDataId, l + 1), s;\n}\n\nfunction slice(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    size: o\n  } = r,\n      [i, l] = parseSliceParams(a, s, o);\n  if (assertParamsValid(a, i, l), 0 === sizeFromShape(l)) return n.makeTensorInfo(l, a.dtype, []);\n\n  if (n.shouldExecuteOnCPU([a]) || \"string\" === a.dtype) {\n    var _e1017 = n.texData.get(a.dataId),\n        _t702 = sliceImplCPU(_e1017.values, i, l, a.shape, a.dtype);\n\n    return n.makeTensorInfo(l, a.dtype, _t702);\n  }\n\n  var {\n    isPacked: u\n  } = n.texData.get(a.dataId),\n      c = isSliceContinous(a.shape, i, l);\n\n  if (u || !c) {\n    var _e1018 = env().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new SlicePackedProgram(l) : new SliceProgram(l);\n\n    return n.runWebGLProgram(_e1018, [a], a.dtype, [i]);\n  }\n\n  return n.uploadToGPU(a.dataId), shallowSlice(a, i, l, n);\n}\n\nvar sliceConfig = {\n  kernelName: Slice,\n  backendName: \"webgl\",\n  kernelFunc: slice\n},\n    batchToSpaceND = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    crops: o\n  } = r;\n  assert$4(a.shape.length <= 4, () => \"batchToSpaceND for rank > 4 with a WebGL backend not implemented yet\");\n  var i = s.reduce((e, t) => e * t),\n      l = getReshaped(a.shape, s, i),\n      u = getPermuted(l.length, s.length),\n      c = getReshapedPermuted(a.shape, s, i),\n      p = getSliceBeginCoords(o, s.length),\n      d = getSliceSize(c, o, s.length),\n      h = [],\n      m = reshape({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      f = transpose({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }),\n      g = reshape({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: c\n    }\n  }),\n      $ = slice({\n    inputs: {\n      x: g\n    },\n    backend: n,\n    attrs: {\n      begin: p,\n      size: d\n    }\n  });\n  return h.push(m), h.push(f), h.push(g), h.forEach(e => n.disposeIntermediateTensorInfo(e)), $;\n},\n    batchToSpaceNDConfig = {\n  kernelName: BatchToSpaceND,\n  backendName: \"webgl\",\n  kernelFunc: batchToSpaceND\n};\n\nfunction bincount(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o\n  } = r,\n      i = n.readSync(a.dataId),\n      l = n.readSync(s.dataId),\n      u = bincountImplCPU(i, l, s.dtype, s.shape, o);\n  return n.makeTensorInfo([o], s.dtype, u);\n}\n\nvar bincountConfig = {\n  kernelName: Bincount,\n  backendName: \"webgl\",\n  kernelFunc: bincount\n},\n    NOT_EQUAL = \"return float(a != b);\",\n    notEqual = binaryKernelFunc({\n  opSnippet: NOT_EQUAL,\n  cpuKernelImpl: notEqualImplCPU,\n  dtype: \"bool\"\n}),\n    notEqualConfig = {\n  kernelName: NotEqual,\n  backendName: \"webgl\",\n  kernelFunc: notEqual\n};\n\nfunction real(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return identity({\n    inputs: {\n      x: n.texData.get(r.dataId).complexTensorInfos.real\n    },\n    backend: n\n  });\n}\n\nvar realConfig = {\n  kernelName: Real,\n  backendName: \"webgl\",\n  kernelFunc: real\n},\n    TO_INT = \"return float(int(x));\";\n\nfunction int(e, t) {\n  var n = new UnaryOpProgram(e.shape, TO_INT),\n      r = t.runWebGLProgram(n, [e], \"int32\");\n  return {\n    dataId: r.dataId,\n    shape: r.shape,\n    dtype: r.dtype\n  };\n}\n\nfunction cast(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dtype: s\n  } = r;\n\n  if (\"complex64\" === s) {\n    if (\"complex64\" === a.dtype) return identity({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    var _e1019 = zeros$2(a.shape),\n        _t703 = cast({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        dtype: \"float32\"\n      }\n    }),\n        _r416 = complex({\n      inputs: {\n        real: _t703,\n        imag: _e1019\n      },\n      backend: n\n    });\n\n    return _e1019.dispose(), n.disposeIntermediateTensorInfo(_t703), _r416;\n  }\n\n  if (\"complex64\" === a.dtype) {\n    var _e1020 = real({\n      inputs: {\n        input: a\n      },\n      backend: n\n    }),\n        _t704 = cast({\n      inputs: {\n        x: _e1020\n      },\n      backend: n,\n      attrs: {\n        dtype: s\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(_e1020), _t704;\n  }\n\n  if (!hasEncodingLoss(a.dtype, s)) {\n    var _e1021 = identity({\n      inputs: {\n        x: a\n      },\n      backend: n\n    });\n\n    return {\n      dataId: _e1021.dataId,\n      shape: _e1021.shape,\n      dtype: s\n    };\n  }\n\n  if (\"int32\" === s) return int(a, n);\n\n  if (\"bool\" === s) {\n    var _e1022 = n.makeTensorInfo([], \"bool\", getTypedArrayFromDType(\"bool\", 1)),\n        _t705 = notEqual({\n      inputs: {\n        a,\n        b: _e1022\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e1022), _t705;\n  }\n\n  throw new Error(\"Error in Cast: failed to cast \".concat(a.dtype, \" to \").concat(s));\n}\n\nvar castConfig = {\n  kernelName: Cast,\n  backendName: \"webgl\",\n  kernelFunc: cast\n},\n    CEIL = \"return ceil(x);\",\n    ceil = unaryKernelFunc({\n  opSnippet: CEIL,\n  packedOpSnippet: CEIL,\n  cpuKernelImpl: ceilImplCPU\n}),\n    ceilConfig = {\n  kernelName: Ceil,\n  backendName: \"webgl\",\n  kernelFunc: ceil\n};\n\nclass ClipProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.customUniforms = [{\n      name: \"minVal\",\n      type: \"float\"\n    }, {\n      name: \"maxVal\",\n      type: \"float\"\n    }], this.outputShape = e, this.userCode = \"\\n\\n      void main() {\\n        float value = getAAtOutCoords();\\n        if (isnan(value)) {\\n          setOutput(value);\\n          return;\\n        }\\n\\n        setOutput(clamp(value, minVal, maxVal));\\n      }\\n    \";\n  }\n\n}\n\nclass ClipPackedProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.customUniforms = [{\n      name: \"minVal\",\n      type: \"float\"\n    }, {\n      name: \"maxVal\",\n      type: \"float\"\n    }], this.outputShape = e, this.userCode = \"\\n      void main() {\\n        vec4 value = getAAtOutCoords();\\n\\n        if (any(isnan(value))) {\\n          setOutput(value);\\n          return;\\n        }\\n\\n        setOutput(clamp(value, vec4(minVal), vec4(maxVal)));\\n      }\\n    \";\n  }\n\n}\n\nfunction clipByValue(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    clipValueMin: s,\n    clipValueMax: o\n  } = r;\n  var i;\n  return i = env().getBool(\"WEBGL_PACK_CLIP\") ? new ClipPackedProgram(a.shape) : new ClipProgram(a.shape), n.runWebGLProgram(i, [a], a.dtype, [[s], [o]]);\n}\n\nvar clipByValueConfig = {\n  kernelName: ClipByValue,\n  backendName: \"webgl\",\n  kernelFunc: clipByValue\n};\n\nclass ComplexAbsProgram {\n  constructor(e) {\n    this.variableNames = [\"real\", \"imag\"], this.outputShape = e, this.userCode = \"\\n      void main() {\\n        float re = abs(getRealAtOutCoords());\\n        float im = abs(getImagAtOutCoords());\\n        float mx = max(re, im);\\n\\n        // sadly the length function in glsl is not underflow-safe\\n        // (at least not on Intel GPUs). So the safe solution is\\n        // to ensure underflow-safety in all cases.\\n        setOutput(\\n          mx == 0.0 ? 0.0 : mx * length(vec2(1, min(re, im)/mx))\\n        );\\n      }\\n    \";\n  }\n\n}\n\nfunction makeComplexComponentTensorInfo(e, t) {\n  return {\n    dataId: t.dataId,\n    dtype: t.dtype,\n    shape: e.shape\n  };\n}\n\nfunction complexAbs(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t,\n      a = n.texData.get(r.dataId),\n      s = new ComplexAbsProgram(r.shape),\n      o = [makeComplexComponentTensorInfo(r, a.complexTensorInfos.real), makeComplexComponentTensorInfo(r, a.complexTensorInfos.imag)];\n  return n.runWebGLProgram(s, o, o[0].dtype);\n}\n\nvar complexAbsConfig = {\n  kernelName: ComplexAbs,\n  backendName: \"webgl\",\n  kernelFunc: complexAbs\n};\n\nclass ConcatProgram {\n  constructor(e) {\n    this.outputShape = [], this.outputShape = computeOutShape$1(e, 1), this.variableNames = e.map((e, t) => \"T\".concat(t));\n    var t = new Array(e.length - 1);\n    t[0] = e[0][1];\n\n    for (var _n433 = 1; _n433 < t.length; _n433++) {\n      t[_n433] = t[_n433 - 1] + e[_n433][1];\n    }\n\n    var n = [\"if (yC < \".concat(t[0], \") setOutput(getT0(yR, yC));\")];\n\n    for (var _e1023 = 1; _e1023 < t.length; _e1023++) {\n      n.push(\"else if (yC < \".concat(t[_e1023], \") setOutput(getT\").concat(_e1023, \"(yR, yC-\").concat(t[_e1023 - 1], \"));\"));\n    }\n\n    n.push(\"else setOutput(getT\".concat(t.length, \"(yR, yC-\").concat(t[t.length - 1], \"));\")), this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int yR = coords.x;\\n        int yC = coords.y;\\n\\n        \".concat(n.join(\"\\n        \"), \"\\n      }\\n    \");\n  }\n\n}\n\nclass ConcatPackedProgram {\n  constructor(e, t) {\n    this.packedInputs = !0, this.packedOutput = !0, this.outputShape = [], this.outputShape = computeOutShape$1(e, t);\n    var n = this.outputShape,\n        r = n.length,\n        a = getCoordsDataType(r),\n        s = getChannels(\"coords\", r),\n        o = [\"x\", \"y\", \"z\", \"w\", \"u\", \"v\"].slice(0, r);\n    this.variableNames = e.map((e, t) => \"T\".concat(t));\n    var i = new Array(e.length - 1);\n    i[0] = e[0][t];\n\n    for (var _n434 = 1; _n434 < i.length; _n434++) {\n      i[_n434] = i[_n434 - 1] + e[_n434][t];\n    }\n\n    var l = o[t],\n        u = o.slice(-2),\n        c = o.join();\n    var p = \"if (\".concat(l, \" < \").concat(i[0], \") {\\n        return getChannel(\\n            getT0(\").concat(c, \"), vec2(\").concat(u.join(), \"));\\n        }\");\n\n    for (var _e1024 = 1; _e1024 < i.length; _e1024++) {\n      var _t706 = i[_e1024 - 1];\n      p += \"\\n        if (\".concat(l, \" < \").concat(i[_e1024], \"  && \").concat(l, \" >= \").concat(i[_e1024 - 1], \") {\\n          return getChannel(\\n            getT\").concat(_e1024, \"(\").concat(shiftedChannels(o, l, _t706), \"),\\n            vec2(\").concat(shiftedChannels(u, l, _t706), \"));\\n        }\");\n    }\n\n    var d = i[i.length - 1];\n    p += \"\\n        return getChannel(\\n          getT\".concat(i.length, \"(\").concat(shiftedChannels(o, l, d), \"),\\n          vec2(\").concat(shiftedChannels(u, l, d), \"));\"), this.userCode = \"\\n      float getValue(\".concat(o.map(e => \"int \" + e), \") {\\n        \").concat(p, \"\\n      }\\n\\n      void main() {\\n        \").concat(a, \" coords = getOutputCoords();\\n        vec4 result = vec4(getValue(\").concat(s, \"), 0., 0., 0.);\\n\\n        \").concat(s[r - 1], \" = \").concat(s[r - 1], \" + 1;\\n        if (\").concat(s[r - 1], \" < \").concat(n[r - 1], \") {\\n          result.g = getValue(\").concat(s, \");\\n        }\\n\\n        \").concat(s[r - 2], \" = \").concat(s[r - 2], \" + 1;\\n        if (\").concat(s[r - 2], \" < \").concat(n[r - 2], \") {\\n          result.a = getValue(\").concat(s, \");\\n        }\\n\\n        \").concat(s[r - 1], \" = \").concat(s[r - 1], \" - 1;\\n        if (\").concat(s[r - 2], \" < \").concat(n[r - 2], \" &&\\n            \").concat(s[r - 1], \" < \").concat(n[r - 1], \") {\\n          result.b = getValue(\").concat(s, \");\\n        }\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction shiftedChannels(e, t, n) {\n  var r = e.indexOf(t);\n  return e.map((e, t) => t === r ? \"\".concat(e, \" - \").concat(n) : e).join();\n}\n\nfunction imag(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return identity({\n    inputs: {\n      x: n.texData.get(r.dataId).complexTensorInfos.imag\n    },\n    backend: n\n  });\n}\n\nvar imagConfig = {\n  kernelName: Imag,\n  backendName: \"webgl\",\n  kernelFunc: imag\n};\n\nfunction concatImpl(e, t, n) {\n  var r = e[0].dtype;\n\n  if (\"complex64\" === r) {\n    var _r417 = e.map(e => real({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _a301 = e.map(e => imag({\n      inputs: {\n        input: e\n      },\n      backend: n\n    })),\n        _s216 = concatImpl(_r417, t, n),\n        _o151 = concatImpl(_a301, t, n),\n        _i85 = complex({\n      inputs: {\n        real: _s216,\n        imag: _o151\n      },\n      backend: n\n    });\n\n    return _r417.forEach(e => n.disposeIntermediateTensorInfo(e)), _a301.forEach(e => n.disposeIntermediateTensorInfo(e)), n.disposeIntermediateTensorInfo(_s216), n.disposeIntermediateTensorInfo(_o151), _i85;\n  }\n\n  var a = n.shouldExecuteOnCPU(e);\n\n  if (\"string\" === r && (a = !0), a) {\n    var _a302 = e.map(e => {\n      var r = sizeFromShape(e.shape.slice(t));\n      return reshape({\n        inputs: {\n          x: e\n        },\n        backend: n,\n        attrs: {\n          shape: [-1, r]\n        }\n      });\n    }),\n        _s217 = _a302.map(e => ({\n      vals: n.readSync(e.dataId),\n      shape: e.shape\n    })),\n        _o152 = computeOutShape$1(_a302.map(e => e.shape), 1),\n        _i86 = concatImplCPU(_s217, _o152, r, 1 === _a302[0].shape[0]),\n        _l67 = computeOutShape$1(e.map(e => e.shape), t),\n        _u59 = n.makeTensorInfo(_l67, r, _i86);\n\n    return _a302.forEach(e => n.disposeIntermediateTensorInfo(e)), _u59;\n  }\n\n  if (e.length > env().getNumber(\"WEBGL_MAX_TEXTURES_IN_SHADER\")) {\n    var _r418 = Math.floor(e.length / 2),\n        _a303 = concatImpl(e.slice(0, _r418), t, n),\n        _s218 = concatImpl(e.slice(_r418), t, n),\n        _o153 = concatImpl([_a303, _s218], t, n);\n\n    return n.disposeIntermediateTensorInfo(_a303), n.disposeIntermediateTensorInfo(_s218), _o153;\n  }\n\n  if (env().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") && e[0].shape.length > 1) {\n    var _a304 = new ConcatPackedProgram(e.map(e => e.shape), t);\n\n    return n.runWebGLProgram(_a304, e, r);\n  }\n\n  var {\n    tensors2D: s,\n    outShape: o\n  } = computeTensors2D(e, t, n),\n      i = new ConcatProgram(s.map(e => e.shape)),\n      l = n.runWebGLProgram(i, s, r);\n  s.forEach(e => n.disposeIntermediateTensorInfo(e));\n  var u = reshape({\n    inputs: {\n      x: l\n    },\n    attrs: {\n      shape: o\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(l), u;\n}\n\nfunction computeTensors2D(e, t, n) {\n  var r = computeOutShape$1(e.map(e => e.shape), t);\n  return {\n    tensors2D: e.map(e => reshape({\n      inputs: {\n        x: e\n      },\n      attrs: {\n        shape: [-1, sizeFromShape(e.shape.slice(t))]\n      },\n      backend: n\n    })),\n    outShape: r\n  };\n}\n\nfunction concat(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r,\n      s = parseAxisParam(a, t[0].shape)[0],\n      o = computeOutShape$1(t.map(e => e.shape), s);\n  if (0 === sizeFromShape(o)) return n.makeTensorInfo(o, t[0].dtype, []);\n  var i = t.filter(e => sizeFromShape(e.shape) > 0);\n  return 1 === i.length ? identity({\n    inputs: {\n      x: i[0]\n    },\n    backend: n\n  }) : (assertParamsConsistent(i.map(e => e.shape), s), concatImpl(i, s, n));\n}\n\nvar concatConfig = {\n  kernelName: Concat,\n  backendName: \"webgl\",\n  kernelFunc: concat\n};\n\nclass Conv2DProgram {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var s = e.padInfo.top,\n        o = e.padInfo.left,\n        i = e.strideHeight,\n        l = e.strideWidth,\n        u = e.dilationHeight,\n        c = e.dilationWidth,\n        p = e.filterHeight,\n        d = e.filterWidth,\n        h = 4 * Math.floor(e.inChannels / 4),\n        m = e.inChannels % 4,\n        f = \"channelsLast\" === e.dataFormat,\n        g = f ? 1 : 2,\n        $ = f ? 2 : 3,\n        y = f ? 3 : 1;\n    var b = \"\",\n        x = \"\";\n    n && (b = r ? \"float activation(float a) {\\n          float b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(n, \"\\n        }\") : a ? \"float activation(float a) {\\n          float b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(n, \"\\n        }\") : \"\\n          float activation(float x) {\\n            \".concat(n, \"\\n          }\\n        \"), x = \"result = activation(result);\");\n    var v = t ? \"result += getBiasAtOutCoords();\" : \"\";\n    t && this.variableNames.push(\"bias\"), r && this.variableNames.push(\"preluActivationWeights\"), a && this.variableNames.push(\"leakyreluAlpha\"), this.userCode = \"\\n      \".concat(b, \"\\n\\n      const ivec2 strides = ivec2(\").concat(i, \", \").concat(l, \");\\n      const ivec2 pads = ivec2(\").concat(s, \", \").concat(o, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d2 = coords[\").concat(y, \"];\\n\\n        ivec2 xRCCorner =\\n            ivec2(coords[\").concat(g, \"], coords[\").concat($, \"]) * strides - pads;\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        // Convolve x(?, ?, d1) with w(:, :, d1, d2) to get y(yR, yC, d2).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(p, \"; wR++) {\\n          int xR = xRCorner + wR * \").concat(u, \";\\n\\n          if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n            continue;\\n          }\\n\\n          for (int wC = 0; wC < \").concat(d, \"; wC++) {\\n            int xC = xCCorner + wC * \").concat(c, \";\\n\\n            if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n              continue;\\n            }\\n\\n            for (int d1 = 0; d1 < \").concat(h, \"; d1 += 4) {\\n              vec4 wValues = vec4(\\n                getW(wR, wC, d1, d2),\\n                getW(wR, wC, d1 + 1, d2),\\n                getW(wR, wC, d1 + 2, d2),\\n                getW(wR, wC, d1 + 3, d2)\\n              );\\n\\n              if (\").concat(f, \") {\\n                vec4 xValues = vec4(\\n                  getX(batch, xR, xC, d1),\\n                  getX(batch, xR, xC, d1 + 1),\\n                  getX(batch, xR, xC, d1 + 2),\\n                  getX(batch, xR, xC, d1 + 3)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else {\\n                vec4 xValues = vec4(\\n                  getX(batch, d1, xR, xC),\\n                  getX(batch, d1 + 1, xR, xC),\\n                  getX(batch, d1 + 2, xR, xC),\\n                  getX(batch, d1 + 3, xR, xC)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n            }\\n\\n            if (\").concat(1 === m, \") {\\n\\n              if (\").concat(f, \") {\\n                dotProd +=\\n                    getX(batch, xR, xC, \").concat(h, \") *\\n                    getW(wR, wC, \").concat(h, \", d2);\\n              } else {\\n                dotProd +=\\n                    getX(batch, \").concat(h, \", xR, xC) *\\n                    getW(wR, wC, \").concat(h, \", d2);\\n              }\\n\\n            } else if (\").concat(2 === m, \") {\\n              vec2 wValues = vec2(\\n                getW(wR, wC, \").concat(h, \", d2),\\n                getW(wR, wC, \").concat(h, \" + 1, d2)\\n              );\\n\\n              if (\").concat(f, \") {\\n                vec2 xValues = vec2(\\n                  getX(batch, xR, xC, \").concat(h, \"),\\n                  getX(batch, xR, xC, \").concat(h, \" + 1)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else {\\n                vec2 xValues = vec2(\\n                  getX(batch, \").concat(h, \", xR, xC),\\n                  getX(batch, \").concat(h, \" + 1, xR, xC)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n\\n            } else if (\").concat(3 === m, \") {\\n              vec3 wValues = vec3(\\n                getW(wR, wC, \").concat(h, \", d2),\\n                getW(wR, wC, \").concat(h, \" + 1, d2),\\n                getW(wR, wC, \").concat(h, \" + 2, d2)\\n              );\\n\\n              if (\").concat(f, \") {\\n                vec3 xValues = vec3(\\n                  getX(batch, xR, xC, \").concat(h, \"),\\n                  getX(batch, xR, xC, \").concat(h, \" + 1),\\n                  getX(batch, xR, xC, \").concat(h, \" + 2)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else {\\n                vec3 xValues = vec3(\\n                  getX(batch, \").concat(h, \", xR, xC),\\n                  getX(batch, \").concat(h, \" + 1, xR, xC),\\n                  getX(batch, \").concat(h, \" + 2, xR, xC)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n\\n            }\\n          }\\n        }\\n\\n        float result = dotProd;\\n        \").concat(v, \"\\n        \").concat(x, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv3DProgram {\n  constructor(e) {\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var t = e.padInfo.front,\n        n = e.padInfo.top,\n        r = e.padInfo.left,\n        a = e.strideDepth,\n        s = e.strideHeight,\n        o = e.strideWidth,\n        i = e.dilationDepth,\n        l = e.dilationHeight,\n        u = e.dilationWidth,\n        c = e.filterDepth,\n        p = e.filterHeight,\n        d = e.filterWidth,\n        h = 4 * Math.floor(e.inChannels / 4),\n        m = e.inChannels % 4;\n    this.userCode = \"\\n      const ivec3 strides = ivec3(\".concat(a, \", \").concat(s, \", \").concat(o, \");\\n      const ivec3 pads = ivec3(\").concat(t, \", \").concat(n, \", \").concat(r, \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int d2 = coords.u;\\n\\n        ivec3 xFRCCorner = ivec3(coords.y, coords.z, coords.w) * strides - pads;\\n        int xFCorner = xFRCCorner.x;\\n        int xRCorner = xFRCCorner.y;\\n        int xCCorner = xFRCCorner.z;\\n\\n        // Convolve x(?, ?, ?, d1) with w(:, :, :, d1, d2) to get\\n        // y(yF, yR, yC, d2). ? = to be determined. : = across all\\n        // values in that axis.\\n        float dotProd = 0.0;\\n        for (int wF = 0; wF < \").concat(c, \"; wF++) {\\n          int xF = xFCorner + wF * \").concat(i, \";\\n\\n          if (xF < 0 || xF >= \").concat(e.inDepth, \") {\\n            continue;\\n          }\\n\\n          for (int wR = 0; wR < \").concat(p, \"; wR++) {\\n            int xR = xRCorner + wR * \").concat(l, \";\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int wC = 0; wC < \").concat(d, \"; wC++) {\\n              int xC = xCCorner + wC * \").concat(u, \";\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              for (int d1 = 0; d1 < \").concat(h, \"; d1 += 4) {\\n                vec4 xValues = vec4(\\n                  getX(batch, xF, xR, xC, d1),\\n                  getX(batch, xF, xR, xC, d1 + 1),\\n                  getX(batch, xF, xR, xC, d1 + 2),\\n                  getX(batch, xF, xR, xC, d1 + 3)\\n                );\\n                vec4 wValues = vec4(\\n                  getW(wF, wR, wC, d1, d2),\\n                  getW(wF, wR, wC, d1 + 1, d2),\\n                  getW(wF, wR, wC, d1 + 2, d2),\\n                  getW(wF, wR, wC, d1 + 3, d2)\\n                );\\n\\n                dotProd += dot(xValues, wValues);\\n              }\\n\\n              if (\").concat(1 === m, \") {\\n                dotProd +=\\n                  getX(batch, xF, xR, xC, \").concat(h, \") *\\n                  getW(wF, wR, wC, \").concat(h, \", d2);\\n              } else if (\").concat(2 === m, \") {\\n                vec2 xValues = vec2(\\n                  getX(batch, xF, xR, xC, \").concat(h, \"),\\n                  getX(batch, xF, xR, xC, \").concat(h, \" + 1)\\n                );\\n                vec2 wValues = vec2(\\n                  getW(wF, wR, wC, \").concat(h, \", d2),\\n                  getW(wF, wR, wC, \").concat(h, \" + 1, d2)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              } else if (\").concat(3 === m, \") {\\n                vec3 xValues = vec3(\\n                  getX(batch, xF, xR, xC, \").concat(h, \"),\\n                  getX(batch, xF, xR, xC, \").concat(h, \" + 1),\\n                  getX(batch, xF, xR, xC, \").concat(h, \" + 2)\\n                );\\n                vec3 wValues = vec3(\\n                  getW(wF, wR, wC, \").concat(h, \", d2),\\n                  getW(wF, wR, wC, \").concat(h, \" + 1, d2),\\n                  getW(wF, wR, wC, \").concat(h, \" + 2, d2)\\n                );\\n                dotProd += dot(xValues, wValues);\\n              }\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Im2ColPackedProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e;\n    var {\n      filterWidth: r,\n      inChannels: a,\n      strideWidth: s,\n      strideHeight: o,\n      padInfo: i,\n      outWidth: l,\n      dilationWidth: u,\n      dilationHeight: c,\n      dataFormat: p\n    } = n,\n        {\n      left: d,\n      top: h\n    } = i,\n        m = a * r,\n        f = getGlslDifferences(),\n        g = \"channelsLast\" === p,\n        $ = g ? 0 : 1,\n        y = g ? 1 : 2;\n    var b = \"\";\n\n    for (var _n435 = 0; _n435 <= 1; _n435++) {\n      for (var _r419 = 0; _r419 <= 1; _r419++) {\n        b += \"\\n          blockIndex = rc.y + \".concat(_r419, \";\\n          pos = rc.x + \").concat(_n435, \";\\n\\n          if(blockIndex < \").concat(e[1], \" && pos < \").concat(e[0], \") {\\n            offsetY = int(blockIndex / (\").concat(l, \")) * \").concat(o, \" - \").concat(h, \";\\n            d0 = offsetY + \").concat(c, \" * (pos / \").concat(m, \");\\n\\n            if(d0 < \").concat(t[$], \" && d0 >= 0) {\\n\\n              offsetX = int(mod(float(blockIndex), \").concat(l, \".) * \").concat(s, \". - \").concat(d, \".);\\n              d1 = offsetX + \").concat(u, \" * (int(mod(float(pos), \").concat(m, \".) / \").concat(a, \".));\\n\\n              if(d1 < \").concat(t[y], \" && d1 >= 0) {\\n\\n                ch = int(mod(float(pos), \").concat(a, \".));\\n\\n                if (\").concat(g, \") {\\n                  innerDims = vec2(d1, ch);\\n                  result[\").concat(2 * _n435 + _r419, \"] = getChannel(\\n                    getA(d0, int(innerDims.x),\\n                    int(innerDims.y)), innerDims);\\n                } else {\\n                  innerDims = vec2(d0, d1);\\n                  result[\").concat(2 * _n435 + _r419, \"] = getChannel(\\n                    getA(ch, int(innerDims.x),\\n                    int(innerDims.y)), innerDims);\\n                }\\n              }\\n            }\\n          }\\n        \");\n      }\n    }\n\n    this.userCode = \"\\n      void main() {\\n        ivec2 rc = getOutputCoords();\\n\\n        vec4 result = vec4(0);\\n\\n        int blockIndex, pos, offsetY, d0, offsetX, d1, ch;\\n        vec2 innerDims;\\n\\n        \".concat(b, \"\\n\\n        \").concat(f.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nfunction conv2dByMatMul(_ref67) {\n  var {\n    x: e,\n    filter: t,\n    convInfo: n,\n    backend: r,\n    bias: a = null,\n    preluActivationWeights: s = null,\n    leakyreluAlpha: o = 0,\n    activation: i = null\n  } = _ref67;\n  var l = e.shape,\n      u = r.texData.get(e.dataId),\n      c = \"channelsLast\" === n.dataFormat;\n  var p;\n  var d = [],\n      h = l[2] % 2 != 0 && !!u.isPacked;\n\n  if ((1 != l[0] * l[1] * l[2] && 1 !== n.outChannels || !(n.inChannels > MATMUL_SHARED_DIM_THRESHOLD)) && env().getBool(\"WEBGL_LAZILY_UNPACK\") && env().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") && h) {\n    var _h23 = {\n      dataId: e.dataId,\n      shape: [1, c ? l[0] * l[1] * (l[2] + 1) : l[0] * l[2] * (l[3] + 1), n.inChannels],\n      dtype: e.dtype\n    },\n        m = u.shape;\n    u.shape = u.shape.slice(), u.shape[u.shape.length - 2]++, assert$4(isReshapeFree(u.shape, _h23.shape), () => \"packed reshape \".concat(u.shape, \" to \").concat(_h23.shape, \" isn't free\"));\n    var f = reshape({\n      inputs: {\n        x: t\n      },\n      backend: r,\n      attrs: {\n        shape: [1, n.inChannels, n.outChannels]\n      }\n    });\n    d.push(f);\n    var g = batchMatMulImpl({\n      a: _h23,\n      b: f,\n      backend: r,\n      transposeA: !1,\n      transposeB: !1,\n      bias: a,\n      activation: i,\n      preluActivationWeights: s,\n      leakyreluAlpha: o\n    }),\n        $ = r.texData.get(g.dataId);\n    assert$4($.isPacked, () => \"batchMatMul result is expected to be packed\"), u.shape = m, $.shape = n.outShape, p = identity({\n      inputs: {\n        x: g\n      },\n      backend: r\n    }), p.shape = n.outShape, d.push(g);\n  } else {\n    var _u60 = reshape({\n      inputs: {\n        x: e\n      },\n      backend: r,\n      attrs: {\n        shape: [1, c ? l[0] * l[1] * l[2] : l[0] * l[2] * l[3], n.inChannels]\n      }\n    }),\n        _h24 = reshape({\n      inputs: {\n        x: t\n      },\n      backend: r,\n      attrs: {\n        shape: [1, n.inChannels, n.outChannels]\n      }\n    }),\n        _m10 = batchMatMulImpl({\n      a: _u60,\n      b: _h24,\n      transposeA: !1,\n      transposeB: !1,\n      backend: r,\n      bias: a,\n      activation: i,\n      preluActivationWeights: s,\n      leakyreluAlpha: o\n    });\n\n    p = reshape({\n      inputs: {\n        x: _m10\n      },\n      backend: r,\n      attrs: {\n        shape: n.outShape\n      }\n    }), d.push(_u60), d.push(_h24), d.push(_m10);\n  }\n\n  for (var _e1025 of d) {\n    r.disposeIntermediateTensorInfo(_e1025);\n  }\n\n  return p;\n}\n\nfunction conv2dWithIm2Row(_ref68) {\n  var {\n    x: e,\n    filter: t,\n    convInfo: n,\n    backend: r,\n    bias: a = null,\n    preluActivationWeights: s = null,\n    leakyreluAlpha: o = 0,\n    activation: i = null\n  } = _ref68;\n  var {\n    filterWidth: l,\n    filterHeight: u,\n    inChannels: c,\n    outWidth: p,\n    outHeight: d,\n    dataFormat: h\n  } = n,\n      m = \"channelsLast\" === h,\n      f = l * u * c,\n      g = d * p,\n      $ = [f, g],\n      y = [],\n      b = reshape({\n    inputs: {\n      x: e\n    },\n    backend: r,\n    attrs: {\n      shape: e.shape.slice(1)\n    }\n  }),\n      x = reshape({\n    inputs: {\n      x: t\n    },\n    backend: r,\n    attrs: {\n      shape: [1, f, sizeFromShape(t.shape) / f]\n    }\n  });\n  y.push(b), y.push(x);\n  var v = new Im2ColPackedProgram($, b.shape, n),\n      I = r.runWebGLProgram(v, [b], \"float32\"),\n      C = reshape({\n    inputs: {\n      x: I\n    },\n    backend: r,\n    attrs: {\n      shape: [1, $[0], $[1]]\n    }\n  });\n  y.push(I), y.push(C);\n  var S = null != a,\n      k = null != s,\n      T = \"leakyrelu\" === i,\n      N = i ? mapActivationToShaderProgram(i, !0) : null,\n      w = new MatMulPackedProgram(C.shape, x.shape, [1, g, n.outChannels], !0, !1, S, N, k, T),\n      E = [C, x];\n\n  if (a && E.push(a), k && E.push(s), T) {\n    var _e1026 = r.makeTensorInfo([], \"float32\", createScalarValue(o, \"float32\"));\n\n    E.push(_e1026), y.push(_e1026);\n  }\n\n  var A = r.runWebGLProgram(w, E, \"float32\"),\n      D = reshape({\n    inputs: {\n      x: A\n    },\n    backend: r,\n    attrs: {\n      shape: m ? [1, d, p, n.outChannels] : [1, n.outChannels, d, p]\n    }\n  });\n  y.push(A);\n\n  for (var _e1027 of y) {\n    r.disposeIntermediateTensorInfo(_e1027);\n  }\n\n  return D;\n}\n\nfunction conv2d(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dilations: u,\n    dimRoundingMode: c\n  } = r,\n      p = convertConv2DDataFormat(l),\n      d = computeConv2DInfo(a.shape, s.shape, o, u, i, c, !1, p);\n  var h;\n  if (1 !== d.filterHeight || 1 !== d.filterWidth || 1 !== d.dilationHeight || 1 !== d.dilationWidth || 1 !== d.strideHeight || 1 !== d.strideWidth || \"SAME\" !== d.padInfo.type && \"VALID\" !== d.padInfo.type) {\n    if (env().getBool(\"WEBGL_CONV_IM2COL\") && 1 === a.shape[0]) h = conv2dWithIm2Row({\n      x: a,\n      filter: s,\n      convInfo: d,\n      backend: n\n    });else {\n      var _e1028 = new Conv2DProgram(d);\n\n      h = n.runWebGLProgram(_e1028, [a, s], \"float32\");\n    }\n  } else h = conv2dByMatMul({\n    x: a,\n    filter: s,\n    convInfo: d,\n    backend: n\n  });\n  var m = reshape({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      shape: d.outShape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), m;\n}\n\nvar conv2DConfig = {\n  kernelName: Conv2D$1,\n  backendName: \"webgl\",\n  kernelFunc: conv2d\n};\n\nclass Conv2DDerFilterProgram {\n  constructor(e) {\n    this.variableNames = [\"x\", \"dy\"], this.outputShape = e.filterShape, this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int wR = coords.x;\\n        int wC = coords.y;\\n        int d1 = coords.z;\\n        int d2 = coords.w;\\n\\n        // Convolve x(?, ?, d1) with dy(:, :, d2) to get dw(wR, wC, d1, d2).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n\\n        for (int b = 0; b < \".concat(e.batchSize, \"; b++) {\\n          for (int yR = 0; yR < \").concat(e.outHeight, \"; yR++) {\\n            int xR = wR + yR * \").concat(e.strideHeight, \" - \").concat(e.padInfo.top, \";\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int yC = 0; yC < \").concat(e.outWidth, \"; yC++) {\\n              int xC = wC + yC * \").concat(e.strideWidth, \" - \").concat(e.padInfo.left, \";\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              if (\").concat(\"channelsLast\" === e.dataFormat, \") {\\n                float dyValue = getDy(b, yR, yC, d2);\\n                float xValue = getX(b, xR, xC, d1);\\n                dotProd += (xValue * dyValue);\\n              } else {\\n                float dyValue = getDy(b, d2, yR, yC);\\n                float xValue = getX(b, d1, xR, xC);\\n                dotProd += (xValue * dyValue);\\n              }\\n\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv2DDerInputProgram {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"W\"], this.outputShape = e.inShape;\n    var t = e.filterHeight,\n        n = e.filterWidth,\n        r = \"channelsLast\" === e.dataFormat;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d1 = coords[\").concat(r ? 3 : 1, \"];\\n\\n        ivec2 dyCorner = ivec2(coords[\").concat(r ? 1 : 2, \"], coords[\").concat(r ? 2 : 3, \"]) - pads;\\n        int dyRCorner = dyCorner.x;\\n        int dyCCorner = dyCorner.y;\\n\\n        // Convolve dy(?, ?, d2) with w(:, :, d1, d2) to compute dx(xR, xC, d1).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(t, \"; wR++) {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          int wRPerm = \").concat(t, \" - 1 - wR;\\n\\n          for (int wC = 0; wC < \").concat(n, \"; wC++) {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            int wCPerm = \").concat(n, \" - 1 - wC;\\n\\n            for (int d2 = 0; d2 < \").concat(e.outChannels, \"; d2++) {\\n\\n              if (\").concat(r, \") {\\n                float xValue = getDy(batch, idyR, idyC, d2);\\n                float wValue = getW(wRPerm, wCPerm, d1, d2);\\n                dotProd += xValue * wValue;\\n              } else {\\n                float xValue = getDy(batch, d2, idyR, idyC);\\n                float wValue = getW(wRPerm, wCPerm, d1, d2);\\n                dotProd += xValue * wValue;\\n              }\\n\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv3DDerFilterProgram {\n  constructor(e) {\n    this.variableNames = [\"x\", \"dy\"], this.outputShape = e.filterShape, this.userCode = \"\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int wF = coords.x;\\n        int wR = coords.y;\\n        int wC = coords.z;\\n        int d1 = coords.w;\\n        int d2 = coords.u;\\n\\n        float dotProd = 0.0;\\n\\n        for (int b = 0; b < \".concat(e.batchSize, \"; b++) {\\n          for (int yF = 0; yF < \").concat(e.outDepth, \"; yF++) {\\n            int xF = wF + yF * \").concat(e.strideDepth, \" - \").concat(e.padInfo.front, \";\\n\\n            if (xF < 0 || xF >= \").concat(e.inDepth, \") {\\n              continue;\\n            }\\n\\n            for (int yR = 0; yR < \").concat(e.outHeight, \"; yR++) {\\n              int xR = wR + yR * \").concat(e.strideHeight, \" - \").concat(e.padInfo.top, \";\\n\\n              if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n                continue;\\n              }\\n\\n              for (int yC = 0; yC < \").concat(e.outWidth, \"; yC++) {\\n                int xC = wC + yC * \").concat(e.strideWidth, \" - \").concat(e.padInfo.left, \";\\n\\n                if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                  continue;\\n                }\\n\\n                float dyValue = getDy(b, yF, yR, yC, d2);\\n                float xValue = getX(b, xF, xR, xC, d1);\\n                dotProd += (xValue * dyValue);\\n              }\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass Conv3DDerInputProgram {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"W\"], this.outputShape = e.inShape;\n    var t = e.filterDepth,\n        n = e.filterHeight,\n        r = e.filterWidth;\n    this.userCode = \"\\n      const ivec3 pads = ivec3(\".concat(t - 1 - e.padInfo.front, \", \").concat(n - 1 - e.padInfo.top, \", \").concat(r - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int d1 = coords.u;\\n\\n\\n        ivec3 dyCorner = ivec3(coords.y, coords.z, coords.w) - pads;\\n        int dyFCorner = dyCorner.x;\\n        int dyRCorner = dyCorner.y;\\n        int dyCCorner = dyCorner.z;\\n\\n        float dotProd = 0.0;\\n        for (int wF = 0; wF < \").concat(t, \"; wF++) {\\n          float dyF = float(dyFCorner + wF) / \").concat(e.strideDepth, \".0;\\n\\n          if (dyF < 0.0 || dyF >= \").concat(e.outDepth, \".0 || fract(dyF) > 0.0) {\\n            continue;\\n          }\\n          int idyF = int(dyF);\\n\\n          int wFPerm = \").concat(t, \" - 1 - wF;\\n\\n          for (int wR = 0; wR < \").concat(n, \"; wR++) {\\n            float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n            if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 ||\\n              fract(dyR) > 0.0) {\\n              continue;\\n            }\\n            int idyR = int(dyR);\\n\\n            int wRPerm = \").concat(n, \" - 1 - wR;\\n\\n            for (int wC = 0; wC < \").concat(r, \"; wC++) {\\n              float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n              if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                  fract(dyC) > 0.0) {\\n                continue;\\n              }\\n              int idyC = int(dyC);\\n\\n              int wCPerm = \").concat(r, \" - 1 - wC;\\n\\n              for (int d2 = 0; d2 < \").concat(e.outChannels, \"; d2++) {\\n                float xValue = getDy(batch, idyF, idyR, idyC, d2);\\n                float wValue = getW(wFPerm, wRPerm, wCPerm, d1, d2);\\n                dotProd += xValue * wValue;\\n              }\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction conv2DBackpropFilter(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r,\n      p = convertConv2DDataFormat(l),\n      d = computeConv2DInfo(a.shape, c, o, 1, i, u, !1, p),\n      h = new Conv2DDerFilterProgram(d);\n  return n.runWebGLProgram(h, [a, s], \"float32\");\n}\n\nvar conv2DBackpropFilterConfig = {\n  kernelName: Conv2DBackpropFilter,\n  backendName: \"webgl\",\n  kernelFunc: conv2DBackpropFilter\n};\n\nfunction conv2DBackpropInput(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    inputShape: o,\n    strides: i,\n    pad: l,\n    dataFormat: u,\n    dimRoundingMode: c\n  } = r,\n      p = convertConv2DDataFormat(u),\n      d = computeConv2DInfo(o, s.shape, i, 1, l, c, !1, p),\n      h = new Conv2DDerInputProgram(d);\n  return n.runWebGLProgram(h, [a, s], \"float32\");\n}\n\nvar conv2DBackpropInputConfig = {\n  kernelName: Conv2DBackpropInput,\n  backendName: \"webgl\",\n  kernelFunc: conv2DBackpropInput\n};\n\nfunction conv3D(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l\n  } = r,\n      u = computeConv3DInfo(a.shape, s.shape, o, l, i),\n      c = new Conv3DProgram(u);\n  return n.runWebGLProgram(c, [a, s], \"float32\");\n}\n\nvar conv3DConfig = {\n  kernelName: Conv3D$1,\n  backendName: \"webgl\",\n  kernelFunc: conv3D\n};\n\nfunction conv3DBackpropFilterV2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    filterShape: l\n  } = r,\n      u = computeConv3DInfo(a.shape, l, o, 1, i),\n      c = new Conv3DDerFilterProgram(u);\n  return n.runWebGLProgram(c, [a, s], \"float32\");\n}\n\nvar conv3DBackpropFilterV2Config = {\n  kernelName: Conv3DBackpropFilterV2,\n  backendName: \"webgl\",\n  kernelFunc: conv3DBackpropFilterV2\n};\n\nfunction conv3DBackpropInput(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    pad: o,\n    strides: i,\n    inputShape: l\n  } = r,\n      u = computeConv3DInfo(l, s.shape, i, 1, o),\n      c = new Conv3DDerInputProgram(u);\n  return n.runWebGLProgram(c, [a, s], \"float32\");\n}\n\nvar conv3DBackpropInputConfig = {\n  kernelName: Conv3DBackpropInputV2,\n  backendName: \"webgl\",\n  kernelFunc: conv3DBackpropInput\n},\n    COS = CHECK_NAN_SNIPPET_UNARY + \"\\n  return cos(x);\\n\",\n    cos = unaryKernelFunc({\n  opSnippet: COS\n}),\n    cosConfig = {\n  kernelName: Cos,\n  backendName: \"webgl\",\n  kernelFunc: cos\n},\n    COSH = \"\\n  float e2x = exp(-x);\\n  return (e2x + 1.0 / e2x) / 2.0;\\n\",\n    cosh = unaryKernelFunc({\n  opSnippet: COSH\n}),\n    coshConfig = {\n  kernelName: Cosh,\n  backendName: \"webgl\",\n  kernelFunc: cosh\n};\n\nclass CropAndResizeProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"Image\", \"Boxes\", \"BoxInd\"], this.outputShape = [];\n    var [s, o, i, l] = e,\n        [u] = t,\n        [c, p] = n;\n    this.outputShape = [u, c, p, l];\n    var d = \"bilinear\" === r ? 1 : 0,\n        [h, m] = [o - 1 + \".0\", i - 1 + \".0\"],\n        [f, g, $] = c > 1 ? [\"\" + (o - 1) / (c - 1), \"(y2-y1) * height_ratio\", \"y1*\".concat(h, \" + float(y)*(height_scale)\")] : [\"0.0\", \"0.0\", \"0.5 * (y1+y2) * \".concat(h)],\n        [y, b, x] = p > 1 ? [\"\" + (i - 1) / (p - 1), \"(x2-x1) * width_ratio\", \"x1*\".concat(m, \" + float(x)*(width_scale)\")] : [\"0.0\", \"0.0\", \"0.5 * (x1+x2) * \".concat(m)];\n    this.userCode = \"\\n      const float height_ratio = float(\".concat(f, \");\\n      const float width_ratio = float(\").concat(y, \");\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int y = coords[1];\\n        int x = coords[2];\\n        int d = coords[3];\\n\\n        // get box vals\\n        float y1 = getBoxes(b,0);\\n        float x1 = getBoxes(b,1);\\n        float y2 = getBoxes(b,2);\\n        float x2 = getBoxes(b,3);\\n\\n        // get image in batch index\\n        int bInd = round(getBoxInd(b));\\n        if(bInd < 0 || bInd >= \").concat(s, \") {\\n          return;\\n        }\\n\\n        float height_scale = \").concat(g, \";\\n        float width_scale = \").concat(b, \";\\n\\n        float in_y = \").concat($, \";\\n        if( in_y < 0.0 || in_y > \").concat(h, \" ) {\\n          setOutput(float(\").concat(a, \"));\\n          return;\\n        }\\n        float in_x = \").concat(x, \";\\n        if( in_x < 0.0 || in_x > \").concat(m, \" ) {\\n          setOutput(float(\").concat(a, \"));\\n          return;\\n        }\\n\\n        vec2 sourceFracIndexCR = vec2(in_x,in_y);\\n        if(\").concat(d, \" == 1) {\\n          // Compute the four integer indices.\\n          ivec2 sourceFloorCR = ivec2(sourceFracIndexCR);\\n          ivec2 sourceCeilCR = ivec2(ceil(sourceFracIndexCR));\\n\\n          float topLeft = getImage(b, sourceFloorCR.y, sourceFloorCR.x, d);\\n          float bottomLeft = getImage(b, sourceCeilCR.y, sourceFloorCR.x, d);\\n          float topRight = getImage(b, sourceFloorCR.y, sourceCeilCR.x, d);\\n          float bottomRight = getImage(b, sourceCeilCR.y, sourceCeilCR.x, d);\\n\\n          vec2 fracCR = sourceFracIndexCR - vec2(sourceFloorCR);\\n\\n          float top = topLeft + (topRight - topLeft) * fracCR.x;\\n          float bottom = bottomLeft + (bottomRight - bottomLeft) * fracCR.x;\\n          float newValue = top + (bottom - top) * fracCR.y;\\n          setOutput(newValue);\\n        } else {\\n          // Compute the coordinators of nearest neighbor point.\\n          ivec2 sourceNearestCR = ivec2(floor(\\n            sourceFracIndexCR + vec2(0.5,0.5)));\\n          float newValue = getImage(b, sourceNearestCR.y, sourceNearestCR.x, d);\\n          setOutput(newValue);\\n        }\\n      }\\n    \");\n  }\n\n}\n\nvar cropAndResize = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    image: a,\n    boxes: s,\n    boxInd: o\n  } = t,\n      {\n    cropSize: i,\n    method: l,\n    extrapolationValue: u\n  } = r,\n      c = new CropAndResizeProgram(a.shape, s.shape, i, l, u);\n  return n.runWebGLProgram(c, [a, s, o], \"float32\");\n},\n    cropAndResizeConfig = {\n  kernelName: CropAndResize,\n  backendName: \"webgl\",\n  kernelFunc: cropAndResize\n};\n\nclass CumSumProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.customUniforms = [{\n      name: \"index\",\n      type: \"float\"\n    }], this.outputShape = e;\n    var r = e.length,\n        a = t ? \"0.0\" : \"getX(\".concat(getCoords(r, \"coords\"), \")\"),\n        s = e[e.length - 1];\n    var o = \"\",\n        i = \"\";\n    t ? (o = n ? \"end != \" + (s - 1) : \"end != 0\", i = n ? \"end + 1\" : \"end - 1\") : (o = n ? \"end + pow2 < \".concat(s) : \"end >= pow2\", i = n ? \"end + pow2\" : \"end - pow2\"), this.userCode = \"\\n      void main() {\\n        \".concat(getCoordsDataType(r), \" coords = getOutputCoords();\\n        int end = \").concat(getFinalCoord(r, \"coords\"), \";\\n        float val = \").concat(a, \";\\n        int pow2 = int(pow(2.0, index));\\n        if (\").concat(o, \") {\\n          int idx = \").concat(i, \";\\n          \").concat(getFinalCoord(r, \"coords\"), \" = idx;\\n          val += getX(\").concat(getCoords(r, \"coords\"), \");\\n        }\\n        setOutput(val);\\n      }\\n    \");\n  }\n\n}\n\nfunction getCoords(e, t) {\n  if (1 === e) return \"\".concat(t);\n  if (2 === e) return \"\".concat(t, \".x, \").concat(t, \".y\");\n  if (3 === e) return \"\".concat(t, \".x, \").concat(t, \".y, \").concat(t, \".z\");\n  if (4 === e) return \"\".concat(t, \".x, \").concat(t, \".y, \").concat(t, \".z, \").concat(t, \".w\");\n  throw Error(\"Cumulative sum for rank \".concat(e, \" is not yet supported\"));\n}\n\nfunction getFinalCoord(e, t) {\n  if (1 === e) return \"\".concat(t);\n  if (2 === e) return \"\".concat(t, \".y\");\n  if (3 === e) return \"\".concat(t, \".z\");\n  if (4 === e) return \"\".concat(t, \".w\");\n  throw Error(\"Cumulative sum for rank \".concat(e, \" is not yet supported\"));\n}\n\nfunction cumsum(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    exclusive: o,\n    reverse: i\n  } = r,\n      l = a.shape.length,\n      u = getAxesPermutation([s], l);\n  var c = a;\n  null != u && (c = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: u\n    }\n  }));\n  var p = getInnerMostAxes(1, l)[0];\n  if (p !== l - 1) throw new Error(\"WebGL cumsum shader expects an inner-most axis=\".concat(a.shape.length - 1, \" but got axis=\").concat(s));\n  var d = c.shape[p];\n  var h = identity({\n    inputs: {\n      x: c\n    },\n    backend: n\n  });\n\n  for (var _e1029 = 0; _e1029 <= Math.ceil(Math.log2(d)) - 1; _e1029++) {\n    var _t707 = new CumSumProgram(c.shape, !1, i),\n        _r420 = h;\n\n    h = n.runWebGLProgram(_t707, [h], h.dtype, [[_e1029]]), n.disposeIntermediateTensorInfo(_r420);\n  }\n\n  if (o) {\n    var _e1030 = new CumSumProgram(c.shape, o, i),\n        _t708 = h;\n\n    h = n.runWebGLProgram(_e1030, [h], h.dtype), n.disposeIntermediateTensorInfo(_t708);\n  }\n\n  if (null != u) {\n    var _e1031 = transpose({\n      inputs: {\n        x: h\n      },\n      backend: n,\n      attrs: {\n        perm: getUndoAxesPermutation(u)\n      }\n    });\n\n    return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(c), _e1031;\n  }\n\n  return h;\n}\n\nvar cumsumConfig = {\n  kernelName: Cumsum,\n  backendName: \"webgl\",\n  kernelFunc: cumsum\n};\n\nfunction denseBincount(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    weights: s\n  } = t,\n      {\n    size: o,\n    binaryOutput: i\n  } = r;\n\n  if (1 === a.shape.length) {\n    var _e1032 = n.readSync(a.dataId),\n        _t709 = n.readSync(s.dataId),\n        _r421 = bincountImplCPU(_e1032, _t709, s.dtype, s.shape, o);\n\n    return n.makeTensorInfo([o], s.dtype, _r421);\n  }\n\n  if (2 === a.shape.length) {\n    var _e1033 = n.bufferSync(a),\n        _t710 = n.bufferSync(s),\n        _r422 = bincountReduceImplCPU(_e1033, _t710, o, i);\n\n    return n.makeTensorInfo(_r422.shape, s.dtype, _r422.values);\n  }\n\n  throw new Error(\"Error in denseBincount: input must be at most rank 2, but got rank\".concat(a.shape.length, \".\"));\n}\n\nvar denseBincountConfig = {\n  kernelName: DenseBincount,\n  backendName: \"webgl\",\n  kernelFunc: denseBincount\n};\n\nclass DepthToSpaceProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.outputShape = [], this.outputShape = e, this.blockSize = t, this.dataFormat = n, this.userCode = \"\\n    void main() {\\n      ivec4 coords = getOutputCoords();\\n      int b = coords[0];\\n      int h = \".concat(this.getHeightCoordString(), \";\\n      int w = \").concat(this.getWidthCoordString(), \";\\n      int d = \").concat(this.getDepthCoordString(), \";\\n\\n      int in_h = h / \").concat(t, \";\\n      int offset_h = imod(h, \").concat(t, \");\\n      int in_w = w / \").concat(t, \";\\n      int offset_w = imod(w, \").concat(t, \");\\n      int offset_d = (offset_h * \").concat(t, \" + offset_w) *\\n        \").concat(this.getOutputDepthSize(), \";\\n      int in_d = d + offset_d;\\n\\n      float result = \").concat(this.getInputSamplingString(), \";\\n      setOutput(result);\\n    }\\n  \");\n  }\n\n  getHeightCoordString() {\n    return \"NHWC\" === this.dataFormat ? \"coords[1]\" : \"coords[2]\";\n  }\n\n  getWidthCoordString() {\n    return \"NHWC\" === this.dataFormat ? \"coords[2]\" : \"coords[3]\";\n  }\n\n  getDepthCoordString() {\n    return \"NHWC\" === this.dataFormat ? \"coords[3]\" : \"coords[1]\";\n  }\n\n  getOutputDepthSize() {\n    return \"NHWC\" === this.dataFormat ? this.outputShape[3] : this.outputShape[1];\n  }\n\n  getInputSamplingString() {\n    return \"NHWC\" === this.dataFormat ? \"getX(b, in_h, in_w, in_d)\" : \"getX(b, in_d, in_h, in_w)\";\n  }\n\n}\n\nfunction depthToSpace(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockSize: s,\n    dataFormat: o\n  } = r;\n  assert$4(s > 1, () => \"blockSize should be > 1 for depthToSpace, but was: \".concat(s));\n  var i = a.shape[0],\n      l = (\"NHWC\" === o ? a.shape[1] : a.shape[2]) * s,\n      u = (\"NHWC\" === o ? a.shape[2] : a.shape[3]) * s,\n      c = (\"NHWC\" === o ? a.shape[3] : a.shape[1]) / (s * s),\n      p = new DepthToSpaceProgram(\"NHWC\" === o ? [i, l, u, c] : [i, c, l, u], s, o);\n  return n.runWebGLProgram(p, [a], a.dtype);\n}\n\nvar depthToSpaceConfig = {\n  kernelName: DepthToSpace,\n  backendName: \"webgl\",\n  kernelFunc: depthToSpace\n};\n\nclass DepthwiseConv2DProgram {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var s = e.inHeight,\n        o = e.inWidth,\n        i = e.padInfo.top,\n        l = e.padInfo.left,\n        u = e.strideHeight,\n        c = e.strideWidth,\n        p = e.dilationHeight,\n        d = e.dilationWidth,\n        h = e.filterHeight,\n        m = e.filterWidth,\n        f = e.outChannels / e.inChannels;\n    var g = \"\",\n        $ = \"\";\n    n && (g = r ? \"float activation(float a) {\\n          float b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(n, \"\\n        }\") : a ? \"float activation(float a) {\\n          float b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(n, \"\\n        }\") : \"\\n          float activation(float x) {\\n            \".concat(n, \"\\n          }\\n        \"), $ = \"result = activation(result);\");\n    var y = t ? \"result += getBiasAtOutCoords();\" : \"\";\n    t && this.variableNames.push(\"bias\"), r && this.variableNames.push(\"preluActivationWeights\"), a && this.variableNames.push(\"leakyreluAlpha\"), this.userCode = \"\\n      \".concat(g, \"\\n\\n      const ivec2 strides = ivec2(\").concat(u, \", \").concat(c, \");\\n      const ivec2 pads = ivec2(\").concat(i, \", \").concat(l, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords.x;\\n        ivec2 xRCCorner = coords.yz * strides - pads;\\n        int d2 = coords.w;\\n        int d1 = d2 / \").concat(f, \";\\n        int q = d2 - d1 * \").concat(f, \";\\n\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        // Convolve x(?, ?, d1) with w(:, :, d1, q) to get y(yR, yC, d2).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        // TO DO(dsmilkov): Flatten the two for loops and vec4 the operations.\\n        for (int wR = 0; wR < \").concat(h, \"; wR++) {\\n          int xR = xRCorner + wR * \").concat(p, \";\\n\\n          if (xR < 0 || xR >= \").concat(s, \") {\\n            continue;\\n          }\\n\\n          for (int wC = 0; wC < \").concat(m, \"; wC++) {\\n            int xC = xCCorner + wC * \").concat(d, \";\\n\\n            if (xC < 0 || xC >= \").concat(o, \") {\\n              continue;\\n            }\\n\\n            float xVal = getX(batch, xR, xC, d1);\\n            float wVal = getW(wR, wC, d1, q);\\n            dotProd += xVal * wVal;\\n          }\\n        }\\n\\n        float result = dotProd;\\n        \").concat(y, \"\\n        \").concat($, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nclass DepthwiseConvPacked2DProgram {\n  constructor(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : !1;\n    this.variableNames = [\"x\", \"W\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = e.outShape;\n    var s = e.outChannels / e.inChannels,\n        o = e.inHeight,\n        i = e.inWidth,\n        l = e.padInfo.top,\n        u = e.padInfo.left,\n        c = e.strideHeight,\n        p = e.strideWidth,\n        d = e.dilationHeight,\n        h = e.dilationWidth,\n        m = e.filterHeight,\n        f = e.filterWidth,\n        g = f;\n    var $ = \"\\n      int xR; int xC; int xCOffset;\\n      vec4 wTexel; vec4 previous; vec4 final;\";\n\n    for (var _e1034 = 0; _e1034 < f; _e1034++) {\n      $ += \"\\n          vec4 xTexelC\".concat(2 * _e1034, \";\\n          int xTexelC\").concat(2 * _e1034, \"Ready;\\n          vec4 xTexelC\").concat(2 * _e1034 + 1, \";\\n          int xTexelC\").concat(2 * _e1034 + 1, \"Ready;\\n          vec4 xC\").concat(_e1034, \";\");\n    }\n\n    for (var _e1035 = 0; _e1035 < m; _e1035++) {\n      for (var _e1036 = 0; _e1036 < f; _e1036++) {\n        $ += \"\\n          xTexelC\".concat(2 * _e1036, \" = vec4(0.0);\\n          xTexelC\").concat(2 * _e1036, \"Ready = 0;\\n          xTexelC\").concat(2 * _e1036 + 1, \" = vec4(0.0);\\n          xTexelC\").concat(2 * _e1036 + 1, \"Ready = 0;\\n          xC\").concat(_e1036, \" = vec4(0.0);\");\n      }\n\n      $ += \"\\n        xR = xRCorner + \".concat(_e1035 * d, \";\\n        if (xR >=0 && xR < \").concat(o, \") {\\n      \");\n\n      for (var _t711 = 0; _t711 < (g + 1) / 2; _t711++) {\n        var _n436 = 2 * _t711,\n            _r423 = _n436 * h;\n\n        if ($ += \"\\n          xC = xCCorner + \".concat(_r423, \";\\n          \"), 1 === p) {\n          if (_n436 < f && (u % 2 == 1 ? ($ += \"\\n                xCOffset = xC + 1;\\n                if (xCOffset >= 0 && xCOffset < \".concat(i, \" && xTexelC\").concat(_n436, \"Ready == 0) {\\n                  xTexelC\").concat(_n436, \" = getX(batch, xR, xCOffset, d1);\\n\\n                  // Need to manually clear unused channels in case\\n                  // we're reading from recycled texture.\\n                  if (xCOffset + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n436, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n436, \"Ready = 1;\\n                }\\n              \"), $ += 1 === h && _r423 > 0 ? \"\\n                xC\".concat(_n436, \" = vec4(xTexelC\").concat(_n436 - 2, \".zw, xTexelC\").concat(_n436, \".xy);\\n                \") : \"\\n                  xCOffset = xC + 1 - 2;\\n\\n                  if (xCOffset >= 0 && xCOffset < \".concat(i, \") {\\n                    previous = getX(batch, xR, xCOffset, d1);\\n\\n                    // Need to manually clear unused channels in case\\n                    // we're reading from recycled texture.\\n                    if (xCOffset + 1 >= \").concat(i, \") {\\n                      previous.zw = vec2(0.0);\\n                    }\\n\\n                    xC\").concat(_n436, \" = vec4(previous.zw, xTexelC\").concat(_n436, \".xy);\\n                  } else {\\n                    xC\").concat(_n436, \" = vec4(0.0, 0.0, xTexelC\").concat(_n436, \".xy);\\n                  }\\n                  \")) : $ += \"\\n                if (xC >= 0 && xC < \".concat(i, \" && xTexelC\").concat(_n436, \"Ready == 0) {\\n                  xTexelC\").concat(_n436, \" = getX(batch, xR, xC, d1);\\n                  if (xC + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n436, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n436, \"Ready = 1;\\n                }\\n\\n                xC\").concat(_n436, \" = xTexelC\").concat(_n436, \";\\n                \"), _r423 + 1 < f)) {\n            var _e1037 = u % 2 == 0 ? nearestLargerEven(h) : h;\n\n            h % 2 == 0 && u % 2 == 1 || h % 2 != 0 && u % 2 != 1 ? ($ += \"\\n                  xCOffset = xC + \".concat(u % 2, \" + \").concat(_e1037, \";\\n\\n                  if (xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n436 + 1, \"Ready == 0) {\\n                    xTexelC\").concat(_n436 + 1, \" = getX(batch, xR, xCOffset, d1);\\n\\n                    // Need to manually clear unused channels in case\\n                    // we're reading from recycled texture.\\n                    if (xCOffset + 1 >= \").concat(i, \") {\\n                      xTexelC\").concat(_n436 + 1, \".zw = vec2(0.0);\\n                    }\\n                    xTexelC\").concat(_n436 + 1, \"Ready = 1;\\n                  }\\n                  \"), h > 1 && ($ += \"\\n                    xCOffset -= 2;\\n                    if (xCOffset >= 0 && xCOffset < \".concat(i, \" && xTexelC\").concat(_n436, \"Ready == 0) {\\n                      xTexelC\").concat(_n436, \" = getX(batch, xR, xCOffset, d1);\\n                      xTexelC\").concat(_n436, \"Ready = 1;\\n                    }\\n                    \")), $ += \"\\n                  xC\".concat(_n436 + 1, \" = vec4(xTexelC\").concat(_n436, \".zw, xTexelC\").concat(_n436 + 1, \".xy);\\n                  \")) : $ += 1 === _e1037 ? \"\\n                    xC\".concat(_n436 + 1, \" = xTexelC\").concat(_n436, \";\\n                    \") : \"\\n                    xCOffset = xC + \".concat(_e1037, \";\\n\\n                    if (xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n436 + 1, \"Ready == 0) {\\n                      xTexelC\").concat(_n436 + 1, \" = getX(batch, xR, xCOffset, d1);\\n                      if (xCOffset + 1 >= \").concat(i, \") {\\n                        xTexelC\").concat(_n436 + 1, \".zw = vec2(0.0);\\n                      }\\n                      xTexelC\").concat(_n436 + 1, \"Ready = 1;\\n                    }\\n\\n                    xC\").concat(_n436 + 1, \" = xTexelC\").concat(_n436 + 1, \";\\n                    \");\n          }\n        } else _r423 < f && (u % 2 == 1 ? ($ += \"\\n                xCOffset = xC + 1 - \".concat(p, \";\\n                if(xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n436, \"Ready == 0) {\\n                  xTexelC\").concat(_n436, \" = getX(batch, xR, xCOffset, d1);\\n                  // Need to manually clear unused channels in case\\n                  // we're reading from recycled texture.\\n                  if (xCOffset + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n436, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n436, \"Ready = 1;\\n                }\\n\\n                if(xC + 1 >= 0 && xC + 1 < \").concat(i, \" && xTexelC\").concat(_n436 + 1, \"Ready == 0) {\\n                  xTexelC\").concat(_n436 + 1, \" = getX(batch, xR, xC + 1, d1);\\n                  // Need to manually clear unused channels in case\\n                  // we're reading from recycled texture.\\n                  if (xC + 2 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n436 + 1, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n436 + 1, \"Ready = 1;\\n                }\\n\\n                xC\").concat(_n436, \" = vec4(xTexelC\").concat(_n436, \".zw, xTexelC\").concat(_n436 + 1, \".zw);\\n              \"), _r423 + 1 < f && ($ += \"\\n                  final = vec4(0.0);\\n                  xCOffset = xC + 1 + \".concat(p, \";\\n                  if(xCOffset >= 0 && xCOffset < \").concat(i, \") {\\n                    final = getX(batch, xR, xCOffset, d1);\\n                  }\\n                  xC\").concat(_n436 + 1, \" = vec4(xTexelC\").concat(_n436 + 1, \".xy, final.xy);\\n                \"))) : ($ += \"\\n                if(xC >= 0 && xC < \".concat(i, \" && xTexelC\").concat(_n436, \"Ready == 0) {\\n                  xTexelC\").concat(_n436, \" = getX(batch, xR, xC, d1);\\n                  if (xC + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n436, \".zw = vec2(0.0);\\n                  }\\n                  xTexelC\").concat(_n436, \"Ready = 1;\\n                }\\n\\n                xCOffset = xC + \").concat(p, \";\\n                if(xCOffset >= 0 && xCOffset < \").concat(i, \" && xTexelC\").concat(_n436 + 1, \"Ready == 0) {\\n                  xTexelC\").concat(_n436 + 1, \" = getX(batch, xR, xCOffset, d1);\\n                  if (xCOffset + 1 >= \").concat(i, \") {\\n                    xTexelC\").concat(_n436 + 1, \".zw = vec2(0.);\\n                  }\\n                  xTexelC\").concat(_n436 + 1, \"Ready = 1;\\n                }\\n\\n                xC\").concat(_n436, \" = vec4(\\n                  xTexelC\").concat(_n436, \".xy, xTexelC\").concat(_n436 + 1, \".xy);\\n              \"), _r423 + 1 < f && ($ += \"\\n                  xC\".concat(_n436 + 1, \" = vec4(xTexelC\").concat(_n436, \".zw, xTexelC\").concat(_n436 + 1, \".zw);\\n                \"))));\n\n        _n436 < f && ($ += \"\\n            wTexel = getW(\".concat(_e1035, \", \").concat(_r423, \", d1, q);\\n            dotProd += xC\").concat(_n436, \" * vec4(wTexel.xz, wTexel.xz);\\n          \"), _r423 + 1 < f && ($ += \"\\n              wTexel = getW(\".concat(_e1035, \", \").concat(_r423 + 1, \", d1, q);\\n              dotProd += xC\").concat(_n436 + 1, \" * vec4(wTexel.xz, wTexel.xz);\\n            \")));\n      }\n\n      $ += \"\\n        }\\n      \";\n    }\n\n    var y = \"\",\n        b = \"\";\n    n && (y = r ? \"vec4 activation(vec4 a) {\\n          vec4 b = getPreluActivationWeightsAtOutCoords();\\n          \".concat(n, \"\\n        }\") : a ? \"vec4 activation(vec4 a) {\\n          vec4 b = getLeakyreluAlphaAtOutCoords();\\n          \".concat(n, \"\\n        }\") : \"vec4 activation(vec4 x) {\\n          \".concat(n, \"\\n        }\"), b = \"result = activation(result);\");\n    var x = t ? \"result += getBiasAtOutCoords();\" : \"\";\n    t && this.variableNames.push(\"bias\"), r && this.variableNames.push(\"preluActivationWeights\"), a && this.variableNames.push(\"leakyreluAlpha\"), this.userCode = \"\\n      \".concat(y, \"\\n\\n      const ivec2 strides = ivec2(\").concat(c, \", \").concat(p, \");\\n      const ivec2 pads = ivec2(\").concat(l, \", \").concat(u, \");\\n\\n      void main() {\\n\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords.x;\\n        ivec2 xRCCorner = coords.yz * strides - pads;\\n        int d2 = coords.w;\\n        int d1 = d2 / \").concat(s, \";\\n        int q = d2 - d1 * \").concat(s, \";\\n        int xRCorner = xRCCorner.x;\\n        int xCCorner = xRCCorner.y;\\n\\n        //intialize dotProd with a small epsilon seems to reduce GPU accuracy loss.\\n        vec4 dotProd = vec4(0.000000000000001);\\n\\n        \").concat($, \"\\n\\n        vec4 result = dotProd - vec4(0.000000000000001);\\n        \").concat(x, \"\\n        \").concat(b, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction depthwiseConv2dNative(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l,\n    dimRoundingMode: u\n  } = r;\n  var c = l;\n  null == c && (c = [1, 1]), assert$4(eitherStridesOrDilationsAreOne(o, c), () => \"Error in depthwiseConv2d: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '\").concat(c, \"'\"));\n  var p = computeConv2DInfo(a.shape, s.shape, o, c, i, u, !0);\n  var d;\n  return d = env().getBool(\"WEBGL_PACK_DEPTHWISECONV\") && p.strideWidth <= 2 && p.outChannels / p.inChannels == 1 ? new DepthwiseConvPacked2DProgram(p) : new DepthwiseConv2DProgram(p), n.runWebGLProgram(d, [a, s], \"float32\");\n}\n\nvar depthwiseConv2dNativeConfig = {\n  kernelName: DepthwiseConv2dNative,\n  backendName: \"webgl\",\n  kernelFunc: depthwiseConv2dNative\n};\n\nclass DepthwiseConv2DDerFilterProgram {\n  constructor(e) {\n    this.variableNames = [\"x\", \"dy\"], this.outputShape = e.filterShape, this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int wR = coords.x;\\n        int wC = coords.y;\\n        int d1 = coords.z;\\n        int dm = coords.w;\\n        int d2 = d1 * \".concat(e.outChannels / e.inChannels, \" + dm;\\n\\n        float dotProd = 0.0;\\n\\n        // TO DO: Vec4 over the batch size\\n        for (int b = 0; b < \").concat(e.batchSize, \"; b++) {\\n          for (int yR = 0; yR < \").concat(e.outHeight, \"; yR++) {\\n            int xR = wR + yR * \").concat(e.strideHeight, \" - \").concat(e.padInfo.top, \";\\n\\n            if (xR < 0 || xR >= \").concat(e.inHeight, \") {\\n              continue;\\n            }\\n\\n            for (int yC = 0; yC < \").concat(e.outWidth, \"; yC++) {\\n              int xC = wC + yC * \").concat(e.strideWidth, \" - \").concat(e.padInfo.left, \";\\n\\n              if (xC < 0 || xC >= \").concat(e.inWidth, \") {\\n                continue;\\n              }\\n\\n              float dyValue = getDy(b, yR, yC, d2);\\n              float xValue = getX(b, xR, xC, d1);\\n              dotProd += (xValue * dyValue);\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass DepthwiseConv2DDerInputProgram {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"W\"], this.outputShape = e.inShape;\n    var t = e.filterHeight,\n        n = e.filterWidth,\n        r = e.outChannels / e.inChannels;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int d1 = coords[3];\\n        ivec2 dyCorner = coords.yz - pads;\\n        int dyRCorner = dyCorner.x;\\n        int dyCCorner = dyCorner.y;\\n\\n        float dotProd = 0.0;\\n\\n        for (int wR = 0; wR < \").concat(t, \"; wR++) {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          int wRPerm = \").concat(t, \" - 1 - wR;\\n\\n          for (int wC = 0; wC < \").concat(n, \"; wC++) {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            int wCPerm = \").concat(n, \" - 1 - wC;\\n\\n            // TO DO: Vec4 over the channelMul\\n            for (int dm = 0; dm < \").concat(r, \"; dm++) {\\n              int d2 = d1 * \").concat(r, \" + dm;\\n              float xValue = getDy(batch, idyR, idyC, d2);\\n              float wValue = getW(wRPerm, wCPerm, d1, dm);\\n              dotProd += xValue * wValue;\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction depthwiseConv2dNativeBackpropFilter(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    dy: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    filterShape: c\n  } = r,\n      p = computeConv2DInfo(a.shape, c, o, i, l, u, !0),\n      d = new DepthwiseConv2DDerFilterProgram(p);\n  return n.runWebGLProgram(d, [a, s], \"float32\");\n}\n\nvar depthwiseConv2dNativeBackpropFilterConfig = {\n  kernelName: DepthwiseConv2dNativeBackpropFilter,\n  backendName: \"webgl\",\n  kernelFunc: depthwiseConv2dNativeBackpropFilter\n};\n\nfunction depthwiseConv2dNativeBackpropInput(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    dilations: i,\n    pad: l,\n    dimRoundingMode: u,\n    inputShape: c\n  } = r,\n      p = computeConv2DInfo(c, s.shape, o, i, l, u, !0),\n      d = new DepthwiseConv2DDerInputProgram(p);\n  return n.runWebGLProgram(d, [a, s], \"float32\");\n}\n\nvar depthwiseConv2dNativeBackpropInputConfig = {\n  kernelName: DepthwiseConv2dNativeBackpropInput,\n  backendName: \"webgl\",\n  kernelFunc: depthwiseConv2dNativeBackpropInput\n};\n\nclass DiagProgram {\n  constructor(e) {\n    this.variableNames = [\"X\"], this.outputShape = [e, e], this.userCode = \"\\n      void main() {\\n          ivec2 coords = getOutputCoords();\\n          float val = coords[0] == coords[1] ? getX(coords[0]) : 0.0;\\n          setOutput(val);\\n      }\\n    \";\n  }\n\n}\n\nfunction diag(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t,\n      a = [...r.shape, ...r.shape],\n      s = sizeFromShape(r.shape),\n      o = reshape({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [s]\n    }\n  }),\n      i = new DiagProgram(s),\n      l = n.runWebGLProgram(i, [o], o.dtype),\n      u = reshape({\n    inputs: {\n      x: l\n    },\n    backend: n,\n    attrs: {\n      shape: a\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(l), u;\n}\n\nvar diagConfig = {\n  kernelName: Diag,\n  backendName: \"webgl\",\n  kernelFunc: diag\n};\n\nclass Dilation2DProgram {\n  constructor(e) {\n    this.variableNames = [\"x\", \"W\"], this.outputShape = e.outShape;\n    var {\n      inHeight: t,\n      inWidth: n,\n      padInfo: r,\n      strideHeight: a,\n      strideWidth: s,\n      filterHeight: o,\n      filterWidth: i,\n      dilationHeight: l,\n      dilationWidth: u\n    } = e,\n        {\n      top: c,\n      left: p\n    } = r;\n    this.userCode = \"\\n      const ivec2 strides = ivec2(\".concat(a, \", \").concat(s, \");\\n      const ivec2 pads = ivec2(\").concat(c, \", \").concat(p, \");\\n      const float neg_infinity = -3.4e38;\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int d1 = coords.w;\\n        ivec2 outTopLeftCorner =\\n            coords.yz * strides - pads;\\n        int hBeg = outTopLeftCorner.x;\\n        int wBeg = outTopLeftCorner.y;\\n\\n        float curVal = neg_infinity;\\n        for (int h = 0; h < \").concat(o, \"; h++) {\\n          int hIn = hBeg + h * \").concat(l, \";\\n\\n          if (hIn >= 0 && hIn < \").concat(t, \") {\\n            for (int w = 0; w < \").concat(i, \"; w++) {\\n              int wIn = wBeg + w * \").concat(u, \";\\n\\n              if (wIn >= 0 && wIn < \").concat(n, \") {\\n                float xVal = getX(batch, hIn, wIn, d1);\\n                float wVal = getW(h, w, d1);\\n\\n                float val = xVal + wVal;\\n                if (val > curVal) {\\n                  curVal = val;\\n                }\\n              }\\n            }\\n          }\\n        }\\n\\n        float result = curVal;\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nfunction dilation2D(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s\n  } = t,\n      {\n    strides: o,\n    pad: i,\n    dilations: l\n  } = r,\n      u = computeDilation2DInfo(a.shape, s.shape, o, i, \"NHWC\", l);\n  var c;\n  var p = new Dilation2DProgram(u);\n  c = n.runWebGLProgram(p, [a, s], \"float32\");\n  var d = reshape({\n    inputs: {\n      x: c\n    },\n    backend: n,\n    attrs: {\n      shape: u.outShape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(c), d;\n}\n\nvar dilation2DConfig = {\n  kernelName: Dilation2D,\n  backendName: \"webgl\",\n  kernelFunc: dilation2D\n};\n\nfunction einsum(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    equation: a\n  } = r,\n      s = t,\n      {\n    allDims: o,\n    summedDims: i,\n    idDims: l\n  } = decodeEinsumEquation(a, s.length);\n  checkEinsumDimSizes(o.length, l, s);\n  var {\n    path: u,\n    steps: c\n  } = getEinsumComputePath(i, l),\n      p = c.length;\n  var d = null,\n      h = o.length;\n  var m = [];\n\n  for (var _e1038 = 0; _e1038 < p; ++_e1038) {\n    for (var _t712 of c[_e1038]) {\n      var {\n        permutationIndices: _e1039,\n        expandDims: _r424\n      } = getEinsumPermutation(h, l[_t712]);\n\n      var _a305 = void 0;\n\n      isIdentityPermutation(_e1039) ? _a305 = s[_t712] : (_a305 = transpose({\n        inputs: {\n          x: s[_t712]\n        },\n        backend: n,\n        attrs: {\n          perm: _e1039\n        }\n      }), m.push(_a305));\n\n      var _o154 = _a305.shape.slice();\n\n      for (var _e1040 = 0; _e1040 < _r424.length; ++_e1040) {\n        _o154.splice(_r424[_e1040], 0, 1);\n      }\n\n      arraysEqual(_a305.shape, _o154) || (_a305 = reshape({\n        inputs: {\n          x: _a305\n        },\n        backend: n,\n        attrs: {\n          shape: _o154\n        }\n      }), m.push(_a305)), null === d ? d = _a305 : (d = multiply({\n        inputs: {\n          a: _a305,\n          b: d\n        },\n        backend: n\n      }), m.push(d));\n    }\n\n    _e1038 < p - 1 && (u[_e1038] >= 0 && (d = sum({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        axis: u[_e1038] - (o.length - h),\n        keepDims: !1\n      }\n    }), m.push(d)), h--);\n  }\n\n  for (var _e1041 of m) {\n    _e1041 !== d && n.disposeIntermediateTensorInfo(_e1041);\n  }\n\n  return d;\n}\n\nvar einsumConfig = {\n  kernelName: Einsum,\n  backendName: \"webgl\",\n  kernelFunc: einsum\n},\n    ELU = \"return (x >= 0.0) ? x : (exp(x) - 1.0);\",\n    ELU_PACKED = \"\\n  vec4 result;\\n\\n  result.r = (x.r >= 0.0) ? x.r : (exp(x.r) - 1.0);\\n  result.g = (x.g >= 0.0) ? x.g : (exp(x.g) - 1.0);\\n  result.b = (x.b >= 0.0) ? x.b : (exp(x.b) - 1.0);\\n  result.a = (x.a >= 0.0) ? x.a : (exp(x.a) - 1.0);\\n\\n  return result;\\n\",\n    elu = unaryKernelFunc({\n  opSnippet: ELU,\n  packedOpSnippet: ELU_PACKED\n}),\n    eluConfig = {\n  kernelName: Elu$1,\n  backendName: \"webgl\",\n  kernelFunc: elu\n},\n    ELU_DER = \"return (b >= 1.0) ? a : a * (b + 1.0);\",\n    ELU_DER_PACKED = \"\\n  vec4 bGTEZero = vec4(greaterThanEqual(b, vec4(0.)));\\n  return (bGTEZero * a) + ((vec4(1.0) - bGTEZero) * (a * (b + vec4(1.0))));\\n\",\n    eluGrad = e => {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    dy: r,\n    y: a\n  } = t,\n      s = env().getBool(\"WEBGL_PACK_BINARY_OPERATIONS\") ? new BinaryOpPackedProgram(ELU_DER_PACKED, r.shape, a.shape) : new BinaryOpProgram(ELU_DER, r.shape, a.shape);\n  return n.runWebGLProgram(s, [r, a], r.dtype);\n},\n    eluGradConfig = {\n  kernelName: EluGrad,\n  backendName: \"webgl\",\n  kernelFunc: eluGrad\n},\n    PACKED_EQUAL = \"\\n  return vec4(equal(a, b));\\n\",\n    EQUAL = \"return float(a == b);\",\n    equal = binaryKernelFunc({\n  opSnippet: EQUAL,\n  packedOpSnippet: PACKED_EQUAL,\n  dtype: \"bool\",\n  cpuKernelImpl: equalImplCPU\n}),\n    equalConfig = {\n  kernelName: Equal,\n  backendName: \"webgl\",\n  kernelFunc: equal\n},\n    ERF = \"\\n  // Error function is calculated approximately with elementary function.\\n  // See \\\"Handbook of Mathematical Functions with Formulas,\\n  // Graphs, and Mathematical Tables\\\", Abramowitz and Stegun.\\n  float p = \".concat(ERF_P, \";\\n  float a1 = \").concat(ERF_A1, \";\\n  float a2 = \").concat(ERF_A2, \";\\n  float a3 = \").concat(ERF_A3, \";\\n  float a4 = \").concat(ERF_A4, \";\\n  float a5 = \").concat(ERF_A5, \";\\n\\n  float sign = sign(x);\\n  x = abs(x);\\n  float t = 1.0 / (1.0 + p * x);\\n  return sign * (1.0 - (((((a5*t + a4)*t) + a3)*t + a2)*t + a1)*t*exp(-x*x));\\n\"),\n    erf = unaryKernelFunc({\n  opSnippet: ERF\n}),\n    erfConfig = {\n  kernelName: Erf,\n  backendName: \"webgl\",\n  kernelFunc: erf\n},\n    EXP = \"return exp(x);\",\n    exp = unaryKernelFunc({\n  opSnippet: EXP,\n  packedOpSnippet: EXP,\n  cpuKernelImpl: expImplCPU\n}),\n    expConfig = {\n  kernelName: Exp,\n  backendName: \"webgl\",\n  kernelFunc: exp\n};\n\nfunction expandDims(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    dim: a\n  } = n,\n      {\n    input: s\n  } = t,\n      o = s.shape.length,\n      i = s.shape.slice();\n  var l = a;\n  return a < 0 && (assert$4(-(o + 1) <= a, () => \"Axis must be in the interval [\".concat(-(o + 1), \", \").concat(o, \"]\")), l = o + a + 1), i.splice(l, 0, 1), reshape({\n    inputs: {\n      x: s\n    },\n    backend: r,\n    attrs: {\n      shape: i\n    }\n  });\n}\n\nvar expandDimsConfig = {\n  kernelName: ExpandDims,\n  backendName: \"webgl\",\n  kernelFunc: expandDims\n},\n    EXPM1 = \"return exp(x) - 1.0;\",\n    expm1 = unaryKernelFunc({\n  opSnippet: EXPM1,\n  packedOpSnippet: EXPM1,\n  cpuKernelImpl: expm1ImplCPU\n}),\n    expm1Config = {\n  kernelName: Expm1,\n  backendName: \"webgl\",\n  kernelFunc: expm1\n};\n\nclass FFTProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"real\", \"imag\"];\n    var r = t[1];\n    this.outputShape = t;\n    var a = n ? \"2.0 * \".concat(Math.PI) : \"-2.0 * \".concat(Math.PI),\n        s = n ? \"\".concat(r, \".0\") : \"1.0\";\n    var o;\n    if (\"real\" === e) o = \"return real * expR - imag * expI;\";else {\n      if (\"imag\" !== e) throw new Error(\"FFT component must be either \\\"real\\\" or \\\"imag\\\", got \".concat(e, \".\"));\n      o = \"return real * expI + imag * expR;\";\n    }\n    this.userCode = \"\\n      const float exponentMultiplier = \".concat(a, \";\\n\\n      float unaryOpComplex(float real, float expR, float imag, float expI) {\\n        \").concat(o, \"\\n      }\\n\\n      float mulMatDFT(int batch, int index) {\\n        float indexRatio = float(index) / float(\").concat(r, \");\\n        float exponentMultiplierTimesIndexRatio =\\n            exponentMultiplier * indexRatio;\\n\\n        float result = 0.0;\\n\\n        for (int i = 0; i < \").concat(r, \"; i++) {\\n          // x = (-2|2 * PI / N) * index * i;\\n          float x = exponentMultiplierTimesIndexRatio * float(i);\\n          float expR = cos(x);\\n          float expI = sin(x);\\n          float real = getReal(batch, i);\\n          float imag = getImag(batch, i);\\n\\n          result +=\\n              unaryOpComplex(real, expR, imag, expI) / \").concat(s, \";\\n        }\\n\\n        return result;\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        setOutput(mulMatDFT(coords[0], coords[1]));\\n      }\\n    \");\n  }\n\n}\n\nfunction fftImpl(e, t, n) {\n  var r = n.texData.get(e.dataId),\n      a = sizeFromShape(e.shape),\n      s = e.shape[e.shape.length - 1],\n      o = reshape({\n    inputs: {\n      x: e\n    },\n    backend: n,\n    attrs: {\n      shape: [a / s, s]\n    }\n  }),\n      i = o.shape,\n      l = new FFTProgram(\"real\", i, t),\n      u = new FFTProgram(\"imag\", i, t),\n      c = [{\n    dataId: r.complexTensorInfos.real.dataId,\n    dtype: r.complexTensorInfos.real.dtype,\n    shape: i\n  }, {\n    dataId: r.complexTensorInfos.imag.dataId,\n    dtype: r.complexTensorInfos.imag.dtype,\n    shape: i\n  }],\n      p = n.runWebGLProgram(l, c, \"float32\"),\n      d = n.runWebGLProgram(u, c, \"float32\"),\n      h = complex({\n    inputs: {\n      real: p,\n      imag: d\n    },\n    backend: n\n  });\n  n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d);\n  var m = reshape({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      shape: e.shape\n    }\n  });\n  return n.disposeIntermediateTensorInfo(o), n.disposeIntermediateTensorInfo(h), m;\n}\n\nfunction fft(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return fftImpl(r, !1, n);\n}\n\nvar fftConfig = {\n  kernelName: FFT,\n  backendName: \"webgl\",\n  kernelFunc: fft\n};\n\nclass FillProgram {\n  constructor(e, t) {\n    this.outputShape = [], this.customUniforms = [{\n      name: \"value\",\n      type: \"float\"\n    }], this.variableNames = [\"x\"], this.outputShape = e, this.userCode = \"\\n      void main() {\\n        // Input can be obtained from uniform value.\\n        setOutput(value);\\n      }\\n    \";\n  }\n\n}\n\nfunction fill(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    shape: r,\n    value: a\n  } = n;\n  var {\n    dtype: s\n  } = n;\n\n  if (s = s || inferDtype(a), \"string\" === s) {\n    var _e1042 = getArrayFromDType(s, sizeFromShape(r));\n\n    return _e1042.fill(a), t.makeTensorInfo(r, s, _e1042);\n  }\n\n  {\n    var _e1043 = new FillProgram(r, a);\n\n    return t.runWebGLProgram(_e1043, [], s, [[a]]);\n  }\n}\n\nvar fillConfig = {\n  kernelName: Fill,\n  backendName: \"webgl\",\n  kernelFunc: fill\n};\n\nclass FlipLeftRightProgram {\n  constructor(e) {\n    this.variableNames = [\"Image\"], this.outputShape = [];\n    var t = e[2];\n    this.outputShape = e, this.userCode = \"\\n        void main() {\\n          ivec4 coords = getOutputCoords();\\n          int x = coords[2];\\n\\n          int coordX = \".concat(t, \" - x - 1;\\n          float outputValue;\\n          if(coordX >= 0 && coordX < \").concat(t, \") {\\n            outputValue = getImage(coords[0], coords[1], coordX, coords[3]);\\n          } else {\\n            outputValue = getImage(coords[0], coords[1], coords[2], coords[3]);\\n          }\\n          setOutput(outputValue);\\n        }\\n    \");\n  }\n\n}\n\nvar flipLeftRightConfig = {\n  kernelName: FlipLeftRight,\n  backendName: \"webgl\",\n  kernelFunc: _ref69 => {\n    var {\n      inputs: e,\n      backend: t\n    } = _ref69;\n    var {\n      image: n\n    } = e,\n        r = t,\n        a = new FlipLeftRightProgram(n.shape);\n    return r.runWebGLProgram(a, [n], n.dtype);\n  }\n},\n    FLOOR = \"return floor(x);\",\n    floor = unaryKernelFunc({\n  opSnippet: FLOOR,\n  packedOpSnippet: FLOOR,\n  cpuKernelImpl: floorImplCPU\n}),\n    floorConfig = {\n  kernelName: Floor,\n  backendName: \"webgl\",\n  kernelFunc: floor\n},\n    INT_DIV = \"\\n  float s = sign(a) * sign(b);\\n  int ia = round(a);\\n  int ib = round(b);\\n  if (ib != 0) {\\n    // Windows (D3D) wants guaranteed non-zero int division at compile-time.\\n    return float(idiv(ia, ib, s));\\n  } else {\\n    return NAN;\\n  }\\n\",\n    INT_DIV_PACKED = \"\\n  ivec4 ia = round(a);\\n  ivec4 ib = round(b);\\n  bvec4 cond = notEqual(ib, ivec4(0));\\n  ivec4 result = ivec4(0);\\n  vec4 s = sign(a) * sign(b);\\n\\n  // Windows (D3D) wants guaranteed non-zero int division at compile-time.\\n  if (cond[0]) {\\n    result[0] = idiv(ia[0], ib[0], s[0]);\\n  }\\n  if (cond[1]) {\\n    result[1] = idiv(ia[1], ib[1], s[1]);\\n  }\\n  if (cond[2]) {\\n    result[2] = idiv(ia[2], ib[2], s[2]);\\n  }\\n  if (cond[3]) {\\n    result[3] = idiv(ia[3], ib[3], s[3]);\\n  }\\n  return vec4(result);\\n\",\n    floorDiv = binaryKernelFunc({\n  opSnippet: INT_DIV,\n  packedOpSnippet: INT_DIV_PACKED,\n  dtype: \"int32\"\n}),\n    floorDivConfig = {\n  kernelName: FloorDiv,\n  backendName: \"webgl\",\n  kernelFunc: floorDiv\n};\n\nclass FromPixelsProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"];\n    var t = getGlslDifferences(),\n        [n, r] = e;\n    this.outputShape = e, this.userCode = \"\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n        int texR = coords[0];\\n        int texC = coords[1];\\n        int depth = coords[2];\\n        vec2 uv = (vec2(texC, texR) + halfCR) / vec2(\".concat(r, \".0, \").concat(n, \".0);\\n\\n        vec4 values = \").concat(t.texture2D, \"(A, uv);\\n        float value;\\n        if (depth == 0) {\\n          value = values.r;\\n        } else if (depth == 1) {\\n          value = values.g;\\n        } else if (depth == 2) {\\n          value = values.b;\\n        } else if (depth == 3) {\\n          value = values.a;\\n        }\\n\\n        setOutput(floor(value * 255.0 + 0.5));\\n      }\\n    \");\n  }\n\n}\n\nclass FromPixelsPackedProgram {\n  constructor(e) {\n    this.variableNames = [\"A\"], this.packedInputs = !1, this.packedOutput = !0;\n    var t = getGlslDifferences(),\n        [n, r] = e;\n    this.outputShape = e, this.userCode = \"\\n      void main() {\\n        ivec3 coords = getOutputCoords();\\n        int texR = coords[0];\\n        int texC = coords[1];\\n        int depth = coords[2];\\n\\n        vec4 result = vec4(0.);\\n\\n        for(int row=0; row<=1; row++) {\\n          for(int col=0; col<=1; col++) {\\n            texC = coords[1] + row;\\n            depth = coords[2] + col;\\n\\n            vec2 uv = (vec2(texC, texR) + halfCR) /\\n                       vec2(\".concat(r, \".0, \").concat(n, \".0);\\n            vec4 values = \").concat(t.texture2D, \"(A, uv);\\n            float value;\\n            if (depth == 0) {\\n              value = values.r;\\n            } else if (depth == 1) {\\n              value = values.g;\\n            } else if (depth == 2) {\\n              value = values.b;\\n            } else if (depth == 3) {\\n              value = values.a;\\n            }\\n\\n            result[row * 2 + col] = floor(value * 255.0 + 0.5);\\n          }\\n        }\\n\\n        \").concat(t.output, \" = result;\\n      }\\n    \");\n  }\n\n}\n\nvar fromPixelsConfig = {\n  kernelName: FromPixels,\n  backendName: \"webgl\",\n  kernelFunc: fromPixels\n};\nvar fromPixels2DContext;\n\nfunction fromPixels(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e;\n  var {\n    pixels: a\n  } = t;\n  var {\n    numChannels: s\n  } = r,\n      o = \"undefined\" != typeof HTMLVideoElement && a instanceof HTMLVideoElement,\n      i = \"undefined\" != typeof HTMLImageElement && a instanceof HTMLImageElement,\n      [l, u] = o ? [a.videoWidth, a.videoHeight] : [a.width, a.height],\n      c = [u, l],\n      p = [u, l, s];\n  (i || o) && (null == fromPixels2DContext && (fromPixels2DContext = document.createElement(\"canvas\").getContext(\"2d\")), fromPixels2DContext.canvas.width = l, fromPixels2DContext.canvas.height = u, fromPixels2DContext.drawImage(a, 0, 0, l, u), a = fromPixels2DContext.canvas);\n  var d = n.makeTensorInfo(c, \"int32\");\n  n.texData.get(d.dataId).usage = TextureUsage.PIXELS, n.gpgpu.uploadPixelDataToTexture(n.getTexture(d.dataId), a);\n  var h = env().getBool(\"WEBGL_PACK\") ? new FromPixelsPackedProgram(p) : new FromPixelsProgram(p),\n      m = n.runWebGLProgram(h, [d], \"int32\");\n  return n.disposeData(d.dataId), m;\n}\n\nfunction fusedConv2d(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dataFormat: c,\n    dilations: p,\n    dimRoundingMode: d,\n    activation: h,\n    leakyreluAlpha: m\n  } = r,\n      f = convertConv2DDataFormat(c),\n      g = computeConv2DInfo(a.shape, s.shape, l, p, u, d, !1, f);\n  var $;\n  var y = [];\n  if (1 !== g.filterHeight || 1 !== g.filterWidth || 1 !== g.dilationHeight || 1 !== g.dilationWidth || 1 !== g.strideHeight || 1 !== g.strideWidth || \"SAME\" !== g.padInfo.type && \"VALID\" !== g.padInfo.type) {\n    if (env().getBool(\"WEBGL_CONV_IM2COL\") && 1 === a.shape[0]) $ = conv2dWithIm2Row({\n      x: a,\n      filter: s,\n      convInfo: g,\n      backend: n,\n      bias: o,\n      activation: h,\n      preluActivationWeights: i,\n      leakyreluAlpha: m\n    });else {\n      var _e1044 = null != o,\n          _t713 = null != i,\n          _r425 = \"leakyrelu\" === h,\n          _l68 = h ? mapActivationToShaderProgram(h, !1) : null,\n          _u61 = new Conv2DProgram(g, _e1044, _l68, _t713, _r425),\n          _c39 = [a, s];\n\n      if (o && _c39.push(o), i && _c39.push(i), _r425) {\n        var _e1045 = n.makeTensorInfo([], \"float32\", createScalarValue(m, \"float32\"));\n\n        _c39.push(_e1045), y.push(_e1045);\n      }\n\n      $ = n.runWebGLProgram(_u61, _c39, \"float32\");\n    }\n  } else $ = conv2dByMatMul({\n    x: a,\n    filter: s,\n    convInfo: g,\n    backend: n,\n    bias: o,\n    activation: h,\n    preluActivationWeights: i,\n    leakyreluAlpha: m\n  });\n  var b = reshape({\n    inputs: {\n      x: $\n    },\n    backend: n,\n    attrs: {\n      shape: g.outShape\n    }\n  });\n  return y.push($), y.forEach(e => n.disposeIntermediateTensorInfo(e)), b;\n}\n\nvar fusedConv2DConfig = {\n  kernelName: FusedConv2D,\n  backendName: \"webgl\",\n  kernelFunc: fusedConv2d\n};\n\nfunction fusedDepthwiseConv2D(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    filter: s,\n    bias: o,\n    preluActivationWeights: i\n  } = t,\n      {\n    strides: l,\n    pad: u,\n    dilations: c,\n    dimRoundingMode: p,\n    activation: d,\n    leakyreluAlpha: h\n  } = r,\n      m = [];\n  var f = c;\n  null == f && (f = [1, 1]), assert$4(eitherStridesOrDilationsAreOne(l, f), () => \"Error in depthwiseConv2d: Either strides or dilations must be 1. Got strides \".concat(l, \" and dilations '\").concat(f, \"'\"));\n  var g = computeConv2DInfo(a.shape, s.shape, l, f, u, p, !0),\n      $ = env().getBool(\"WEBGL_PACK_DEPTHWISECONV\") && g.strideWidth <= 2 && g.outChannels / g.inChannels == 1,\n      y = d ? mapActivationToShaderProgram(d, $) : null,\n      b = [a, s],\n      x = null != o,\n      v = null != i,\n      I = \"leakyrelu\" === d;\n\n  if (x && b.push(o), v && b.push(i), I) {\n    var _e1046 = n.makeTensorInfo([], \"float32\", createScalarValue(h, \"float32\"));\n\n    b.push(_e1046), m.push(_e1046);\n  }\n\n  var C;\n  C = $ ? new DepthwiseConvPacked2DProgram(g, x, y, v, I) : new DepthwiseConv2DProgram(g, x, y, v, I);\n  var S = n.runWebGLProgram(C, b, \"float32\");\n  return m.forEach(e => n.disposeIntermediateTensorInfo(e)), S;\n}\n\nvar fusedDepthwiseConv2DConfig = {\n  kernelName: FusedDepthwiseConv2D,\n  backendName: \"webgl\",\n  kernelFunc: fusedDepthwiseConv2D\n};\n\nclass GatherNDProgram {\n  constructor(e, t, n) {\n    this.sliceDim = e, this.strides = t, this.variableNames = [\"x\", \"indices\"], this.outputShape = n;\n    var r = getCoordsDataType(t.length),\n        a = getCoordsDataType(n.length);\n    this.userCode = \"\\n        \".concat(r, \" strides = \").concat(r, \"(\").concat(this.strides, \");\\n         void main() {\\n          \").concat(a, \" coords = getOutputCoords();\\n          int flattenIndex = 0;\\n          for (int j = 0; j < \").concat(this.sliceDim, \"; j++) {\\n            int index = round(getIndices(coords[0], j));\\n            flattenIndex += index * \").concat(this.sliceDim > 1 ? \"strides[j]\" : \"strides\", \";\\n          }\\n          setOutput(getX(flattenIndex, coords[1]));\\n        }\\n      \");\n  }\n\n}\n\nfunction gatherNd(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    params: r,\n    indices: a\n  } = t,\n      s = a.shape,\n      o = s[s.length - 1],\n      i = sizeFromShape(r.shape),\n      [l, u, c, p] = prepareAndValidate(r, a),\n      d = reshape({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [u, o]\n    }\n  }),\n      h = reshape({\n    inputs: {\n      x: r\n    },\n    backend: n,\n    attrs: {\n      shape: [sizeFromShape(r.shape) / c, c]\n    }\n  });\n\n  if (n.shouldExecuteOnCPU([r, a]) || \"string\" === r.dtype) {\n    var _e1047 = n.readSync(a.dataId),\n        _t714 = n.bufferSync(r),\n        _s219 = gatherNdImplCPU(_e1047, _t714, r.dtype, u, o, c, p, r.shape, i);\n\n    return n.makeTensorInfo(l, r.dtype, _s219.values);\n  }\n\n  var m = new GatherNDProgram(o, p, [u, c]),\n      f = n.runWebGLProgram(m, [h, d], h.dtype),\n      g = reshape({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  });\n  return n.disposeIntermediateTensorInfo(d), n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(f), g;\n}\n\nvar gatherNdConfig = {\n  kernelName: GatherNd,\n  backendName: \"webgl\",\n  kernelFunc: gatherNd\n};\n\nclass GatherProgram {\n  constructor(e, t) {\n    this.variableNames = [\"A\", \"indices\"], this.outputShape = t, this.rank = t.length;\n    var n = getCoordsDataType(this.rank),\n        r = getSourceCoords$1(e);\n    this.userCode = \"\\n      void main() {\\n        \".concat(n, \" resRC = getOutputCoords();\\n        setOutput(getA(\").concat(r, \"));\\n      }\\n    \");\n  }\n\n}\n\nfunction getSourceCoords$1(e, t) {\n  var n = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\"],\n      r = [];\n\n  for (var _t715 = 0; _t715 < e.length; _t715++) {\n    r.push(2 === _t715 ? \"int(getIndices(resRC.x, resRC.z))\" : \"\".concat(n[_t715]));\n  }\n\n  return r.join();\n}\n\nfunction gatherV2(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    indices: s\n  } = t,\n      {\n    axis: o,\n    batchDims: i\n  } = r,\n      l = collectGatherOpShapeInfo(a, s, parseAxisParam(o, a.shape)[0], i),\n      u = sizeFromShape(s.shape),\n      c = [],\n      p = reshape({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [l.batchSize, l.outerSize, l.dimSize, l.sliceSize]\n    }\n  }),\n      d = reshape({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: [l.batchSize, u / l.batchSize]\n    }\n  });\n  c.push(p), c.push(d);\n  var h = [l.batchSize, l.outerSize, u / l.batchSize, l.sliceSize];\n\n  if (n.shouldExecuteOnCPU([a, s]) || \"string\" === a.dtype) {\n    var _e1048 = n.bufferSync(d),\n        _t716 = n.bufferSync(p),\n        _r426 = gatherV2ImplCPU(_t716, _e1048, h);\n\n    return c.forEach(e => n.disposeIntermediateTensorInfo(e)), n.makeTensorInfo(l.outputShape, _r426.dtype, _r426.values);\n  }\n\n  var m = new GatherProgram(p.shape, h),\n      f = n.runWebGLProgram(m, [p, d], p.dtype);\n  c.push(f);\n  var g = reshape({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: l.outputShape\n    }\n  });\n  return c.forEach(e => n.disposeIntermediateTensorInfo(e)), g;\n}\n\nvar gatherV2Config = {\n  kernelName: GatherV2,\n  backendName: \"webgl\",\n  kernelFunc: gatherV2\n},\n    GREATER = \"return float(a > b);\",\n    GREATER_PACKED = \"\\n  return vec4(greaterThan(a, b));\\n\",\n    greater = binaryKernelFunc({\n  opSnippet: GREATER,\n  packedOpSnippet: GREATER_PACKED,\n  cpuKernelImpl: greaterImplCPU,\n  dtype: \"bool\"\n}),\n    greaterConfig = {\n  kernelName: Greater,\n  backendName: \"webgl\",\n  kernelFunc: greater\n},\n    GREATER_EQUAL = \"return float(a >= b);\",\n    GREATER_EQUAL_PACKED = \"\\n  return vec4(greaterThanEqual(a, b));\\n\",\n    greaterEqual = binaryKernelFunc({\n  opSnippet: GREATER_EQUAL,\n  packedOpSnippet: GREATER_EQUAL_PACKED,\n  dtype: \"bool\",\n  cpuKernelImpl: greaterEqualImplCPU\n}),\n    greaterEqualConfig = {\n  kernelName: GreaterEqual,\n  backendName: \"webgl\",\n  kernelFunc: greaterEqual\n};\n\nfunction ifft(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    input: r\n  } = t;\n  return fftImpl(r, !0, n);\n}\n\nvar ifftConfig = {\n  kernelName: IFFT,\n  backendName: \"webgl\",\n  kernelFunc: ifft\n},\n    IS_FINITE = \"return float(!isnan(x) && !isinf(x));\",\n    isFinite$1 = unaryKernelFunc({\n  opSnippet: IS_FINITE,\n  dtype: \"bool\"\n}),\n    isFiniteConfig = {\n  kernelName: IsFinite,\n  backendName: \"webgl\",\n  kernelFunc: isFinite$1\n},\n    IS_INF = \"return float(isinf(x));\",\n    isInf = unaryKernelFunc({\n  opSnippet: IS_INF,\n  dtype: \"bool\"\n}),\n    isInfConfig = {\n  kernelName: IsInf,\n  backendName: \"webgl\",\n  kernelFunc: isInf\n},\n    IS_NAN = \"return float(isnan(x));\",\n    isNaN$1 = unaryKernelFunc({\n  opSnippet: IS_NAN,\n  dtype: \"bool\"\n}),\n    isNaNConfig = {\n  kernelName: IsNan,\n  backendName: \"webgl\",\n  kernelFunc: isNaN$1\n},\n    LESS = \"return float(a < b);\",\n    LESS_PACKED = \"\\n  return vec4(lessThan(a, b));\\n\",\n    less = binaryKernelFunc({\n  opSnippet: LESS,\n  packedOpSnippet: LESS_PACKED,\n  cpuKernelImpl: lessImplCPU,\n  dtype: \"bool\"\n}),\n    lessConfig = {\n  kernelName: Less,\n  backendName: \"webgl\",\n  kernelFunc: less\n},\n    LESS_EQUAL = \"return float(a <= b);\",\n    LESS_EQUAL_PACKED = \"\\n  return vec4(lessThanEqual(a, b));\\n\",\n    lessEqual = binaryKernelFunc({\n  opSnippet: LESS_EQUAL,\n  packedOpSnippet: LESS_EQUAL_PACKED,\n  cpuKernelImpl: lessEqualImplCPU,\n  dtype: \"bool\"\n}),\n    lessEqualConfig = {\n  kernelName: LessEqual,\n  backendName: \"webgl\",\n  kernelFunc: lessEqual\n};\n\nfunction linSpace(e) {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    num: s\n  } = n,\n      o = linSpaceImplCPU(r, a, s);\n  return t.makeTensorInfo([o.length], \"float32\", o);\n}\n\nvar linSpaceConfig = {\n  kernelName: LinSpace,\n  backendName: \"webgl\",\n  kernelFunc: linSpace\n},\n    LOG = \"if (x < 0.0) return NAN;\\n  return log(x);\",\n    LOG_PACKED = \"\\n  vec4 result = log(x);\\n  vec4 isNaN = vec4(lessThan(x, vec4(0.0)));\\n  result.r = isNaN.r == 1.0 ? NAN : result.r;\\n  result.g = isNaN.g == 1.0 ? NAN : result.g;\\n  result.b = isNaN.b == 1.0 ? NAN : result.b;\\n  result.a = isNaN.a == 1.0 ? NAN : result.a;\\n\\n  return result;\\n\",\n    log = unaryKernelFunc({\n  opSnippet: LOG,\n  packedOpSnippet: LOG_PACKED,\n  cpuKernelImpl: logImplCPU\n}),\n    logConfig = {\n  kernelName: Log,\n  backendName: \"webgl\",\n  kernelFunc: log\n},\n    LOG1P = \"return log(1.0 + x);\",\n    log1p = unaryKernelFunc({\n  opSnippet: LOG1P\n}),\n    log1pConfig = {\n  kernelName: Log1p,\n  backendName: \"webgl\",\n  kernelFunc: log1p\n},\n    LOGICAL_AND = \"return float(a >= 1.0 && b >= 1.0);\",\n    LOGICAL_AND_PACKED = \"\\n  return vec4(\\n    vec4(greaterThanEqual(a, vec4(1.0))) *\\n    vec4(greaterThanEqual(b, vec4(1.0))));\\n\",\n    logicalAnd = binaryKernelFunc({\n  opSnippet: LOGICAL_AND,\n  packedOpSnippet: LOGICAL_AND_PACKED,\n  dtype: \"bool\"\n}),\n    logicalAndConfig = {\n  kernelName: LogicalAnd,\n  backendName: \"webgl\",\n  kernelFunc: logicalAnd\n},\n    LOGICAL_NOT = \"return float(!(x >= 1.0));\",\n    logicalNot = unaryKernelFunc({\n  opSnippet: LOGICAL_NOT\n}),\n    logicalNotConfig = {\n  kernelName: LogicalNot,\n  backendName: \"webgl\",\n  kernelFunc: logicalNot\n},\n    LOGICAL_OR = \"return float(a >= 1.0 || b >= 1.0);\",\n    LOGICAL_OR_PACKED = \"\\n  return min(\\n    vec4(greaterThanEqual(a, vec4(1.0))) +\\n    vec4(greaterThanEqual(b, vec4(1.0))),\\n    vec4(1.0));\\n\",\n    logicalOr = binaryKernelFunc({\n  opSnippet: LOGICAL_OR,\n  packedOpSnippet: LOGICAL_OR_PACKED,\n  dtype: \"bool\"\n}),\n    logicalOrConfig = {\n  kernelName: LogicalOr,\n  backendName: \"webgl\",\n  kernelFunc: logicalOr\n};\n\nclass LRNProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"x\"], this.outputShape = [];\n    var s = t,\n        o = e[3] - 1;\n    var i;\n    this.outputShape = e;\n    var l = \"float(\".concat(n, \") + float(\").concat(r, \") * sum\");\n    i = .5 === a ? \"inversesqrt(\".concat(l, \")\") : 1 === a ? \"1.0/(\".concat(l, \")\") : \"exp(log(\".concat(l, \") * float(-\").concat(a, \"));\"), this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int r = coords[1];\\n        int c = coords[2];\\n        int d = coords[3];\\n        float x = getX(b, r, c, d);\\n        float sum = 0.0;\\n        for (int j = -\".concat(s, \"; j <= \").concat(s, \"; j++) {\\n          int idx = d + j;\\n          if (idx >= 0 && idx <=  \").concat(o, \") {\\n            float z = getX(b, r, c, idx);\\n            sum += z * z;\\n          }\\n        }\\n        float val = x * \").concat(i, \";\\n        setOutput(val);\\n      }\\n    \");\n  }\n\n}\n\nclass LRNPackedProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"x\"], this.outputShape = [], this.packedInputs = !0, this.packedOutput = !0;\n    var s = t,\n        o = e[3] - 1;\n    var i;\n    this.outputShape = e;\n    var l = \"float(\".concat(n, \") + float(\").concat(r, \") * sum\");\n    i = .5 === a ? \"inversesqrt(\".concat(l, \")\") : 1 === a ? \"1.0/(\".concat(l, \")\") : \"exp(log(\".concat(l, \") * float(-\").concat(a, \"));\"), this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords.x;\\n        int r = coords.y;\\n        int c = coords.z;\\n        int d = coords.w;\\n\\n        bool hasNextCol = d < \".concat(this.outputShape[3], \";\\n        bool hasNextRow = c < \").concat(this.outputShape[2], \";\\n\\n        vec4 sum = vec4(0.);\\n        vec4 xFragAtOutputCoords = getX(b, r, c, d);\\n\\n        vec4 xAtOutputCoords = vec4(\\n          getChannel(xFragAtOutputCoords, vec2(c, d)),\\n          hasNextCol ?\\n            getChannel(xFragAtOutputCoords, vec2(c, d + 1)) : 0.0,\\n          hasNextRow ?\\n            getChannel(xFragAtOutputCoords , vec2(c + 1, d)) : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getChannel(xFragAtOutputCoords, vec2(c + 1, d + 1)) : 0.0\\n        );\\n\\n        int firstChannel = d - \").concat(s, \";\\n        vec2 cache = vec2(0.);\\n        if(firstChannel >= 0){\\n          vec4 firstChannelFrag = getX(b, r, c, firstChannel);\\n          cache.x = getChannel(firstChannelFrag, vec2(c, firstChannel));\\n            if(hasNextRow){\\n              cache.y = getChannel(firstChannelFrag, vec2(c + 1, firstChannel));\\n            }\\n        }\\n\\n        ivec2 depth = ivec2(d, d + 1);\\n        for (int j = - \").concat(s, \"; j <= \").concat(s, \"; j++) {\\n          ivec2 idx = depth + j;\\n          bvec2 aboveLowerBound = greaterThanEqual(idx, ivec2(0));\\n          bvec2 belowUpperBound = lessThanEqual(idx, ivec2(\").concat(o, \"));\\n\\n          bool depthInRange = aboveLowerBound.x && belowUpperBound.x;\\n          bool depthPlusOneInRange = aboveLowerBound.y && belowUpperBound.y;\\n\\n          if(depthInRange || depthPlusOneInRange){\\n            vec4 z = vec4(0.);\\n            vec4 xFragAtCurrentDepth;\\n            z.xz = cache.xy;\\n            if(depthPlusOneInRange && hasNextCol){\\n              xFragAtCurrentDepth = idx.y != d ?\\n                getX(b, r, c, idx.y) : xFragAtOutputCoords;\\n              z.y = getChannel(xFragAtCurrentDepth, vec2(c, idx.y));\\n              if(hasNextRow){\\n                z.w = getChannel(xFragAtCurrentDepth, vec2(c + 1, idx.y));\\n              }\\n            }\\n            cache.xy = z.yw;\\n            sum += z * z;\\n          }\\n        }\\n        vec4 result = xAtOutputCoords * \").concat(i, \";\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar lrn = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    depthRadius: s,\n    bias: o,\n    alpha: i,\n    beta: l\n  } = r,\n      u = env().getBool(\"WEBGL_PACK_NORMALIZATION\") ? new LRNPackedProgram(a.shape, s, o, i, l) : new LRNProgram(a.shape, s, o, i, l);\n  return n.runWebGLProgram(u, [a], a.dtype);\n},\n    LRNConfig = {\n  kernelName: LRN,\n  backendName: \"webgl\",\n  kernelFunc: lrn\n};\n\nclass LRNGradProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"inputImage\", \"outputImage\", \"dy\"], this.outputShape = [], this.outputShape = e, this.depth = e[3], this.depthRadius = t, this.bias = n, this.alpha = r, this.beta = a, this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int r = coords[1];\\n        int c = coords[2];\\n\\n        float result = 0.0;\\n        for (int d = 0; d < \".concat(this.depth, \"; ++d) {\\n          int depthBegin = int(max(0.0, float(d - \").concat(t, \")));\\n          int depthEnd = int(min(float(\").concat(this.depth, \"),\\n              float(d + \").concat(t, \" + 1)));\\n\\n          const int MIN_DEPTH_BEGIN = 0;\\n          const int MAX_DEPTH_END = \").concat(this.depth, \";\\n\\n          float norm = 0.0;\\n          for (int k = MIN_DEPTH_BEGIN; k < MAX_DEPTH_END; ++k) {\\n            if (k < depthBegin){\\n              continue;\\n            }\\n            else if (k >= depthBegin && k < depthEnd) {\\n              norm += getInputImage(b, r, c, k) * getInputImage(b, r, c, k);\\n            }\\n            else {\\n              break;\\n            }\\n          }\\n\\n          norm = float(\").concat(r, \") * norm + float(\").concat(n, \");\\n\\n          for(int k = MIN_DEPTH_BEGIN; k < MAX_DEPTH_END; ++k){\\n            if (k < depthBegin){\\n              continue;\\n            }\\n            else if (k >= depthBegin && k < depthEnd){\\n              float dyi = -2.0 * float(\").concat(r, \")\\n                * float(\").concat(a, \")\\n                * getInputImage(b ,r ,c, k) * getOutputImage(b, r, c, d)\\n                / norm;\\n              if (k == d) {\\n                dyi += pow(norm, -1.0 * \").concat(a, \");\\n              }\\n              if (k == coords[3]) {\\n                dyi *= getDy(b, r, c, d);\\n                result += dyi;\\n              }\\n            }\\n            else {\\n              break;\\n            }\\n          }\\n      }\\n      setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar lrnGrad = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    y: s,\n    dy: o\n  } = t,\n      {\n    depthRadius: i,\n    bias: l,\n    alpha: u,\n    beta: c\n  } = r,\n      p = new LRNGradProgram(a.shape, i, l, u, c);\n  return n.runWebGLProgram(p, [a, s, o], a.dtype);\n},\n    LRNGradConfig = {\n  kernelName: LRNGrad,\n  backendName: \"webgl\",\n  kernelFunc: lrnGrad\n};\n\nfunction maxImpl(e, t, n, r) {\n  var a = sizeFromShape(t),\n      s = reshape({\n    inputs: {\n      x: e\n    },\n    attrs: {\n      shape: [sizeFromShape(e.shape) / a, a]\n    },\n    backend: r\n  }),\n      o = reduce(s, e.dtype, \"max\", r),\n      i = reshape({\n    inputs: {\n      x: o\n    },\n    attrs: {\n      shape: n\n    },\n    backend: r\n  });\n  return r.disposeIntermediateTensorInfo(s), r.disposeIntermediateTensorInfo(o), i;\n}\n\nfunction max(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reductionIndices: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation(u, i),\n      p = null != c,\n      d = n.shouldExecuteOnCPU([a]);\n  var h = a;\n\n  if (p) {\n    if (d) {\n      var _e1049 = n.texData.get(h.dataId).values,\n          _t717 = new Array(i);\n\n      for (var _e1050 = 0; _e1050 < _t717.length; _e1050++) {\n        _t717[_e1050] = a.shape[c[_e1050]];\n      }\n\n      var _r427 = transposeImplCPU(_e1049, a.shape, a.dtype, c, _t717);\n\n      h = n.makeTensorInfo(_t717, a.dtype), n.texData.get(h.dataId).values = _r427;\n    } else h = transposeImpl(a, c, n);\n\n    u = getInnerMostAxes(u.length, i);\n  }\n\n  assertAxesAreInnerMostDims(\"max\", u, i);\n  var [m, f] = computeOutAndReduceShapes(h.shape, u);\n  var g,\n      $ = m;\n\n  if (o && ($ = expandShapeToKeepDim(m, l)), d) {\n    var _e1051 = n.texData.get(h.dataId),\n        _t718 = maxImplCPU(_e1051.values, sizeFromShape(f), $, a.dtype);\n\n    g = n.makeTensorInfo($, a.dtype), n.texData.get(g.dataId).values = _t718;\n  } else g = maxImpl(h, f, $, n);\n\n  return p && n.disposeIntermediateTensorInfo(h), g;\n}\n\nvar maxConfig = {\n  kernelName: Max,\n  backendName: \"webgl\",\n  kernelFunc: max\n},\n    MAXIMUM = CHECK_NAN_SNIPPET$1 + \"\\n  return max(a, b);\\n\",\n    MAXIMUM_PACKED = \"\\n  vec4 result = vec4(max(a, b));\\n  vec4 isNaN = min(vec4(isnan(a)) + vec4(isnan(b)), vec4(1.0));\\n  \" + CHECK_NAN_SNIPPET + \"\\n  return result;\\n\",\n    maximum = binaryKernelFunc({\n  opSnippet: MAXIMUM,\n  packedOpSnippet: MAXIMUM_PACKED,\n  cpuKernelImpl: maximumImplCPU\n}),\n    maximumConfig = {\n  kernelName: Maximum$1,\n  backendName: \"webgl\",\n  kernelFunc: maximum\n};\n\nfunction maxPool(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t;\n  assertNotComplex(a, \"maxPool\");\n  var {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dimRoundingMode: l\n  } = r;\n  assert$4(eitherStridesOrDilationsAreOne(o, 1), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(o, \" and dilations '1'\"));\n  var u = computePool2DInfo(a.shape, s, o, 1, i, l);\n  if (1 === u.filterWidth && 1 === u.filterHeight && arraysEqual(u.inShape, u.outShape)) return identity({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var c = new Pool2DProgram(u, \"max\", !1);\n  return n.runWebGLProgram(c, [a], a.dtype);\n}\n\nvar maxPoolConfig = {\n  kernelName: MaxPool,\n  backendName: \"webgl\",\n  kernelFunc: maxPool\n};\n\nfunction maxPool3d(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    filterSize: s,\n    strides: o,\n    pad: i,\n    dataFormat: l,\n    dimRoundingMode: u\n  } = r,\n      c = computePool3DInfo(a.shape, s, o, [1, 1, 1], i, u, l),\n      p = new Pool3DProgram(c, \"max\", !1);\n  return n.runWebGLProgram(p, [a], a.dtype);\n}\n\nvar maxPool3DConfig = {\n  kernelName: MaxPool3D,\n  backendName: \"webgl\",\n  kernelFunc: maxPool3d\n};\n\nclass MaxPool2DBackpropProgram {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"maxPos\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterHeight,\n        n = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec2 pads = ivec2(\".concat(t - 1 - e.padInfo.top, \", \").concat(n - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n\\n        ivec2 dyRCCorner = coords.yz - pads;\\n        int dyRCorner = dyRCCorner.x;\\n        int dyCCorner = dyRCCorner.y;\\n\\n        // Convolve dy(?, ?, d) with pos mask(:, :, d) to get dx(xR, xC, d).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n        for (int wR = 0; wR < \").concat(t, \";\\n          wR += \").concat(e.dilationHeight, \") {\\n          float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n          if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 || fract(dyR) > 0.0) {\\n            continue;\\n          }\\n          int idyR = int(dyR);\\n\\n          for (int wC = 0; wC < \").concat(n, \"; wC++) {\\n            float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n            if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                fract(dyC) > 0.0) {\\n              continue;\\n            }\\n            int idyC = int(dyC);\\n\\n            float dyValue = getDy(b, idyR, idyC, d);\\n            int maxPosValue = \").concat(t * n - 1, \" - int(getMaxPos(b, idyR, idyC, d));\\n\\n            // Get the current value, check it against the value from the\\n            // position matrix.\\n            int curPosValue = wR * \").concat(n, \" + wC;\\n            float mask = float(maxPosValue == curPosValue ? 1.0 : 0.0);\\n\\n            dotProd += dyValue * mask;\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nclass MaxPool3DBackpropProgram {\n  constructor(e) {\n    this.variableNames = [\"dy\", \"maxPos\"], this.outputShape = e.inShape;\n    var t = e.effectiveFilterDepth,\n        n = e.effectiveFilterHeight,\n        r = e.effectiveFilterWidth;\n    this.userCode = \"\\n      const ivec3 pads = ivec3(\".concat(t - 1 - e.padInfo.front, \", \").concat(n - 1 - e.padInfo.top, \", \").concat(r - 1 - e.padInfo.left, \");\\n\\n      void main() {\\n        ivec5 coords = getOutputCoords();\\n        int batch = coords.x;\\n        int ch = coords.u;\\n\\n        ivec3 dyCorner = ivec3(coords.y, coords.z, coords.w) - pads;\\n        int dyDCorner = dyCorner.x;\\n        int dyRCorner = dyCorner.y;\\n        int dyCCorner = dyCorner.z;\\n\\n        // Convolve dy(?, ?, ?, ch) with pos mask(:, :, :, d) to get\\n        // dx(xD, xR, xC, ch).\\n        // ? = to be determined. : = across all values in that axis.\\n        float dotProd = 0.0;\\n\\n        for (int wD = 0; wD < \").concat(t, \";\\n           wD += \").concat(e.dilationDepth, \") {\\n          float dyD = float(dyDCorner + wD) / \").concat(e.strideDepth, \".0;\\n\\n          if (dyD < 0.0 || dyD >= \").concat(e.outDepth, \".0 || fract(dyD) > 0.0) {\\n            continue;\\n          }\\n          int idyD = int(dyD);\\n\\n          for (int wR = 0; wR < \").concat(n, \";\\n              wR += \").concat(e.dilationHeight, \") {\\n            float dyR = float(dyRCorner + wR) / \").concat(e.strideHeight, \".0;\\n\\n            if (dyR < 0.0 || dyR >= \").concat(e.outHeight, \".0 ||\\n                fract(dyR) > 0.0) {\\n              continue;\\n            }\\n            int idyR = int(dyR);\\n\\n            for (int wC = 0; wC < \").concat(r, \";\\n                wC += \").concat(e.dilationWidth, \") {\\n              float dyC = float(dyCCorner + wC) / \").concat(e.strideWidth, \".0;\\n\\n              if (dyC < 0.0 || dyC >= \").concat(e.outWidth, \".0 ||\\n                  fract(dyC) > 0.0) {\\n                continue;\\n              }\\n              int idyC = int(dyC);\\n\\n              float dyValue = getDy(batch, idyD, idyR, idyC, ch);\\n              int maxPosValue = \").concat(t * n * r - 1, \" -\\n                  int(getMaxPos(batch, idyD, idyR, idyC, ch));\\n\\n              // Get the current value, check it against the value from the\\n              // position matrix.\\n              int curPosValue =\\n                  wD * \").concat(n, \" * \").concat(r, \" +\\n                  wR * \").concat(r, \" + wC;\\n              float mask = float(maxPosValue == curPosValue ? 1.0 : 0.0);\\n\\n              dotProd += dyValue * mask;\\n            }\\n          }\\n        }\\n        setOutput(dotProd);\\n      }\\n    \");\n  }\n\n}\n\nfunction maxPool3DGrad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s\n  } = t,\n      o = s,\n      {\n    filterSize: i,\n    strides: l,\n    pad: u,\n    dimRoundingMode: c\n  } = r,\n      p = computePool3DInfo(o.shape, i, l, [1, 1, 1], u, c),\n      d = new Pool3DProgram(p, \"max\", !0),\n      h = n.runWebGLProgram(d, [o], o.dtype),\n      m = new MaxPool3DBackpropProgram(p),\n      f = n.runWebGLProgram(m, [a, h], o.dtype);\n  return n.disposeIntermediateTensorInfo(h), f;\n}\n\nvar maxPoolGrad3DConfig = {\n  kernelName: MaxPool3DGrad,\n  backendName: \"webgl\",\n  kernelFunc: maxPool3DGrad\n};\n\nfunction maxPoolGrad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    dy: a,\n    input: s,\n    output: o\n  } = t,\n      i = s;\n  assertNotComplex([s, o], \"maxPoolGrad\");\n  var {\n    filterSize: l,\n    strides: u,\n    pad: c,\n    dimRoundingMode: p\n  } = r,\n      d = computePool2DInfo(i.shape, l, u, 1, c, p),\n      h = new Pool2DProgram(d, \"max\", !0),\n      m = n.runWebGLProgram(h, [i], i.dtype),\n      f = new MaxPool2DBackpropProgram(d),\n      g = n.runWebGLProgram(f, [a, m], i.dtype);\n  return n.disposeIntermediateTensorInfo(m), g;\n}\n\nvar maxPoolGradConfig = {\n  kernelName: MaxPoolGrad,\n  backendName: \"webgl\",\n  kernelFunc: maxPoolGrad\n};\n\nfunction maxPoolWithArgmaxImpl(e, t, n, r) {\n  var a = new Pool2DProgram(n, \"max\", !1);\n  var s = r.runWebGLProgram(a, [e], \"float32\");\n  return a = new Pool2DProgram(n, \"max\", !0, !0, t), [s, r.runWebGLProgram(a, [e], \"float32\")];\n}\n\nvar maxPoolWithArgmaxConfig = {\n  kernelName: MaxPoolWithArgmax,\n  backendName: \"webgl\",\n  kernelFunc: _ref70 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref70;\n    var {\n      x: r\n    } = e,\n        {\n      filterSize: a,\n      strides: s,\n      pad: o,\n      includeBatchInIndex: i\n    } = t,\n        l = n;\n    assert$4(4 === r.shape.length, () => \"Error in maxPool: input must be rank 4 but got rank \".concat(r.shape.length, \".\"));\n    var u = [1, 1];\n    assert$4(eitherStridesOrDilationsAreOne(s, u), () => \"Error in maxPool: Either strides or dilations must be 1. Got strides \".concat(s, \" and dilations '\").concat(u, \"'\"));\n    var c = computePool2DInfo(r.shape, a, s, u, o),\n        [p, d] = maxPoolWithArgmaxImpl(r, i, c, l);\n    return [p, d];\n  }\n};\n\nfunction meanImpl(e, t, n, r) {\n  var a = sizeFromShape(t),\n      s = reshape({\n    inputs: {\n      x: e\n    },\n    attrs: {\n      shape: [sizeFromShape(e.shape) / a, a]\n    },\n    backend: r\n  }),\n      o = reduce(s, \"float32\", \"mean\", r),\n      i = reshape({\n    inputs: {\n      x: o\n    },\n    attrs: {\n      shape: n\n    },\n    backend: r\n  });\n  return r.disposeIntermediateTensorInfo(s), r.disposeIntermediateTensorInfo(o), i;\n}\n\nvar meanConfig = {\n  kernelName: Mean,\n  backendName: \"webgl\",\n  kernelFunc: _ref71 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref71;\n    var {\n      x: r\n    } = e,\n        {\n      keepDims: a,\n      axis: s\n    } = t,\n        o = n,\n        i = r.shape.length,\n        l = parseAxisParam(s, r.shape);\n    var u = l;\n    var c = getAxesPermutation(u, i),\n        p = null != c,\n        d = o.shouldExecuteOnCPU([r]),\n        h = [];\n    var m = r;\n\n    if (p) {\n      if (d) {\n        var _e1052 = o.texData.get(m.dataId).values,\n            _t719 = new Array(i);\n\n        for (var _e1053 = 0; _e1053 < _t719.length; _e1053++) {\n          _t719[_e1053] = r.shape[c[_e1053]];\n        }\n\n        var _n437 = transposeImplCPU(_e1052, r.shape, r.dtype, c, _t719);\n\n        m = o.makeTensorInfo(_t719, r.dtype), o.texData.get(m.dataId).values = _n437;\n      } else m = transposeImpl(r, c, o);\n\n      h.push(m), u = getInnerMostAxes(u.length, i);\n    }\n\n    assertAxesAreInnerMostDims(\"sum\", u, i);\n    var [f, g] = computeOutAndReduceShapes(m.shape, u);\n    var $ = f;\n    a && ($ = expandShapeToKeepDim(f, l));\n    var y = meanImpl(m, g, $, o);\n\n    for (var _e1054 of h) {\n      o.disposeIntermediateTensorInfo(_e1054);\n    }\n\n    return y;\n  }\n};\n\nfunction min(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = parseAxisParam(s, a.shape);\n  var u = l;\n  var c = getAxesPermutation(u, i);\n  var p = a;\n  null != c && (p = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), u = getInnerMostAxes(u.length, a.shape.length)), assertAxesAreInnerMostDims(\"min\", u, i);\n  var [d, h] = computeOutAndReduceShapes(p.shape, u),\n      m = reshape({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, sizeFromShape(h)]\n    }\n  }),\n      f = reduce(m, m.dtype, \"min\", n);\n  var g;\n  return g = reshape(o ? {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: expandShapeToKeepDim(d, l)\n    }\n  } : {\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  }), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo(f), null != c && n.disposeIntermediateTensorInfo(p), g;\n}\n\nvar minConfig = {\n  kernelName: Min,\n  backendName: \"webgl\",\n  kernelFunc: min\n},\n    MINIMUM = CHECK_NAN_SNIPPET$1 + \"\\n  return min(a, b);\\n\",\n    MINIMUM_PACKED = \"\\n  vec4 result = vec4(min(a, b));\\n  vec4 isNaN = min(vec4(isnan(a)) + vec4(isnan(b)), vec4(1.0));\\n  \" + CHECK_NAN_SNIPPET + \"\\n  return result;\\n\",\n    minimum = binaryKernelFunc({\n  opSnippet: MINIMUM,\n  packedOpSnippet: MINIMUM_PACKED,\n  cpuKernelImpl: minimumImplCPU\n}),\n    minimumConfig = {\n  kernelName: Minimum$1,\n  backendName: \"webgl\",\n  kernelFunc: minimum\n};\n\nclass MirrorPadProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = [\"coords[0]\", \"coords[1]\", \"coords[2]\", \"coords[3]\"].slice(0, r),\n        l = \"reflect\" === n ? 0 : 1;\n    this.userCode = 1 !== r ? \"\\n      \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outC = getOutputCoords();\\n        for (int i = 0; i < \").concat(r, \"; i++) {\\n          if (outC[i] < start[i]) {\\n            outC[i] = start[i] * 2 - outC[i] - \").concat(l, \";\\n          } else if(outC[i] >= end[i]) {\\n            outC[i] = (end[i] - 1) * 2 - outC[i] + \").concat(l, \";\\n          }\\n        }\\n        \").concat(a, \" coords = outC - start;\\n        setOutput(getX(\").concat(i, \"));\\n      }\\n    \") : \"\\n        int start = \".concat(s, \";\\n        int end = \").concat(o, \";\\n\\n        void main() {\\n          int outC = getOutputCoords();\\n          if (outC < start) {\\n            outC = start * 2 - outC - \").concat(l, \";\\n          } else if(outC >= end) {\\n            outC = (end - 1) * 2 - outC + \").concat(l, \";\\n          }\\n          setOutput(getX(outC - start));\\n        }\\n      \");\n  }\n\n}\n\nclass MirrorPadPackedProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = getChannels(\"rc\", r),\n        l = getChannels(\"source\", r),\n        u = \"\".concat(i[r - 1], \" < \").concat(this.outputShape[r - 1]),\n        c = 1 === r ? \"source\" : \"vec2(\".concat(l.slice(-2).join(), \")\"),\n        p = \"reflect\" === n ? 0 : 1;\n    var d = \"\";\n\n    if (1 === r) {\n      var _e1055 = \"\\n        \".concat(a, \" source = rc;\\n        if (source < start) {\\n          source = start * 2 - source - \").concat(p, \";\\n        } else if (source >= end) {\\n          source = (end - 1) * 2 - source + \").concat(p, \";\\n        }\\n        source -= start;\\n      \");\n\n      d = \"\\n        \".concat(a, \" rc = outputLoc;\\n        \").concat(_e1055, \"\\n        result[0] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        \").concat(i[r - 1], \" += 1;\\n        if(\").concat(u, \") {\\n          \").concat(_e1055, \"\\n          result[1] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        }\\n      \");\n    } else {\n      var _e1056 = \"\\n        \".concat(a, \" source = rc;\\n        \").concat(a, \" lt = \").concat(a, \"(lessThan(source, start));\\n        \").concat(a, \" gte = \").concat(a, \"(greaterThanEqual(source, end));\\n        \").concat(a, \" orig = 1 - (lt + gte);\\n        source = orig * source +\\n                lt * (start * 2 - source - \").concat(p, \") +\\n                gte * ((end - 1) * 2 - source + \").concat(p, \");\\n        source -= start;\\n      \");\n\n      d = \"\\n        \".concat(a, \" rc = outputLoc;\\n        \").concat(_e1056, \"\\n        result[0] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        \").concat(i[r - 1], \" += 1;\\n        if(\").concat(u, \") {\\n          \").concat(_e1056, \"\\n          result[1] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        }\\n        rc = outputLoc;\\n        \").concat(i[r - 2], \" += 1;\\n        if(\").concat(i[r - 2], \" < \").concat(this.outputShape[r - 2], \") {\\n          \").concat(_e1056, \"\\n          result[2] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n          \").concat(i[r - 1], \" += 1;\\n          if(\").concat(u, \") {\\n            \").concat(_e1056, \"\\n            result[3] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n          }\\n        }\\n      \");\n    }\n\n    this.userCode = \"\\n      const \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      const \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outputLoc = getOutputCoords();\\n        vec4 result = vec4(0.);\\n        \").concat(d, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar mirrorPadKernelFunc = _ref72 => {\n  var {\n    inputs: e,\n    backend: t,\n    attrs: n\n  } = _ref72;\n  var {\n    x: r\n  } = e,\n      {\n    paddings: a,\n    mode: s\n  } = n,\n      o = env().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new MirrorPadPackedProgram(r.shape, a, s) : new MirrorPadProgram(r.shape, a, s);\n  return t.runWebGLProgram(o, [r], r.dtype);\n},\n    mirrorPadConfig = {\n  kernelName: MirrorPad,\n  backendName: \"webgl\",\n  kernelFunc: mirrorPadKernelFunc\n},\n    MOD = \"if (b == 0.0) return NAN;\\n  return mod(a, b);\",\n    MOD_PACKED = \"\\n  vec4 result = mod(a, b);\\n  vec4 isNaN = vec4(equal(b, vec4(0.0)));\\n  \" + CHECK_NAN_SNIPPET + \"\\n  return result;\\n\",\n    mod = binaryKernelFunc({\n  opSnippet: MOD,\n  packedOpSnippet: MOD_PACKED\n}),\n    modConfig = {\n  kernelName: Mod,\n  backendName: \"webgl\",\n  kernelFunc: mod\n};\n\nclass MultinomialProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"probs\"], this.customUniforms = [{\n      name: \"seed\",\n      type: \"float\"\n    }], this.outputShape = [e, n], this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n\\n        float r = random(seed);\\n        float cdf = 0.0;\\n\\n        for (int i = 0; i < \".concat(t - 1, \"; i++) {\\n          cdf += getProbs(batch, i);\\n\\n          if (r < cdf) {\\n            setOutput(float(i));\\n            return;\\n          }\\n        }\\n\\n        // If no other event happened, last event happened.\\n        setOutput(float(\").concat(t - 1, \"));\\n      }\\n    \");\n  }\n\n}\n\nvar DIV = \"\\nif (a == b) {\\n  return 1.0;\\n};\\nreturn a / b;\",\n    DIV_PACKED = \"\\n  // vec4 one = vec4(equal(a, b));\\n  // return one + (vec4(1.0) - one) * a / b;\\n  vec4 result = a / b;\\n  if(a.x == b.x) {\\n    result.x = 1.;\\n  }\\n  if(a.y == b.y) {\\n    result.y = 1.;\\n  }\\n  if(a.z == b.z) {\\n    result.z = 1.;\\n  }\\n  if(a.w == b.w) {\\n    result.w = 1.;\\n  }\\n\\n  return result;\\n\",\n    realDiv = binaryKernelFunc({\n  opSnippet: DIV,\n  packedOpSnippet: DIV_PACKED,\n  checkOutOfBounds: !0\n}),\n    realDivConfig = {\n  kernelName: RealDiv,\n  backendName: \"webgl\",\n  kernelFunc: realDiv\n},\n    SUB = \"return a - b;\",\n    sub = binaryKernelFunc({\n  opSnippet: SUB,\n  packedOpSnippet: SUB,\n  supportsComplex: !0,\n  cpuKernelImpl: subImplCPU\n}),\n    subConfig = {\n  kernelName: Sub,\n  backendName: \"webgl\",\n  kernelFunc: sub\n};\n\nfunction softmax(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    dim: s\n  } = r,\n      o = parseAxisParam([s], a.shape),\n      i = max({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      reductionIndices: o,\n      keepDims: !1\n    }\n  }),\n      l = expandShapeToKeepDim(i.shape, o),\n      u = reshape({\n    inputs: {\n      x: i\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      c = sub({\n    inputs: {\n      a,\n      b: u\n    },\n    backend: n\n  }),\n      p = exp({\n    inputs: {\n      x: c\n    },\n    backend: n\n  }),\n      d = sum({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      axis: o,\n      keepDims: !1\n    }\n  }),\n      h = reshape({\n    inputs: {\n      x: d\n    },\n    backend: n,\n    attrs: {\n      shape: l\n    }\n  }),\n      m = realDiv({\n    inputs: {\n      a: p,\n      b: h\n    },\n    backend: n\n  });\n  return n.disposeIntermediateTensorInfo(i), n.disposeIntermediateTensorInfo(u), n.disposeIntermediateTensorInfo(c), n.disposeIntermediateTensorInfo(p), n.disposeIntermediateTensorInfo(d), n.disposeIntermediateTensorInfo(h), m;\n}\n\nvar softmaxConfig = {\n  kernelName: Softmax$2,\n  backendName: \"webgl\",\n  kernelFunc: softmax\n};\n\nfunction multinomial(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    logits: a\n  } = t,\n      {\n    numSamples: s,\n    seed: o,\n    normalized: i\n  } = r,\n      l = i ? a : softmax({\n    inputs: {\n      logits: a\n    },\n    backend: n,\n    attrs: {\n      dim: a.shape.length - 1\n    }\n  }),\n      u = new MultinomialProgram(l.shape[0], l.shape[1], s),\n      c = n.runWebGLProgram(u, [l], \"int32\", [[o]]);\n  return i || n.disposeIntermediateTensorInfo(l), c;\n}\n\nvar multinomialConfig = {\n  kernelName: Multinomial,\n  backendName: \"webgl\",\n  kernelFunc: multinomial\n},\n    NEG = \"return -x;\";\n\nfunction neg(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n\n  if (n.shouldExecuteOnCPU([r])) {\n    var _e1057 = n.texData.get(r.dataId),\n        [_t720, _a306] = negImplCPU(_e1057.values, r.shape, r.dtype);\n\n    return n.makeTensorInfo(_a306, r.dtype, _t720);\n  }\n\n  var a;\n  return a = env().getBool(\"WEBGL_PACK_UNARY_OPERATIONS\") ? new UnaryOpPackedProgram(r.shape, NEG) : new UnaryOpProgram(r.shape, NEG), n.runWebGLProgram(a, [r], r.dtype);\n}\n\nvar negConfig = {\n  kernelName: Neg,\n  backendName: \"webgl\",\n  kernelFunc: neg\n},\n    nonMaxSuppressionV3Impl = nonMaxSuppressionV3Impl$2;\n\nfunction nonMaxSuppressionV3(e) {\n  warn(\"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead\");\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l\n  } = r,\n      u = n.readSync(a.dataId),\n      c = n.readSync(s.dataId),\n      {\n    selectedIndices: p\n  } = nonMaxSuppressionV3Impl(u, c, o, i, l);\n  return n.makeTensorInfo([p.length], \"int32\", new Int32Array(p));\n}\n\nvar nonMaxSuppressionV3Config = {\n  kernelName: NonMaxSuppressionV3,\n  backendName: \"webgl\",\n  kernelFunc: nonMaxSuppressionV3\n},\n    nonMaxSuppressionV4Impl = nonMaxSuppressionV4Impl$2;\n\nfunction nonMaxSuppressionV4(e) {\n  warn(\"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead\");\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    padToMaxOutputSize: u\n  } = r,\n      c = n.readSync(a.dataId),\n      p = n.readSync(s.dataId),\n      {\n    selectedIndices: d,\n    validOutputs: h\n  } = nonMaxSuppressionV4Impl(c, p, o, i, l, u);\n  return [n.makeTensorInfo([d.length], \"int32\", new Int32Array(d)), n.makeTensorInfo([], \"int32\", new Int32Array([h]))];\n}\n\nvar nonMaxSuppressionV4Config = {\n  kernelName: NonMaxSuppressionV4,\n  backendName: \"webgl\",\n  kernelFunc: nonMaxSuppressionV4\n},\n    nonMaxSuppressionV5Impl = nonMaxSuppressionV5Impl$2;\n\nfunction nonMaxSuppressionV5(e) {\n  warn(\"tf.nonMaxSuppression() in webgl locks the UI thread. Call tf.nonMaxSuppressionAsync() instead\");\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    boxes: a,\n    scores: s\n  } = t,\n      {\n    maxOutputSize: o,\n    iouThreshold: i,\n    scoreThreshold: l,\n    softNmsSigma: u\n  } = r,\n      c = n.readSync(a.dataId),\n      p = n.readSync(s.dataId),\n      d = o,\n      h = i,\n      m = l,\n      f = u,\n      {\n    selectedIndices: g,\n    selectedScores: $\n  } = nonMaxSuppressionV5Impl(c, p, d, h, m, f);\n  return [n.makeTensorInfo([g.length], \"int32\", new Int32Array(g)), n.makeTensorInfo([$.length], \"float32\", new Float32Array($))];\n}\n\nvar nonMaxSuppressionV5Config = {\n  kernelName: NonMaxSuppressionV5,\n  backendName: \"webgl\",\n  kernelFunc: nonMaxSuppressionV5\n};\n\nclass OneHotProgram {\n  constructor(e, t, n, r) {\n    this.variableNames = [\"indices\"], this.outputShape = [e, t], this.userCode = \"\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int index = round(getIndices(coords.x));\\n        setOutput(mix(float(\".concat(r, \"), float(\").concat(n, \"),\\n                      float(index == coords.y)));\\n      }\\n    \");\n  }\n\n}\n\nvar oneHot = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a\n  } = t,\n      {\n    depth: s,\n    onValue: o,\n    offValue: i\n  } = r,\n      l = sizeFromShape(a.shape),\n      u = new OneHotProgram(l, s, o, i),\n      c = reshape({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [l]\n    }\n  }),\n      p = n.runWebGLProgram(u, [c], a.dtype);\n  n.disposeIntermediateTensorInfo(c);\n  var d = reshape({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [...a.shape, s]\n    }\n  });\n  return n.disposeIntermediateTensorInfo(p), d;\n},\n    oneHotConfig = {\n  kernelName: OneHot,\n  backendName: \"webgl\",\n  kernelFunc: oneHot\n};\n\nfunction zerosLike(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n\n  if (\"complex64\" === r.dtype) {\n    var _e1058 = real({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t721 = zerosLike({\n      inputs: {\n        x: _e1058\n      },\n      backend: n\n    }),\n        a = imag({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex({\n      inputs: {\n        real: _t721,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e1058), n.disposeIntermediateTensorInfo(_t721), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill({\n    attrs: {\n      shape: r.shape,\n      dtype: r.dtype,\n      value: \"string\" === r.dtype ? \"\" : 0\n    },\n    backend: n\n  });\n}\n\nvar zerosLikeConfig = {\n  kernelName: ZerosLike,\n  backendName: \"webgl\",\n  kernelFunc: zerosLike\n};\n\nfunction onesLike(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    x: r\n  } = t;\n  if (\"string\" === r.dtype) throw new Error(\"onesLike is not supported under string dtype\");\n\n  if (\"complex64\" === r.dtype) {\n    var _e1059 = real({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        _t722 = onesLike({\n      inputs: {\n        x: _e1059\n      },\n      backend: n\n    }),\n        a = imag({\n      inputs: {\n        input: r\n      },\n      backend: n\n    }),\n        s = zerosLike({\n      inputs: {\n        x: a\n      },\n      backend: n\n    }),\n        o = complex({\n      inputs: {\n        real: _t722,\n        imag: s\n      },\n      backend: n\n    });\n\n    return n.disposeIntermediateTensorInfo(_e1059), n.disposeIntermediateTensorInfo(_t722), n.disposeIntermediateTensorInfo(a), n.disposeIntermediateTensorInfo(s), o;\n  }\n\n  return fill({\n    attrs: {\n      shape: r.shape,\n      dtype: r.dtype,\n      value: 1\n    },\n    backend: n\n  });\n}\n\nvar onesLikeConfig = {\n  kernelName: OnesLike,\n  backendName: \"webgl\",\n  kernelFunc: onesLike\n};\n\nfunction pack(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    axis: a\n  } = r;\n  if (1 === t.length) return expandDims({\n    inputs: {\n      input: t[0]\n    },\n    backend: n,\n    attrs: {\n      dim: a\n    }\n  });\n  var s = t[0].shape,\n      o = t[0].dtype;\n  t.forEach(e => {\n    assertShapesMatch(s, e.shape, \"All tensors passed to stack must have matching shapes\"), assert$4(o === e.dtype, () => \"All tensors passed to stack must have matching dtypes\");\n  });\n  var i = [],\n      l = concat({\n    inputs: t.map(e => {\n      var t = expandDims({\n        inputs: {\n          input: e\n        },\n        backend: n,\n        attrs: {\n          dim: a\n        }\n      });\n      return i.push(t), t;\n    }),\n    backend: n,\n    attrs: {\n      axis: a\n    }\n  });\n  return i.forEach(e => n.disposeIntermediateTensorInfo(e)), l;\n}\n\nvar packConfig = {\n  kernelName: Pack,\n  backendName: \"webgl\",\n  kernelFunc: pack\n};\n\nclass PadProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.customUniforms = [{\n      name: \"value\",\n      type: \"float\"\n    }], this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = [\"coords[0]\", \"coords[1]\", \"coords[2]\", \"coords[3]\"].slice(0, r);\n    this.userCode = 1 !== r ? \"\\n      \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outC = getOutputCoords();\\n        if (any(lessThan(outC, start)) || any(greaterThanEqual(outC, end))) {\\n          setOutput(value);\\n        } else {\\n          \").concat(a, \" coords = outC - start;\\n          setOutput(getX(\").concat(i, \"));\\n        }\\n      }\\n    \") : \"\\n        int start = \".concat(s, \";\\n        int end = \").concat(o, \";\\n\\n        void main() {\\n          int outC = getOutputCoords();\\n          if (outC < start || outC >= end) {\\n            setOutput(value);\\n          } else {\\n            setOutput(getX(outC - start));\\n          }\\n        }\\n      \");\n  }\n\n}\n\nclass PadPackedProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.packedInputs = !0, this.packedOutput = !0, this.customUniforms = [{\n      name: \"value\",\n      type: \"float\"\n    }], this.outputShape = t.map((t, n) => t[0] + e[n] + t[1]);\n    var r = e.length,\n        a = getCoordsDataType(r),\n        s = t.map(e => e[0]).join(\",\"),\n        o = t.map((t, n) => t[0] + e[n]).join(\",\"),\n        i = getChannels(\"rc\", r),\n        l = getChannels(\"source\", r),\n        u = \"\".concat(i[r - 1], \" < \").concat(this.outputShape[r - 1]),\n        c = 1 === r ? \"source\" : \"vec2(\".concat(l.slice(-2).join(), \")\"),\n        p = [\"\".concat(a, \" rc = outputLoc;\"), \"\".concat(i[r - 1], \" += 1;\\n       if(\").concat(u, \") {\\n      \"), 1 === r ? \"\" : \"}\\n       rc = outputLoc;\\n       \".concat(i[r - 2], \" += 1;\\n       if(\").concat(i[r - 2], \" < \").concat(this.outputShape[r - 2], \") {\"), 1 === r ? \"\" : \"  \".concat(i[r - 1], \" += 1;\\n         if(\").concat(u, \") {\")],\n        d = 1 === r ? \"rc < start || rc >= end\" : \"any(lessThan(rc, start)) || any(greaterThanEqual(rc, end))\";\n    var h = \"\";\n\n    for (var _e1060 = 0, _t723 = 1 === r ? 2 : 4; _e1060 < _t723; _e1060++) {\n      h += \"\\n        \".concat(p[_e1060], \"\\n        if (\").concat(d, \") {\\n          result[\").concat(_e1060, \"] = float(value);\\n        } else {\\n          \").concat(a, \" source = rc - start;\\n          result[\").concat(_e1060, \"] = getChannel(getX(\").concat(l.join(), \"), \").concat(c, \");\\n        }\\n      \");\n    }\n\n    h += 1 === r ? \"} \" : \"}}\", this.userCode = \"\\n      const \".concat(a, \" start = \").concat(a, \"(\").concat(s, \");\\n      const \").concat(a, \" end = \").concat(a, \"(\").concat(o, \");\\n\\n      void main() {\\n        \").concat(a, \" outputLoc = getOutputCoords();\\n        vec4 result = vec4(0.);\\n        \").concat(h, \"\\n        setOutput(result);\\n      }\\n    \");\n  }\n\n}\n\nvar padV2 = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    paddings: s,\n    constantValue: o\n  } = r,\n      i = env().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new PadPackedProgram(a.shape, s, o) : new PadProgram(a.shape, s, o);\n  return n.runWebGLProgram(i, [a], a.dtype, [[o]]);\n},\n    padV2Config = {\n  kernelName: PadV2,\n  backendName: \"webgl\",\n  kernelFunc: padV2\n},\n    POW = \"\\n  if(a < 0.0 && floor(b) < b){\\n    return NAN;\\n  }\\n  if (b == 0.0) {\\n    return 1.0;\\n  }\\n  return (round(mod(b, 2.0)) != 1) ?\\n      pow(abs(a), b) : sign(a) * pow(abs(a), b);\\n\",\n    POW_PACKED = \"\\n  // isModRound1 has 1 for components with round(mod(b, 2.0)) == 1, 0 otherwise.\\n  vec4 isModRound1 = vec4(equal(round(mod(b, 2.0)), ivec4(1)));\\n  vec4 multiplier = sign(a) * isModRound1 + (vec4(1.0) - isModRound1);\\n  vec4 result = multiplier * pow(abs(a), b);\\n\\n  // Ensure that a^0 = 1, including 0^0 = 1 as this correspond to TF and JS\\n  bvec4 isExpZero = equal(b, vec4(0.0));\\n  result.r = isExpZero.r ? 1.0 : result.r;\\n  result.g = isExpZero.g ? 1.0 : result.g;\\n  result.b = isExpZero.b ? 1.0 : result.b;\\n  result.a = isExpZero.a ? 1.0 : result.a;\\n\\n  vec4 isNaN = vec4(lessThan(a, vec4(0.0))) * vec4(lessThan(floor(b), b));\\n  \" + CHECK_NAN_SNIPPET + \"\\n  return result;\\n\",\n    pow = binaryKernelFunc({\n  opSnippet: POW,\n  packedOpSnippet: POW_PACKED\n}),\n    powConfig = {\n  kernelName: Pow,\n  backendName: \"webgl\",\n  kernelFunc: pow\n};\n\nfunction prod(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    axis: s,\n    keepDims: o\n  } = r,\n      i = a.shape.length,\n      l = [],\n      u = parseAxisParam(s, a.shape);\n  var c = u;\n  var p = getAxesPermutation(c, i);\n  var d,\n      h = a;\n\n  if (null != p && (h = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: p\n    }\n  }), c = getInnerMostAxes(c.length, i), l.push(h)), assertAxesAreInnerMostDims(\"prod\", c, i), n.shouldExecuteOnCPU([h])) {\n    var _e1061 = n.texData.get(h.dataId).values,\n        {\n      outVals: _t724,\n      outShape: _r428,\n      outDtype: _a307\n    } = prodImplCPU(h.shape, h.dtype, _e1061, c);\n    d = n.makeTensorInfo(_r428, _a307, _t724);\n  } else {\n    var [_e1062, _t725] = computeOutAndReduceShapes(h.shape, c),\n        _r429 = sizeFromShape(_t725),\n        _s220 = reshape({\n      inputs: {\n        x: h\n      },\n      backend: n,\n      attrs: {\n        shape: [-1, _r429]\n      }\n    }),\n        _o155 = reduce(_s220, sumOutType(a.dtype), \"prod\", n);\n\n    d = reshape({\n      inputs: {\n        x: _o155\n      },\n      backend: n,\n      attrs: {\n        shape: _e1062\n      }\n    }), l.push(_s220), l.push(_o155);\n  }\n\n  if (o) {\n    l.push(d);\n\n    var _e1063 = expandShapeToKeepDim(d.shape, u);\n\n    d = reshape({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        shape: _e1063\n      }\n    });\n  }\n\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), d;\n}\n\nvar prodConfig = {\n  kernelName: Prod,\n  backendName: \"webgl\",\n  kernelFunc: prod\n},\n    range$1 = e => {\n  var {\n    backend: t,\n    attrs: n\n  } = e,\n      {\n    start: r,\n    stop: a,\n    step: s,\n    dtype: o\n  } = n,\n      i = rangeImplCPU(r, a, s, o);\n  return t.makeTensorInfo([i.length], o, i);\n},\n    rangeConfig = {\n  kernelName: Range,\n  backendName: \"webgl\",\n  kernelFunc: range$1\n},\n    RECIPROCAL = \"return 1.0 / x;\",\n    reciprocal = unaryKernelFunc({\n  opSnippet: RECIPROCAL\n}),\n    reciprocalConfig = {\n  kernelName: Reciprocal,\n  backendName: \"webgl\",\n  kernelFunc: reciprocal\n},\n    RELU = CHECK_NAN_SNIPPET$2 + \"\\n  return (x < 0.0) ? 0.0 : x;\\n\",\n    RELU_PACKED = \"\\n  vec4 result = x * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    relu$1 = unaryKernelFunc({\n  opSnippet: RELU,\n  packedOpSnippet: RELU_PACKED\n}),\n    reluConfig = {\n  kernelName: Relu$1,\n  backendName: \"webgl\",\n  kernelFunc: relu$1\n},\n    RELU6 = CHECK_NAN_SNIPPET$2 + \"\\n  return (x < 0.0) ? 0.0 : min(6.0, x);\\n\",\n    RELU6_PACKED = \"\\n  vec4 result = min(x, vec4(6.)) * vec4(greaterThanEqual(x, vec4(0.0)));\\n  bvec4 isNaN = isnan(x);\\n\\n  result.r = isNaN.r ? x.r : result.r;\\n  result.g = isNaN.g ? x.g : result.g;\\n  result.b = isNaN.b ? x.b : result.b;\\n  result.a = isNaN.a ? x.a : result.a;\\n\\n  return result;\\n\",\n    relu6 = unaryKernelFunc({\n  opSnippet: RELU6,\n  packedOpSnippet: RELU6_PACKED\n}),\n    relu6Config = {\n  kernelName: Relu6$1,\n  backendName: \"webgl\",\n  kernelFunc: relu6\n};\n\nclass ResizeBilinearProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"(vec2(yRC) + vec2(0.5)) * effectiveInputOverOutputRatioRC - vec2(0.5)\" : \"vec2(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec2 effectiveInputOverOutputRatioRC = vec2(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec2 inputShapeRC = vec2(\").concat(o, \".0, \").concat(i, \".0);\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        ivec2 yRC = coords.yz;\\n\\n        // Fractional source index.\\n        vec2 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the four integer indices.\\n        ivec2 sourceFloorRC = ivec2(max(sourceFracIndexRC, vec2(0.0)));\\n        ivec2 sourceCeilRC = ivec2(\\n          min(inputShapeRC - 1.0, ceil(sourceFracIndexRC)));\\n\\n        float topLeft = getA(b, sourceFloorRC.x, sourceFloorRC.y, d);\\n        float bottomLeft = getA(b, sourceCeilRC.x, sourceFloorRC.y, d);\\n        float topRight = getA(b, sourceFloorRC.x, sourceCeilRC.y, d);\\n        float bottomRight = getA(b, sourceCeilRC.x, sourceCeilRC.y, d);\\n\\n        vec2 fracRC = sourceFracIndexRC - vec2(sourceFloorRC);\\n\\n        float top = topLeft + (topRight - topLeft) * fracRC.y;\\n        float bottom = bottomLeft + (bottomRight - bottomLeft) * fracRC.y;\\n        float newValue = top + (bottom - top) * fracRC.x;\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nclass ResizeBilinearPackedProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"(vec3(yRC) + vec3(0.5)) * effectiveInputOverOutputRatioRC - vec3(0.5)\" : \"vec3(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec3 effectiveInputOverOutputRatioRC = vec3(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec3 inputShapeRC = vec3(\").concat(o, \".0, \").concat(i, \".0,\\n                                     \").concat(i, \".0);\\n\\n      float getAValue(int b, int r, int c, int d) {\\n        return getChannel(getA(b, r, c, d), vec2(c, d));\\n      }\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        // Calculate values for next column in yRC.z.\\n        ivec3 yRC = coords.yzz + ivec3(0, 0, 1);\\n\\n        // Fractional source index.\\n        vec3 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the four integer indices.\\n        ivec3 sourceFloorRC = ivec3(max(sourceFracIndexRC, vec3(0.0)));\\n        ivec3 sourceCeilRC = ivec3(\\n          min(inputShapeRC - 1.0, ceil(sourceFracIndexRC)));\\n\\n        // Should we calculate next column and row elements in 2x2 packed cell.\\n        bool hasNextCol = d < \").concat(l - 1, \";\\n        bool hasNextRow = coords.z < \").concat(n - 1, \";\\n\\n        // In parallel, construct four corners for all four components in\\n        // packed 2x2 cell.\\n        vec4 topLeft = vec4(\\n          getAValue(b, sourceFloorRC.x, sourceFloorRC.y, d),\\n          hasNextCol ? getAValue(b, sourceFloorRC.x, sourceFloorRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceFloorRC.x, sourceFloorRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceFloorRC.x, sourceFloorRC.z, d + 1) : 0.0);\\n\\n        vec4 bottomLeft = vec4(\\n          getAValue(b, sourceCeilRC.x, sourceFloorRC.y, d),\\n          hasNextCol ? getAValue(b, sourceCeilRC.x, sourceFloorRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceCeilRC.x, sourceFloorRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceCeilRC.x, sourceFloorRC.z, d + 1) : 0.0);\\n\\n        vec4 topRight = vec4(\\n          getAValue(b, sourceFloorRC.x, sourceCeilRC.y, d),\\n          hasNextCol ? getAValue(b, sourceFloorRC.x, sourceCeilRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceFloorRC.x, sourceCeilRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceFloorRC.x, sourceCeilRC.z, d + 1) : 0.0);\\n\\n        vec4 bottomRight = vec4(\\n          getAValue(b, sourceCeilRC.x, sourceCeilRC.y, d),\\n          hasNextCol ? getAValue(b, sourceCeilRC.x, sourceCeilRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceCeilRC.x, sourceCeilRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceCeilRC.x, sourceCeilRC.z, d + 1) : 0.0);\\n\\n        vec3 fracRC = sourceFracIndexRC - vec3(sourceFloorRC);\\n\\n        vec4 top = mix(topLeft, topRight, fracRC.yyzz);\\n        vec4 bottom = mix(bottomLeft, bottomRight, fracRC.yyzz);\\n        vec4 newValue = mix(top, bottom, fracRC.x);\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeBilinear(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r,\n      [l, u] = i,\n      c = env().getBool(\"WEBGL_PACK_IMAGE_OPERATIONS\") ? new ResizeBilinearPackedProgram(a.shape, l, u, s, o) : new ResizeBilinearProgram(a.shape, l, u, s, o);\n  return n.runWebGLProgram(c, [a], \"float32\");\n}\n\nvar resizeBilinearConfig = {\n  kernelName: ResizeBilinear,\n  backendName: \"webgl\",\n  kernelFunc: resizeBilinear\n};\n\nclass ResizeBilinearBackpropProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"dy\"], this.outputShape = [], this.outputShape = t;\n    var [, r, a] = t,\n        [, s, o] = e,\n        i = [n && s > 1 ? r - 1 : r, n && o > 1 ? a - 1 : a],\n        l = [n && s > 1 ? s - 1 : s, n && o > 1 ? o - 1 : o],\n        u = i[0] / l[0],\n        c = i[1] / l[1],\n        p = 1 / u,\n        d = 1 / c,\n        h = 2 * Math.ceil(p) + 2,\n        m = 2 * Math.ceil(d) + 2;\n    this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        int r = coords[1];\\n        int c = coords[2];\\n\\n        float accumulator = 0.0;\\n\\n        const float heightScale = float(\".concat(u, \");\\n        const float widthScale = float(\").concat(c, \");\\n\\n        const float invHeightScale = float(\").concat(p, \");\\n        const float invWidthScale = float(\").concat(d, \");\\n\\n        const int winHeight = int(\").concat(h, \");\\n        const int winWidth = int(\").concat(m, \");\\n\\n        // Compute bounds for where in dy we will look\\n        float startRLerp = floor(float(r) * invHeightScale);\\n        int startDyR = int(startRLerp - float(winHeight / 2));\\n\\n        float startCLerp = floor(float(c) * invWidthScale);\\n        int startDyC = int(startCLerp - float(winWidth / 2));\\n\\n        // Loop over dy\\n        for (int dyROffset = 0; dyROffset < winHeight; dyROffset++) {\\n          int dyR = dyROffset + startDyR;\\n\\n          // Guard against the window exceeding the bounds of dy\\n          if (dyR < 0 || dyR >= \").concat(s, \") {\\n            continue;\\n          }\\n\\n          for (int dyCOffset = 0; dyCOffset < winWidth; dyCOffset++) {\\n            int dyC = dyCOffset + startDyC;\\n\\n            // Guard against the window exceeding the bounds of dy\\n            if (dyC < 0 || dyC >= \").concat(o, \") {\\n              continue;\\n            }\\n\\n            float dxR = float(dyR) * heightScale;\\n            int topDxRIndex = int(floor(dxR));\\n            int bottomDxRIndex = int(min(ceil(dxR), \").concat(r - 1, \".0));\\n            float dxRLerp = dxR - float(topDxRIndex);\\n            float inverseDxRLerp = 1.0 - dxRLerp;\\n\\n            float dxC = float(dyC) * widthScale;\\n            int leftDxCIndex = int(floor(dxC));\\n            int rightDxCIndex = int(min(ceil(dxC), \").concat(a - 1, \".0));\\n            float dxCLerp = dxC - float(leftDxCIndex);\\n            float inverseDxCLerp = 1.0 - dxCLerp;\\n\\n            if (r == topDxRIndex && c == leftDxCIndex) {\\n              // topLeft\\n              accumulator +=\\n                getDy(b, dyR, dyC, d) * inverseDxRLerp * inverseDxCLerp;\\n            }\\n\\n            if (r == topDxRIndex && c == rightDxCIndex) {\\n              // topRight\\n              accumulator += getDy(b, dyR, dyC, d) * inverseDxRLerp * dxCLerp;\\n            }\\n\\n            if (r == bottomDxRIndex && c == leftDxCIndex) {\\n              // bottomLeft\\n              accumulator += getDy(b, dyR, dyC, d) * dxRLerp * inverseDxCLerp;\\n            }\\n\\n            if (r == bottomDxRIndex && c == rightDxCIndex) {\\n              // bottomRight\\n              accumulator += getDy(b, dyR, dyC, d) * dxRLerp * dxCLerp;\\n            }\\n          }\\n        }\\n        // End loop over dy\\n\\n        setOutput(accumulator);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeBilinearGrad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r,\n      i = new ResizeBilinearBackpropProgram(s.shape, a.shape, o);\n  return n.runWebGLProgram(i, [s], s.dtype);\n}\n\nvar resizeBilinearGradConfig = {\n  kernelName: ResizeBilinearGrad,\n  backendName: \"webgl\",\n  kernelFunc: resizeBilinearGrad\n};\n\nclass ResizeNearestNeighborProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"max((vec2(yRC) + vec2(0.5)) * effectiveInputOverOutputRatioRC, vec2(0.0))\" : \"vec2(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec2 effectiveInputOverOutputRatioRC = vec2(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec2 inputShapeRC = vec2(\").concat(o, \".0, \").concat(i, \".0);\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        ivec2 yRC = coords.yz;\\n\\n        // Fractional source index.\\n        vec2 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the coordinators of nearest neighbor point.\\n        ivec2 sourceNearestRC = ivec2(\\n          min(inputShapeRC - 1.0, floor(sourceFracIndexRC + \").concat(r ? \"0.5\" : \"0.0\", \")));\\n        float newValue = getA(b, sourceNearestRC.x, sourceNearestRC.y, d);\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nclass ResizeNearestNeighborPackedProgram {\n  constructor(e, t, n, r, a) {\n    this.variableNames = [\"A\"], this.packedInputs = !0, this.packedOutput = !0, this.outputShape = [];\n    var [s, o, i, l] = e;\n    this.outputShape = [s, t, n, l];\n    var u = [r && t > 1 ? o - 1 : o, r && n > 1 ? i - 1 : i],\n        c = [r && t > 1 ? t - 1 : t, r && n > 1 ? n - 1 : n];\n    var p;\n    p = a ? \"max((vec3(yRC) + vec3(0.5)) * effectiveInputOverOutputRatioRC, vec3(0.0))\" : \"vec3(yRC) * effectiveInputOverOutputRatioRC\", this.userCode = \"\\n      const vec3 effectiveInputOverOutputRatioRC = vec3(\\n          \".concat(u[0] / c[0], \",\\n          \").concat(u[1] / c[1], \",\\n          \").concat(u[1] / c[1], \");\\n      const vec3 inputShapeRC = vec3(\").concat(o, \".0, \").concat(i, \".0,\\n                                     \").concat(i, \".0);\\n\\n      float getAValue(int b, int r, int c, int d) {\\n        return getChannel(getA(b, r, c, d), vec2(c, d));\\n      }\\n\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        // Calculate values for next column in yRC.z.\\n        ivec3 yRC = coords.yzz + ivec3(0, 0, 1);\\n\\n        // Fractional source index.\\n        vec3 sourceFracIndexRC = \").concat(p, \";\\n\\n        // Compute the coordinators of nearest neighbor point.\\n        ivec3 sourceNearestRC = ivec3(\\n          min(inputShapeRC - 1.0, floor(sourceFracIndexRC + \").concat(r ? \"0.5\" : \"0.0\", \")));\\n\\n        // Should we calculate next column and row elements in 2x2 packed cell.\\n        bool hasNextCol = d < \").concat(l - 1, \";\\n        bool hasNextRow = coords.z < \").concat(n - 1, \";\\n\\n        vec4 newValue = vec4(\\n          getAValue(b, sourceNearestRC.x, sourceNearestRC.y, d),\\n          hasNextCol ? getAValue(b, sourceNearestRC.x, sourceNearestRC.y, d + 1)\\n                     : 0.0,\\n          hasNextRow ? getAValue(b, sourceNearestRC.x, sourceNearestRC.z, d)\\n                     : 0.0,\\n          (hasNextRow && hasNextCol) ?\\n            getAValue(b, sourceNearestRC.x, sourceNearestRC.z, d + 1) : 0.0);\\n\\n        setOutput(newValue);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeNearestNeighbor(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a\n  } = t,\n      {\n    alignCorners: s,\n    halfPixelCenters: o,\n    size: i\n  } = r,\n      [l, u] = i,\n      c = env().getBool(\"WEBGL_PACK_IMAGE_OPERATIONS\") ? new ResizeNearestNeighborPackedProgram(a.shape, l, u, s, o) : new ResizeNearestNeighborProgram(a.shape, l, u, s, o);\n  return n.runWebGLProgram(c, [a], a.dtype);\n}\n\nvar resizeNearestNeighborConfig = {\n  kernelName: ResizeNearestNeighbor,\n  backendName: \"webgl\",\n  kernelFunc: resizeNearestNeighbor\n};\n\nclass ResizeNearestNeigborBackpropProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"dy\"], this.outputShape = [], this.outputShape = t;\n    var [, r, a] = t,\n        [, s, o] = e,\n        i = [n && s > 1 ? r - 1 : r, n && o > 1 ? a - 1 : a],\n        l = [n && s > 1 ? s - 1 : s, n && o > 1 ? o - 1 : o],\n        u = i[0] / l[0],\n        c = i[1] / l[1],\n        p = 1 / u,\n        d = 1 / c,\n        h = 2 * Math.ceil(p) + 2,\n        m = 2 * Math.ceil(d) + 2;\n    this.userCode = \"\\n      void main() {\\n        ivec4 coords = getOutputCoords();\\n        int b = coords[0];\\n        int d = coords[3];\\n        int r = coords[1];\\n        int c = coords[2];\\n\\n        float accumulator = 0.0;\\n\\n        const float heightScale = float(\".concat(u, \");\\n        const float widthScale = float(\").concat(c, \");\\n\\n        const float invHeightScale = float(\").concat(p, \");\\n        const float invWidthScale = float(\").concat(d, \");\\n\\n        const int winHeight = int(\").concat(h, \");\\n        const int winWidth = int(\").concat(m, \");\\n\\n        // Compute bounds for where in dy we will look\\n        float startRLerp = floor(float(r) * invHeightScale);\\n        int startDyR = int(floor(startRLerp - float(winHeight / 2)));\\n\\n        float startCLerp = floor(float(c) * invWidthScale);\\n        int startDyC = int(floor(startCLerp - float(winWidth / 2)));\\n\\n        // Loop over dy\\n        for (int dyROffset = 0; dyROffset < winHeight; dyROffset++) {\\n          int dyR = dyROffset + startDyR;\\n\\n          // Guard against the window exceeding the bounds of dy\\n          if (dyR < 0 || dyR >= \").concat(s, \") {\\n            continue;\\n          }\\n\\n          for (int dyCOffset = 0; dyCOffset < winWidth; dyCOffset++) {\\n            int dyC = dyCOffset + startDyC;\\n\\n            // Guard against the window exceeding the bounds of dy\\n            if (dyC < 0 || dyC >= \").concat(o, \") {\\n              continue;\\n            }\\n\\n            float sourceFracRow =\\n              float(\").concat(i[0], \") *\\n                (float(dyR) / float(\").concat(l[0], \"));\\n\\n            float sourceFracCol =\\n                float(\").concat(i[1], \") *\\n                  (float(dyC) / float(\").concat(l[1], \"));\\n\\n            int sourceNearestRow = int(min(\\n                float(int(\").concat(r, \") - 1),\\n                \").concat(n, \" ? float(round(sourceFracRow)) :\\n                                  float(floor(sourceFracRow))));\\n\\n            int sourceNearestCol = int(min(\\n                float(int(\").concat(a, \") - 1),\\n                \").concat(n, \" ? float(round(sourceFracCol)) :\\n                                  float(floor(sourceFracCol))));\\n\\n            if (r == sourceNearestRow && c == sourceNearestCol) {\\n              accumulator += getDy(b, dyR, dyC, d);\\n            }\\n          }\\n        }\\n        // End loop over dy\\n\\n        setOutput(accumulator);\\n      }\\n    \");\n  }\n\n}\n\nfunction resizeNearestNeighborGrad(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    images: a,\n    dy: s\n  } = t,\n      {\n    alignCorners: o\n  } = r,\n      i = new ResizeNearestNeigborBackpropProgram(s.shape, a.shape, o);\n  return n.runWebGLProgram(i, [s], s.dtype);\n}\n\nvar resizeNearestNeighborGradConfig = {\n  kernelName: ResizeNearestNeighborGrad,\n  backendName: \"webgl\",\n  kernelFunc: resizeNearestNeighborGrad\n};\n\nclass ReverseProgram {\n  constructor(e, t) {\n    this.variableNames = [\"x\"];\n    var n = e.length;\n    if (n > 4) throw new Error(\"WebGL backend: Reverse of rank-\".concat(n, \" tensor is not yet supported\"));\n    if (this.outputShape = e, 1 === n) return void (this.userCode = \"\\n        void main() {\\n          int coord = getOutputCoords();\\n          setOutput(getX(\".concat(e[0], \" - coord - 1));\\n        }\\n      \"));\n    var r = e.map((n, r) => (n => -1 !== t.indexOf(n) && 1 !== e[n] ? \"\".concat(e[n], \" - coords[\").concat(n, \"] - 1\") : \"coords[\".concat(n, \"]\"))(r)).join(\",\"),\n        a = getCoordsDataType(n);\n    this.userCode = \"\\n      void main() {\\n        \".concat(a, \" coords = getOutputCoords();\\n        setOutput(getX(\").concat(r, \"));\\n      }\\n    \");\n  }\n\n}\n\nclass ReversePackedProgram {\n  constructor(e, t) {\n    this.variableNames = [\"x\"], this.packedInputs = !0, this.packedOutput = !0;\n    var n = e.length;\n    if (n > 4) throw new Error(\"WebGL backend: Reverse of rank-\".concat(n, \" tensor is not yet supported\"));\n    this.outputShape = e;\n    var r = getChannels(\"rc\", n),\n        a = \"\".concat(r[n - 1], \" + 1 < \").concat(this.outputShape[n - 1]),\n        s = \"\".concat(r[n - 2], \" + 1 < \").concat(this.outputShape[n - 2]),\n        o = getCoordsDataType(n);\n\n    function i(n) {\n      var r = e.map((r, a) => function (n, r) {\n        return -1 !== t.indexOf(n) && 1 !== e[n] ? \"\".concat(e[n], \" - \").concat(r[n], \" - 1\") : \"\".concat(r[n]);\n      }(a, n));\n      return \"getChannel(getX(\".concat(r.join(\",\"), \"), vec2(\").concat(r.slice(-2).join(\",\"), \"))\");\n    }\n\n    this.userCode = 1 === n ? \"\\n        void main(){\\n          int rc = getOutputCoords();\\n          vec4 result = vec4(0.);\\n          result.r = getChannel(getX(\".concat(e[0], \" - rc - 1),\\n            \").concat(e[0], \" - rc - 1);\\n          if(\").concat(a, \"){\\n              result.g = getChannel(getX(\").concat(e[0], \" - (rc  + 1) - 1),\\n                \").concat(e[0], \" - (rc  + 1) - 1);\\n          }\\n          setOutput(result);\\n        }\\n      \") : \"\\n        void main() {\\n          \".concat(o, \" rc = getOutputCoords();\\n          vec4 result = vec4(0.);\\n          result.r = \").concat(function (e) {\n      return i(e);\n    }(r.slice()), \";\\n          if(\").concat(a, \"){\\n            result.g = \").concat(function (e) {\n      return e[n - 1] = \"(\" + e[n - 1] + \" + 1)\", i(e);\n    }(r.slice()), \";\\n          }\\n          if(\").concat(s, \") {\\n            result.b = \").concat(function (e) {\n      return e[n - 2] = \"(\" + e[n - 2] + \" + 1)\", i(e);\n    }(r.slice()), \";\\n            if(\").concat(a, \") {\\n              result.a = \").concat(function (e) {\n      return e[n - 1] = \"(\" + e[n - 1] + \" + 1)\", e[n - 2] = \"(\" + e[n - 2] + \" + 1)\", i(e);\n    }(r.slice()), \";\\n            }\\n          }\\n          setOutput(result);\\n        }\\n    \");\n  }\n\n}\n\nfunction reverse(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    dims: s\n  } = r,\n      o = a.shape.length,\n      i = parseAxisParam(s, a.shape);\n  if (0 === o) return identity({\n    inputs: {\n      x: a\n    },\n    backend: n\n  });\n  var l = env().getBool(\"WEBGL_PACK_ARRAY_OPERATIONS\") ? new ReversePackedProgram(a.shape, i) : new ReverseProgram(a.shape, i);\n  return n.runWebGLProgram(l, [a], a.dtype);\n}\n\nvar reverseConfig = {\n  kernelName: Reverse,\n  backendName: \"webgl\",\n  kernelFunc: reverse\n};\n\nclass RotateProgram {\n  constructor(e, t) {\n    this.variableNames = [\"Image\"], this.outputShape = [], this.customUniforms = [{\n      name: \"params\",\n      type: \"vec4\"\n    }];\n    var n = e[1],\n        r = e[2];\n    this.outputShape = e;\n    var a = \"\";\n    a = \"number\" == typeof t ? \"float outputValue = \".concat(t.toFixed(2), \";\") : \"\\n        vec3 fill = vec3(\".concat(t.join(\",\"), \");\\n        float outputValue = fill[coords[3]];\"), this.userCode = \"\\n        void main() {\\n          ivec4 coords = getOutputCoords();\\n          int x = coords[2];\\n          int y = coords[1];\\n          float coordXFloat = (float(x) - params[0]) * params[3] -\\n            (float(y) - params[1]) * params[2];\\n          float coordYFloat = (float(x) - params[0]) * params[2] +\\n            (float(y) - params[1]) * params[3];\\n          int coordX = int(round(coordXFloat + params[0]));\\n          int coordY = int(round(coordYFloat + params[1]));\\n          \".concat(a, \"\\n          if(coordX >= 0 && coordX < \").concat(r, \" && coordY >= 0 && coordY < \").concat(n, \") {\\n            outputValue = getImage(coords[0], coordY, coordX, coords[3]);\\n          }\\n          setOutput(outputValue);\\n        }\\n    \");\n  }\n\n}\n\nvar rotateWithOffsetConfig = {\n  kernelName: RotateWithOffset,\n  backendName: \"webgl\",\n  kernelFunc: _ref73 => {\n    var {\n      inputs: e,\n      attrs: t,\n      backend: n\n    } = _ref73;\n    var {\n      image: r\n    } = e,\n        {\n      radians: a,\n      fillValue: s,\n      center: o\n    } = t,\n        i = n,\n        l = new RotateProgram(r.shape, s),\n        [u, c] = getImageCenter(o, r.shape[1], r.shape[2]),\n        p = [[u, c, Math.sin(a), Math.cos(a)]];\n    return i.runWebGLProgram(l, [r], r.dtype, p);\n  }\n},\n    ROUND = \"\\n  // OpenGL ES does not support round function.\\n  // The algorithm is based on banker's rounding.\\n  float base = floor(x);\\n  if ((x - base) < 0.5) {\\n    return floor(x);\\n  } else if ((x - base) > 0.5) {\\n    return ceil(x);\\n  } else {\\n    if (mod(base, 2.0) == 0.0) {\\n      return base;\\n    } else {\\n      return base + 1.0;\\n    }\\n  }\\n\",\n    round = unaryKernelFunc({\n  opSnippet: ROUND\n}),\n    roundConfig = {\n  kernelName: Round,\n  backendName: \"webgl\",\n  kernelFunc: round\n},\n    RSQRT = \"return inversesqrt(x);\",\n    rsqrt = unaryKernelFunc({\n  opSnippet: RSQRT,\n  cpuKernelImpl: rsqrtImplCPU\n}),\n    rsqrtConfig = {\n  kernelName: Rsqrt,\n  backendName: \"webgl\",\n  kernelFunc: rsqrt\n};\n\nclass ScatterProgram {\n  constructor(e, t, n, r, a, s) {\n    var o = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : !0;\n    this.variableNames = [\"updates\", \"indices\", \"defaultValue\"], this.outputShape = s;\n    var i = getCoordsDataType(a.length),\n        l = getCoordsDataType(s.length);\n    var u = \"\";\n    1 === n ? u = \"i\" : 2 === n && (u = \"i, j\");\n    var c = \"\";\n    1 === r ? c = \"i\" : 2 === r && (c = \"i, coords[1]\"), this.userCode = \"\\n        \".concat(i, \" strides = \").concat(i, \"(\").concat(a, \");\\n\\n        void main() {\\n          \").concat(l, \" coords = getOutputCoords();\\n          float sum = 0.0;\\n          bool found = false;\\n          for (int i = 0; i < \").concat(e, \"; i++) {\\n            int flattenedIndex = 0;\\n            for (int j = 0; j < \").concat(t, \"; j++) {\\n              int index = round(getIndices(\").concat(u, \"));\\n              flattenedIndex += index * \").concat(t > 1 ? \"strides[j]\" : \"strides\", \";\\n            }\\n            if (flattenedIndex == coords[0]) {\\n              sum += getUpdates(\").concat(c, \");\\n              found = true;\\n            }\\n          }\\n          setOutput(mix(getDefaultValue(), sum, float(found)));\\n        }\\n      \");\n  }\n\n}\n\nfunction scatterNd(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    indices: a,\n    updates: s\n  } = t,\n      {\n    shape: o\n  } = r,\n      {\n    sliceRank: i,\n    numUpdates: l,\n    sliceSize: u,\n    strides: c,\n    outputSize: p\n  } = calculateShapes(s, a, o),\n      d = [p / u, u];\n  if (0 === p) return n.makeTensorInfo(o, a.dtype);\n  var h = reshape({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: [l, i]\n    }\n  }),\n      m = reshape({\n    inputs: {\n      x: s\n    },\n    backend: n,\n    attrs: {\n      shape: [l, u]\n    }\n  }),\n      f = n.makeTensorInfo([], \"float32\", new Float32Array([0])),\n      g = new ScatterProgram(l, i, h.shape.length, m.shape.length, c, d),\n      $ = n.runWebGLProgram(g, [m, h, f], m.dtype),\n      y = reshape({\n    inputs: {\n      x: $\n    },\n    backend: n,\n    attrs: {\n      shape: o\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), n.disposeIntermediateTensorInfo(m), n.disposeIntermediateTensorInfo($), n.disposeIntermediateTensorInfo(f), y;\n}\n\nvar scatterNdConfig = {\n  kernelName: ScatterNd,\n  backendName: \"webgl\",\n  kernelFunc: scatterNd\n};\n\nclass SelectProgram {\n  constructor(e, t, n) {\n    var r, a;\n    if (this.variableNames = [\"c\", \"a\", \"b\"], this.outputShape = t, n > 4) throw Error(\"Where for rank \".concat(n, \" is not yet supported\"));\n    if (1 === n) a = \"resRC\", r = \"resRC\";else {\n      var _n438 = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\"],\n          _s221 = [],\n          o = [];\n\n      for (var _r430 = 0; _r430 < t.length; _r430++) {\n        o.push(\"\".concat(_n438[_r430])), _r430 < e && _s221.push(\"\".concat(_n438[_r430]));\n      }\n\n      r = _s221.join(), a = o.join();\n    }\n    var s = getCoordsDataType(n);\n    this.userCode = \"\\n      void main() {\\n        \".concat(s, \" resRC = getOutputCoords();\\n        float cVal = getC(\").concat(r, \");\\n        if (cVal >= 1.0) {\\n          setOutput(getA(\").concat(a, \"));\\n        } else {\\n          setOutput(getB(\").concat(a, \"));\\n        }\\n      }\\n    \");\n  }\n\n}\n\nfunction select(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    condition: r,\n    t: a,\n    e: s\n  } = t,\n      o = new SelectProgram(r.shape.length, a.shape, a.shape.length);\n  return n.runWebGLProgram(o, [r, a, s], upcastType(a.dtype, s.dtype));\n}\n\nvar selectConfig = {\n  kernelName: Select,\n  backendName: \"webgl\",\n  kernelFunc: select\n},\n    SELU = \"\\n  // Stable and Attracting Fixed Point (0, 1) for Normalized Weights.\\n  // see: https://arxiv.org/abs/1706.02515\\n  float scaleAlpha = \".concat(SELU_SCALEALPHA, \";\\n  float scale = \").concat(SELU_SCALE, \";\\n  return (x >= 0.0) ? scale * x : scaleAlpha * (exp(x) - 1.0);\\n\"),\n    selu = unaryKernelFunc({\n  opSnippet: SELU\n}),\n    seluConfig = {\n  kernelName: Selu$1,\n  backendName: \"webgl\",\n  kernelFunc: selu\n},\n    SIGMOID = \"return 1.0 / (1.0 + exp(-1.0 * x));\",\n    sigmoid = unaryKernelFunc({\n  opSnippet: SIGMOID\n}),\n    sigmoidConfig = {\n  kernelName: Sigmoid$1,\n  backendName: \"webgl\",\n  kernelFunc: sigmoid\n},\n    SIGN = \"\\n  if (isnan(x)) { return 0.0; }\\n  return sign(x);\\n\",\n    sign = unaryKernelFunc({\n  opSnippet: SIGN\n}),\n    signConfig = {\n  kernelName: Sign,\n  backendName: \"webgl\",\n  kernelFunc: sign\n},\n    SIN = CHECK_NAN_SNIPPET_UNARY + \"\\n  return sin(x);\\n\",\n    sin = unaryKernelFunc({\n  opSnippet: SIN\n}),\n    sinConfig = {\n  kernelName: Sin,\n  backendName: \"webgl\",\n  kernelFunc: sin\n},\n    SINH = \"\\n  float e2x = exp(x);\\n  return (e2x - 1.0 / e2x) / 2.0;\\n\",\n    sinh = unaryKernelFunc({\n  opSnippet: SINH\n}),\n    sinhConfig = {\n  kernelName: Sinh,\n  backendName: \"webgl\",\n  kernelFunc: sinh\n},\n    SOFTPLUS = \"\\n  float epsilon = 1.1920928955078125e-7;\\n  float threshold = log(epsilon) + 2.0;\\n\\n  bool too_large = x > -threshold;\\n  bool too_small = x < threshold;\\n\\n  float result;\\n  float exp_x = exp(x);\\n\\n  if (too_large){\\n    result = x;\\n  }\\n  else if (too_small){\\n    result = exp_x;\\n  }\\n  else{\\n    result = log(exp_x + 1.0);\\n  }\\n  return result;\\n\",\n    softplus = unaryKernelFunc({\n  opSnippet: SOFTPLUS\n}),\n    softplusConfig = {\n  kernelName: Softplus$1,\n  backendName: \"webgl\",\n  kernelFunc: softplus\n},\n    spaceToBatchND = e => {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    blockShape: s,\n    paddings: o\n  } = r;\n  assert$4(a.shape.length <= 4, () => \"spaceToBatchND for rank > 4 with a WebGL backend not implemented yet\");\n  var i = s.reduce((e, t) => e * t),\n      l = [[0, 0]];\n  l.push(...o);\n\n  for (var _e1064 = 1 + s.length; _e1064 < a.shape.length; ++_e1064) {\n    l.push([0, 0]);\n  }\n\n  var u = [],\n      c = padV2({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      paddings: l,\n      constantValue: 0\n    }\n  }),\n      p = getReshaped(c.shape, s, i, !1),\n      d = getPermuted(p.length, s.length, !1),\n      h = getReshapedPermuted(c.shape, s, i, !1),\n      m = reshape({\n    inputs: {\n      x: c\n    },\n    backend: n,\n    attrs: {\n      shape: p\n    }\n  }),\n      f = transpose({\n    inputs: {\n      x: m\n    },\n    backend: n,\n    attrs: {\n      perm: d\n    }\n  }),\n      g = reshape({\n    inputs: {\n      x: f\n    },\n    backend: n,\n    attrs: {\n      shape: h\n    }\n  });\n  return u.push(c), u.push(m), u.push(f), u.forEach(e => n.disposeIntermediateTensorInfo(e)), g;\n},\n    spaceToBatchNDConfig = {\n  kernelName: SpaceToBatchND,\n  backendName: \"webgl\",\n  kernelFunc: spaceToBatchND\n};\n\nfunction sparseFillEmptyRows(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    indices: r,\n    values: a,\n    denseShape: s,\n    defaultValue: o\n  } = t;\n  if (1 !== s.shape.length) throw new Error(\"Dense shape must be a vector, saw:\\n         \".concat(s.shape));\n  if (2 !== r.shape.length) throw new Error(\"Indices must be a matrix, saw:\\n         \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Values must be a vector, saw:\\n         \".concat(a.shape));\n  if (0 !== o.shape.length) throw new Error(\"Default value must be a scalar, saw:\\n        \".concat(o.shape));\n  var i = n.readSync(r.dataId),\n      l = n.readSync(a.dataId),\n      u = n.readSync(s.dataId),\n      c = n.readSync(o.dataId)[0],\n      [p, d, h, m, f] = sparseFillEmptyRowsImplCPU(i, r.shape, r.dtype, l, a.dtype, u, c);\n  return [n.makeTensorInfo(d, r.dtype, p), n.makeTensorInfo([d[0]], a.dtype, h), n.makeTensorInfo([m.length], \"bool\", new Uint8Array(m.map(e => Number(e)))), n.makeTensorInfo([f.length], r.dtype, new Int32Array(f))];\n}\n\nvar sparseFillEmptyRowsConfig = {\n  kernelName: SparseFillEmptyRows,\n  backendName: \"webgl\",\n  kernelFunc: sparseFillEmptyRows\n};\n\nfunction sparseReshape(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    inputIndices: r,\n    inputShape: a,\n    newShape: s\n  } = t;\n  if (2 !== r.shape.length) throw new Error(\"Input indices should be a matrix but received shape \".concat(r.shape));\n  if (1 !== a.shape.length) throw new Error(\"Input shape should be a vector but received shape \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Target shape should be a vector but received shape \".concat(s.shape));\n  var o = Array.from(n.readSync(a.dataId)),\n      i = n.readSync(r.dataId),\n      l = Array.from(n.readSync(s.dataId)),\n      [u, c, p] = sparseReshapeImplCPU(i, r.shape, r.dtype, o, l);\n  return [n.makeTensorInfo(c, r.dtype, u), n.makeTensorInfo([p.length], s.dtype, new Int32Array(p))];\n}\n\nvar sparseReshapeConfig = {\n  kernelName: SparseReshape,\n  backendName: \"webgl\",\n  kernelFunc: sparseReshape\n};\n\nfunction sparseSegmentMean(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n              \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n              \".concat(s.shape));\n  var o = n.readSync(r.dataId),\n      i = n.readSync(a.dataId),\n      l = n.readSync(s.dataId),\n      [u, c] = sparseSegmentReductionImplCPU(o, r.shape, r.dtype, i, l, !0);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentMeanConfig = {\n  kernelName: SparseSegmentMean,\n  backendName: \"webgl\",\n  kernelFunc: sparseSegmentMean\n};\n\nfunction sparseSegmentSum(e) {\n  var {\n    inputs: t,\n    backend: n\n  } = e,\n      {\n    data: r,\n    indices: a,\n    segmentIds: s\n  } = t;\n  if (r.shape.length < 1) throw new Error(\"Data should be at least 1 dimensional but received scalar\");\n  if (1 !== a.shape.length) throw new Error(\"Indices should be a vector but received shape\\n             \".concat(a.shape));\n  if (1 !== s.shape.length) throw new Error(\"Segment ids should be a vector but received shape\\n             \".concat(s.shape));\n  var o = n.readSync(r.dataId),\n      i = n.readSync(a.dataId),\n      l = n.readSync(s.dataId),\n      [u, c] = sparseSegmentReductionImplCPU(o, r.shape, r.dtype, i, l);\n  return n.makeTensorInfo(c, r.dtype, u);\n}\n\nvar sparseSegmentSumConfig = {\n  kernelName: SparseSegmentSum,\n  backendName: \"webgl\",\n  kernelFunc: sparseSegmentSum\n};\n\nfunction sparseToDense(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    sparseIndices: a,\n    sparseValues: s,\n    defaultValue: o\n  } = t,\n      {\n    outputShape: i\n  } = r,\n      {\n    sliceRank: l,\n    numUpdates: u,\n    strides: c,\n    outputSize: p\n  } = calculateShapes(s, a, i),\n      d = new ScatterProgram(u, l, a.shape.length, s.shape.length, c, [p, 1], !1),\n      h = n.runWebGLProgram(d, [s, a, o], s.dtype),\n      m = reshape({\n    inputs: {\n      x: h\n    },\n    backend: n,\n    attrs: {\n      shape: i\n    }\n  });\n  return n.disposeIntermediateTensorInfo(h), m;\n}\n\nvar sparseToDenseConfig = {\n  kernelName: SparseToDense,\n  backendName: \"webgl\",\n  kernelFunc: sparseToDense\n};\n\nfunction splitV(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    numOrSizeSplits: s,\n    axis: o\n  } = r,\n      i = parseAxisParam(o, a.shape)[0],\n      l = prepareSplitSize(a, s, i),\n      u = new Array(a.shape.length).fill(0),\n      c = a.shape.slice();\n  return l.map(e => {\n    var t = [...c];\n    t[i] = e;\n    var r = slice({\n      inputs: {\n        x: a\n      },\n      backend: n,\n      attrs: {\n        begin: u,\n        size: t\n      }\n    });\n    return u[i] += e, r;\n  });\n}\n\nvar splitVConfig = {\n  kernelName: SplitV,\n  backendName: \"webgl\",\n  kernelFunc: splitV\n},\n    SQRT = \"return sqrt(x);\",\n    sqrt = unaryKernelFunc({\n  opSnippet: SQRT\n}),\n    sqrtConfig = {\n  kernelName: Sqrt,\n  backendName: \"webgl\",\n  kernelFunc: sqrt\n},\n    SQUARE = \"return x * x;\",\n    square = unaryKernelFunc({\n  opSnippet: SQUARE\n}),\n    squareConfig = {\n  kernelName: Square,\n  backendName: \"webgl\",\n  kernelFunc: square\n},\n    SQUARED_DIFFERENCE = \"return (a - b) * (a - b);\",\n    squaredDifference = binaryKernelFunc({\n  opSnippet: SQUARED_DIFFERENCE,\n  packedOpSnippet: SQUARED_DIFFERENCE\n}),\n    squaredDifferenceConfig = {\n  kernelName: SquaredDifference,\n  backendName: \"webgl\",\n  kernelFunc: squaredDifference\n};\n\nfunction step(_ref74) {\n  var {\n    inputs: e,\n    attrs: t,\n    backend: n\n  } = _ref74;\n  var {\n    x: r\n  } = e,\n      a = new UnaryOpProgram(r.shape, CHECK_NAN_SNIPPET$2 + \"\\n    return x > 0.0 ? 1.0 : float(\".concat(t.alpha, \");\\n  \"));\n  return n.runWebGLProgram(a, [r], r.dtype);\n}\n\nvar stepConfig = {\n  kernelName: Step,\n  backendName: \"webgl\",\n  kernelFunc: step\n};\n\nclass StridedSliceProgram {\n  constructor(e, t, n) {\n    this.variableNames = [\"x\"], this.outputShape = n;\n    var r = n.length,\n        a = getCoordsDataType(n.length),\n        s = getCoordsDataType(n.length);\n    var o = \"\";\n    if (1 === r) o = \"coords * strides + begin\";else {\n      var _e1065 = 0;\n      o = n.map((t, r) => (_e1065++, 1 === n.length ? \"coords * strides[\".concat(r, \"] + begin[\").concat(r, \"]\") : \"coords[\".concat(_e1065 - 1, \"] * strides[\").concat(r, \"] + begin[\").concat(r, \"]\"))).join(\",\");\n    }\n    this.userCode = \"\\n      \".concat(a, \" begin = \").concat(a, \"(\").concat(e, \");\\n      \").concat(a, \" strides = \").concat(a, \"(\").concat(t, \");\\n\\n      void main() {\\n        \").concat(s, \" coords = getOutputCoords();\\n        setOutput(getX(\").concat(o, \"));\\n      }\\n    \");\n  }\n\n}\n\nfunction stridedSlice(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    begin: s,\n    end: o,\n    strides: i,\n    beginMask: l,\n    endMask: u,\n    ellipsisMask: c,\n    newAxisMask: p,\n    shrinkAxisMask: d\n  } = r,\n      {\n    nonStrided: h,\n    $begin: m,\n    $strides: f,\n    size: g,\n    newShape: $,\n    outShape: y\n  } = sliceInfo(a.shape, s, o, i, l, u, c, p, d),\n      b = reshape({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      shape: $\n    }\n  });\n  var x;\n\n  if (h) {\n    var _e1066 = slice({\n      inputs: {\n        x: b\n      },\n      backend: n,\n      attrs: {\n        begin: m,\n        size: g\n      }\n    });\n\n    x = reshape({\n      inputs: {\n        x: _e1066\n      },\n      backend: n,\n      attrs: {\n        shape: y\n      }\n    }), n.disposeIntermediateTensorInfo(_e1066);\n  } else if (y.some(e => 0 === e)) x = n.makeTensorInfo(y, a.dtype, []);else if (n.shouldExecuteOnCPU([b])) {\n    var _e1067 = n.texData.get(b.dataId),\n        _t726 = buffer(b.shape, b.dtype, _e1067.values),\n        _r431 = stridedSliceImplCPU(y, _t726, f, m);\n\n    x = n.makeTensorInfo(y, b.dtype, _r431.values);\n  } else {\n    var _e1068 = new StridedSliceProgram(m, f, y);\n\n    x = n.runWebGLProgram(_e1068, [b], b.dtype);\n  }\n\n  var v = reshape({\n    inputs: {\n      x\n    },\n    backend: n,\n    attrs: {\n      shape: y\n    }\n  });\n  return n.disposeIntermediateTensorInfo(b), n.disposeIntermediateTensorInfo(x), v;\n}\n\nvar stridedSliceConfig = {\n  kernelName: StridedSlice,\n  backendName: \"webgl\",\n  kernelFunc: stridedSlice\n};\n\nfunction stringNGrams(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    separator: a,\n    nGramWidths: s,\n    leftPad: o,\n    rightPad: i,\n    padWidth: l,\n    preserveShortSequences: u\n  } = r,\n      {\n    data: c,\n    dataSplits: p\n  } = t,\n      d = n.readSync(c.dataId),\n      h = n.readSync(p.dataId),\n      [m, f] = stringNGramsImplCPU(d, h, a, s, o, i, l, u);\n  return [n.makeTensorInfo([m.length], \"string\", m), n.makeTensorInfo(p.shape, \"int32\", f)];\n}\n\nvar stringNGramsConfig = {\n  kernelName: StringNGrams,\n  backendName: \"webgl\",\n  kernelFunc: stringNGrams\n};\n\nfunction stringSplit(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    skipEmpty: a\n  } = r,\n      {\n    input: s,\n    delimiter: o\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (1 !== s.shape.length) throw new Error(\"Input must be a vector, got shape: \".concat(s.shape));\n  if (0 !== o.shape.length) throw new Error(\"Delimiter must be a scalar, got shape: \".concat(o.shape));\n  var i = n.readSync(s.dataId),\n      l = n.readSync(o.dataId)[0],\n      [u, c, p] = stringSplitImplCPU(i, l, a),\n      d = c.length;\n  return [n.makeTensorInfo([d, 2], \"int32\", u), n.makeTensorInfo([d], \"string\", c), n.makeTensorInfo([2], \"int32\", new Int32Array(p))];\n}\n\nvar stringSplitConfig = {\n  kernelName: StringSplit,\n  backendName: \"webgl\",\n  kernelFunc: stringSplit\n};\n\nfunction stringToHashBucketFast(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    numBuckets: a\n  } = r,\n      {\n    input: s\n  } = t;\n  if (\"string\" !== s.dtype) throw new Error(\"Input must be of datatype string\");\n  if (a <= 0) throw new Error(\"Number of buckets must be at least 1\");\n  var o = n.readSync(s.dataId),\n      i = stringToHashBucketFastImplCPU(o, a);\n  return n.makeTensorInfo(s.shape, \"int32\", i);\n}\n\nvar stringToHashBucketFastConfig = {\n  kernelName: StringToHashBucketFast,\n  backendName: \"webgl\",\n  kernelFunc: stringToHashBucketFast\n},\n    TAN = \"return tan(x);\",\n    tan = unaryKernelFunc({\n  opSnippet: TAN\n}),\n    tanConfig = {\n  kernelName: Tan,\n  backendName: \"webgl\",\n  kernelFunc: tan\n},\n    TANH = \"\\n  float e2x = exp(-2.0 * abs(x));\\n  return sign(x) * (1.0 - e2x) / (1.0 + e2x);\\n\",\n    tanh = unaryKernelFunc({\n  opSnippet: TANH\n}),\n    tanhConfig = {\n  kernelName: Tanh$1,\n  backendName: \"webgl\",\n  kernelFunc: tanh\n};\n\nclass TileProgram {\n  constructor(e, t) {\n    this.variableNames = [\"A\"];\n    var n = new Array(e.length);\n\n    for (var _r432 = 0; _r432 < n.length; _r432++) {\n      n[_r432] = e[_r432] * t[_r432];\n    }\n\n    this.outputShape = n, this.rank = n.length;\n    var r = getCoordsDataType(this.rank),\n        a = getSourceCoords(e);\n    this.userCode = \"\\n      void main() {\\n        \".concat(r, \" resRC = getOutputCoords();\\n        setOutput(getA(\").concat(a, \"));\\n      }\\n    \");\n  }\n\n}\n\nfunction getSourceCoords(e) {\n  var t = e.length;\n  if (t > 5) throw Error(\"Tile for rank \".concat(t, \" is not yet supported\"));\n  if (1 === t) return \"imod(resRC, \".concat(e[0], \")\");\n  var n = [\"resRC.x\", \"resRC.y\", \"resRC.z\", \"resRC.w\", \"resRC.u\"],\n      r = [];\n\n  for (var _t727 = 0; _t727 < e.length; _t727++) {\n    r.push(\"imod(\".concat(n[_t727], \", \").concat(e[_t727], \")\"));\n  }\n\n  return r.join();\n}\n\nfunction tile(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    reps: s\n  } = r;\n\n  if (\"string\" === a.dtype || a.shape.length > 5) {\n    var _e1069 = n.readSync(a.dataId),\n        _t728 = \"string\" === a.dtype ? _e1069.map(e => decodeString(e)) : _e1069,\n        _r433 = buffer(a.shape, a.dtype, _t728),\n        _o156 = tileImplCPU(_r433, s);\n\n    return n.makeTensorInfo(_o156.shape, _o156.dtype, _o156.values);\n  }\n\n  var o = new TileProgram(a.shape, s);\n  return n.runWebGLProgram(o, [a], a.dtype);\n}\n\nvar tileConfig = {\n  kernelName: Tile,\n  backendName: \"webgl\",\n  kernelFunc: tile\n};\n\nclass SwapProgram {\n  constructor(e) {\n    this.variableNames = [\"x\", \"indices\"], this.customUniforms = [{\n      name: \"n\",\n      type: \"int\"\n    }, {\n      name: \"firstPass\",\n      type: \"int\"\n    }, {\n      name: \"negativeInf\",\n      type: \"float\"\n    }, {\n      name: \"dir\",\n      type: \"int\"\n    }, {\n      name: \"inc\",\n      type: \"int\"\n    }], this.outputShape = e, this.userCode = \"\\n       void main() {\\n         ivec2 coords = getOutputCoords();\\n         int batch = coords[0];\\n         int elemIdx = coords[1];\\n\\n         // We compare elements pair-wise within a group of size 2 * inc.\\n         // The comparing rule for each group alternates between ascending\\n         // and descending. Within each group, we compare each pair at\\n         // positions i and i+inc. To decide whether an element at position i\\n         // is x0 or x1, we mod it by 2 * inc, if the result is smaller than\\n         // inc, it is in the first half of the group, we denote it as x0,\\n         // otherwise we denote it as x1.\\n         // For example, as shown in the Bitonic top K paper referenced above,\\n         // Figure5(a) shows that element[1] is in the\\n         // second half of the group when group size is 2, but it is in the\\n         // first half of the group when group size is 4.\\n\\n         bool isFirstInPair = imod(elemIdx, 2 * inc) < inc;\\n         int i = isFirstInPair ? elemIdx : elemIdx - inc;\\n\\n         int i0 = firstPass == 1 ? i : int(getIndices(batch, i));\\n         int i1 = firstPass == 1 ? i + inc : int(getIndices(batch, i + inc));\\n         float x0 = i0 < n ? getX(batch, i0) : negativeInf;\\n         float x1 = i1 < n ? getX(batch, i1) : negativeInf;\\n\\n         // Denotes which direction indices are in (ascending or descending).\\n         bool reverse = imod(elemIdx, 2 * dir) >= dir;\\n         bool isGreater = x0 > x1 || (x0 == x1 && i1 > i0);\\n         if (reverse == isGreater) { // Elements in opposite order of direction\\n           int iTemp = i0;\\n           i0 = i1;\\n           i1 = iTemp;\\n         }\\n         if (isFirstInPair) {\\n            setOutput(float(i0));\\n         } else {\\n            setOutput(float(i1));\\n         }\\n       }\\n     \";\n  }\n\n}\n\nclass MergeProgram {\n  constructor(e) {\n    this.variableNames = [\"x\", \"indices\"], this.customUniforms = [{\n      name: \"n\",\n      type: \"int\"\n    }, {\n      name: \"firstPass\",\n      type: \"int\"\n    }, {\n      name: \"k\",\n      type: \"int\"\n    }], this.outputShape = e, this.userCode = \"\\n    void main() {\\n         // Takes max of indices (0, k), (1, k + 1), (2, k + 2) ...\\n         ivec2 coords = getOutputCoords();\\n         int batch = coords[0];\\n         int elemIdx = coords[1];\\n\\n         // The output size is half of the previous size.\\n         // If the previous sequence is | | | | _ _ _ _  | | | |  _ _ _ _ (k=4),\\n         // we only need to output the indices at positions |, the indices at\\n         // positions _ can be thrown away, see Figure5(b) After Phase 2\\n         // (Merge phase) in the Bitonic Top K paper referenced above.\\n         // For example, the paper shows we only need to output the orange bars.\\n         // The output sequence should look like this | | | | | | | |.\\n         // Because the sequence is halved, to map the output index back\\n         // to the previous sequence to find the corresponding value,\\n         // we need to double the index. When we double the index,\\n         // we basically interpolate a position, so 2i looks like\\n         // | _ | _ | _ | _ | _ | _ | _. We move the | to the first k position\\n         // of each 2k positions by - elemIdx % k. E.g. for output at\\n         // index 4,5,6,7, we want to get the corresponding element at\\n         // original index 8,9,10,11, for output at index 8,9,10,11,\\n         // we want to get the corresponding element at original index\\n         // 16,17,18,19, so on and so forth.\\n\\n         int i = elemIdx < k ? elemIdx : (elemIdx * 2 - imod(elemIdx, k));\\n         int i0 = firstPass == 1 ? i : int(getIndices(batch, i));\\n         int i1 = firstPass == 1 ? i + k : int(getIndices(batch, i + k));\\n\\n         float x0 = getX(batch, i0);\\n         float x1 = i1 < n ? getX(batch, i1) : x0;\\n\\n         setOutput(x0 >= x1 ? float(i0) : float(i1));\\n       }\\n     \";\n  }\n\n}\n\nfunction disposeIntermediateTensorInfoOrNull(e, t) {\n  null !== t && e.disposeIntermediateTensorInfo(t);\n}\n\nfunction roundUpToPow2(e) {\n  var t = 1;\n\n  for (; t < e;) {\n    t *= 2;\n  }\n\n  return t;\n}\n\nfunction topK(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a\n  } = t,\n      {\n    k: s,\n    sorted: o\n  } = r,\n      i = env().getNumber(\"TOPK_LAST_DIM_CPU_HANDOFF_SIZE_THRESHOLD\"),\n      l = env().getNumber(\"TOPK_K_CPU_HANDOFF_THRESHOLD\"),\n      u = a.shape,\n      c = u[u.length - 1];\n\n  if (n.shouldExecuteOnCPU([a]) || c < i || s > l) {\n    var _e1070 = n.readSync(a.dataId),\n        [_t729, _r434] = topKImplCPU(_e1070, u, a.dtype, s, o);\n\n    return [n.makeTensorInfo(_t729.shape, _t729.dtype, _t729.values), n.makeTensorInfo(_r434.shape, _r434.dtype, _r434.values)];\n  }\n\n  if (0 === s) return u[u.length - 1] = 0, [n.makeTensorInfo(u, a.dtype, []), n.makeTensorInfo(u, \"int32\", [])];\n  if (1 === c) return [a, fill({\n    attrs: {\n      shape: u,\n      dtype: \"int32\",\n      value: 0\n    },\n    backend: n\n  })];\n  var p = n.texData.get(a.dataId),\n      d = null !== p && p.isPacked,\n      h = d ? n.unpackTensor(a) : a,\n      m = sizeFromShape(u) / c,\n      f = reshape({\n    inputs: {\n      x: h\n    },\n    attrs: {\n      shape: [m, c]\n    },\n    backend: n\n  });\n  d && disposeIntermediateTensorInfoOrNull(n, h);\n  var g = roundUpToPow2(s),\n      $ = roundUpToPow2(c);\n  var y = null;\n\n  var b = () => null === y ? [f, f] : [f, y],\n      x = (e, t, r) => {\n    var a = b(),\n        s = new SwapProgram(r),\n        o = y;\n    y = n.runWebGLProgram(s, a, \"int32\", [[c], [null === y ? 1 : 0], [Number.NEGATIVE_INFINITY], [e], [t]]), disposeIntermediateTensorInfoOrNull(n, o);\n  };\n\n  for (var _e1071 = 1; _e1071 < g; _e1071 *= 2) {\n    var _t730 = 2 * _e1071;\n\n    for (var _n439 = _e1071; _n439 >= 1; _n439 /= 2) {\n      x(_t730, _n439, [m, $]);\n    }\n  }\n\n  for (var _e1072 = $; _e1072 > g; _e1072 /= 2) {\n    var _t731 = b(),\n        _r435 = new MergeProgram([m, _e1072 / 2]),\n        _a308 = y;\n\n    y = n.runWebGLProgram(_r435, _t731, \"int32\", [[c], [null === y ? 1 : 0], [g]]), disposeIntermediateTensorInfoOrNull(n, _a308);\n\n    var _s222 = g / 2,\n        _o157 = 2 * _s222;\n\n    for (var _e1073 = _s222; _e1073 >= 1; _e1073 /= 2) {\n      x(_o157, _e1073, y.shape);\n    }\n  }\n\n  var v = y;\n  y = slice({\n    inputs: {\n      x: y\n    },\n    backend: n,\n    attrs: {\n      begin: 0,\n      size: [m, s]\n    }\n  }), disposeIntermediateTensorInfoOrNull(n, v);\n  var I = gatherV2({\n    inputs: {\n      x: f,\n      indices: y\n    },\n    backend: n,\n    attrs: {\n      axis: 1,\n      batchDims: 1\n    }\n  });\n  disposeIntermediateTensorInfoOrNull(n, f);\n  var C = u.slice(0, -1);\n  C.push(s), v = y, y = reshape({\n    inputs: {\n      x: y\n    },\n    attrs: {\n      shape: C\n    },\n    backend: n\n  }), disposeIntermediateTensorInfoOrNull(n, v);\n  var S = I;\n  return I = reshape({\n    inputs: {\n      x: I\n    },\n    attrs: {\n      shape: C\n    },\n    backend: n\n  }), disposeIntermediateTensorInfoOrNull(n, S), [I, y];\n}\n\nvar topKConfig = {\n  kernelName: TopK,\n  backendName: \"webgl\",\n  kernelFunc: topK\n};\n\nclass TransformProgram {\n  constructor(e, t, n, r, a, s) {\n    this.variableNames = [\"Image\", \"Transforms\"], this.outputShape = s;\n    var o = \"nearest\" === n ? 1 : 2;\n    var i;\n\n    switch (r) {\n      case \"constant\":\n        i = 1;\n        break;\n\n      case \"reflect\":\n        i = 2;\n        break;\n\n      case \"wrap\":\n        i = 3;\n        break;\n\n      case \"nearest\":\n        i = 4;\n        break;\n\n      default:\n        i = 1;\n    }\n\n    this.userCode = \"\\n            float mapCoord(float outCoord, float len) {\\n              float inCoord = outCoord;\\n              if(\".concat(i, \" == 2) {\\n                if (inCoord < 0.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz2 = 2.0 * len;\\n                    if (inCoord < sz2) {\\n                      inCoord = sz2 * float(int(float(-inCoord / sz2))) +\\n                      inCoord;\\n                    }\\n                    inCoord = inCoord < -len ? inCoord + sz2 : -inCoord - 1.0;\\n                  }\\n                } else if (inCoord > len - 1.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz2 = 2.0 * len;\\n                    inCoord -= sz2 * float(int(float(inCoord / sz2)));\\n                    if (inCoord >= len) {\\n                      inCoord = sz2 - inCoord - 1.0;\\n                    }\\n                  }\\n                }\\n                return clamp(inCoord, 0.0, len - 1.0);\\n              } else if (\").concat(i, \" == 3) {\\n                if (inCoord < 0.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz = len - 1.0;\\n                    inCoord += len * (float(int(float(-inCoord / sz))) + 1.0);\\n                  }\\n                } else if (inCoord > len - 1.0) {\\n                  if (len <= 1.0) {\\n                    inCoord = 0.0;\\n                  } else {\\n                    float sz = len - 1.0;\\n                    inCoord -= len * float(int(float(inCoord / sz)));\\n                  }\\n                }\\n                return clamp(inCoord, 0.0, len - 1.0);\\n              } else if (\").concat(i, \" == 4) {\\n                return clamp(outCoord, 0.0, len - 1.0);\\n              } else {\\n                return outCoord;\\n              }\\n            }\\n\\n            float readWithFillValue(int batch, int coordY, int coordX,\\n              int channel) {\\n              float outputValue;\\n              if (0 <= coordY && coordY < \").concat(e, \" && 0 <= coordX && coordX < \").concat(t, \") {\\n                  outputValue = getImage(batch, coordY, coordX, channel);\\n              } else {\\n                outputValue = float(\").concat(a, \");\\n              }\\n              return outputValue;\\n            }\\n\\n            void main() {\\n              ivec4 coords = getOutputCoords();\\n              float outputValue;\\n              int batch = coords[0];\\n              int x = coords[2];\\n              int y = coords[1];\\n              int channel = coords[3];\\n              float xf = float(x);\\n              float yf = float(y);\\n              float a1 = getTransforms(batch, 0);\\n              float a2 = getTransforms(batch, 1);\\n              float a3 = getTransforms(batch, 2);\\n              float b1 = getTransforms(batch, 3);\\n              float b2 = getTransforms(batch, 4);\\n              float b3 = getTransforms(batch, 5);\\n              float c1 = getTransforms(batch, 6);\\n              float c2 = getTransforms(batch, 7);\\n              float projection = c1 * xf + c2 * yf + 1.0;\\n              if (projection == 0.0) {\\n                outputValue = float(\").concat(a, \");\\n              } else {\\n                float inX = (a1 * xf + a2 * yf + a3) / projection;\\n                float inY = (b1 * xf + b2 * yf + b3) / projection;\\n                float mapX = mapCoord(inX, float(\").concat(t, \"));\\n                float mapY = mapCoord(inY, float(\").concat(e, \"));\\n\\n                if (\").concat(o, \" == 1) {\\n                  int coordY = int(round(mapY));\\n                  int coordX = int(round(mapX));\\n                  outputValue = readWithFillValue(batch, coordY, coordX,\\n                    channel);\\n                } else {\\n                  float yFloor = floor(mapY);\\n                  float xFloor = floor(mapX);\\n                  float yCeil = yFloor + 1.0;\\n                  float xCeil = xFloor + 1.0;\\n                  float valueYFloor = (xCeil - mapX) *\\n                  readWithFillValue(batch, int(yFloor), int(xFloor), channel) +\\n                  (mapX - xFloor) *\\n                  readWithFillValue(batch, int(yFloor), int(xCeil), channel);\\n                  float valueYCeil = (xCeil - mapX) *\\n                  readWithFillValue(batch, int(yCeil), int(xFloor), channel) +\\n                  (mapX - xFloor) *\\n                  readWithFillValue(batch, int(yCeil), int(xCeil), channel);\\n                  outputValue = (yCeil - mapY) * valueYFloor +\\n                  (mapY - yFloor) * valueYCeil;\\n                }\\n              }\\n              setOutput(outputValue);\\n            }\\n        \");\n  }\n\n}\n\nfunction transform(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    image: a,\n    transforms: s\n  } = t,\n      {\n    interpolation: o,\n    fillMode: i,\n    fillValue: l,\n    outputShape: u\n  } = r,\n      [c, p, d, h] = a.shape,\n      [m, f] = null != u ? u : [p, d],\n      g = new TransformProgram(p, d, o, i, l, [c, m, f, h]);\n  return n.runWebGLProgram(g, [a, s], \"float32\");\n}\n\nvar transformConfig = {\n  kernelName: Transform,\n  backendName: \"webgl\",\n  kernelFunc: transform\n};\n\nfunction unique(e) {\n  var {\n    inputs: t,\n    attrs: n,\n    backend: r\n  } = e,\n      {\n    axis: a\n  } = n,\n      {\n    x: s\n  } = t;\n  assertNotComplex(s, \"unique\"), console.warn(\"WARNING: \", \"UI might be locked temporarily as data is being downloaded\");\n  var o = r.readSync(s.dataId),\n      {\n    outputValues: i,\n    outputShape: l,\n    indices: u\n  } = uniqueImplCPU(o, a, s.shape, s.dtype);\n  return [r.makeTensorInfo(l, s.dtype, i), r.makeTensorInfo([u.length], \"int32\", u)];\n}\n\nvar uniqueConfig = {\n  kernelName: Unique,\n  backendName: \"webgl\",\n  kernelFunc: unique\n};\n\nfunction unpack(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    value: a\n  } = t;\n  var {\n    axis: s\n  } = r;\n  s < 0 && (s += a.shape.length);\n  var o = a,\n      i = o.shape.length,\n      l = a.shape[s],\n      u = new Array(i - 1);\n  var c = 0;\n\n  for (var _e1074 = 0; _e1074 < i; _e1074++) {\n    _e1074 !== s && (u[c++] = o.shape[_e1074]);\n  }\n\n  var p = [],\n      d = new Array(i).fill(0),\n      h = o.shape.slice();\n  h[s] = 1;\n  var m = new Array(l);\n\n  for (var _e1075 = 0; _e1075 < m.length; _e1075++) {\n    d[s] = _e1075;\n\n    var _t732 = slice({\n      inputs: {\n        x: o\n      },\n      backend: n,\n      attrs: {\n        begin: d,\n        size: h\n      }\n    }),\n        _r436 = reshape({\n      inputs: {\n        x: _t732\n      },\n      backend: n,\n      attrs: {\n        shape: u\n      }\n    });\n\n    m[_e1075] = _r436, p.push(_t732);\n  }\n\n  return p.forEach(e => n.disposeIntermediateTensorInfo(e)), m;\n}\n\nvar unpackConfig = {\n  kernelName: Unpack,\n  backendName: \"webgl\",\n  kernelFunc: unpack\n};\n\nclass SegmentOpProgram {\n  constructor(e, t) {\n    this.variableNames = [\"x\", \"segmentIds\"];\n    var n = e.windowSize,\n        r = e.batchSize,\n        a = e.inSize,\n        s = e.numSegments,\n        o = s * Math.ceil(a / n);\n    this.outputShape = [r, o];\n    var i = 4 * Math.floor(n / 4),\n        l = n % 4,\n        u = \"\\n        sumValue += dot(values, segFilter);\\n    \";\n    var c = \"\";\n    a % n > 0 && (c = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return initializationValue;\\n        }\\n      \"));\n    var p = \"\";\n    a % n > 0 && (p = \"\\n        if (inIdx < 0 || inIdx >= \".concat(a, \") {\\n          return -1.0;\\n        }\\n      \")), this.userCode = \"\\n      const float initializationValue = 0.0;\\n\\n      float getValue(int batch, int inIdx) {\\n        \".concat(c, \"\\n        return getX(batch, inIdx);\\n      }\\n\\n      float getSegmentIdAtIndex(int inIdx) {\\n        \").concat(p, \"\\n        return getSegmentIds(inIdx);\\n      }\\n\\n      void main() {\\n        ivec2 coords = getOutputCoords();\\n        int batch = coords[0];\\n        int outIdx = coords[1];\\n        int inOffset = int(floor(float(outIdx) / float(\\n          \").concat(s, \")) * float(\").concat(n, \"));\\n        int currentSeg = int(mod(float(outIdx), float(\").concat(s, \")));\\n\\n        float sumValue = 0.0;\\n\\n        for (int i = 0; i < \").concat(i, \"; i += 4) {\\n          int inIdx = inOffset + i;\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            getValue(batch, inIdx + 3)\\n          );\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 1)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 2)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 3)) == currentSeg ? 1 : 0\\n          );\\n\\n          \").concat(u, \"\\n        }\\n\\n        int inIdx = inOffset + \").concat(i, \";\\n        if (\").concat(1 === l, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            initializationValue,\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          int inIdxSeg = int(getSegmentIdAtIndex(inIdx));\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            0,\\n            0,\\n            0\\n          );\\n\\n          \").concat(u, \"\\n        } else if (\").concat(2 === l, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            initializationValue,\\n            initializationValue\\n          );\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 1)) == currentSeg ? 1 : 0,\\n              0,\\n              0\\n          );\\n\\n          \").concat(u, \"\\n        } else if (\").concat(3 === l, \") {\\n          vec4 values = vec4(\\n            getValue(batch, inIdx),\\n            getValue(batch, inIdx + 1),\\n            getValue(batch, inIdx + 2),\\n            initializationValue\\n          );\\n\\n          vec4 segFilter = vec4(\\n            int(getSegmentIdAtIndex(inIdx)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 1)) == currentSeg ? 1 : 0,\\n            int(getSegmentIdAtIndex(inIdx + 2)) == currentSeg ? 1 : 0,\\n            0\\n          );\\n\\n          \").concat(u, \"\\n        }\\n        setOutput(sumValue);\\n      }\\n    \");\n  }\n\n}\n\nfunction unsortedSegmentSum(e) {\n  var {\n    inputs: t,\n    backend: n,\n    attrs: r\n  } = e,\n      {\n    x: a,\n    segmentIds: s\n  } = t,\n      {\n    numSegments: o\n  } = r,\n      i = a.shape.length,\n      l = [];\n  var u = 0;\n  var c = getAxesPermutation([u], i);\n  var p = a;\n  null != c && (p = transpose({\n    inputs: {\n      x: a\n    },\n    backend: n,\n    attrs: {\n      perm: c\n    }\n  }), l.push(p), u = getInnerMostAxes(1, i)[0]);\n  var d = computeOutShape(p.shape, u, o),\n      h = sizeFromShape([p.shape[u]]),\n      m = reshape({\n    inputs: {\n      x: p\n    },\n    backend: n,\n    attrs: {\n      shape: [-1, h]\n    }\n  });\n  l.push(m);\n\n  var f = sumOutType(a.dtype),\n      g = (e, t, r, a, s) => {\n    var o = e.shape[0],\n        i = e.shape[1],\n        u = segOpComputeOptimalWindowSize(i, s),\n        c = new SegmentOpProgram({\n      windowSize: u,\n      inSize: i,\n      batchSize: o,\n      numSegments: s\n    }, t),\n        p = n.compileAndRun(c, [e, r], a);\n    if (l.push(p), p.shape[1] === s) return p;\n    var d = range$1({\n      backend: n,\n      attrs: {\n        start: 0,\n        stop: s,\n        step: 1,\n        dtype: \"float32\"\n      }\n    }),\n        h = tile({\n      inputs: {\n        x: d\n      },\n      backend: n,\n      attrs: {\n        reps: [i / u]\n      }\n    });\n    return l.push(d), l.push(h), g(p, t, h, a, s);\n  },\n      $ = reshape({\n    inputs: {\n      x: g(m, \"unsortedSegmentSum\", s, f, o)\n    },\n    backend: n,\n    attrs: {\n      shape: d\n    }\n  });\n\n  var y = $;\n\n  if (null != c) {\n    l.push($);\n\n    var _e1076 = getUndoAxesPermutation(c);\n\n    y = transpose({\n      inputs: {\n        x: y\n      },\n      backend: n,\n      attrs: {\n        perm: _e1076\n      }\n    });\n  }\n\n  return l.forEach(e => n.disposeIntermediateTensorInfo(e)), y;\n}\n\nvar unsortedSegmentSumConfig = {\n  kernelName: UnsortedSegmentSum,\n  backendName: \"webgl\",\n  kernelFunc: unsortedSegmentSum\n},\n    kernelConfigs = [LRNConfig, LRNGradConfig, _fusedMatMulConfig, absConfig, acosConfig, acoshConfig, addConfig, addNConfig, allConfig, anyConfig, argMaxConfig, argMinConfig, asinConfig, asinhConfig, atan2Config, atanConfig, atanhConfig, avgPool3DConfig, avgPoolConfig, avgPoolGrad3DConfig, avgPoolGradConfig, batchMatMulConfig, batchNormConfig, batchToSpaceNDConfig, bincountConfig, castConfig, ceilConfig, clipByValueConfig, complexAbsConfig, complexConfig, concatConfig, conv2DBackpropFilterConfig, conv2DBackpropInputConfig, conv2DConfig, conv3DBackpropFilterV2Config, conv3DBackpropInputConfig, conv3DConfig, cosConfig, coshConfig, cropAndResizeConfig, cumsumConfig, denseBincountConfig, depthToSpaceConfig, depthwiseConv2dNativeBackpropFilterConfig, depthwiseConv2dNativeBackpropInputConfig, depthwiseConv2dNativeConfig, diagConfig, dilation2DConfig, einsumConfig, eluConfig, eluGradConfig, equalConfig, erfConfig, expConfig, expandDimsConfig, expm1Config, fftConfig, fillConfig, flipLeftRightConfig, floorConfig, floorDivConfig, fromPixelsConfig, fusedConv2DConfig, fusedDepthwiseConv2DConfig, gatherNdConfig, gatherV2Config, greaterConfig, greaterEqualConfig, identityConfig, ifftConfig, imagConfig, isFiniteConfig, isInfConfig, isNaNConfig, leakyReluConfig, lessConfig, lessEqualConfig, linSpaceConfig, log1pConfig, logConfig, logicalAndConfig, logicalNotConfig, logicalOrConfig, maxConfig, maxPool3DConfig, maxPoolConfig, maxPoolGrad3DConfig, maxPoolGradConfig, maxPoolWithArgmaxConfig, maximumConfig, meanConfig, minConfig, minimumConfig, mirrorPadConfig, modConfig, multinomialConfig, multiplyConfig, negConfig, nonMaxSuppressionV3Config, nonMaxSuppressionV4Config, nonMaxSuppressionV5Config, notEqualConfig, oneHotConfig, onesLikeConfig, packConfig, padV2Config, powConfig, preluConfig, prodConfig, rangeConfig, realConfig, realDivConfig, reciprocalConfig, relu6Config, reluConfig, reshapeConfig, resizeBilinearConfig, resizeBilinearGradConfig, resizeNearestNeighborConfig, resizeNearestNeighborGradConfig, reverseConfig, rotateWithOffsetConfig, roundConfig, rsqrtConfig, scatterNdConfig, selectConfig, seluConfig, sigmoidConfig, signConfig, sinConfig, sinhConfig, sliceConfig, softmaxConfig, softplusConfig, spaceToBatchNDConfig, sparseFillEmptyRowsConfig, sparseReshapeConfig, sparseSegmentMeanConfig, sparseSegmentSumConfig, sparseToDenseConfig, splitVConfig, sqrtConfig, squareConfig, squaredDifferenceConfig, stepConfig, stridedSliceConfig, stringNGramsConfig, stringSplitConfig, stringToHashBucketFastConfig, subConfig, sumConfig, tanConfig, tanhConfig, tileConfig, topKConfig, transformConfig, transposeConfig, uniqueConfig, unpackConfig, unsortedSegmentSumConfig, zerosLikeConfig];\n\nfor (var _e1077 of kernelConfigs) {\n  registerKernel(_e1077);\n}\n\nvar version$1 = \"3.8.0\",\n    version = {\n  \"tfjs-core\": version$7,\n  \"tfjs-backend-cpu\": version$3,\n  \"tfjs-backend-webgl\": version$2,\n  \"tfjs-data\": version$4,\n  \"tfjs-layers\": version$6,\n  \"tfjs-converter\": version$5,\n  tfjs: version$1\n};\nvar dist = {\n  __proto__: null,\n  data: index,\n  version,\n  AdadeltaOptimizer,\n  AdagradOptimizer,\n  AdamOptimizer,\n  AdamaxOptimizer,\n  MomentumOptimizer,\n  Optimizer,\n  RMSPropOptimizer,\n  SGDOptimizer,\n  Tensor,\n  TensorBuffer,\n  Variable,\n\n  get Rank() {\n    return Rank;\n  },\n\n  sumOutType,\n  upcastType,\n\n  get Reduction() {\n    return Reduction;\n  },\n\n  customGrad,\n  grad,\n  grads,\n  valueAndGrad,\n  valueAndGrads,\n  variableGrads,\n  Environment,\n  env,\n\n  get ENV() {\n    return ENV$2;\n  },\n\n  nextFrame,\n  KernelBackend,\n  DataStorage,\n  abs: abs$2,\n  acos: acos$2,\n  acosh: acosh$2,\n  add: add$2,\n  addN: addN$2,\n  all: all$2,\n  any: any$2,\n  argMax: argMax$2,\n  argMin: argMin$2,\n  asin: asin$2,\n  asinh: asinh$2,\n  atan: atan$2,\n  atan2: atan2$2,\n  atanh: atanh$2,\n  avgPool: avgPool$2,\n  avgPool3d: avgPool3d$1,\n  basicLSTMCell,\n  batchToSpaceND: batchToSpaceND$2,\n  batchNorm: batchNorm$2,\n  batchNorm2d,\n  batchNorm3d,\n  batchNorm4d,\n  bincount: bincount$2,\n  broadcastTo,\n  buffer,\n  cast: cast$3,\n  ceil: ceil$2,\n  clipByValue: clipByValue$1,\n  clone,\n  complex: complex$2,\n  concat: concat$2,\n  concat1d,\n  concat2d,\n  concat3d,\n  concat4d,\n  conv1d: conv1d$1,\n  conv2d: conv2d$3,\n  conv2dTranspose: conv2dTranspose$1,\n  conv3d: conv3d$1,\n  conv3dTranspose: conv3dTranspose$1,\n  cos: cos$2,\n  cosh: cosh$2,\n  cumsum: cumsum$2,\n  denseBincount: denseBincount$2,\n  depthToSpace: depthToSpace$2,\n  depthwiseConv2d: depthwiseConv2d$3,\n  diag: diag$2,\n  dilation2d,\n  div: div$1,\n  divNoNan,\n  dot: dot$2,\n  einsum: einsum$2,\n  elu: elu$4,\n  equal: equal$2,\n  erf: erf$2,\n  exp: exp$2,\n  expandDims: expandDims$3,\n  expm1: expm1$2,\n  eye,\n  fill: fill$2,\n  floor: floor$2,\n  floorDiv: floorDiv$2,\n  gather: gather$1,\n  greater: greater$3,\n  greaterEqual: greaterEqual$2,\n  imag: imag$2,\n  isFinite: isFinite$3,\n  isInf: isInf$2,\n  isNaN: isNaN$3,\n  leakyRelu: leakyRelu$2,\n  less: less$3,\n  lessEqual: lessEqual$2,\n  linspace,\n  localResponseNormalization,\n  log: log$3,\n  log1p: log1p$2,\n  logSigmoid,\n  logSoftmax,\n  logSumExp,\n  logicalAnd: logicalAnd$2,\n  logicalNot: logicalNot$2,\n  logicalOr: logicalOr$2,\n  logicalXor,\n  matMul: matMul$1,\n  max: max$3,\n  maxPool: maxPool$2,\n  maxPool3d: maxPool3d$1,\n  maxPoolWithArgmax,\n  maximum: maximum$3,\n  mean: mean$1,\n  meshgrid,\n  min: min$3,\n  minimum: minimum$3,\n  mirrorPad: mirrorPad$1,\n  mod: mod$2,\n  moments,\n  mul,\n  multiRNNCell,\n  multinomial: multinomial$2,\n  neg: neg$2,\n  notEqual: notEqual$2,\n  oneHot: oneHot$2,\n  ones: ones$1,\n  onesLike: onesLike$2,\n  outerProduct,\n  pad,\n  pad1d,\n  pad2d,\n  pad3d,\n  pad4d,\n  pool: pool$1,\n  pow: pow$2,\n  prelu: prelu$3,\n  print,\n  prod: prod$2,\n  rand,\n  randomGamma,\n  randomNormal: randomNormal$2,\n  randomUniform: randomUniform$1,\n  range: range$4,\n  real: real$2,\n  reciprocal: reciprocal$2,\n  relu: relu$3,\n  relu6: relu6$2,\n  reshape: reshape$3,\n  reverse: reverse$2,\n  reverse1d,\n  reverse2d,\n  reverse3d,\n  reverse4d,\n  round: round$2,\n  rsqrt: rsqrt$2,\n  scalar,\n  selu: selu$2,\n  separableConv2d: separableConv2d$1,\n  setdiff1dAsync,\n  sigmoid: sigmoid$2,\n  sign: sign$2,\n  sin: sin$2,\n  sinh: sinh$2,\n  slice: slice$2,\n  slice1d,\n  slice2d,\n  slice3d,\n  slice4d,\n  softmax: softmax$3,\n  softplus: softplus$2,\n  spaceToBatchND: spaceToBatchND$2,\n  fft: fft$2,\n  ifft: ifft$2,\n  irfft,\n  rfft,\n  split: split$2,\n  sqrt: sqrt$2,\n  square: square$2,\n  squaredDifference: squaredDifference$2,\n  squeeze,\n  stack,\n  step: step$2,\n  stridedSlice: stridedSlice$2,\n  sub: sub$2,\n  sum: sum$2,\n  tan: tan$2,\n  tanh: tanh$2,\n  tensor,\n  tensor1d,\n  tensor2d,\n  tensor3d,\n  tensor4d,\n  tensor5d,\n  tensor6d,\n  tile: tile$3,\n  topk,\n  truncatedNormal: truncatedNormal$1,\n  unique: unique$3,\n  unsortedSegmentSum: unsortedSegmentSum$2,\n  unstack,\n  variable,\n  where,\n  whereAsync,\n  zeros: zeros$2,\n  zerosLike: zerosLike$2,\n  op,\n  OP_SCOPE_SUFFIX,\n  booleanMaskAsync,\n  transpose: transpose$2,\n  norm,\n  movingAverage,\n  scatterND,\n  sparseToDense: sparseToDense$2,\n  gatherND,\n  dropout: dropout$2,\n  enclosingPowerOfTwo,\n  cosineWindow,\n  inTopKAsync,\n  image: image$1,\n  linalg,\n  losses,\n  spectral: spectral$1,\n  fused: fused_ops,\n  signal,\n  sparse: sparse$1,\n  string: string$1,\n  train,\n  enableProdMode,\n  enableDebugMode,\n  disableDeprecationWarnings,\n  deprecationWarn,\n  disposeVariables,\n  engine,\n  memory,\n  profile,\n  tidy,\n  dispose,\n  keep,\n  time,\n  setBackend,\n  ready,\n  getBackend,\n  removeBackend,\n  findBackend,\n  findBackendFactory,\n  registerBackend,\n  backend,\n  setPlatform,\n  getKernel,\n  getGradient,\n  getKernelsForBackend,\n  registerKernel,\n  registerGradient,\n  unregisterKernel,\n  unregisterGradient,\n  copyRegisteredKernels,\n  Abs,\n  Acos,\n  Acosh,\n  Add: Add$1,\n  AddN,\n  All,\n  Any,\n  ArgMax,\n  ArgMin,\n  Asin,\n  Asinh,\n  Atan,\n  Atanh,\n  Atan2,\n  AvgPool,\n  AvgPoolGrad,\n  AvgPool3D,\n  AvgPool3DGrad,\n  BatchMatMul,\n  BatchToSpaceND,\n  Bincount,\n  BroadcastTo,\n  Cast,\n  Ceil,\n  ClipByValue,\n  Complex,\n  ComplexAbs,\n  Concat,\n  Conv2D: Conv2D$1,\n  Conv2DBackpropFilter,\n  Conv2DBackpropInput,\n  Conv3D: Conv3D$1,\n  Conv3DBackpropFilterV2,\n  Conv3DBackpropInputV2,\n  Cos,\n  Cosh,\n  Cumsum,\n  CropAndResize,\n  DenseBincount,\n  DepthToSpace,\n  DepthwiseConv2dNative,\n  DepthwiseConv2dNativeBackpropFilter,\n  DepthwiseConv2dNativeBackpropInput,\n  Diag,\n  Dilation2D,\n  Dilation2DBackpropInput,\n  Dilation2DBackpropFilter,\n  RealDiv,\n  Einsum,\n  Elu: Elu$1,\n  EluGrad,\n  Erf,\n  Equal,\n  Exp,\n  ExpandDims,\n  Expm1,\n  FFT,\n  Fill,\n  FlipLeftRight,\n  Floor,\n  FloorDiv,\n  FusedBatchNorm,\n  GatherV2,\n  GatherNd,\n  Greater,\n  GreaterEqual,\n  Identity: Identity$1,\n  IFFT,\n  Imag,\n  IsFinite,\n  IsInf,\n  IsNan,\n  LeakyRelu,\n  Less,\n  LessEqual,\n  LinSpace,\n  Log,\n  Log1p,\n  LogicalAnd,\n  LogicalNot,\n  LogicalOr,\n  LogSoftmax: LogSoftmax$1,\n  LRN,\n  LRNGrad,\n  Max,\n  Maximum: Maximum$1,\n  MaxPool,\n  MaxPoolGrad,\n  MaxPool3D,\n  MaxPool3DGrad,\n  MaxPoolWithArgmax,\n  Mean,\n  Min,\n  Minimum: Minimum$1,\n  MirrorPad,\n  Mod,\n  Multinomial,\n  Multiply: Multiply$1,\n  Neg,\n  NotEqual,\n  NonMaxSuppressionV3,\n  NonMaxSuppressionV4,\n  NonMaxSuppressionV5,\n  OnesLike,\n  OneHot,\n  Pack,\n  PadV2,\n  Pool,\n  Pow,\n  Prelu,\n  Prod,\n  Range,\n  Real,\n  Reciprocal,\n  Relu: Relu$1,\n  Reshape: Reshape$1,\n  ResizeNearestNeighbor,\n  ResizeNearestNeighborGrad,\n  ResizeBilinear,\n  ResizeBilinearGrad,\n  Relu6: Relu6$1,\n  Reverse,\n  Round,\n  Rsqrt,\n  ScatterNd,\n  Select,\n  Selu: Selu$1,\n  Slice,\n  Sin,\n  Sinh,\n  Sign,\n  Sigmoid: Sigmoid$1,\n  Softplus: Softplus$1,\n  Sqrt,\n  Sum,\n  SpaceToBatchND,\n  SplitV,\n  Softmax: Softmax$2,\n  SparseFillEmptyRows,\n  SparseReshape,\n  SparseSegmentMean,\n  SparseSegmentSum,\n  SparseToDense,\n  SquaredDifference,\n  Square,\n  StridedSlice,\n  StringNGrams,\n  StringSplit,\n  StringToHashBucketFast,\n  Sub,\n  Tan,\n  Tanh: Tanh$1,\n  Tile,\n  TopK,\n  Transform,\n  Transpose,\n  Unique,\n  Unpack,\n  UnsortedSegmentSum,\n  ZerosLike,\n  Step,\n  FromPixels,\n  RotateWithOffset,\n  _FusedMatMul,\n  FusedConv2D,\n  FusedDepthwiseConv2D,\n  version_core: version$7,\n  browser,\n  io,\n  math,\n  serialization,\n  test_util,\n  util,\n  backend_util,\n  tensor_util,\n  slice_util,\n  gather_util: gather_nd_util,\n  scatter_util: scatter_nd_util,\n  device_util,\n  kernel_impls,\n  CallbackList,\n  CustomCallback,\n  History,\n  Callback,\n  callbacks,\n  EarlyStopping,\n  InputSpec,\n  SymbolicTensor,\n  LayersModel,\n  input,\n  loadLayersModel,\n  model,\n  registerCallbackConstructor,\n  sequential,\n  RNN,\n  Sequential,\n  LayerVariable,\n  version_layers: version$6,\n  constraints: exports_constraints,\n  initializers: exports_initializers,\n  layers: exports_layers,\n  metrics: exports_metrics,\n  models: exports_models,\n  regularizers: exports_regularizers,\n  GraphModel,\n  loadGraphModel,\n  deregisterOp,\n  registerOp,\n  version_converter: version$5\n};\nvar basetable = new Uint16Array(512),\n    shifttable = new Uint8Array(512),\n    mantissatable = new Uint32Array(2048),\n    offsettable = new Uint16Array(64),\n    exponenttable = new Uint32Array(64);\nvar inited = !1;\n\nfunction init() {\n  inited = !0;\n\n  for (var _e1078 = 0; _e1078 < 256; ++_e1078) {\n    var t = _e1078 - 127;\n    t < -24 ? (basetable[0 | _e1078] = 0, basetable[256 | _e1078] = 32768, shifttable[0 | _e1078] = 24, shifttable[256 | _e1078] = 24) : t < -14 ? (basetable[0 | _e1078] = 1024 >> -t - 14, basetable[256 | _e1078] = 1024 >> -t - 14 | 32768, shifttable[0 | _e1078] = -t - 1, shifttable[256 | _e1078] = -t - 1) : t <= 15 ? (basetable[0 | _e1078] = t + 15 << 10, basetable[256 | _e1078] = t + 15 << 10 | 32768, shifttable[0 | _e1078] = 13, shifttable[256 | _e1078] = 13) : t < 128 ? (basetable[0 | _e1078] = 31744, basetable[256 | _e1078] = 64512, shifttable[0 | _e1078] = 24, shifttable[256 | _e1078] = 24) : (basetable[0 | _e1078] = 31744, basetable[256 | _e1078] = 64512, shifttable[0 | _e1078] = 13, shifttable[256 | _e1078] = 13);\n  }\n\n  for (var _t733 = 1; _t733 < 2048; ++_t733) {\n    mantissatable[_t733] = _t733 < 1024 ? e(_t733) : 939524096 + (_t733 - 1024 << 13);\n  }\n\n  exponenttable[32] = 2147483648, exponenttable[31] = 1199570944, exponenttable[63] = 3347054592;\n\n  for (var _e1079 = 1; _e1079 <= 30; ++_e1079) {\n    exponenttable[_e1079] = _e1079 << 23;\n  }\n\n  for (var _e1080 = 33; _e1080 <= 62; ++_e1080) {\n    exponenttable[_e1080] = 2147483648 + (_e1080 - 32 << 23);\n  }\n\n  for (var _e1081 = 1; _e1081 < offsettable.length; ++_e1081) {\n    offsettable[_e1081] = 1024;\n  }\n\n  function e(e) {\n    var t = e << 13,\n        n = 0;\n\n    for (; !(8388608 & t);) {\n      n -= 8388608, t <<= 1;\n    }\n\n    return t &= -8388609, n += 947912704, (t | n) >>> 0;\n  }\n\n  offsettable[32] = 0;\n}\n\nfunction float32ToUInt32(e) {\n  var t = new Float32Array(1);\n  return t[0] = e, new Uint32Array(t.buffer)[0];\n}\n\nfunction float16toUInt16(e) {\n  var t = float32ToUInt32(e);\n  return inited || init(), basetable[t >> 23 & 511] | (8388607 & t) >> shifttable[t >> 23 & 511];\n}\n\nfunction float16AsUintToFloat(e) {\n  inited || init();\n  var t = mantissatable[offsettable[e >> 10] + (1023 & e)] + exponenttable[e >> 10],\n      n = new Uint32Array(1);\n  return n[0] = t, new Float32Array(n.buffer)[0];\n}\n\nfunction assert(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"Assertion failed\";\n  if (!e) throw new Error(t);\n}\n\nfunction userError(e) {\n  var t = new Error(e);\n  throw t.isUserError = !0, t;\n}\n\nfunction lookup(e, t) {\n  return e.hasOwnProperty(t) ? e[t] : null;\n}\n\nfunction oops() {\n  var e = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : \"OOPS\";\n  throw new Error(e);\n}\n\nfunction endsWith(e, t) {\n  return !(e.length < t.length || 0 != t.length && e.slice(-t.length) != t);\n}\n\nfunction mapMap(e, t) {\n  var n = {};\n  return Object.keys(e).forEach(r => n[r] = t(r, e[r])), n;\n}\n\nfunction pushRange(e, t) {\n  for (var n = 0; n < t.length; ++n) {\n    e.push(t[n]);\n  }\n}\n\nfunction range(e) {\n  var t = [];\n\n  for (var n = 0; n < e; ++n) {\n    t.push(n);\n  }\n\n  return t;\n}\n\nvar seed = 218109047;\n\nfunction randomUint32() {\n  var e = seed;\n  return e ^= e << 13, e ^= e >>> 17, e ^= e << 5, e >>>= 0, seed = e, e;\n}\n\nfunction randomUFloat() {\n  return randomUint32() / 4294967296;\n}\n\nfunction randomSFloat() {\n  return 2 * randomUFloat() - 1;\n}\n\nfunction flatClone(e) {\n  var t = e,\n      n = {};\n\n  for (var _e1082 of Object.keys(t)) {\n    n[_e1082] = t[_e1082];\n  }\n\n  return n;\n}\n\nfunction lf(e) {\n  for (var _len10 = arguments.length, t = new Array(_len10 > 1 ? _len10 - 1 : 0), _key10 = 1; _key10 < _len10; _key10++) {\n    t[_key10 - 1] = arguments[_key10];\n  }\n\n  return e.replace(/{(\\d+)}/g, (e, n) => t[+n]);\n}\n\nvar badNameError = emitErr(\"opcode name doesn't match\", \"<name>\");\n\nclass Instruction {\n  constructor(e, t, n, r, a) {\n    this.opcode = n, this.mask = r, this.is32bit = a, this.canBeShared = !1, assert((n & r) == n), this.ei = e, this.code = t.replace(/\\s+/g, \" \"), this.friendlyFmt = t.replace(/\\$\\w+/g, e => this.ei.encoders[e] ? this.ei.encoders[e].pretty : e);\n    var s = tokenize(t);\n    this.name = s[0], this.args = s.slice(1);\n  }\n\n  emit(e) {\n    var t = e.words;\n    if (t[0] != this.name) return badNameError;\n    var n = this.opcode,\n        r = 1,\n        a = 0,\n        s = [],\n        o = null,\n        i = null,\n        l = null;\n    var u = this.ei.is32bit(this) && !this.is32bit;\n\n    for (var c = 0; c < this.args.length; ++c) {\n      var _p38 = this.args[c],\n          d = t[r++];\n\n      if (\"$\" == _p38[0]) {\n        var _c40 = this.ei.encoders[_p38],\n            h = null;\n\n        if (_c40.isRegister) {\n          if (h = this.ei.registerNo(d, _c40), null == h) return emitErr(\"expecting register name\", d);\n          this.ei.isPush(this.opcode) ? a++ : this.ei.isPop(this.opcode) && a--;\n        } else if (_c40.isImmediate) {\n          if (d = d.replace(/^#/, \"\"), h = e.bin.parseOneInt(d), null == h) return emitErr(\"expecting number\", d);\n          this.ei.isAddSP(this.opcode) ? a = -h / this.ei.wordSize() : this.ei.isSubSP(this.opcode) && (a = h / this.ei.wordSize());\n        } else if (_c40.isRegList) {\n          if (\"{\" != d) return emitErr(\"expecting {\", d);\n\n          for (h = 0; \"}\" != t[r];) {\n            if (d = t[r++], !d) return emitErr(\"expecting }\", t[r - 2]);\n\n            var _e1083 = this.ei.registerNo(d, _c40);\n\n            if (null == _e1083) return emitErr(\"expecting register name\", d);\n            if (h & 1 << _e1083) return emitErr(\"duplicate register name\", d);\n            h |= 1 << _e1083, this.ei.isPush(this.opcode) ? a++ : this.ei.isPop(this.opcode) && a--, \",\" == t[r] && r++;\n          }\n\n          d = t[r++];\n        } else if (_c40.isLabel) {\n          if (d = d.replace(/^#/, \"\"), /^[+-]?\\d+$/.test(d)) h = parseInt(d, 10), o = \"rel\" + h;else if (/^0x[0-9a-fA-F]+$/.test(d)) h = parseInt(d, 16), o = \"abs\" + h;else {\n            var _t734 = 0;\n\n            if (d.indexOf(\"+\") > 0) {\n              var _e1084 = /(.*)\\+(\\d+)$/.exec(d);\n\n              _e1084 && (d = _e1084[1], _t734 = parseInt(_e1084[2]));\n            }\n\n            if (o = d, h = this.ei.getAddressFromLabel(e.bin, this, d, _c40.isWordAligned), null == h) {\n              if (e.bin.finalEmit) return emitErr(\"unknown label\", d);\n              h = 8;\n            }\n\n            h += _t734;\n          }\n\n          if (u) {\n            i = h, l = d;\n            continue;\n          }\n        } else oops();\n\n        if (null == h) return emitErr(\"didn't understand it\", d);\n        if (s.push(h), h = _c40.encode(h), null == h) return emitErr(\"argument out of range or mis-aligned\", d);\n        assert(0 == (n & h)), n |= h;\n      } else if (_p38 != d) return emitErr(\"expecting \" + _p38, d);\n    }\n\n    return t[r] ? emitErr(\"trailing tokens\", t[r]) : u ? this.ei.emit32(n, i, e.bin.normalizeExternalLabel(l)) : this.is32bit ? {\n      opcode: n >> 16 & 65535 | 32768,\n      opcode2: n >> 0 & 65535,\n      stack: a,\n      numArgs: s,\n      labelName: e.bin.normalizeExternalLabel(o)\n    } : {\n      stack: a,\n      opcode: n,\n      numArgs: s,\n      labelName: e.bin.normalizeExternalLabel(o)\n    };\n  }\n\n  toString() {\n    return this.friendlyFmt;\n  }\n\n}\n\nclass Line {\n  constructor(e, t) {\n    this.bin = e, this.text = t;\n  }\n\n  getOpExt() {\n    return this.instruction ? this.instruction.code : \"\";\n  }\n\n  getOp() {\n    return this.instruction ? this.instruction.name : \"\";\n  }\n\n  update(e) {\n    this.bin.peepOps++, (e = e.replace(/^\\s*/, \"\")) || this.bin.peepDel++, e && (e += \"      \"), this.text = (e = \"    \" + e) + \"; WAS: \" + this.text.trim(), this.instruction = null, this.numArgs = null, this.words = tokenize(e) || [], 0 == this.words.length ? this.type = \"empty\" : \"@\" == this.words[0][0] && (this.type = \"directive\");\n  }\n\n}\n\nclass File$1 {\n  constructor(e) {\n    this.baseOffset = 0, this.checkStack = !0, this.inlineMode = !1, this.normalizeExternalLabel = e => e, this.currLineNo = 0, this.scope = \"\", this.scopeId = 0, this.errors = [], this.labels = {}, this.equs = {}, this.stackpointers = {}, this.stack = 0, this.commPtr = 0, this.peepOps = 0, this.peepDel = 0, this.peepCounts = {}, this.stats = \"\", this.throwOnError = !1, this.disablePeepHole = !1, this.stackAtLabel = {}, this.currLine = new Line(this, \"<start>\"), this.currLine.lineNo = 0, this.ei = e, this.ei.file = this;\n  }\n\n  emitShort(e) {\n    assert(0 <= e && e <= 65535), this.buf.push(e);\n  }\n\n  emitOpCode(e) {\n    this.emitShort(e);\n  }\n\n  location() {\n    return 2 * this.buf.length;\n  }\n\n  pc() {\n    return this.location() + this.baseOffset;\n  }\n\n  parseOneInt(e) {\n    if (!e) return null;\n    if (/^\\d+$/.test(e)) return parseInt(e, 10);\n    var t = e.indexOf(\"-\");\n    if (t > 0) return this.parseOneInt(e.slice(0, t)) - this.parseOneInt(e.slice(t + 1));\n    var n = 1;\n\n    if (e.indexOf(\"*\") >= 0) {\n      var _t735 = null;\n\n      for (; _t735 = /^([^\\*]*)\\*(.*)$/.exec(e);) {\n        var _r437 = this.parseOneInt(_t735[1]);\n\n        if (null == _r437) return null;\n        n *= _r437, e = _t735[2];\n      }\n    }\n\n    if (\"-\" == e[0] ? (n *= -1, e = e.slice(1)) : \"+\" == e[0] && (e = e.slice(1)), /^\\d+$/.test(e)) return n * parseInt(e, 10);\n    if (endsWith(e, \"|1\")) return 1 | this.parseOneInt(e.slice(0, e.length - 2));\n    if (endsWith(e, \"-1\")) return this.parseOneInt(e.slice(0, e.length - 2)) - 1;\n    if (endsWith(e, \"+1\")) return this.parseOneInt(e.slice(0, e.length - 2)) + 1;\n    var r = /(.*)>>(\\d+)$/.exec(e);\n\n    if (r) {\n      var _e1085 = this.parseOneInt(r[1]);\n\n      return _e1085 &= ~(-16777216 & this.baseOffset), _e1085 >> parseInt(r[2]);\n    }\n\n    var a = null;\n    if (\"0\" == e[0]) if (\"x\" == e[1] || \"X\" == e[1]) {\n      var _t736 = /^0x([a-f0-9]+)$/i.exec(e);\n\n      _t736 && (a = parseInt(_t736[1], 16));\n    } else if (\"b\" == e[1] || \"B\" == e[1]) {\n      var _t737 = /^0b([01]+)$/i.exec(e);\n\n      _t737 && (a = parseInt(_t737[1], 2));\n    }\n\n    if (e.indexOf(\"@\") >= 0) {\n      var _t738 = /^(\\w+)@(-?\\d+)$/.exec(e);\n\n      _t738 && (1 != n && this.directiveError(lf(\"multiplication not supported with saved stacks\")), this.stackpointers.hasOwnProperty(_t738[1]) ? a = this.ei.wordSize() * this.ei.computeStackOffset(_t738[1], this.stack - this.stackpointers[_t738[1]] + parseInt(_t738[2])) : this.directiveError(lf(\"saved stack not found\"))), _t738 = /^(.*)@(hi|lo|fn)$/.exec(e), _t738 && this.looksLikeLabel(_t738[1]) && (a = this.lookupLabel(_t738[1], !0), null != a && (\"fn\" == _t738[2] ? a = this.ei.toFnPtr(a, this.baseOffset, _t738[1]) : (a >>= 1, 0 <= a && a <= 65535 ? \"hi\" == _t738[2] ? a = a >> 8 & 255 : \"lo\" == _t738[2] ? a &= 255 : oops() : (this.directiveError(lf(\"@hi/lo out of range\")), a = null))));\n    }\n\n    return null == a && this.looksLikeLabel(e) && (a = this.lookupLabel(e, !0), null != a && 1 == this.ei.postProcessRelAddress(this, 1) && (a += this.baseOffset)), null == a || isNaN(a) ? null : a * n;\n  }\n\n  looksLikeLabel(e) {\n    return !/^(r\\d|pc|sp|lr)$/i.test(e) && /^[\\.a-zA-Z_][\\.:\\w+]*$/.test(e);\n  }\n\n  scopedName(e) {\n    return \".\" == e[0] && this.scope ? this.scope + \"$\" + e : e;\n  }\n\n  lookupLabel(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var n = null,\n        r = this.scopedName(e);\n    return this.labels.hasOwnProperty(r) ? (n = this.labels[r], n = this.ei.postProcessRelAddress(this, n)) : this.lookupExternalLabel && (n = this.lookupExternalLabel(e), null != n && (n = this.ei.postProcessAbsAddress(this, n))), null == n && this.equs.hasOwnProperty(r) && (n = this.equs[r]), null == n && t && (this.finalEmit ? this.directiveError(lf(\"unknown label: {0}\", e)) : n = 11111), n;\n  }\n\n  align(e) {\n    for (assert(2 == e || 4 == e || 8 == e || 16 == e); this.location() % e != 0;) {\n      this.emitOpCode(0);\n    }\n  }\n\n  pushError(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"\";\n    var n = {\n      scope: this.scope,\n      message: lf(\"  -> Line {2} ('{1}'), error: {0}\\n{3}\", e, this.currLine.text, this.currLine.lineNo, t),\n      lineNo: this.currLine.lineNo,\n      line: this.currLine.text,\n      coremsg: e,\n      hints: t\n    };\n    if (this.errors.push(n), this.throwOnError) throw new Error(n.message);\n  }\n\n  directiveError(e) {\n    this.pushError(e);\n  }\n\n  emitString(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n\n    function n(e, t) {\n      return 255 & (e.charCodeAt(t) || 0);\n    }\n\n    var r,\n        a = /^\\s*([\\w\\.]+\\s*:\\s*)?.\\w+\\s+(\".*\")\\s*$/.exec(e);\n    if (a && null != (r = parseString(a[2]))) {\n      if (this.align(2), t) for (var _e1086 = 0; _e1086 < r.length; _e1086++) {\n        this.emitShort(r.charCodeAt(_e1086));\n      } else for (var _e1087 = 0; _e1087 < r.length + 1; _e1087 += 2) {\n        this.emitShort(n(r, _e1087 + 1) << 8 | n(r, _e1087));\n      }\n    } else this.directiveError(lf(\"expecting string\"));\n  }\n\n  parseNumber(e) {\n    var t = this.parseOneInt(e.shift());\n    return null == t ? null : t;\n  }\n\n  parseNumbers(e) {\n    e = e.slice(1);\n    var t = [];\n\n    for (;;) {\n      var n = this.parseNumber(e);\n\n      if (null == n) {\n        this.directiveError(lf(\"cannot parse number at '{0}'\", e[0]));\n        break;\n      }\n\n      if (t.push(n), \",\" != e[0]) {\n        if (null == e[0]) break;\n        this.directiveError(lf(\"expecting number, got '{0}'\", e[0]));\n        break;\n      }\n\n      if (e.shift(), null == e[0]) break;\n    }\n\n    return t;\n  }\n\n  emitSpace(e) {\n    var t = this.parseNumbers(e);\n    if (1 == t.length && t.push(0), 2 != t.length) this.directiveError(lf(\"expecting one or two numbers\"));else if (t[0] % 2 != 0) this.directiveError(lf(\"only even space supported\"));else {\n      var _e1088 = 255 & t[1];\n\n      _e1088 |= _e1088 << 8;\n\n      for (var n = 0; n < t[0]; n += 2) {\n        this.emitShort(_e1088);\n      }\n    }\n  }\n\n  emitBytes(e) {\n    var t = this.parseNumbers(e);\n    t.length % 2 != 0 && (this.directiveError(\".bytes needs an even number of arguments\"), t.push(0));\n\n    for (var _e1089 = 0; _e1089 < t.length; _e1089 += 2) {\n      var n = t[_e1089],\n          r = t[_e1089 + 1];\n      0 <= n && r <= 255 && 0 <= r && n <= 255 ? this.emitShort(255 & n | (255 & r) << 8) : this.directiveError(lf(\"expecting uint8\"));\n    }\n  }\n\n  emitHex(e) {\n    e.slice(1).forEach(e => {\n      if (\",\" != e) if (e.length % 4 != 0) this.directiveError(\".hex needs an even number of bytes\");else if (/^[a-f0-9]+$/i.test(e)) for (var t = 0; t < e.length; t += 4) {\n        var n = parseInt(e.slice(t, t + 4), 16);\n        n = (255 & n) << 8 | n >> 8 & 255, this.emitShort(n);\n      } else this.directiveError(\".hex needs a hex number\");\n    });\n  }\n\n  emitFloats(e) {\n    e.slice(1).forEach(e => {\n      if (\",\" == e) return;\n      var t = parseFloat(e);\n      isNaN(t) && this.directiveError(\"invalid .float\");\n      var n = float32ToUInt32(t);\n      this.emitShort(65535 & n), this.emitShort(n >> 16 & 65535);\n    });\n  }\n\n  emitFloats16(e) {\n    e.slice(1).forEach(e => {\n      if (\",\" == e) return;\n      var t = parseFloat(e);\n      isNaN(t) && this.directiveError(\"invalid .float16\");\n      var n = float16toUInt16(t);\n      this.emitShort(65535 & n);\n    });\n  }\n\n  handleDirective(e) {\n    var t,\n        n = e.words,\n        r = () => {\n      2 != n.length && this.directiveError(lf(\"expecting one argument\"));\n    };\n\n    switch (n[0]) {\n      case \".ascii\":\n      case \".asciz\":\n      case \".string\":\n        this.emitString(e.text);\n        break;\n\n      case \".utf16\":\n        this.emitString(e.text, !0);\n        break;\n\n      case \".align\":\n        if (r(), t = this.parseOneInt(n[1]), null != t) {\n          if (0 == t) return;\n          t <= 4 ? this.align(1 << t) : this.directiveError(lf(\"expecting 1, 2, 3 or 4 (for 2, 4, 8, or 16 byte alignment)\"));\n        } else this.directiveError(lf(\"expecting number\"));\n\n        break;\n\n      case \".balign\":\n        if (r(), t = this.parseOneInt(n[1]), null != t) {\n          if (1 == t) return;\n          2 == t || 4 == t || 8 == t || 16 == t ? this.align(t) : this.directiveError(lf(\"expecting 2, 4, 8, or 16\"));\n        } else this.directiveError(lf(\"expecting number\"));\n\n        break;\n\n      case \".p2align\":\n        r(), t = this.parseOneInt(n[1]), null != t ? this.align(1 << t) : this.directiveError(lf(\"expecting number\"));\n        break;\n\n      case \".byte\":\n        this.emitBytes(n);\n        break;\n\n      case \".hex\":\n        this.emitHex(n);\n        break;\n\n      case \".float\":\n        this.emitFloats(n);\n        break;\n\n      case \".float16\":\n        this.emitFloats16(n);\n        break;\n\n      case \".hword\":\n      case \".short\":\n      case \".2bytes\":\n        this.parseNumbers(n).forEach(e => {\n          -32768 <= e && e <= 65535 ? this.emitShort(65535 & e) : this.directiveError(lf(\"expecting int16\"));\n        });\n        break;\n\n      case \".word\":\n      case \".4bytes\":\n      case \".long\":\n        this.parseNumbers(n).forEach(e => {\n          -2147483648 <= e && e <= 4294967295 ? (this.emitShort(65535 & e), this.emitShort(e >> 16 & 65535)) : this.directiveError(lf(\"expecting int32\"));\n        });\n        break;\n\n      case \".skip\":\n      case \".space\":\n        this.emitSpace(n);\n        break;\n\n      case \".set\":\n      case \".equ\":\n        /^\\w+$/.test(n[1]) || this.directiveError(lf(\"expecting name\"));\n        var a = this.parseNumbers(n.slice(\",\" == n[2] || \"=\" == n[2] ? 2 : 1));\n        1 != a.length && this.directiveError(lf(\"expecting one value\")), void 0 !== this.equs[n[1]] && this.equs[n[1]] != a[0] && this.directiveError(lf(\"redefinition of {0}\", n[1])), this.equs[n[1]] = a[0];\n        break;\n\n      case \".startaddr\":\n        this.location() && this.directiveError(lf(\".startaddr can be only be specified at the beginning of the file\")), r(), this.baseOffset = this.parseOneInt(n[1]);\n        break;\n\n      case \"@stackmark\":\n        r(), this.stackpointers[n[1]] = this.stack;\n        break;\n\n      case \"@stackempty\":\n        this.checkStack && (null == this.stackpointers[n[1]] ? this.directiveError(lf(\"no such saved stack\")) : this.stackpointers[n[1]] != this.stack && this.directiveError(lf(\"stack mismatch\")));\n        break;\n\n      case \"@scope\":\n        this.scope = n[1] || \"\", this.currLineNo = this.scope ? 0 : this.realCurrLineNo;\n        break;\n\n      case \".syntax\":\n      case \"@nostackcheck\":\n        this.checkStack = !1;\n        break;\n\n      case \"@dummystack\":\n        r(), this.stack += this.parseOneInt(n[1]);\n        break;\n\n      case \".section\":\n      case \".global\":\n        this.stackpointers = {}, this.stack = 0, this.scope = \"$S\" + this.scopeId++;\n        break;\n\n      case \".comm\":\n        {\n          n = n.filter(e => \",\" != e), n.shift();\n\n          var _e1090 = this.parseOneInt(n[1]),\n              _t739 = 0;\n\n          if (_t739 = n[2] ? this.parseOneInt(n[2]) : 4, null == this.lookupLabel(n[0])) {\n            for (this.commPtr || (this.commPtr = this.lookupExternalLabel(\"_pxt_comm_base\") || 0, this.commPtr || this.directiveError(lf(\"PXT_COMM_BASE not defined\"))); this.commPtr & _t739 - 1;) {\n              this.commPtr++;\n            }\n\n            this.labels[this.scopedName(n[0])] = this.commPtr - this.baseOffset, this.commPtr += _e1090;\n          }\n\n          break;\n        }\n\n      case \".arch\":\n      case \".thumb\":\n      case \".file\":\n      case \".text\":\n      case \".cpu\":\n      case \".fpu\":\n      case \".eabi_attribute\":\n      case \".code\":\n      case \".thumb_func\":\n      case \".type\":\n      case \".fnstart\":\n      case \".save\":\n      case \".size\":\n      case \".fnend\":\n      case \".pad\":\n      case \".globl\":\n      case \".local\":\n      case \"@\":\n        break;\n\n      default:\n        /^\\.cfi_/.test(n[0]) || this.directiveError(lf(\"unknown directive\"));\n    }\n  }\n\n  handleOneInstruction(e, t) {\n    var n = t.emit(e);\n    return !n.error && (this.stack += n.stack, this.checkStack && this.stack < 0 && this.pushError(lf(\"stack underflow\")), e.location = this.location(), e.opcode = n.opcode, e.stack = n.stack, this.emitOpCode(n.opcode), null != n.opcode2 && this.emitOpCode(n.opcode2), null != n.opcode3 && this.emitOpCode(n.opcode3), e.instruction = t, e.numArgs = n.numArgs, !0);\n  }\n\n  handleInstruction(e) {\n    if (e.instruction && this.handleOneInstruction(e, e.instruction)) return;\n\n    var t = e => this.ei.instructions.hasOwnProperty(e) ? this.ei.instructions[e] : [];\n\n    var n = t(e.words[0]);\n\n    for (var _t740 = 0; _t740 < n.length; ++_t740) {\n      if (this.handleOneInstruction(e, n[_t740])) return;\n    }\n\n    var r = this.ei.stripCondition(e.words[0]);\n\n    if (r && (n = t(r), n.length > 0)) {\n      e.words[0] = r;\n\n      for (var _t741 = 0; _t741 < n.length; ++_t741) {\n        if (this.handleOneInstruction(e, n[_t741])) return;\n      }\n    }\n\n    var a = e.words[0].toLowerCase().replace(/s$/, \"\").replace(/[^a-z]/g, \"\");\n    a = this.ei.stripCondition(a) || a;\n    var s = \"\",\n        o = t(a).concat(t(a + \"s\"));\n    o.length > 0 && o.forEach(t => {\n      var n = t.emit(e);\n      s += lf(\"   Maybe: {0} ({1} at '{2}')\\n\", t.toString(), n.error, n.errorAt);\n    }), this.pushError(lf(\"assembly error\"), s);\n  }\n\n  buildLine(e, t) {\n    var n = e => {\n      var n = new Line(this, e);\n      return n.scope = this.scope, n.lineNo = this.currLineNo, t.push(n), n;\n    },\n        r = n(e),\n        a = tokenize(r.text) || [];\n\n    r.words = a;\n    var s = a[0] || \"\";\n\n    if (\":\" == s.charAt(s.length - 1)) {\n      var _t742 = /^([\\.\\w]+):$/.exec(a[0]);\n\n      if (_t742) {\n        if (r.type = \"label\", r.text = _t742[1] + \":\", r.words = [_t742[1]], !(a.length > 1)) return;\n        a.shift(), r = n(e.replace(/^[^:]*:/, \"\")), r.words = a, s = a[0] || \"\";\n      }\n    }\n\n    var o = s.charAt(0);\n    \".\" == o || \"@\" == o ? (r.type = \"directive\", \"@scope\" == r.words[0] && this.handleDirective(r)) : r.type = 0 == r.words.length ? \"empty\" : \"instruction\";\n  }\n\n  prepLines(e) {\n    this.currLineNo = 0, this.realCurrLineNo = 0, this.lines = [], e.split(/\\r?\\n/).forEach(e => {\n      this.errors.length > 10 || (this.currLineNo++, this.realCurrLineNo++, this.buildLine(e, this.lines));\n    });\n  }\n\n  iterLines() {\n    this.stack = 0, this.buf = [], this.scopeId = 0, this.lines.forEach(e => {\n      if (!(this.errors.length > 10) && (this.currLine = e, 0 != e.words.length)) if (\"label\" == e.type) {\n        var t = this.scopedName(e.words[0]);\n\n        if (this.prevLabel = t, this.finalEmit) {\n          null != this.equs[t] && this.directiveError(lf(\".equ redefined as label\"));\n          var _e1091 = this.labels[t];\n          null == _e1091 && oops(), 0 == this.errors.length && _e1091 != this.location() && oops(\"invalid location: \".concat(this.location(), \" != \").concat(_e1091, \" at \").concat(t)), assert(this.errors.length > 0 || _e1091 == this.location()), this.reallyFinalEmit && (this.stackAtLabel[t] = this.stack);\n        } else this.labels.hasOwnProperty(t) ? this.directiveError(lf(\"label redefinition\")) : this.inlineMode && /^_/.test(t) ? this.directiveError(lf(\"labels starting with '_' are reserved for the compiler\")) : this.labels[t] = this.location();\n\n        e.location = this.location();\n      } else \"directive\" == e.type ? this.handleDirective(e) : \"instruction\" == e.type ? this.handleInstruction(e) : \"empty\" == e.type || oops();\n    });\n  }\n\n  getSource(e) {\n    var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n    var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0;\n\n    var r = 0,\n        a = e => {\n      var t = this.labels[e] || r,\n          n = t - r;\n      return r = t, n;\n    },\n        s = this.buf ? this.location() : 0,\n        o = a(\"_code_end\"),\n        i = a(\"_helpers_end\"),\n        l = a(\"_vtables_end\"),\n        u = a(\"_literals_end\"),\n        c = r,\n        p = s + this.baseOffset & 16777215;\n\n    n && p > n && userError(lf(\"program too big by {0} bytes!\", p - n));\n    var d = lf(\"; total bytes: {0} ({1}% of {2}k flash with {3} free)\", p, (100 * p / (n = n || 131072)).toFixed(1), (n / 1024).toFixed(1), n - p),\n        h = lf(\"; generated code sizes (bytes): {0} (incl. {1} user, {2} helpers, {3} vtables, {4} lits); src size {5}\\n\", c, o, i, l, u, s - c) + lf(\"; assembly: {0} lines; density: {1} bytes/stmt; ({2} stmts)\\n\", this.lines.length, Math.round(100 * o / t) / 100, t) + d + \"\\n\" + this.stats + \"\\n\\n\",\n        m = !1;\n    return this.lines.forEach((t, n) => {\n      if (\"_stored_program\" == t.words[0]) return h += '_stored_program: .string \"...\"\\n', void (m = !0);\n      if (m) return void (m = !1);\n      var r = t.text;\n\n      if (e) {\n        if (\"@stackempty\" == t.words[0] && this.lines[n - 1].text == t.text) return;\n        if (r = r.replace(/; WAS: .*/, \"\"), !r.trim()) return;\n      }\n\n      h += r + \"\\n\";\n    }), h;\n  }\n\n  peepHole() {\n    var e = this.lines.filter(e => \"empty\" != e.type);\n\n    for (var t = 0; t < e.length; ++t) {\n      var n = e[t];\n      if (/^user/.test(n.scope)) continue;\n      var r = e[t + 1];\n      if (!r) continue;\n      var a = e[t + 2];\n      \"instruction\" == n.type && this.ei.peephole(n, r, a);\n    }\n  }\n\n  clearLabels() {\n    this.labels = {}, this.commPtr = 0;\n  }\n\n  peepPass(e) {\n    this.peepOps = 0, this.peepDel = 0, this.peepCounts = {}, this.peepHole(), this.throwOnError = !0, this.finalEmit = !1, this.clearLabels(), this.iterLines(), assert(!this.checkStack || 0 == this.stack), this.finalEmit = !0, this.reallyFinalEmit = e || 0 == this.peepOps, this.iterLines(), this.stats += lf(\"; peep hole pass: {0} instructions removed and {1} updated\\n\", this.peepDel, this.peepOps - this.peepDel);\n  }\n\n  getLabels() {\n    return this.userLabelsCache || (this.userLabelsCache = mapMap(this.labels, (e, t) => t + this.baseOffset)), this.userLabelsCache;\n  }\n\n  emit(e) {\n    if (assert(null == this.buf), this.prepLines(e), !(this.errors.length > 0 || (this.clearLabels(), this.iterLines(), this.checkStack && 0 != this.stack && this.directiveError(lf(\"stack misaligned at the end of the file\")), this.errors.length > 0 || (this.ei.expandLdlit(this), this.clearLabels(), this.iterLines(), this.finalEmit = !0, this.reallyFinalEmit = this.disablePeepHole, this.iterLines(), this.errors.length > 0 || this.disablePeepHole)))) {\n      var _e1092 = 5;\n\n      for (var t = 0; t < _e1092 && (console.debug(\"Peephole OPT, pass \".concat(t)), this.peepPass(t == _e1092), 0 != this.peepOps); ++t) {\n        ;\n      }\n    }\n  }\n\n}\n\nclass AbstractProcessor {\n  constructor() {\n    this.file = null, this.encoders = {}, this.instructions = {};\n  }\n\n  toFnPtr(e, t, n) {\n    return e;\n  }\n\n  wordSize() {\n    return -1;\n  }\n\n  computeStackOffset(e, t) {\n    return t;\n  }\n\n  is32bit(e) {\n    return !1;\n  }\n\n  emit32(e, t, n) {\n    return null;\n  }\n\n  postProcessRelAddress(e, t) {\n    return t;\n  }\n\n  postProcessAbsAddress(e, t) {\n    return t;\n  }\n\n  peephole(e, t, n) {}\n\n  registerNo(e, t) {\n    return null;\n  }\n\n  getAddressFromLabel(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    return null;\n  }\n\n  isPop(e) {\n    return !1;\n  }\n\n  isPush(e) {\n    return !1;\n  }\n\n  isAddSP(e) {\n    return !1;\n  }\n\n  isSubSP(e) {\n    return !1;\n  }\n\n  testAssembler() {\n    assert(!1);\n  }\n\n  expandLdlit(e) {}\n\n  addEnc(e, t, n) {\n    var r = {\n      name: e,\n      pretty: t,\n      encode: n,\n      isRegister: /^\\$[sr]\\d/i.test(e),\n      isImmediate: /^\\$i\\d/i.test(e),\n      isRegList: /^\\$[sr]l\\d/i.test(e),\n      isLabel: /^\\$l[a-z]/i.test(e)\n    };\n    return this.encoders[e] = r, r;\n  }\n\n  inrange(e, t, n) {\n    return Math.floor(t) != t || t < 0 || t > e ? null : n;\n  }\n\n  inminmax(e, t, n, r) {\n    return Math.floor(n) != n || n < e || n > t ? null : r;\n  }\n\n  inseq(e, t) {\n    var n = e.indexOf(t);\n    return n < 0 ? null : n;\n  }\n\n  inrangeSigned(e, t, n) {\n    return Math.floor(t) != t || t < -(e + 1) || t > e ? null : n & (e << 1 | 1);\n  }\n\n  addInst(e, t, n, r) {\n    var a = new Instruction(this, e, t, n, r);\n    return this.instructions.hasOwnProperty(a.name) || (this.instructions[a.name] = []), this.instructions[a.name].push(a), a;\n  }\n\n  addInst32(e, t, n) {\n    var r = 2147483648;\n    return assert(!!(t & r)), assert(!!(n & r)), this.addInst(e, t &= ~r, n &= ~r, !0);\n  }\n\n}\n\nfunction tokenize(e) {\n  var t = [],\n      n = \"\";\n\n  e: for (var r = 0; r < e.length; ++r) {\n    switch (e[r]) {\n      case \"[\":\n      case \"]\":\n      case \"!\":\n      case \"{\":\n      case \"}\":\n      case \",\":\n        n && (t.push(n), n = \"\"), t.push(e[r]);\n        break;\n\n      case \" \":\n      case \"\\t\":\n      case \"\\r\":\n      case \"\\n\":\n        n && (t.push(n), n = \"\");\n        break;\n\n      case \"/\":\n        if (\"/\" == e[r + 1]) break e;\n        break;\n\n      case \";\":\n        break e;\n\n      default:\n        n += e[r];\n    }\n  }\n\n  return n && (t.push(n), n = \"\"), t[0] ? t : null;\n}\n\nfunction parseString(e) {\n  e = e.replace(/\\\\\\\\/g, \"\\\\B\").replace(/\\\\(['\\?])/g, (e, t) => t).replace(/\\\\[z0]/g, \"\\0\").replace(/\\\\x([0-9a-f][0-9a-f])/gi, (e, t) => \"\\\\u00\" + t).replace(/\\\\B/g, \"\\\\\\\\\");\n\n  try {\n    return JSON.parse(e);\n  } catch (e) {\n    return null;\n  }\n}\n\nfunction emitErr(e, t) {\n  return {\n    stack: null,\n    opcode: null,\n    error: e,\n    errorAt: t\n  };\n}\n\nfunction expectError(e, t) {\n  var n = new File$1(e);\n  n.emit(t), 0 == n.errors.length && oops(\"ASMTEST: expecting error for: \" + t);\n}\n\nfunction tohex(e) {\n  return e < 0 || e > 65535 ? (\"0x\" + e.toString(16)).toLowerCase() : (\"0x\" + (\"000\" + e.toString(16)).slice(-4)).toLowerCase();\n}\n\nfunction expect$1(e, t) {\n  var n = [],\n      r = t.replace(/^([0-9a-fA-F]{4,8})\\s/gm, (e, t) => (n.push(parseInt(t.slice(0, 4), 16)), 8 == t.length && n.push(parseInt(t.slice(4, 8), 16)), \"\")),\n      a = new File$1(e);\n  a.throwOnError = !0, a.disablePeepHole = !0, a.emit(r), a.errors.length > 0 && (console.debug(a.errors[0].message), oops(\"ASMTEST: not expecting errors\")), a.buf.length != n.length && oops(\"ASMTEST: wrong buf len\");\n\n  for (var _e1093 = 0; _e1093 < n.length; ++_e1093) {\n    a.buf[_e1093] != n[_e1093] && oops(\"ASMTEST: wrong buf content at \" + _e1093 + \" , exp:\" + tohex(n[_e1093]) + \", got: \" + tohex(a.buf[_e1093]));\n  }\n}\n\nvar asmDeps = {\n  softmax: [\"expf_asm\"]\n},\n    asmFns = {\n  expf_asm: \"\\n// based on https://stackoverflow.com/questions/29381117\\nexpf_asm:\\n\\tvldr.32\\ts15, .L10\\n\\tvcmpe.f32\\ts0, s15\\n\\tvmrs\\tAPSR_nzcv, FPSCR\\n\\tbmi\\t.L5\\n\\tvldr.32\\ts15, .L10+4\\n\\tvcmpe.f32\\ts0, s15\\n\\tvmrs\\tAPSR_nzcv, FPSCR\\n\\tbgt\\t.L9\\n\\tvldr.32\\ts15, .L10+8\\n\\tvldr.32\\ts9, .L10+12\\n\\tvldr.32\\ts6, .L10+16\\n\\tvldr.32\\ts7, .L10+20\\n\\tvldr.32\\ts10, .L10+24\\n\\tvldr.32\\ts8, .L10+28\\n\\tvldr.32\\ts11, .L10+32\\n\\tvldr.32\\ts12, .L10+36\\n\\tvldr.32\\ts13, .L10+40\\n\\tvmul.f32\\ts15, s0, s15\\n\\tvmov.f32\\ts14, #1.0\\n\\tvadd.f32\\ts15, s15, s9\\n\\tvsub.f32\\ts15, s15, s9\\n\\tvfma.f32\\ts0, s15, s6\\n\\tvcvt.s32.f32\\ts9, s15\\n\\tvfma.f32\\ts0, s15, s7\\n\\tvmov.f32\\ts15, s10\\n\\tvfma.f32\\ts15, s8, s0\\n\\tvmov\\tr3, s9\\t// int\\n\\tvfma.f32\\ts11, s15, s0\\n\\tvfma.f32\\ts12, s11, s0\\n\\tvfma.f32\\ts13, s12, s0\\n\\tvmov.f32\\ts15, s13\\n\\tvmov.f32\\ts13, s14\\n\\tvfma.f32\\ts13, s15, s0\\n\\tvfma.f32\\ts14, s13, s0\\n\\tvmov\\tr2, s14\\t// int\\n\\tadd\\tr3, r2, r3, lsl #23\\n\\tvmov\\ts0, r3\\t// int\\n\\tbx\\tlr\\n.L9:\\n\\tvldr.32\\ts15, .L10+44\\n\\tvmov.f32\\ts14, #1.0\\n\\tvdiv.f32\\ts0, s14, s15\\n\\tbx\\tlr\\n.L5:\\n\\tvldr.32\\ts0, .L10+44\\n\\tbx\\tlr\\n.L11:\\n\\t.align\\t2\\n.L10:\\n\\t.word\\t3265921024\\n\\t.word\\t1118699520\\n\\t.word\\t1069066811\\n\\t.word\\t1262485504\\n\\t.word\\t3207688704\\n\\t.word\\t3049242254\\n\\t.word\\t1007234926\\n\\t.word\\t984915968\\n\\t.word\\t1026207149\\n\\t.word\\t1042983464\\n\\t.word\\t1056964603\\n\\t.word\\t0\\n\",\n  softmax: \"\\nsoftmax:\\n\\tcmp\\tr1, #1\\n\\tpush\\t{r3, r4, r5, lr}\\n\\tvldr.32\\ts5, [r0]\\n\\tbls\\t.L13\\n\\tadds\\tr3, r0, #4\\n\\tadd\\tr2, r0, r1, lsl #2\\n.L16:\\n\\tvldmia.32\\tr3!, {s15}\\n\\tvcmp.f32\\ts15, s5\\n\\tvmrs\\tAPSR_nzcv, FPSCR\\n\\tit\\tgt\\n\\tvmovgt.f32\\ts5, s15\\n\\tcmp\\tr2, r3\\n\\tbne\\t.L16\\n.L17:\\n\\tmovs\\tr4, #0\\n\\tvmov\\ts4, r4\\n\\tmov\\tr5, r0\\n.L19:\\n\\tvldr.32\\ts0, [r5]\\n\\tvsub.f32\\ts0, s0, s5\\n\\tbl\\texpf_asm\\n\\tadds\\tr4, #1\\n\\tcmp\\tr1, r4\\n\\tvadd.f32\\ts4, s4, s0\\n\\tvstmia.32\\tr5!, {s0}\\n\\tbhi\\t.L19\\n\\tmovs\\tr3, #0\\n.L20:\\n\\tvldr.32\\ts14, [r0]\\n\\tvdiv.f32\\ts15, s14, s4\\n\\tadds\\tr3, #1\\n\\tcmp\\tr1, r3\\n\\tvstmia.32\\tr0!, {s15}\\n\\tbhi\\t.L20\\n\\tpop\\t{r3, r4, r5, pc}\\n.L13:\\n\\tcmp\\tr1, #0\\n\\tbne\\t.L17\\n\\tpop\\t{r3, r4, r5, pc}\\n\"\n},\n    unrollLimit = 10;\nvar OpCode, Reg, F16Mode;\n\nfunction assert$1(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"assertion failed\";\n  if (!e) throw new Error(\"ir: \" + t);\n}\n\nfunction addParamBytes(e, t) {\n  assert$1(0 == (e.weightPtr & t.length - 1)), e.weightBuffer || (e.weightBuffer = new Uint8Array(128));\n  var n = e.weightPtr + t.length;\n\n  if (n + 3 > e.weightBuffer.length) {\n    var _t743 = new Uint8Array(2 * n);\n\n    _t743.set(e.weightBuffer), e.weightBuffer = _t743;\n  }\n\n  e.weightBuffer.set(t, e.weightPtr), e.weightPtr = n;\n}\n\nfunction addFloat32(e, t) {\n  assert$1(null != t && !isNaN(t)), e.weightAsm += \".float \".concat(t, \"\\n\");\n  var n = float32ToUInt32(t);\n  addParamBytes(e, [n >> 0 & 255, n >> 8 & 255, n >> 16 & 255, n >> 24 & 255]);\n}\n\nfunction addFloat16(e, t) {\n  assert$1(null != t && !isNaN(t)), e.weightAsm += \".float16 \".concat(t, \"\\n\");\n  var n = float16toUInt16(t);\n  addParamBytes(e, [n >> 0 & 255, n >> 8 & 255]);\n}\n\nfunction alignWeights(e) {\n  for (; 3 & e.weightPtr;) {\n    addParamBytes(e, [0]);\n  }\n\n  e.weightAsm += \".balign 4\\n\";\n}\n\nfunction addWeight(e, t) {\n  e.opts.float16weights ? addFloat16(e, t) : addFloat32(e, t);\n}\n\nfunction addBias(e, t) {\n  addFloat32(e, t);\n}\n\nfunction weightOffset(e) {\n  return assert$1(0 == (3 & e.weightPtr)), e.weightPtr >> 2;\n}\n\nfunction stringifyComment(e) {\n  return e ? \"// \" + e.replace(/\\n/g, \"\\n// \") : \"\";\n}\n\nfunction indent(e) {\n  return \"  \" + e.replace(/\\n$/, \"\").replace(/\\n/g, \"\\n  \") + \"\\n\";\n}\n\nfunction numCycles(e) {\n  var t = 0,\n      n = null;\n\n  var r = e => e < 4096 ? 1 : 2;\n\n  for (var a of e) {\n    switch (a.opcode) {\n      case OpCode.comment:\n      case OpCode.label:\n        break;\n\n      case OpCode.repeat:\n        t += (numCycles(a.body) + 4 + (a.isDef ? 1 : 0)) * a.num + 1;\n        break;\n\n      case OpCode.loadWeightAddr:\n        t += 2 + r(4 * a.num);\n        break;\n\n      case OpCode.loadDataAddr:\n        t += r(4 * a.num + 8);\n        break;\n\n      case OpCode.addPtr:\n        null == a.src ? t += r(4 * a.num) : (1 != a.num && (a.src > Reg.Zero ? a.src == Reg.Zero + 1 || (a.src == Reg.Zero + 2 ? t++ : t += 2) : t++), t += 2), t += 1 == a.num ? 1 : 3;\n        break;\n\n      case OpCode.loadFConst:\n        t += 0 == a.num ? 2 : 1 == a.num ? 1 : 4;\n        break;\n\n      case OpCode.load:\n      case OpCode.store:\n        t += 1 + a.num;\n        break;\n\n      case OpCode.relu:\n        t += 6;\n        break;\n\n      case OpCode.vmax:\n        t += 4, a.src != a.dst && t++;\n        break;\n\n      case OpCode.vmul:\n      case OpCode.vadd:\n        t += a.src === n || a.srcAlt === n ? 2 : 1, n = a.dst;\n        break;\n\n      case OpCode.vcvt:\n        t += 1;\n        break;\n\n      case OpCode.fcall:\n        t += \"softmax\" == a.fname ? 200 + 150 * a.num : 500 + 500 * a.num;\n        break;\n\n      default:\n        throw new Error(\"bad op \" + a.opcode);\n    }\n  }\n\n  return t;\n}\n\nfunction toThumb(e, t) {\n  var n;\n  var r = {},\n      a = !!e.opts.testInput && !!e.opts.includeTest;\n  var s = \"\";\n\n  var o = e => 4 * (e + 2),\n      i = [\"0x30470f62  // magic\", \"0x46344c4d  // more magic; ML4F\", \"_start_model-_header // header size\", \"_end-_header // total size of compiled object\", \"_weights-_header // offset of weights\", a ? \"_testInput-_header\" : \"0 // no tests\", a ? \"_testOutput-_header\" : \"0 // no tests\", \"\".concat(o(e.arenaSize), \" // arena size\"), \"\".concat(o(0), \"  // offset of input data\"), \"1 // input type - float32\", \"\".concat(o(e.outputOffset), \"  // offset of output data\"), \"1 // output type - float32\"];\n\n  for (var _e1094 = 0; _e1094 < 4; ++_e1094) {\n    i.push(\"0 // padding\");\n  }\n\n  h(e.inputShape, \"input\"), h(e.outputShape, \"output\");\n  var l = \"\";\n\n  for (; (null === (n = t[0]) || void 0 === n ? void 0 : n.opcode) == OpCode.comment;) {\n    l += stringifyComment(t.shift().fname) + \"\\n\";\n  }\n\n  var u = {},\n      c = \"\".concat(stringifyComment(e.stats), \"\\n    .cpu cortex-m4\\n    .text\\n    .arch armv7e-m\\n    .syntax unified\\n    .thumb\\n    .thumb_func\\n    .fpu fpv4-sp-d16\\n// ABI: r0 -> points to magic, r1 -> points to RAM arena\\n_header:\\n\");\n\n  for (var _e1095 of i) {\n    f(\".word \".concat(_e1095));\n  }\n\n  var p = 0;\n  u[Reg.InputPtr] = 1, u[Reg.OutputPtr] = 2, u[Reg.KernelPtr] = 3, u[Reg.DataDescPtr] = 7, f(\"_start_model:\"), f(\"push {r4,r5,r6,r7,r8,r9,r10,r11,r12,lr}\"), f(\"mov \".concat($(Reg.DataDescPtr), \", r1\")), f(\"ldr r1, [r0, #4*4] // weight offset\"), f(\"adds r1, r0 // weight addr\"), f(\"str r1, [\".concat($(Reg.DataDescPtr), \", #0]\")), f(\"movs r1, #0\"), f(\"str r1, [\".concat($(Reg.DataDescPtr), \", #4]\")), I(t), f(\"pop {r4,r5,r6,r7,r8,r9,r10,r11,r12,pc}\");\n\n  for (var _e1096 of Object.keys(r)) {\n    for (var _t744 of asmDeps[_e1096] || []) {\n      r[_t744] = !0;\n    }\n  }\n\n  for (var _e1097 of Object.keys(r)) {\n    f(asmFns[_e1097]);\n  }\n\n  return f(\".balign 4\"), f(\"_weights:\\n\".concat(e.weightAsm)), a && (d(\"_testInput\", e.opts.testInput), d(\"_testOutput\", e.opts.testOutput)), f(\"_end:\"), c;\n\n  function d(e, t) {\n    f(\"\".concat(e, \":\"));\n\n    for (var _e1098 of t) {\n      f(\".float \".concat(_e1098));\n    }\n  }\n\n  function h(e, t) {\n    for (var _n440 of e) {\n      null != _n440 && i.push(\"\".concat(_n440, \" // \").concat(t, \" shape\"));\n    }\n\n    i.push(\"0 // end of \".concat(t, \" shape\"));\n  }\n\n  function m(e, t) {\n    assert$1(!u[e]);\n    var n = {},\n        r = {};\n\n    for (var _e1099 of Object.keys(u)) {\n      n[_e1099] = u[_e1099], r[n[_e1099]] = !0;\n    }\n\n    var a = -1;\n\n    for (var _e1100 = 4; _e1100 <= 12; ++_e1100) {\n      if (!r[_e1100]) {\n        a = _e1100;\n        break;\n      }\n    }\n\n    if (a < 0 && g(\"can't alloc \" + e), u[e] = a, t) {\n      var _e1101 = s;\n\n      try {\n        s += \"    \", t();\n      } finally {\n        s = _e1101, u = n;\n      }\n    }\n  }\n\n  function f(e) {\n    y(e) && g(\"wrong reg: \" + e), c += s + e + \"\\n\";\n  }\n\n  function g(e) {\n    throw new Error(\"internal thumb error: \" + e);\n  }\n\n  function $(e) {\n    if (null == e) return \"<fake>\";\n    if (e <= Reg.S31) return \"s\" + (e - Reg.S0);\n    if (e >= Reg.Zero) return \"#\" + (e - Reg.Zero);\n    var t = u[e];\n    return null == t ? \"<fake:\" + regName(e) + \">\" : \"r\" + t;\n  }\n\n  function y(e) {\n    return e.indexOf(\"<fake\") >= 0;\n  }\n\n  function b(e) {\n    return /^r[0-7]$/.test(e);\n  }\n\n  function x(e, t) {\n    t <= 255 && b(e) ? f(\"movs \".concat(e, \", #\").concat(t)) : f(\"movw \".concat(e, \", #\").concat(t));\n  }\n\n  function v(e, t, n) {\n    Math.abs(n) < 4096 ? f(n < 0 ? \"subw \".concat(e, \", \").concat(t, \", #\").concat(-n) : \"addw \".concat(e, \", \").concat(t, \", #\").concat(n)) : (assert$1(t != e), x(e, n), f(\"adds \".concat(e, \", \").concat(t, \", \").concat(e)));\n  }\n\n  function I(e) {\n    for (var _t745 of e) {\n      S(_t745);\n    }\n  }\n\n  function C(e) {\n    return \"{\" + range(e.num).map(t => $(e.dst + t)).join(\",\") + \"}\";\n  }\n\n  function S(e) {\n    var t = $(e.dst);\n    var n = $(e.src),\n        a = $(e.srcAlt),\n        s = e.increment ? \"!\" : \"\";\n\n    switch (e.opcode) {\n      case OpCode.label:\n        f(\"\".concat(e.fname, \":\"));\n        break;\n\n      case OpCode.comment:\n        f(stringifyComment(e.fname));\n        break;\n\n      case OpCode.repeat:\n        assert$1(e.num >= 1), m(e.dst, () => {\n          t = $(e.dst);\n          var n = \".l.\" + p++;\n          x(t, e.isDef ? 0 : e.num), f(\"\".concat(n, \":  // rep \").concat(e.num)), I(e.body), e.isDef ? (f(\"adds \".concat(t, \", #1\")), f(\"cmp \".concat(t, \", #\").concat(e.num)), f(\"blt \".concat(n))) : (b(t) ? f(\"subs \".concat(t, \", #1\")) : f(\"subs \".concat(t, \", \").concat(t, \", #1\")), f(\"bne \".concat(n)));\n        });\n        break;\n\n      case OpCode.loadWeightAddr:\n        f(\"ldr r0, [\".concat($(Reg.DataDescPtr), \", #0]\")), v(t, \"r0\", 4 * e.num);\n        break;\n\n      case OpCode.loadDataAddr:\n        v(t, $(Reg.DataDescPtr), o(e.num));\n        break;\n\n      case OpCode.addPtr:\n        if (y(t) && e.isDef && (m(e.dst), t = $(e.dst)), null == e.src) v(t, a, 4 * e.num);else {\n          if (1 != e.num) {\n            if (x(\"r0\", 4 * e.num), \"#\" == n[0]) {\n              var _e1102 = +n.slice(1);\n\n              0 == _e1102 ? x(\"r0\", 0) : 1 == _e1102 || (2 == _e1102 ? f(\"adds r0,r0\") : (assert$1(t != a), x(t, _e1102), f(\"muls r0, \".concat(t))));\n            } else f(\"muls r0, \".concat(n));\n          } else \"#\" == n[0] ? x(\"r0\", +n.slice(1) << 2) : f(\"lsls r0, \".concat(n, \", #2\"));\n          f(\"adds \".concat(t, \", \").concat(a, \", r0\"));\n        }\n        break;\n\n      case OpCode.loadFConst:\n        0 == e.num ? f(\"vldr \".concat(t, \", [\").concat($(Reg.DataDescPtr), \", #4]\")) : e.num == Number.NEGATIVE_INFINITY ? (f(\"movw r0, #0xff80\"), f(\"lsls r0, r0, #16\"), f(\"vmov \".concat(t, \", r0\"))) : f(\"vmov \".concat(t, \", #\").concat(e.num, \"e+0\"));\n        break;\n\n      case OpCode.load:\n        assert$1(e.f16Mode != F16Mode.On), f(\"vldm \".concat(n).concat(s, \", \").concat(C(e)));\n        break;\n\n      case OpCode.store:\n        f(\"vstm \".concat(n).concat(s, \", \").concat(C(e)));\n        break;\n\n      case OpCode.relu:\n        f(\"ldr r0, [\".concat(t, \", #0]\")), f(\"cmp r0, #0\"), f(\"it lt\"), f(\"movwlt r0, #0\"), f(\"stm \".concat(t, \"!, {r0}\"));\n        break;\n\n      case OpCode.vmul:\n        f(\"vmul.f32 \".concat(t, \", \").concat(n, \", \").concat(a));\n        break;\n\n      case OpCode.vadd:\n        f(\"vadd.f32 \".concat(t, \", \").concat(n, \", \").concat(a));\n        break;\n\n      case OpCode.vcvt:\n        f(\"\".concat(e.fname, \" \").concat(t, \", \").concat(n));\n        break;\n\n      case OpCode.vmax:\n        assert$1(t != a), n != t && f(\"vmov \".concat(t, \", \").concat(n)), f(\"vcmp.f32 \".concat(t, \", \").concat(a)), f(\"vmrs APSR_nzcv, FPSCR\"), f(\"it mi\"), f(\"vmovmi.f32 \".concat(t, \", \").concat(a));\n        break;\n\n      case OpCode.fcall:\n        f(\"mov r0, \".concat(t)), x(\"r1\", e.num), f(\"bl \".concat(e.fname)), r[e.fname] = !0;\n        break;\n\n      default:\n        g(\"bad op \" + e.opcode);\n    }\n  }\n}\n\nfunction toJS(e, t) {\n  var n = \"\";\n\n  if (t.opcode == OpCode.repeat) {\n    var r = regName(t.dst);\n    n = \"for (let \".concat(r, \" = 0; \").concat(r, \" < \").concat(t.num, \"; \").concat(r, \"++) {\\n\").concat(indent(toJSs(e, t.body)), \"}\\n\");\n  } else n = stringify1(t);\n\n  return n.indexOf(\"???\") >= 0 && oops(\"invalid register in: \" + n), n;\n}\n\nfunction stringify(e) {\n  return e.map(stringify1).join(\"\");\n}\n\nfunction stringify1(e) {\n  var t = null == e.dst ? null : regName(e.dst),\n      n = null == e.src ? null : regName(e.src),\n      r = null == e.srcAlt ? null : regName(e.srcAlt);\n\n  switch (e.opcode) {\n    case OpCode.label:\n      return stringifyComment(\"label: \" + e.fname) + \"\\n\";\n\n    case OpCode.comment:\n      return isBreak(e) ? \"debugger\\n\" : stringifyComment(e.fname) + \"\\n\";\n\n    case OpCode.repeat:\n      return \"for (let \".concat(t, \" = 0; \").concat(t, \" < \").concat(e.num, \"; \").concat(t, \"++) {\\n\").concat(indent(stringify(e.body)), \"}\\n\");\n\n    case OpCode.loadWeightAddr:\n      return \"\".concat(t, \" = weightOff + \").concat(e.num, \"\\n\");\n\n    case OpCode.loadDataAddr:\n      return \"\".concat(t, \" = dataOff + \").concat(e.num, \"\\n\");\n\n    case OpCode.addPtr:\n      return null == e.src ? \"\".concat(t, \" = \").concat(r, \" + \").concat(e.num, \"\\n\") : \"\".concat(t, \" = \").concat(r, \" + \").concat(n).concat(1 == e.num ? \"\" : \" * \" + e.num, \"\\n\");\n\n    case OpCode.loadFConst:\n      return \"\".concat(t, \" = \").concat(e.num, \"\\n\");\n\n    case OpCode.load:\n      {\n        var _t746 = \"\",\n            _r438 = e.dst + 0;\n\n        if (e.increment) for (var a = 0; a < e.num; ++a) {\n          _t746 += \"\".concat(regName(_r438++), \" = \").concat(e.fname || \"mem\", \"[\").concat(n, \"++]\\n\");\n        } else for (var _a309 = 0; _a309 < e.num; ++_a309) {\n          _t746 += \"\".concat(regName(_r438++), \" = mem[\").concat(n, \" + \").concat(_a309, \"]\\n\");\n        }\n        return _t746;\n      }\n\n    case OpCode.store:\n      {\n        var _t747 = \"\",\n            _r439 = e.dst + 0;\n\n        if (e.increment) for (var _a310 = 0; _a310 < e.num; ++_a310) {\n          _t747 += \"mem[\".concat(n, \"++] = \").concat(regName(_r439++), \"\\n\");\n        } else for (var _a311 = 0; _a311 < e.num; ++_a311) {\n          _t747 += \"mem[\".concat(n, \" + \").concat(_a311, \"] = \").concat(regName(_r439++), \"\\n\");\n        }\n        return _t747;\n      }\n\n    case OpCode.relu:\n      return \"if (mem[\".concat(t, \"] < 0) mem[\").concat(t, \"] = 0; \").concat(t, \"++\\n\");\n\n    case OpCode.vmul:\n      return \"\".concat(t, \" = f32(\").concat(n, \" * \").concat(r, \")\\n\");\n\n    case OpCode.vadd:\n      return \"\".concat(t, \" = f32(\").concat(n, \" + \").concat(r, \")\\n\");\n\n    case OpCode.vmax:\n      return \"\".concat(t, \" = Math.max(\").concat(n, \", \").concat(r, \")\\n\");\n\n    case OpCode.fcall:\n      return \"\".concat(e.fname, \"(\").concat(t, \", \").concat(e.num, \")\\n\");\n\n    case OpCode.vcvt:\n      return \"\".concat(t, \" = rt.\").concat(e.fname.replace(/\\./g, \"_\"), \"(\").concat(n, \")\\n\");\n\n    default:\n      throw new Error(\"bad op \" + e.opcode);\n  }\n}\n\nfunction regName(e) {\n  if (e <= Reg.S31) return \"s\" + (e - Reg.S0);\n  if (e >= Reg.Zero) return \"\" + (e - Reg.Zero);\n  if (e >= Reg.Tmp0) return \"tmp\" + (e - Reg.Tmp0);\n  if (e >= Reg.Index0) return \"idx\" + (e - Reg.Index0);\n\n  switch (e) {\n    case Reg.InputPtr:\n      return \"input\";\n\n    case Reg.KernelPtr:\n      return \"kernel\";\n\n    case Reg.OutputPtr:\n      return \"output\";\n\n    default:\n      return \"???\" + e;\n  }\n}\n\nfunction toJSs(e, t) {\n  return t.map(t => toJS(e, t)).join(\"\");\n}\n\n!function (e) {\n  e[e.comment = 0] = \"comment\", e[e.label = 1] = \"label\", e[e.repeat = 2] = \"repeat\", e[e.loadWeightAddr = 3] = \"loadWeightAddr\", e[e.loadDataAddr = 4] = \"loadDataAddr\", e[e.addPtr = 5] = \"addPtr\", e[e.loadFConst = 6] = \"loadFConst\", e[e.load = 7] = \"load\", e[e.store = 8] = \"store\", e[e.vmul = 9] = \"vmul\", e[e.vmax = 10] = \"vmax\", e[e.vadd = 11] = \"vadd\", e[e.vcvt = 12] = \"vcvt\", e[e.relu = 13] = \"relu\", e[e.fcall = 14] = \"fcall\";\n}(OpCode || (OpCode = {})), function (e) {\n  e[e.S0 = 0] = \"S0\", e[e.S1 = 1] = \"S1\", e[e.S15 = 15] = \"S15\", e[e.S31 = 32] = \"S31\", e[e.InputPtr = 200] = \"InputPtr\", e[e.OutputPtr = 201] = \"OutputPtr\", e[e.KernelPtr = 202] = \"KernelPtr\", e[e.DataDescPtr = 203] = \"DataDescPtr\", e[e.Index0 = 300] = \"Index0\", e[e.Tmp0 = 400] = \"Tmp0\", e[e.Zero = 500] = \"Zero\", e[e.One = 501] = \"One\";\n}(Reg || (Reg = {})), function (e) {\n  e[e.Off = 0] = \"Off\", e[e.On = 1] = \"On\", e[e.Even = 2] = \"Even\", e[e.Odd = 3] = \"Odd\";\n}(F16Mode || (F16Mode = {}));\nvar repIdx = 0;\n\nfunction repeatIdx(e, t) {\n  var n = Reg.Index0 + repIdx++;\n  return {\n    opcode: OpCode.repeat,\n    dst: n,\n    num: e,\n    body: t(n),\n    isDef: !0\n  };\n}\n\nfunction repeat(e, t) {\n  var n = repeatIdx(e, t);\n  return n.isDef = !1, n;\n}\n\nfunction comment(e) {\n  return {\n    opcode: OpCode.comment,\n    fname: e\n  };\n}\n\nfunction label(e) {\n  return {\n    opcode: OpCode.label,\n    fname: e\n  };\n}\n\nfunction loadWeightAddr(e, t) {\n  return assert$1(t >= 0), {\n    opcode: OpCode.loadWeightAddr,\n    dst: e,\n    num: t\n  };\n}\n\nfunction relaxWeights() {\n  var e = addPtr(Reg.KernelPtr, null, 0);\n  return e.fname = \"relax\", e;\n}\n\nfunction loadDataAddr(e, t) {\n  return assert$1(t >= 0), {\n    opcode: OpCode.loadDataAddr,\n    dst: e,\n    num: t\n  };\n}\n\nfunction addPtr(e, t) {\n  var n = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n  var r = arguments.length > 3 ? arguments[3] : undefined;\n  return r || (r = e), {\n    opcode: OpCode.addPtr,\n    dst: e,\n    src: t,\n    srcAlt: r,\n    num: n\n  };\n}\n\nfunction load0(e) {\n  return {\n    opcode: OpCode.loadFConst,\n    dst: e,\n    num: 0\n  };\n}\n\nfunction loadMInf(e) {\n  return {\n    opcode: OpCode.loadFConst,\n    dst: e,\n    num: Number.NEGATIVE_INFINITY\n  };\n}\n\nfunction load(e, t, n, r) {\n  return {\n    opcode: OpCode.load,\n    dst: e,\n    src: n,\n    num: t,\n    increment: r\n  };\n}\n\nfunction load16(e, t, n) {\n  return {\n    opcode: OpCode.load,\n    dst: e,\n    src: n,\n    num: t,\n    increment: !0,\n    f16Mode: F16Mode.On\n  };\n}\n\nfunction loadWeight(e, t, n) {\n  var r = Reg.KernelPtr;\n  return e.opts.float16weights ? load16(t, n, r) : load(t, n, r, !0);\n}\n\nfunction store(e, t, n, r) {\n  return {\n    opcode: OpCode.store,\n    src: e,\n    dst: t,\n    num: n,\n    increment: r\n  };\n}\n\nfunction relu(e) {\n  return {\n    opcode: OpCode.relu,\n    dst: e,\n    increment: !0\n  };\n}\n\nfunction vmul(e, t, n) {\n  return {\n    opcode: OpCode.vmul,\n    dst: e,\n    src: t,\n    srcAlt: n\n  };\n}\n\nfunction vmax(e, t, n) {\n  return n == e && ([t, n] = [n, t]), {\n    opcode: OpCode.vmax,\n    dst: e,\n    src: t,\n    srcAlt: n\n  };\n}\n\nfunction vadd(e, t, n) {\n  return {\n    opcode: OpCode.vadd,\n    dst: e,\n    src: t,\n    srcAlt: n\n  };\n}\n\nfunction vcvt(e, t, n) {\n  return {\n    opcode: OpCode.vcvt,\n    dst: t,\n    src: n,\n    fname: e\n  };\n}\n\nfunction fcall(e, t, n) {\n  return {\n    opcode: OpCode.fcall,\n    fname: e,\n    dst: t,\n    num: n\n  };\n}\n\nfunction flatten() {\n  var t = [],\n      n = e => {\n    e && t.push(e);\n  };\n\n  for (var _len11 = arguments.length, e = new Array(_len11), _key11 = 0; _key11 < _len11; _key11++) {\n    e[_key11] = arguments[_key11];\n  }\n\n  for (var _t748 of e) {\n    if (Array.isArray(_t748)) for (var _e1103 of _t748) {\n      if (Array.isArray(_e1103)) for (var _t749 of _e1103) {\n        n(_t749);\n      } else n(_e1103);\n    } else n(_t748);\n  }\n\n  return t;\n}\n\nfunction isRelax(e) {\n  return e.opcode == OpCode.addPtr && \"relax\" == e.fname;\n}\n\nfunction isBreak(e) {\n  return e.opcode == OpCode.comment && \"BREAK\" == e.fname;\n}\n\nfunction isOddF16(e) {\n  var t = 0;\n\n  for (var n of e) {\n    n.opcode == OpCode.load && n.f16Mode && (t += n.num), isRelax(n) && (t = t + 1 & -2);\n  }\n\n  return !!(1 & t);\n}\n\nfunction fixupAndMarkF16(e) {\n  return function e(t) {\n    var n = [];\n\n    for (var r of t) {\n      if (r.opcode == OpCode.repeat) assert$1(!isOddF16(r.body)), r.body = e(r.body), n.push(r);else if (r.opcode == OpCode.load && r.f16Mode) {\n        var _e1104 = 0,\n            _t750 = !1;\n\n        r.f16Mode == F16Mode.Odd ? (_e1104 = 1 + (r.num >> 1), n.push(addPtr(r.src, Reg.One, -1)), 1 & r.num || (_t750 = !0)) : r.f16Mode == F16Mode.Even ? (_e1104 = r.num + 1 >> 1, 1 & r.num && (_t750 = !0)) : assert$1(!1);\n        var a = load(r.dst, _e1104, r.src, !0);\n        a.fname = \"memU32\", n.push(a);\n        var s = r.dst + _e1104 - 1;\n\n        for (var _e1105 = r.num - 1; _e1105 >= 0; --_e1105) {\n          n.push(vcvt(_t750 ? \"vcvtb.f32.f16\" : \"vcvtt.f32.f16\", r.dst + _e1105, s)), _t750 && s--, _t750 = !_t750;\n        }\n      } else n.push(r);\n    }\n\n    return n;\n  }(function e(t) {\n    var n = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n    var r = n ? 1 : 0;\n\n    var a = () => !!(1 & r),\n        s = [];\n\n    for (var _n441 of t) {\n      if (_n441 = cloneOp(_n441), _n441.opcode != OpCode.repeat) s.push(_n441), _n441.opcode == OpCode.load && _n441.f16Mode && (assert$1(_n441.f16Mode == F16Mode.On), _n441.f16Mode = a() ? F16Mode.Odd : F16Mode.Even, r += _n441.num), isRelax(_n441) && (r = r + 1 & -2);else {\n        if (0 == _n441.num) continue;\n\n        var _t751 = a(),\n            o = _n441.body,\n            i = e(o, _t751);\n\n        if (_n441.body = i.ops, i.odd != _t751) {\n          if (_n441.isDef && (console.log(stringify([_n441])), assert$1(!1)), 1 == _n441.num) pushRange(s, i.ops), r++;else {\n            var _a312 = 1 & _n441.num;\n\n            _n441.num >>= 1;\n            var l = e(o, i.odd);\n            assert$1(l.odd == _t751), _n441.body = i.ops.concat(l.ops), s.push(_n441), _a312 && (pushRange(s, e(o, _t751).ops), r++);\n          }\n        } else s.push(_n441);\n      }\n    }\n\n    return {\n      ops: s,\n      odd: !!(1 & r)\n    };\n  }(e).ops);\n}\n\nfunction cloneOp(e) {\n  return {\n    opcode: e.opcode,\n    dst: e.dst,\n    src: e.src,\n    srcAlt: e.srcAlt,\n    isDef: e.isDef,\n    f16Mode: e.f16Mode,\n    increment: e.increment,\n    num: e.num,\n    body: e.body,\n    fname: e.fname\n  };\n}\n\nfunction optimize(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n\n  var n = e => e && null != t[e] ? t[e] : e,\n      r = [];\n\n  for (var a of e) {\n    switch (a = cloneOp(a), a.dst = n(a.dst), a.src = n(a.src), a.srcAlt = n(a.srcAlt), a.opcode) {\n      case OpCode.repeat:\n        if (0 == a.num) ;else if (1 == a.num) t[a.dst] = Reg.Zero, pushRange(r, optimize(a.body, t));else {\n          a.body = optimize(a.body, t);\n\n          var _e1106 = !a.isDef && 2 * a.body.length < unrollLimit;\n\n          if (a.num * a.body.length < 2 * unrollLimit) for (var _e1107 = 0; _e1107 < a.num; ++_e1107) {\n            t[a.dst] = Reg.Zero + _e1107, pushRange(r, optimize(a.body, t));\n          } else if (_e1106) {\n            var _e1108 = unrollLimit / a.body.length | 0,\n                _t752 = a.body.slice();\n\n            for (var _n443 = 1; _n443 < _e1108; ++_n443) {\n              pushRange(a.body, _t752);\n            }\n\n            var _n442 = a.num / _e1108 | 0;\n\n            r.push(a);\n            var s = a.num - _n442 * _e1108;\n            a.num = _n442;\n\n            for (var _e1109 = 0; _e1109 < s; ++_e1109) {\n              pushRange(r, _t752);\n            }\n          } else r.push(a);\n        }\n        break;\n\n      case OpCode.addPtr:\n        (a.dst != a.srcAlt || 0 != a.num && a.src != Reg.Zero) && r.push(a);\n        break;\n\n      default:\n        r.push(a);\n    }\n  }\n\n  return r;\n}\n\nfunction reset() {\n  repIdx = 0;\n}\n\nvar inited$1 = !1;\nvar compilers = {\n  Conv2D: {\n    compile: compileConv,\n    computePaddedInputShape: paddingConv\n  },\n  Conv1D: {\n    compile: compileConv,\n    computePaddedInputShape: paddingConv\n  },\n  MaxPooling1D: {\n    compile: compileMaxPooling,\n    computePaddedInputShape: paddingPool,\n    needsMInfPadding: !0\n  },\n  MaxPooling2D: {\n    compile: compileMaxPooling,\n    computePaddedInputShape: paddingPool,\n    needsMInfPadding: !0\n  },\n  Dense: {\n    compile: compileDense\n  },\n  Dropout: {},\n  Flatten: {},\n  InputLayer: {},\n  Reshape: {}\n},\n    numFPRegs = 32,\n    numTmpRegs = 8;\n\nfunction unsupported(e) {\n  throw new Error(\"Unsupported operator or config: \" + e);\n}\n\nfunction assert$2(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"assertion failed\";\n  e || unsupported(t);\n}\n\nfunction getLayerInfo(e) {\n  var t = e.__ml4f_info;\n  return t || (t = {\n    layer: e\n  }, e.__ml4f_info = t), t;\n}\n\nfunction validateConfig(e) {\n  var t = e.layer.getConfig();\n  e.model.opts.verbose && console.log(e.inputShape, e.outputShape, t), 4 == e.inputShape.length ? (4 != e.inputShape.length && 3 != e.inputShape.length && unsupported(\"inputShape: \" + e.inputShape.length), \"channelsLast\" != t.dataFormat && unsupported(\"dataFormat: \" + t.dataFormat)) : 3 != e.inputShape.length && unsupported(\"inputShape: \" + e.inputShape.length), t.dtype && \"float32\" != t.dtype && unsupported(\"dtype: \" + t.dtype);\n}\n\nfunction addActivation(e, t) {\n  var n = t.layer.getConfig(),\n      r = shapeElts(t.outputShape);\n  \"linear\" != n.activation && (e.push(loadDataAddr(Reg.OutputPtr, t.outputOff)), \"relu\" == n.activation ? e.push(repeat(r, () => [relu(Reg.OutputPtr)])) : \"softmax\" == n.activation ? e.push(fcall(\"softmax\", Reg.OutputPtr, r)) : unsupported(\"activation: \" + n.activation));\n}\n\nfunction paddingConv(e) {\n  var t = e.layer.getConfig(),\n      n = e.inputShape.slice();\n\n  for (var r = 1; r <= t.kernelSize.length; ++r) {\n    var a = t.strides[r - 1],\n        s = e.outputShape[r] * a + t.kernelSize[r - 1] - a;\n    assert$2(s >= n[r]), n[r] = s;\n  }\n\n  return n;\n}\n\nfunction paddingPool(e) {\n  var t = e.layer.getConfig(),\n      n = e.inputShape.slice();\n\n  for (var r = 1; r <= t.poolSize.length; ++r) {\n    var a = e.outputShape[r] * t.strides[r - 1];\n    a > n[r] && (n[r] = a);\n  }\n\n  return n;\n}\n\nfunction compileConv(e) {\n  var t = e.layer.getConfig(),\n      n = numFPRegs >> 1,\n      r = numFPRegs >> 1;\n  validateConfig(e);\n\n  var a = 2 == t.kernelSize.length,\n      s = e.layer.weights[0].read().arraySync(),\n      o = a ? s : [s],\n      i = e => (e = e.slice(), a || e.unshift(1), e),\n      [l, u] = i(t.kernelSize),\n      [c, p] = i(t.strides),\n      [d, h, m] = i(e.inputShape.slice(1)),\n      [f, g, $] = i(e.outputShape.slice(1));\n\n  assert$2(l <= d, \"KH2\"), assert$2(u <= h, \"KW2\"), assert$2(o.length == l, \"KH\"), assert$2(o[0].length == u, \"KW\"), assert$2(o[0][0].length == m, \"CH\"), assert$2(o[0][0][0].length == t.filters, \"F\"), assert$2($ == t.filters, \"FF\");\n  var y = e.model,\n      b = weightOffset(y),\n      x = t.useBias ? e.layer.weights[1].read().arraySync() : null;\n\n  for (var _e1110 = 0; _e1110 < t.filters; _e1110++) {\n    x && addBias(y, x[_e1110]);\n\n    for (var _t753 = 0; _t753 < l; _t753++) {\n      for (var _n444 = 0; _n444 < u; _n444++) {\n        for (var _r440 = 0; _r440 < m; ++_r440) {\n          addWeight(y, o[_t753][_n444][_r440][_e1110]);\n        }\n      }\n\n      alignWeights(y);\n    }\n  }\n\n  var v = [loadWeightAddr(Reg.KernelPtr, b), repeatIdx(t.filters, a => {\n    var s = [],\n        o = t => {\n      t.push(loadDataAddr(Reg.OutputPtr, e.outputOff)), t.push(addPtr(Reg.OutputPtr, a));\n    };\n\n    return o(s), s.push(t.useBias ? load(Reg.S0, 1, Reg.KernelPtr, !0) : load0(Reg.S0)), s.push(repeat(g * f, () => [store(Reg.OutputPtr, Reg.S0, 1, !1), addPtr(Reg.OutputPtr, null, t.filters)])), s.push(repeatIdx(l, a => {\n      var s = [],\n          i = u * m;\n      var l = 0;\n\n      var _loop58 = function _loop58(_u62) {\n        l = i - _u62, l > r && (l = r), s.push(loadWeight(y, n, l)), s.push(loadDataAddr(Reg.InputPtr, e.inputOff + _u62)), s.push(addPtr(Reg.InputPtr, a, h * m)), o(s);\n        var d = p * m,\n            $ = c * h * m;\n        s.push(repeat(f, () => [repeat(g, () => flatten(load(Reg.S0, l, Reg.InputPtr, !0), addPtr(Reg.InputPtr, null, d - l), range(l + 1).map(e => [e < l ? vmul(e, e, e + n) : null, e >= 2 ? vadd(Reg.S0, Reg.S0, e - 1) : null]), load(Reg.S1, 1, Reg.OutputPtr, !1), vadd(Reg.S0, Reg.S0, Reg.S1), store(Reg.OutputPtr, Reg.S0, 1, !1), addPtr(Reg.OutputPtr, null, t.filters))), addPtr(Reg.InputPtr, null, $ - g * d)]));\n      };\n\n      for (var _u62 = 0; _u62 < i; _u62 += l) {\n        _loop58(_u62);\n      }\n\n      return s.push(relaxWeights()), s;\n    })), s.push(relaxWeights()), s;\n  })];\n  return addActivation(v, e), v;\n}\n\nfunction compileMaxPooling(e) {\n  var t = e.layer.getConfig(),\n      n = 2 == t.poolSize.length;\n  validateConfig(e);\n\n  var r = e => (e = e.slice(), n || e.unshift(1), e),\n      [a, s] = r(t.poolSize),\n      [o, i] = r(t.strides),\n      [l, u, c] = r(e.inputShape.slice(1)),\n      [p, d, h] = r(e.outputShape.slice(1));\n\n  assert$2(a <= l, \"KH2\"), assert$2(s <= u, \"KW2\"), assert$2(c == h, \"CH\"), a - 1 > numTmpRegs && unsupported(\"too high MaxPool2D area\");\n  var m = u * c;\n  return [repeatIdx(c, t => {\n    var n = [loadDataAddr(Reg.OutputPtr, e.outputOff), addPtr(Reg.OutputPtr, t), loadDataAddr(Reg.InputPtr, e.inputOff), addPtr(Reg.InputPtr, t)],\n        r = range(a - 1).map(e => Reg.Tmp0 + e);\n    r.unshift(Reg.InputPtr);\n\n    for (var _e1111 = 1; _e1111 < a; ++_e1111) {\n      var _t754 = addPtr(r[_e1111], null, m * _e1111, Reg.InputPtr);\n\n      _t754.isDef = !0, n.push(_t754);\n    }\n\n    return n.push(repeat(p, () => flatten(repeat(d, () => {\n      var e = [];\n\n      for (var _t755 = 0; _t755 < a; ++_t755) {\n        for (var _n445 = 0; _n445 < s; ++_n445) {\n          var _a313 = 0 == _t755 && 0 == _n445 ? Reg.S0 : Reg.S1;\n\n          e.push(load(_a313, 1, r[_t755], !0), addPtr(r[_t755], null, c - 1)), _a313 != Reg.S0 && e.push(vmax(Reg.S0, Reg.S0, _a313));\n        }\n\n        e.push(addPtr(r[_t755], null, (i - s) * c));\n      }\n\n      return e.push(store(Reg.OutputPtr, Reg.S0, 1, !0), addPtr(Reg.OutputPtr, null, c - 1)), e;\n    }), r.map(e => addPtr(e, null, o * m - d * i * c))))), n;\n  })];\n}\n\nfunction compileDense(e) {\n  var t = e.layer.getConfig(),\n      n = (numFPRegs >> 1) - 2,\n      r = Reg.S1,\n      a = r + n;\n  2 != e.inputShape.length && unsupported(\"inputShape: \" + e.inputShape.length), t.dtype && \"float32\" != t.dtype && unsupported(\"dtype: \" + t.dtype);\n  var s = e.layer.weights[0].read().arraySync(),\n      o = e.inputShape[1];\n  assert$2(s.length == o, \"IH\"), assert$2(s[0].length == t.units, \"UN\");\n  var i = e.model,\n      l = weightOffset(i),\n      u = t.useBias ? e.layer.weights[1].read().arraySync() : null;\n\n  for (var _e1112 = 0; _e1112 < t.units; _e1112++) {\n    u && addBias(i, u[_e1112]);\n\n    for (var _t756 = 0; _t756 < o; ++_t756) {\n      addWeight(i, s[_t756][_e1112]);\n    }\n\n    alignWeights(i);\n  }\n\n  var c = [loadWeightAddr(Reg.KernelPtr, l), loadDataAddr(Reg.OutputPtr, e.outputOff), repeat(t.units, () => {\n    var s = [];\n    s.push(t.useBias ? load(Reg.S0, 1, Reg.KernelPtr, !0) : load0(Reg.S0)), s.push(loadDataAddr(Reg.InputPtr, e.inputOff));\n\n    var l = e => flatten(load(r, e, Reg.InputPtr, !0), loadWeight(i, a, e), range(e + 1).map(t => [t < e ? vmul(r + t, r + t, a + t) : null, t >= 1 ? vadd(Reg.S0, Reg.S0, r + t - 1) : null])),\n        u = o / n | 0;\n\n    u > 0 && s.push(repeat(u, () => l(n)));\n    var c = o - u * n;\n    return c > 0 && pushRange(s, l(c)), s.push(store(Reg.OutputPtr, Reg.S0, 1, !0)), s.push(relaxWeights()), s;\n  })];\n  return addActivation(c, e), c;\n}\n\nfunction noop(e) {\n  return [];\n}\n\nfunction shapeElts(e) {\n  var t = 1;\n\n  for (var n of e) {\n    null != n && (t *= n);\n  }\n\n  return t;\n}\n\nfunction fixupCompileInfo(e) {\n  void 0 === e.testable && (e.testable = !!e.compile), e.compile || (void 0 === e.inPlace && (e.inPlace = !0), e.compile = noop), e.computePaddedInputShape || (e.computePaddedInputShape = e => e.inputShape.slice());\n}\n\nfunction isInPlace(e) {\n  var t;\n  return !!(null === (t = compilers[e.getClassName()]) || void 0 === t ? void 0 : t.inPlace);\n}\n\nfunction needMInfPadding(e) {\n  var t;\n  return !!(null === (t = compilers[e.getClassName()]) || void 0 === t ? void 0 : t.needsMInfPadding);\n}\n\nfunction shapeToString(e) {\n  return \"[\".concat(e.filter(e => null != e).join(\",\"), \"]\");\n}\n\nfunction assignLayerInfos(e, t) {\n  inited$1 || (inited$1 = !0, Object.values(compilers).forEach(fixupCompileInfo)), reset(), t.verbose && e.summary();\n  var n = e.layers[0].batchInputShape,\n      r = {\n    weightPtr: 0,\n    weightBuffer: new Uint8Array(128),\n    weightAsm: \"\",\n    inputShape: n,\n    outputShape: null,\n    outputOffset: -1,\n    arenaSize: -1,\n    minArenaSize: -1,\n    opts: t,\n    stats: \"\"\n  };\n  var a,\n      s = [shapeElts(n), 0],\n      o = 0,\n      i = s[0];\n\n  var l = e => i = Math.max(e, i);\n\n  for (var _t757 of e.layers) {\n    var _e1113 = getLayerInfo(_t757);\n\n    _e1113.model = r, _e1113.inputShape = a ? a.outputShape : n, _e1113.outputShape = _t757.computeOutputShape(_e1113.inputShape);\n\n    var _i87 = compilers[_t757.getClassName()],\n        _u63 = _i87 ? _i87.computePaddedInputShape(_e1113) : _e1113.inputShape.slice();\n\n    _e1113.inputOff = o, _e1113.rawInputShape = _e1113.inputShape.slice();\n\n    var _c41 = shapeElts(_u63);\n\n    shapeElts(_e1113.inputShape) != _c41 ? (o = 0 == o ? 1 : 0, _e1113.rawInputOff = _e1113.inputOff, _e1113.inputOff = o, _e1113.inputShape = _u63, _c41 > s[o] && (s[o] = _c41), l(_c41 + shapeElts(_e1113.rawInputShape))) : _e1113.rawInputOff = null;\n\n    var _p39 = shapeElts(_e1113.outputShape);\n\n    isInPlace(_t757) ? (l(shapeElts(_e1113.inputShape)), l(shapeElts(_e1113.outputShape))) : (l(shapeElts(_e1113.inputShape) + shapeElts(_e1113.outputShape)), o = 0 == o ? 1 : 0), _e1113.outputOff = o, _p39 > s[o] && (s[o] = _p39), a = _e1113;\n  }\n\n  r.outputShape = a.outputShape;\n  var u = s[0];\n\n  for (var _t758 of e.layers) {\n    var _e1114 = getLayerInfo(_t758);\n\n    _e1114.inputOff && (_e1114.inputOff = u), _e1114.outputOff && (_e1114.outputOff = u), _e1114.rawInputOff && (_e1114.rawInputOff = u), _e1114.stats = {\n      name: _t758.name\n    };\n  }\n\n  var c = s[0] + s[1];\n  return r.arenaSize = c, r.minArenaSize = i, c > 1.2 * i && console.log(\"possible arena shrink with wiser allocation: \" + (c / i).toFixed(3) + \"x\"), r;\n}\n\nfunction compilePadding(e) {\n  var t = [comment(\"padding\")];\n  if (null == e.rawInputOff) return t;\n\n  var n = e.rawInputShape.length >= 4,\n      r = e => ((e = e.slice()).shift(), n || e.unshift(1), e),\n      [a, s, o] = r(e.rawInputShape),\n      [i, l, u] = r(e.inputShape);\n\n  assert$2(o == u);\n  var c = l - s,\n      p = c >> 1,\n      d = c - p,\n      h = i - a,\n      m = h >> 1,\n      f = h - m,\n      g = numFPRegs >> 1,\n      $ = numFPRegs - g,\n      y = Reg.S0 + g;\n  t.push(needMInfPadding(e.layer) ? loadMInf(Reg.S0) : load0(Reg.S0));\n\n  for (var _e1115 = 1; _e1115 < g; ++_e1115) {\n    t.push(vadd(Reg.S0 + _e1115, Reg.S0, Reg.S0));\n  }\n\n  t.push(loadDataAddr(Reg.InputPtr, e.rawInputOff)), t.push(loadDataAddr(Reg.OutputPtr, e.inputOff));\n  var b = d + p,\n      x = d + f * l;\n  return t.push(...v(m * l + p)), t.push(repeat(a - 1, () => flatten(I(s), v(b)))), t.push(...I(s)), t.push(...v(x)), t;\n\n  function v(e) {\n    var t = [],\n        n = (e *= o) % g,\n        r = (e - n) / g;\n    return r && t.push(repeat(r, () => [store(Reg.OutputPtr, Reg.S0, g, !0)])), n && t.push(store(Reg.OutputPtr, Reg.S0, n, !0)), t;\n  }\n\n  function I(e) {\n    var t = [],\n        n = (e *= o) % $,\n        r = (e - n) / $;\n    return r && t.push(repeat(r, () => [load(y, $, Reg.InputPtr, !0), store(Reg.OutputPtr, y, $, !0)])), n && t.push(load(y, n, Reg.InputPtr, !0), store(Reg.OutputPtr, y, n, !0)), t;\n  }\n}\n\nfunction optimizeWithComment(e, t, n) {\n  e.float16weights && (t = fixupAndMarkF16(t));\n  var r = numCycles(t);\n  e.optimize && (t = optimize(t));\n  var a = numCycles(t);\n  n.unoptimizedCycles += r, n.optimizedCycles += a;\n  var s = r ? \"\".concat(a, \" cycles (\").concat((100 * (r - a) / r).toFixed(1), \"% opt)\") : \"(no computation)\";\n  return r && t.unshift(comment(s)), {\n    opcodes: t,\n    optinfo: s\n  };\n}\n\nfunction statsShape(e) {\n  return e.filter(e => null != e);\n}\n\nfunction compileModelCore(m, opts) {\n  var modelInfo = assignLayerInfos(m, opts);\n  void 0 === opts.optimize && (opts.optimize = !0);\n  var ops = [],\n      layerStats = [],\n      layer0 = getLayerInfo(m.layers[0]),\n      layerN = getLayerInfo(m.layers[m.layers.length - 1]),\n      totalStats = {\n    name: \"TOTAL\",\n    inputShape: statsShape(layer0.rawInputShape || layer0.inputShape),\n    outputShape: statsShape(layerN.outputShape),\n    arenaBytes: 0,\n    codeBytes: 0,\n    weightBytes: 0,\n    unoptimizedCycles: 0,\n    optimizedCycles: 0\n  };\n\n  for (var _e1116 of m.layers) {\n    var t = getLayerInfo(_e1116);\n    t.stats.unoptimizedCycles = 0, t.stats.optimizedCycles = 0, t.stats.arenaBytes = 0, t.stats.inputShape = statsShape(t.rawInputShape || t.inputShape), t.stats.outputShape = statsShape(t.outputShape);\n    var n = layerStats.length;\n\n    if (layerStats.push(t.stats), ops.push([label(\"begin_\" + n)]), null != t.rawInputOff) {\n      var _e1117 = optimizeWithComment(opts, compilePadding(t), t.stats);\n\n      ops.push(_e1117.opcodes), t.stats.arenaBytes = shapeElts(t.rawInputShape) + shapeElts(t.inputShape) << 2, t.stats.hasPadding = !0;\n    }\n\n    var r = compilers[_e1116.getClassName()];\n\n    if (r) {\n      var _n446 = weightOffset(modelInfo),\n          a = optimizeWithComment(opts, r.compile(t), t.stats);\n\n      t.stats.weightBytes = weightOffset(modelInfo) - _n446 << 2;\n      var s = \"data: \".concat(shapeToString(t.inputShape), \"@\").concat(t.inputOff, \" => \").concat(shapeToString(t.outputShape), \"@\").concat(t.outputOff),\n          o = \"Layer: \".concat(_e1116.getClassName(), \"; \").concat(s);\n      a.opcodes.unshift(comment(o)), opts.verbose && console.log(o + \" \" + a.optinfo), ops.push(a.opcodes);\n    } else unsupported(\"layer: \" + _e1116.getClassName());\n\n    t.stats.unoptimizedCycles && (t.stats.arenaBytes = Math.max(t.stats.arenaBytes, shapeElts(t.inputShape) + shapeElts(t.outputShape) << 2)), totalStats.unoptimizedCycles += t.stats.unoptimizedCycles, ops.push([label(\"end_\" + n)]);\n  }\n\n  var flat = flatten(ops);\n  var lastInfo = getLayerInfo(m.layers[m.layers.length - 1]);\n  modelInfo.outputOffset = lastInfo.outputOff;\n  var cycles = numCycles(flat),\n      cycleinfo = \"total cycles: \".concat(cycles, \" (\").concat((cycles / 84e3).toFixed(3), \"ms at 84MHz)\");\n  modelInfo.stats = cycleinfo, totalStats.optimizedCycles = cycles, opts.verbose && console.log(modelInfo.stats), modelInfo.weightBuffer = modelInfo.weightBuffer.slice(0, modelInfo.weightPtr);\n  var js = \"\\n\".concat(stringifyComment(modelInfo.stats), \"\\n((weights, mkRuntime) => {\\n    \\\"use strict\\\";\\n    const weightOff = \").concat(modelInfo.arenaSize, \"\\n    const dataOff = 0\\n    const mem = new Float32Array(weightOff + \").concat(weightOffset(modelInfo), \")\\n    mem.fill(1000.2342)\\n    new Uint8Array(mem.buffer).set(weights, weightOff << 2)\\n    const memU32 = new Uint32Array(mem.buffer)\\n    const rt = mkRuntime(mem)\\n    const { softmax, f32 } = rt\\n    return (inputs => {\\n        if (inputs.length != \").concat(shapeElts(getLayerInfo(m.layers[0]).rawInputShape), \")\\n            throw new Error(\\\"invalid input size\\\")\\n        mem.set(inputs, dataOff)\\n        let input, output, kernel\\n        let \").concat(range(numTmpRegs).map(e => \"tmp\" + e).join(\", \"), \"\\n        let \").concat(range(numFPRegs).map(e => \"s\" + e).join(\", \"), \"\\n\\n\").concat(toJSs(modelInfo, flat), \"\\n        \\n        return mem.slice(\").concat(lastInfo.outputOff, \", \").concat(lastInfo.outputOff + shapeElts(lastInfo.outputShape), \")\\n    })\\n})\\n\"),\n      execute = eval(js)(modelInfo.weightBuffer, mkRuntime);\n  var thumb = \"\";\n\n  if (opts.includeTest && opts.testOutput && opts.testOutputFromJS) {\n    var _e1118 = opts.testOutput;\n    opts.testOutput = execute(opts.testInput), thumb = toThumb(modelInfo, flat), opts.testOutput = _e1118;\n  } else thumb = toThumb(modelInfo, flat);\n\n  var res = {\n    execute,\n    js,\n    thumb,\n    machineCode: null,\n    options: opts,\n    memInfo: null,\n    timeInfo: modelInfo.stats,\n    stats: {\n      total: totalStats,\n      layers: layerStats\n    }\n  };\n  return res;\n}\n\nfunction mkRuntime(e) {\n  return {\n    softmax: (t, n) => {\n      var r = e[t];\n\n      for (var _a314 = 1; _a314 < n; ++_a314) {\n        r = Math.max(e[t + _a314], r);\n      }\n\n      var a = 0;\n\n      for (var s = 0; s < n; ++s) {\n        a += e[t + s] = Math.exp(e[t + s] - r);\n      }\n\n      for (var _r441 = 0; _r441 < n; ++_r441) {\n        e[t + _r441] /= a;\n      }\n    },\n    f32: e => {\n      var t = new Float32Array(1);\n      return t[0] = e, t[0];\n    },\n    vcvtb_f32_f16: e => float16AsUintToFloat(65535 & e),\n    vcvtt_f32_f16: e => float16AsUintToFloat(e >> 16 & 65535)\n  };\n}\n\nvar thumbRegs = {\n  r0: 0,\n  r1: 1,\n  r2: 2,\n  r3: 3,\n  r4: 4,\n  r5: 5,\n  r6: 6,\n  r7: 7,\n  r8: 8,\n  r9: 9,\n  r10: 10,\n  r11: 11,\n  r12: 12,\n  sp: 13,\n  r13: 13,\n  lr: 14,\n  r14: 14,\n  pc: 15,\n  r15: 15\n},\n    armConditions = {\n  eq: 0,\n  ne: 1,\n  cs: 2,\n  hs: 2,\n  cc: 3,\n  lo: 3,\n  mi: 4,\n  pl: 5,\n  vs: 6,\n  vc: 7,\n  hi: 8,\n  ls: 9,\n  ge: 10,\n  lt: 11,\n  gt: 12,\n  le: 13,\n  \"\": 14,\n  al: 14\n};\nvar fpRegs;\n\nclass ThumbProcessor extends AbstractProcessor {\n  constructor() {\n    if (super(), this.runtimeIsARM = !1, !fpRegs) {\n      fpRegs = {};\n\n      for (var _e1119 = 0; _e1119 < 32; ++_e1119) {\n        fpRegs[\"s\" + _e1119] = _e1119;\n      }\n    }\n\n    var e = function e(_e1120) {\n      var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : !1;\n\n      for (var n of Object.keys(armConditions)) {\n        (14 != armConditions[n] || t) && _e1120(n, armConditions[n]);\n      }\n    };\n\n    this.addEnc(\"$r0\", \"R0-7\", e => this.inrange(7, e, e)), this.addEnc(\"$r1\", \"R0-7\", e => this.inrange(7, e, e << 3)), this.addEnc(\"$r2\", \"R0-15\", e => this.inrange(15, e, 7 & e | (8 & e) << 4)), this.addEnc(\"$r3\", \"R0-15\", e => this.inrange(15, e, e << 3)), this.addEnc(\"$r4\", \"R0-7\", e => this.inrange(7, e, e << 6)), this.addEnc(\"$r5\", \"R0-7\", e => this.inrange(7, e, e << 8)), this.addEnc(\"$r01\", \"R0-7\", e => this.inrange(7, e, e | e << 3)), this.addEnc(\"$i0\", \"#0-255\", e => this.inrange(255, e, e)), this.addEnc(\"$i1\", \"#0-1020\", e => this.inrange(255, e / 4, e >> 2)), this.addEnc(\"$i2\", \"#0-510\", e => this.inrange(127, e / 4, e >> 2)), this.addEnc(\"$i3\", \"#0-7\", e => this.inrange(7, e, e << 6)), this.addEnc(\"$i4\", \"#0-31\", e => this.inrange(31, e, e << 6)), this.addEnc(\"$i5\", \"#0-124\", e => this.inrange(31, e / 4, e >> 2 << 6)), this.addEnc(\"$i6\", \"#1-32\", e => 0 == e ? null : 32 == e ? 0 : this.inrange(31, e, e << 6)), this.addEnc(\"$i7\", \"#0-62\", e => this.inrange(31, e / 2, e >> 1 << 6)), this.addEnc(\"$i32\", \"#0-2^32\", e => 1), this.addEnc(\"$rl0\", \"{R0-7,...}\", e => this.inrange(255, e, e)), this.addEnc(\"$rl1\", \"{LR,R0-7,...}\", e => 16384 & e ? this.inrange(255, -16385 & e, 256 | 255 & e) : this.inrange(255, e, e)), this.addEnc(\"$rl2\", \"{PC,R0-7,...}\", e => 32768 & e ? this.inrange(255, -32769 & e, 256 | 255 & e) : this.inrange(255, e, e)), this.addEnc(\"$la\", \"LABEL\", e => this.inrange(255, e / 4, e >> 2)).isWordAligned = !0, this.addEnc(\"$lb\", \"LABEL\", e => this.inrangeSigned(127, e / 2, e >> 1)), this.addEnc(\"$lb11\", \"LABEL\", e => this.inrangeSigned(1023, e / 2, e >> 1)), this.addInst(\"adcs  $r0, $r1\", 16704, 65472), this.addInst(\"add   $r2, $r3\", 17408, 65280), this.addInst(\"add   $r5, pc, $i1\", 40960, 63488), this.addInst(\"add   $r5, sp, $i1\", 43008, 63488), this.addInst(\"add   sp, $i2\", 45056, 65408).canBeShared = !0, this.addInst(\"adds  $r0, $r1, $i3\", 7168, 65024), this.addInst(\"adds  $r0, $r1, $r4\", 6144, 65024), this.addInst(\"adds  $r01, $r4\", 6144, 65024), this.addInst(\"adds  $r5, $i0\", 12288, 63488), this.addInst(\"adr   $r5, $la\", 40960, 63488), this.addInst(\"ands  $r0, $r1\", 16384, 65472), this.addInst(\"asrs  $r0, $r1\", 16640, 65472), this.addInst(\"asrs  $r0, $r1, $i6\", 4096, 63488), this.addInst(\"bics  $r0, $r1\", 17280, 65472), this.addInst(\"bkpt  $i0\", 48640, 65280), this.addInst(\"blx   $r3\", 18304, 65415), this.addInst(\"bx    $r3\", 18176, 65408), this.addInst(\"cmn   $r0, $r1\", 17088, 65472), this.addInst(\"cmp   $r0, $r1\", 17024, 65472), this.addInst(\"cmp   $r2, $r3\", 17664, 65280), this.addInst(\"cmp   $r5, $i0\", 10240, 63488), this.addInst(\"eors  $r0, $r1\", 16448, 65472), this.addInst(\"ldmia $r5!, $rl0\", 51200, 63488), this.addInst(\"ldmia $r5, $rl0\", 51200, 63488), this.addInst(\"ldr   $r0, [$r1, $i5]\", 26624, 63488), this.addInst(\"ldr   $r0, [$r1, $r4]\", 22528, 65024), this.addInst(\"ldr   $r5, [pc, $i1]\", 18432, 63488), this.addInst(\"ldr   $r5, $la\", 18432, 63488), this.addInst(\"ldr   $r5, [sp, $i1]\", 38912, 63488).canBeShared = !0, this.addInst(\"ldr   $r5, [sp]\", 38912, 63488).canBeShared = !0, this.addInst(\"ldrb  $r0, [$r1, $i4]\", 30720, 63488), this.addInst(\"ldrb  $r0, [$r1, $r4]\", 23552, 65024), this.addInst(\"ldrh  $r0, [$r1, $i7]\", 34816, 63488), this.addInst(\"ldrh  $r0, [$r1, $r4]\", 23040, 65024), this.addInst(\"ldrsb $r0, [$r1, $r4]\", 22016, 65024), this.addInst(\"ldrsh $r0, [$r1, $r4]\", 24064, 65024), this.addInst(\"lsls  $r0, $r1\", 16512, 65472), this.addInst(\"lsls  $r0, $r1, $i4\", 0, 63488), this.addInst(\"lsrs  $r0, $r1\", 16576, 65472), this.addInst(\"lsrs  $r0, $r1, $i6\", 2048, 63488), this.addInst(\"mov   $r2, $r3\", 17920, 65280), this.addInst(\"movs  $r0, $r1\", 0, 65472), this.addInst(\"movs  $r5, $i0\", 8192, 63488), this.addInst(\"muls  $r0, $r1\", 17216, 65472), this.addInst(\"mvns  $r0, $r1\", 17344, 65472), this.addInst(\"negs  $r0, $r1\", 16960, 65472), this.addInst(\"nop\", 18112, 65535), this.addInst(\"orrs  $r0, $r1\", 17152, 65472), this.addInst(\"pop   $rl2\", 48128, 65024), this.addInst(\"push  $rl1\", 46080, 65024), this.addInst(\"rev   $r0, $r1\", 47616, 65472), this.addInst(\"rev16 $r0, $r1\", 47680, 65472), this.addInst(\"revsh $r0, $r1\", 47808, 65472), this.addInst(\"rors  $r0, $r1\", 16832, 65472), this.addInst(\"sbcs  $r0, $r1\", 16768, 65472), this.addInst(\"sev\", 48960, 65535), this.addInst(\"stm   $r5!, $rl0\", 49152, 63488), this.addInst(\"stmia $r5!, $rl0\", 49152, 63488), this.addInst(\"stmea $r5!, $rl0\", 49152, 63488), this.addInst(\"str   $r0, [$r1, $i5]\", 24576, 63488).canBeShared = !0, this.addInst(\"str   $r0, [$r1]\", 24576, 63488).canBeShared = !0, this.addInst(\"str   $r0, [$r1, $r4]\", 20480, 65024), this.addInst(\"str   $r5, [sp, $i1]\", 36864, 63488).canBeShared = !0, this.addInst(\"str   $r5, [sp]\", 36864, 63488).canBeShared = !0, this.addInst(\"strb  $r0, [$r1, $i4]\", 28672, 63488), this.addInst(\"strb  $r0, [$r1, $r4]\", 21504, 65024), this.addInst(\"strh  $r0, [$r1, $i7]\", 32768, 63488), this.addInst(\"strh  $r0, [$r1, $r4]\", 20992, 65024), this.addInst(\"sub   sp, $i2\", 45184, 65408), this.addInst(\"subs  $r0, $r1, $i3\", 7680, 65024), this.addInst(\"subs  $r0, $r1, $r4\", 6656, 65024), this.addInst(\"subs  $r01, $r4\", 6656, 65024), this.addInst(\"subs  $r5, $i0\", 14336, 63488), this.addInst(\"svc   $i0\", 57088, 65280), this.addInst(\"sxtb  $r0, $r1\", 45632, 65472), this.addInst(\"sxth  $r0, $r1\", 45568, 65472), this.addInst(\"tst   $r0, $r1\", 16896, 65472), this.addInst(\"udf   $i0\", 56832, 65280), this.addInst(\"uxtb  $r0, $r1\", 45760, 65472), this.addInst(\"uxth  $r0, $r1\", 45696, 65472), this.addInst(\"wfe\", 48928, 65535), this.addInst(\"wfi\", 48944, 65535), this.addInst(\"yield\", 48912, 65535), this.addInst(\"cpsid i\", 46706, 65535), this.addInst(\"cpsie i\", 46690, 65535), e((e, t) => this.addInst(\"b\".concat(e, \" $lb\"), 53248 | t << 8, 65280)), this.addInst(\"b     $lb11\", 57344, 63488), this.addInst(\"bal   $lb11\", 57344, 63488), this.addInst(\"bl    $lb\", 61440, 63488), this.addInst(\"bb    $lb\", 57344, 63488), this.addInst(\"ldlit   $r5, $i32\", 18432, 63488), this.addEnc(\"$RL0\", \"{R0-15,...}\", e => this.inrange(65535, e, e)), this.addEnc(\"$R0\", \"R0-15\", e => this.inrange(15, e, e << 8)), this.addEnc(\"$R1\", \"R0-15\", e => this.inrange(15, e, e << 16)), this.addEnc(\"$R2\", \"R0-15\", e => this.inrange(15, e, e << 12)), this.addEnc(\"$R3\", \"R0-15\", e => this.inrange(15, e, e << 0)), this.addEnc(\"$I0\", \"#0-4095\", e => this.inrange(4095, e, 255 & e | (1792 & e) << 4 | (2048 & e) << 15)), this.addEnc(\"$I1\", \"#0-4095\", e => this.inrange(4095, e, e)), this.addEnc(\"$I2\", \"#0-65535\", e => this.inrange(65535, e, 255 & e | (1792 & e) << 4 | (2048 & e) << 15 | (61440 & e) << 4)), this.addEnc(\"$I3\", \"#0-31\", e => this.inrange(31, e, (3 & e) << 6 | e >> 2 << 12)), this.addEnc(\"$LB\", \"LABEL\", e => {\n      var t = e >> 1 & 2047 | (e >> 12 & 63) << 16 | (e >> 18 & 1) << 13 | (e >> 19 & 1) << 11 | (e >> 20 & 1) << 26;\n      return null == this.inrangeSigned(1048575, e / 2, t) ? null : t;\n    }), this.addEnc(\"$S0\", \"S0-31\", e => this.inrange(31, e, e >> 1 << 0 | (1 & e) << 5)), this.addEnc(\"$S1\", \"S0-31\", e => this.inrange(31, e, e >> 1 << 12 | (1 & e) << 22)), this.addEnc(\"$S2\", \"S0-31\", e => this.inrange(31, e, e >> 1 << 16 | (1 & e) << 7)), this.addEnc(\"$SL0\", \"{S0-S31}\", e => {\n      if (!(e |= 0)) return null;\n      var t = 0;\n\n      for (; t < 32 && 0 == (e & 1 << t);) {\n        t++;\n      }\n\n      if (!(e >>>= t)) return null;\n      var n = 0;\n\n      for (; 1 & e;) {\n        e >>= 1, n++;\n      }\n\n      return e ? null : (e = t) >> 1 << 12 | (1 & e) << 22 | n;\n    }), this.addInst32(\"push  $RL0\", 3912040448, 4294901760), this.addInst32(\"pop   $RL0\", 3904700416, 4294901760), this.addInst32(\"addw  $R0, $R1, $I0\", 4060086272, 4226842624), this.addInst32(\"subw  $R0, $R1, $I0\", 4070572032, 4226842624), this.addInst32(\"ldr   $R2, [$R1, $I1]\", 4174381056, 4293918720), this.addInst32(\"str   $R2, [$R1, $I1]\", 4173332480, 4293918720), this.addInst32(\"movw  $R0, $I2\", 4064280576, 4226842624), this.addInst32(\"add   $R0, $R1, $R3, lsl $I3\", 3942645760, 4293951488), this.addInst32(\"subs  $R0, $R1, $i0\", 4054843392, 4293951488), this.addInst32(\"sub   $R0, $R1, $i0\", 4053794816, 4293951488), this.addInst32(\"adds  $R0, $R1, $i0\", 4044357632, 4293951488), this.addInst32(\"add   $R0, $R1, $i0\", 4043309056, 4293951488), e((e, t) => this.addInst32(\"b\".concat(e, \" $LB\"), 4026564608 | t << 22, 4223717376), !0), e((e, t) => this.addInst(\"it \".concat(e), 48904 | t << 4, 65535), !0), this.addInst32(\"vabs.f32     $S1, $S0\", 4004514496, 4290711504), this.addInst32(\"vadd.f32     $S1, $S2, $S0\", 3996125696, 4289728336), this.addInst32(\"vmul.f32     $S1, $S2, $S0\", 3995077120, 4289728336), this.addInst32(\"vcmpe.f32    $S1, #0.0\", 4004842176, 4290711536), this.addInst32(\"vcmpe.f32    $S1, $S0\", 4004776640, 4290711504), this.addInst32(\"vcmp.f32     $S1, #0.0\", 4004842048, 4290711536), this.addInst32(\"vcmp.f32     $S1, $S0\", 4004776512, 4290711504), this.addInst32(\"vdiv.f32     $S1, $S2, $S0\", 4001368576, 4289728336), this.addInst32(\"vfma.f32     $S1, $S2, $S0\", 4003465728, 4289728336), this.addInst32(\"vfms.f32     $S1, $S2, $S0\", 4003465792, 4289728336), this.addInst32(\"vfnma.f32    $S1, $S2, $S0\", 4002417216, 4289728336), this.addInst32(\"vfnms.f32    $S1, $S2, $S0\", 4002417152, 4289728336), this.addInst32(\"vmla.f32     $S1, $S2, $S0\", 3791654160, 4289728272), this.addInst32(\"vmls.f32     $S1, $S2, $S0\", 3793751312, 4289728272), this.addInst32(\"vneg.f32     $S1, $S0\", 4004579904, 4290711504), this.addInst32(\"vsqrt.f32    $S1, $S0\", 4004580032, 4290711504), this.addInst32(\"vsub.f32     $S1, $S2, $S0\", 3996125760, 4289728336), this.addInst32(\"vstmdb       $R1!, $SL0\", 3978299904, 4289728256), this.addInst32(\"vstmia       $R1!, $SL0\", 3969911296, 4289728256), this.addInst32(\"vstmia       $R1, $SL0\", 3967814144, 4289728256), this.addInst32(\"vstm         $R1!, $SL0\", 3969911296, 4289728256), this.addInst32(\"vstm         $R1, $SL0\", 3967814144, 4289728256), this.addInst32(\"vldmdb       $R1!, $SL0\", 3979348480, 4289728256), this.addInst32(\"vldmia       $R1!, $SL0\", 3970959872, 4289728256), this.addInst32(\"vldmia       $R1, $SL0\", 3968862720, 4289728256), this.addInst32(\"vldm         $R1!, $SL0\", 3970959872, 4289728256), this.addInst32(\"vldm         $R1, $SL0\", 3968862720, 4289728256), this.addInst32(\"vldr         $S1, [$R1, $i1]\", 3985639936, 4289728256), this.addInst32(\"vstr         $S1, [$R1, $i1]\", 3984591360, 4289728256), this.addInst32(\"vldr         $S1, [$R1]\", 3985639936, 4289728256), this.addInst32(\"vmrs         APSR_nzcv, fpscr\", 4008835600, 4294967295), this.addInst32(\"vmrs         APSR_nzcv, FPSCR\", 4008835600, 4294967295), this.addInst32(\"vmov.f32     $S1, $S0\", 4004514368, 4290711504), this.addInst32(\"vmov         $S2, $R2\", 3992979984, 4293922687), this.addInst32(\"vmov         $R2, $S2\", 3994028560, 4293922687), this.addInst32(\"vldr         $S1, $la\", 3986622976, 4290711296), this.addInst32(\"vmov.f32     $S1, #1.0\", 4004973056, 4290711536), this.addInst32(\"vcvt.s32.f32 $S1, $S0\", 4005366464, 4290711504), this.addInst32(\"vcvtb.f32.f16 $S1, $S0\", 4004645440, 4290711504), this.addInst32(\"vcvtt.f32.f16 $S1, $S0\", 4004645568, 4290711504), this.addInst32(\"vcvtb.f16.f32 $S1, $S0\", 4004710976, 4290711504), this.addInst32(\"vcvtt.f16.f32 $S1, $S0\", 4004711104, 4290711504);\n  }\n\n  stripCondition(e) {\n    if (e.length >= 5) {\n      var t = e.indexOf(\".\");\n      var n = \"\",\n          r = !1;\n      if (t > 0 && (n = e.slice(t), e = e.slice(0, t), \".32\" == n && (r = !0, n = \"\")), armConditions[e.slice(-2)]) return e.slice(0, -2) + n;\n      if (r) return e;\n    }\n\n    return null;\n  }\n\n  toFnPtr(e, t, n) {\n    return this.runtimeIsARM && /::/.test(n) ? e + t & -2 : e + t | 1;\n  }\n\n  wordSize() {\n    return 4;\n  }\n\n  is32bit(e) {\n    return \"bl\" == e.name || \"bb\" == e.name || e.is32bit;\n  }\n\n  postProcessAbsAddress(e, t) {\n    return (t ^= 1) - e.baseOffset;\n  }\n\n  emit32(e, t, n) {\n    var r = !!(t % 2);\n    r && (t = t + 1 & -4);\n    var a = t >> 1;\n    if (assert(null != a), (0 | a) != a || !(-2097152 < a && a < 2097152)) return emitErr(\"jump out of range\", n);\n    var s = 2047 & a,\n        o = a >> 11 & 1023;\n    return {\n      opcode: 4026531840 & a ? 62464 | o : 61440 | o,\n      opcode2: r ? 59392 | s : 63488 | s,\n      stack: 0,\n      numArgs: [t],\n      labelName: n\n    };\n  }\n\n  expandLdlit(e) {\n    var t,\n        n = !1,\n        r = [],\n        a = {},\n        s = 1;\n\n    for (var o = 0; o < e.lines.length; ++o) {\n      var i = e.lines[o];\n\n      if (r.push(i), \"instruction\" == i.type && i.instruction && \"ldlit\" == i.instruction.name) {\n        if (!t) {\n          var _r443 = i.location + 900,\n              _a315 = o + 1;\n\n          for (; _a315 < e.lines.length && !(e.lines[_a315].location > _r443); ++_a315) {\n            var _n447 = e.lines[_a315].getOp();\n\n            (\"b\" == _n447 || \"bb\" == _n447 || \"pop\" == _n447 && \"pc\" == e.lines[_a315].words[2]) && (t = e.lines[_a315]);\n          }\n\n          if (t) n = !1;else for (n = !0; --_a315 > o;) {\n            if (\"instruction\" == e.lines[_a315].type) {\n              t = e.lines[_a315];\n              break;\n            }\n          }\n        }\n\n        var _r442 = i.words[1],\n            l = \"#\" + i.words[3],\n            u = lookup(a, l);\n        u || (u = \"_ldlit_\" + ++s, a[l] = u), i.update(\"ldr \".concat(_r442, \", \").concat(u));\n      }\n\n      if (i === t) {\n        t = null;\n\n        var _o158 = [],\n            _l69 = \"_jmpwords_\" + ++s;\n\n        n && _o158.push(\"bb \" + _l69), _o158.push(\".balign 4\");\n\n        for (var _e1121 of Object.keys(a)) {\n          _o158.push(a[_e1121] + \": .word \" + _e1121.slice(1));\n        }\n\n        n && _o158.push(_l69 + \":\");\n\n        for (var _t759 of _o158) {\n          e.buildLine(_t759, r);\n          var _n448 = r[r.length - 1];\n          _n448.scope = i.scope, _n448.lineNo = i.lineNo;\n        }\n\n        a = {};\n      }\n    }\n\n    e.lines = r;\n  }\n\n  getAddressFromLabel(e, t, n) {\n    var r = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : !1;\n    var a = e.lookupLabel(n);\n    if (null == a) return null;\n    var s = e.location() + 4;\n    return r && (s &= 4294967292), a - s;\n  }\n\n  isPop(e) {\n    return 48128 == e;\n  }\n\n  isPush(e) {\n    return 46080 == e;\n  }\n\n  isAddSP(e) {\n    return 45056 == e;\n  }\n\n  isSubSP(e) {\n    return 45184 == e;\n  }\n\n  peephole(e, t, n) {\n    var r = this.encoders.$lb11,\n        a = this.encoders.$lb;\n\n    function s(e, t) {\n      return null != e.encode(t.numArgs[0] + 8) && null != e.encode(t.numArgs[0] - 8) && null != e.encode(t.numArgs[0]);\n    }\n\n    var o = e.getOp(),\n        i = !1;\n    \"bne\" != o && \"beq\" != o || (\"b\" == t.getOp() && 0 == e.numArgs[0] && (i = !0), \"bb\" == t.getOp() && 2 == e.numArgs[0] && (i = !0)), \"bb\" == o && s(r, e) ? e.update(\"b \" + e.words[1]) : \"b\" == o && -2 == e.numArgs[0] ? e.update(\"\") : \"bne\" == o && i && s(a, t) ? (e.update(\"beq \" + t.words[1]), t.update(\"\")) : \"beq\" == o && i && s(a, t) ? (e.update(\"bne \" + t.words[1]), t.update(\"\")) : \"push\" != o || 16384 != e.numArgs[0] || \"push\" != t.getOp() || 16384 & t.numArgs[0] ? \"pop\" == o && \"pop\" == t.getOp() && 32768 == t.numArgs[0] ? (e.update(e.text.replace(\"}\", \", pc}\")), t.update(\"\")) : \"push\" == o && \"pop\" == t.getOp() && e.numArgs[0] == t.numArgs[0] ? (assert(e.numArgs[0] > 0), e.update(\"\"), t.update(\"\")) : \"push\" == o && \"pop\" == t.getOp() && 4 == e.words.length && 4 == t.words.length ? (assert(\"{\" == e.words[1]), e.update(\"mov \" + t.words[2] + \", \" + e.words[2]), t.update(\"\")) : n && \"movs $r5, $i0\" == e.getOpExt() && \"mov $r0, $r1\" == t.getOpExt() && e.numArgs[0] == t.numArgs[1] && clobbersReg(n, e.numArgs[0]) ? (e.update(\"movs r\" + t.numArgs[0] + \", #\" + e.numArgs[1]), t.update(\"\")) : \"pop\" == o && singleReg(e) >= 0 && \"push\" == t.getOp() && singleReg(e) == singleReg(t) ? (e.update(\"ldr r\" + singleReg(e) + \", [sp, #0]\"), t.update(\"\")) : \"push\" == o && \"ldr $r5, [sp, $i1]\" == t.getOpExt() && singleReg(e) == t.numArgs[0] && 0 == t.numArgs[1] ? t.update(\"\") : n && \"push\" == o && singleReg(e) >= 0 && preservesReg(t, singleReg(e)) && \"pop\" == n.getOp() && singleReg(e) == singleReg(n) && (e.update(\"\"), n.update(\"\")) : (e.update(t.text.replace(\"{\", \"{lr, \")), t.update(\"\"));\n  }\n\n  registerNo(e, t) {\n    if (!e) return null;\n    e = e.toLowerCase();\n    var n = thumbRegs;\n    \"S\" == t.name[1] && (n = fpRegs);\n    var r = n[e];\n    return void 0 === r ? null : r;\n  }\n\n  testAssembler() {\n    expectError(this, \"lsl r0, r0, #8\"), expectError(this, \"push {r17}\"), expectError(this, \"mov r0, r1 foo\"), expectError(this, \"movs r14, #100\"), expectError(this, \"push {r0\"), expectError(this, \"push lr,r0}\"), expectError(this, \"b #+11\"), expectError(this, \"b #+10240000\"), expectError(this, \"bne undefined_label\"), expectError(this, \".foobar\"), expect$1(this, \"0200      lsls    r0, r0, #8\\nb500      push    {lr}\\n2064      movs    r0, #100        ; 0x64\\nb401      push    {r0}\\nbc08      pop     {r3}\\nb501      push    {r0, lr}\\nbd20      pop {r5, pc}\\nbc01      pop {r0}\\n4770      bx      lr\\n0000      .balign 4\\ne6c0      .word   -72000\\nfffe\\n\"), expect$1(this, \"4291      cmp     r1, r2\\nd100      bne     l6\\ne000      b       l8\\n1840  l6: adds    r0, r0, r1\\n4718  l8: bx      r3\\n\"), expect$1(this, \"          @stackmark base\\nb403      push    {r0, r1}\\n          @stackmark locals\\n9801      ldr     r0, [sp, locals@1]\\nb401      push    {r0}\\n9802      ldr     r0, [sp, locals@1]\\nbc01      pop     {r0}\\n          @stackempty locals\\n9901      ldr     r1, [sp, locals@1]\\n9102      str     r1, [sp, base@0]\\n          @stackempty locals\\nb002      add     sp, #8\\n          @stackempty base\\n\"), expect$1(this, \"b090      sub sp, #4*16\\nb010      add sp, #4*16\\n\"), expect$1(this, '6261      .string \"abc\"\\n0063      \\n'), expect$1(this, '6261      .string \"abcde\"\\n6463      \\n0065      \\n'), expect$1(this, \"3042      adds r0, 0x42\\n1c0d      adds r5, r1, #0\\nd100      bne #0\\n2800      cmp r0, #0\\n6b28      ldr r0, [r5, #48]\\n0200      lsls r0, r0, #8\\n2063      movs r0, 0x63\\n4240      negs r0, r0\\n46c0      nop\\nb500      push {lr}\\nb401      push {r0}\\nb402      push {r1}\\nb404      push {r2}\\nb408      push {r3}\\nb520      push {r5, lr}\\nbd00      pop {pc}\\nbc01      pop {r0}\\nbc02      pop {r1}\\nbc04      pop {r2}\\nbc08      pop {r3}\\nbd20      pop {r5, pc}\\n9003      str r0, [sp, #4*3]\\n\");\n  }\n\n}\n\nfunction preservesReg(e, t) {\n  return \"movs $r5, $i0\" == e.getOpExt() && e.numArgs[0] != t;\n}\n\nfunction clobbersReg(e, t) {\n  return !!(\"pop\" == e.getOp() && e.numArgs[0] & 1 << t);\n}\n\nfunction singleReg(e) {\n  assert(\"push\" == e.getOp() || \"pop\" == e.getOp());\n  var t = 0,\n      n = -1,\n      r = e.numArgs[0];\n\n  for (; r > 0;) {\n    1 & r && (n = -1 == n ? t : -2), r >>= 1, t++;\n  }\n\n  return n >= 0 ? n : -1;\n}\n\nvar epsF32 = 2e-5,\n    epsF16 = .0045;\n\nfunction mkProcessorFile() {\n  var e = new File$1(new ThumbProcessor());\n  return e.ei.testAssembler(), e.disablePeepHole = !0, e.lookupExternalLabel = e => null, e.normalizeExternalLabel = e => e, e.throwOnError = !0, e;\n}\n\nfunction throwAssemblerErrors(e) {\n  if (e.errors.length > 0) throw new Error(e.errors[0].message);\n}\n\nfunction assemble(e) {\n  var t = mkProcessorFile();\n  t.emit(e), throwAssemblerErrors(t);\n  var n = new Uint8Array(t.buf.length << 1);\n\n  for (var _e1122 = 0; _e1122 < t.buf.length; ++_e1122) {\n    n[_e1122 << 1] = 255 & t.buf[_e1122], n[1 + (_e1122 << 1)] = t.buf[_e1122] >> 8 & 255;\n  }\n\n  return {\n    binary: n,\n    procFile: t\n  };\n}\n\nfunction randomTensor(e) {\n  var t = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n  var n = shapeElts(e = e.map(e => null == e ? 1 : e));\n  return dist.tidy(() => dist.tensor(range(n).map(e => t * randomSFloat())).reshape(e));\n}\n\nfunction isNear(e, t, n) {\n  var r = Math.abs(e - t);\n  return r < n || r / (Math.abs(e) + Math.abs(t)) < n;\n}\n\nfunction optionsWithTestData(e, t) {\n  t = flatClone(t);\n  var n = 0,\n      r = 0;\n\n  var _loop59 = function _loop59() {\n    var s = randomTensor(e.inputs[0].shape),\n        o = e.predict(s).flatten().arraySync();\n    var i = 0,\n        l = 1;\n\n    for (var _e1123 of o) {\n      i += _e1123, l *= _e1123;\n    }\n\n    if (!(Math.abs(i - 1) < .1)) {\n      a();\n      return \"break\";\n    }\n\n    if (l > r && (r = l, a()), n++ > (t.includeTest ? 1e3 : 100) || r > .1) {\n      l || a();\n      return \"break\";\n    }\n\n    function a() {\n      t.testInput = s.flatten().arraySync(), t.testOutput = o;\n    }\n  };\n\n  for (;;) {\n    var _ret4 = _loop59();\n\n    if (_ret4 === \"break\") break;\n  }\n\n  return t;\n}\n\nfunction compileModel(e, t) {\n  var n = compileModelCore(e, t),\n      r = assemble(n.thumb);\n  n.machineCode = r.binary;\n  var a = 0;\n\n  for (var _e1124 of n.stats.layers) {\n    _e1124.codeBytes = r.procFile.lookupLabel(\"end_\" + a) - r.procFile.lookupLabel(\"begin_\" + a), a++;\n  }\n\n  var s = getStatsFromBin(n.machineCode, n.stats.total);\n  return n.memInfo = s.info, n;\n}\n\nfunction validateCompilation(e) {\n  var t = e.options,\n      n = t.testOutput,\n      r = e.execute(t.testInput);\n  e.options.verbose && console.log(\"Test output\", r);\n  var a = 0;\n\n  for (var _e1125 = 0; _e1125 < r.length && (isNear(n[_e1125], r[_e1125], t.float16weights ? epsF16 : epsF32) || (console.log(\"at \".concat(_e1125, \" \").concat(n[_e1125], \"[exp] - \").concat(r[_e1125], \" = \").concat(n[_e1125] - r[_e1125])), a++, !(a > 5))); ++_e1125) {\n    ;\n  }\n\n  if (a) throw new Error(\"mismatch\");\n}\n\nfunction compileAndTest(e, t) {\n  var n;\n\n  try {\n    return n = compileModel(e, t = optionsWithTestData(e, t)), validateCompilation(n), n;\n  } catch (r) {\n    throw t.info && console.log(t.info), n && t.verbose || (t.verbose = !0, n = compileModelCore(e, t)), console.log(n.js), console.log(\"Failing model: \", e.name), r;\n  }\n}\n\nfunction readU32(e, t) {\n  return (e[t] | e[t + 1] << 8 | e[t + 2] << 16 | e[t + 3] << 24) >>> 0;\n}\n\nfunction readU32s(e) {\n  var t = [];\n\n  for (var n = 0; n < e.length; n += 4) {\n    t.push(readU32(e, n));\n  }\n\n  return t;\n}\n\nfunction getStatsFromBin(e, t) {\n  var [n, r, a, s, o, i, l, u] = readU32s(e.slice(0, 64));\n  if (809963362 != n) return null;\n  var c = i || s,\n      p = o - a,\n      d = 100 * p / c,\n      h = s - c;\n\n  function m(e) {\n    return (e / 1024).toFixed(2) + \"k\";\n  }\n\n  var f = \"model: \".concat(m(c), \"; code: \").concat(m(p), \" (\").concat(d.toFixed(1), \"%); arena: \").concat(m(u), \"; test \").concat(m(h));\n  return t && (t.arenaBytes = u, t.codeBytes = p, t.weightBytes = c - p), {\n    info: f,\n    modelSize: c,\n    codeSize: p,\n    testSize: h,\n    totalSize: s,\n    arenaSize: u\n  };\n}\n\nvar compileAndTest_1 = compileAndTest,\n    compileModel_1 = compileModel;\nvar _excluded = [\"worker\"];\n\nfunction addLayer(e, t, n) {\n  var r;\n  var a = e.inputs[0].fields.expand_button.value;\n\n  switch (e.type) {\n    case \"model_block_conv1d_layer\":\n      r = conv1d$2(t ? {\n        inputShape: t,\n        kernelSize: [a.kernelSize],\n        strides: a.strideSize,\n        filters: a.numFilters,\n        activation: a.activation,\n        padding: \"same\"\n      } : {\n        kernelSize: [a.kernelSize],\n        strides: a.strideSize,\n        filters: a.numFilters,\n        activation: a.activation,\n        padding: \"same\"\n      });\n      break;\n\n    case \"model_block_maxpool1d_layer\":\n      r = maxPooling1d$1(t ? {\n        inputShape: t,\n        poolSize: [a.poolSize],\n        padding: \"same\"\n      } : {\n        poolSize: [a.poolSize],\n        padding: \"same\"\n      });\n      break;\n\n    case \"model_block_avgpool1d_layer\":\n      break;\n\n    case \"model_block_conv2d_layer\":\n      r = conv2d$5(t ? {\n        inputShape: t,\n        kernelSize: [a.kernelSize, a.kernelSize],\n        strides: a.strideSize,\n        filters: a.numFilters,\n        activation: a.activation,\n        padding: \"same\"\n      } : {\n        kernelSize: [a.kernelSize, a.kernelSize],\n        strides: a.strideSize,\n        filters: a.numFilters,\n        activation: a.activation,\n        padding: \"same\"\n      });\n      break;\n\n    case \"model_block_maxpool2d_layer\":\n      r = maxPooling2d$1(t ? {\n        inputShape: t,\n        poolSize: [a.poolSize, a.poolSize],\n        padding: \"same\"\n      } : {\n        poolSize: [a.poolSize, a.poolSize],\n        padding: \"same\"\n      });\n      break;\n\n    case \"model_block_avgpool2d_layer\":\n      break;\n\n    case \"model_block_dropout_layer\":\n      r = dropout$3({\n        rate: a.rate\n      });\n      break;\n\n    case \"model_block_flatten_layer\":\n      r = t ? flatten$4({\n        inputShape: t\n      }) : flatten$4();\n      break;\n\n    case \"model_block_dense_layer\":\n      r = dense$1({\n        units: n || a.numUnits,\n        activation: n ? \"softmax\" : a.activation\n      });\n      break;\n\n    default:\n      console.error(\"Received an invalid layer type\");\n  }\n\n  return r;\n}\n\nfunction buildDefaultModel(e, t, n) {\n  e.add(conv2d$5({\n    inputShape: t,\n    kernelSize: [4, 4],\n    strides: 1,\n    filters: 16,\n    padding: \"same\",\n    activation: \"relu\"\n  })), e.add(maxPooling2d$1({\n    poolSize: [2, 2],\n    padding: \"same\"\n  })), e.add(dropout$3({\n    rate: .1\n  })), e.add(conv2d$5({\n    kernelSize: [2, 2],\n    strides: 1,\n    filters: 16,\n    padding: \"same\",\n    activation: \"relu\"\n  })), e.add(maxPooling2d$1({\n    poolSize: [2, 2],\n    padding: \"same\"\n  })), e.add(dropout$3({\n    rate: .1\n  })), e.add(conv2d$5({\n    kernelSize: [2, 2],\n    strides: 1,\n    filters: 16,\n    padding: \"same\",\n    activation: \"relu\"\n  })), e.add(dropout$3({\n    rate: .1\n  })), e.add(flatten$4()), e.add(dense$1({\n    units: n,\n    activation: \"softmax\"\n  }));\n}\n\nfunction buildModelFromJSON(e, t) {\n  var n = sequential$1();\n  var r;\n  if (\"default\" == t) e.inputShape.push(1), buildDefaultModel(n, e.inputShape, e.outputShape), r = {\n    loss: \"categoricalCrossentropy\",\n    optimizer: \"adam\",\n    metrics: [\"acc\"],\n    epochs: 250\n  };else {\n    \"2d\" == e.convolutionType && e.inputShape.push(1);\n    var s = t.inputs.filter(e => \"LAYER_INPUTS\" == e.name)[0].child;\n    var o = addLayer(s, e.inputShape, null);\n    var a;\n    n.add(o), s && (null == (a = s.children) || a.forEach((t, r) => {\n      o = addLayer(t, null, r == s.children.length - 1 ? e.outputShape : null), n.add(o);\n    }));\n    var i = t.inputs[1].fields.expand_button.value;\n    r = {\n      loss: i.lossFn,\n      optimizer: i.optimizer,\n      metrics: [i.metrics],\n      epochs: i.numEpochs\n    };\n  }\n  return {\n    model: n,\n    params: r\n  };\n}\n\nvar handlers = {\n  compile: function () {\n    var _compile = _asyncToGenerator(function* (e) {\n      var {\n        data: t\n      } = e,\n          {\n        model: n,\n        params: r\n      } = buildModelFromJSON(t.model, t.modelBlockJSON);\n      var a, s;\n      yield n.save({\n        save: e => {\n          a = e;\n          var t = {\n            modelArtifactsInfo: {\n              dateSaved: new Date(),\n              modelTopologyType: \"JSON\"\n            }\n          };\n          return Promise.resolve(t);\n        }\n      }), a.weightData = null;\n\n      try {\n        s = yield compileModel_1(n, {\n          verbose: !1,\n          float16weights: !0,\n          optimize: !0\n        });\n      } catch (t) {\n        return console.error(\"Error with getting model stats during compiling: \", t), _extends({}, e, {\n          data: void 0\n        });\n      }\n\n      var o = [...s.stats.layers, s.stats.total];\n      return _extends({}, e, {\n        data: {\n          modelJSON: a,\n          modelStats: o,\n          trainingParams: r\n        }\n      });\n    });\n\n    function compile(_x140) {\n      return _compile.apply(this, arguments);\n    }\n\n    return compile;\n  }(),\n  train: function () {\n    var _train = _asyncToGenerator(function* (e) {\n      var {\n        data: t\n      } = e;\n      if (t.xData[0].length != t.model.inputShape[0] || t.xData[0][0].length != t.model.inputShape[1]) return console.error(\"Input data does not match expected shape of model.\"), _extends({}, e, {\n        data: void 0\n      });\n      var n = t.model.modelJSON,\n          r = yield loadLayersModel$1({\n        load: () => Promise.resolve(n)\n      });\n      var a = tensor3d$1(t.xData, [t.xData.length, t.model.inputShape[0], t.model.inputShape[1]]);\n      n.modelTopology.config.layers[0].class_name.indexOf(\"2D\") > -1 && (a = a.expandDims(3));\n      var s = oneHot$5(tensor1d$1(t.yData, \"int32\"), t.model.labels.length),\n          o = t.trainingParams;\n\n      try {\n        r.compile({\n          loss: o.loss,\n          optimizer: o.optimizer,\n          metrics: o.metrics\n        });\n      } catch (t) {\n        return console.error(\"Error with model.compile during training: \", t), _extends({}, e, {\n          data: void 0\n        });\n      }\n\n      var i,\n          l = [];\n\n      try {\n        yield r.fit(a, s, {\n          epochs: o.epochs,\n          validationSplit: t.xData.length > 40 ? .1 : 0,\n          callbacks: {\n            onEpochEnd\n          }\n        }).then(e => {\n          l = e.history.acc;\n        });\n      } catch (t) {\n        return console.error(\"Error with model.fit during training: \", t), _extends({}, e, {\n          data: void 0\n        });\n      }\n\n      yield r.save({\n        save: e => {\n          i = e;\n          var t = {\n            modelArtifactsInfo: {\n              dateSaved: new Date(),\n              modelTopologyType: \"JSON\"\n            }\n          };\n          return Promise.resolve(t);\n        }\n      });\n      var u = i.weightData;\n      var c;\n      i.weightData = null;\n\n      try {\n        c = yield compileAndTest_1(r, {\n          verbose: !1,\n          includeTest: !0,\n          float16weights: !0,\n          optimize: !0\n        });\n      } catch (e) {\n        console.error(\"Error compiling arm model during training: \", e);\n      }\n\n      return _extends({}, e, {\n        data: {\n          modelWeights: u,\n          trainingLogs: l,\n          armModel: JSON.stringify(c)\n        }\n      });\n    });\n\n    function train(_x141) {\n      return _train.apply(this, arguments);\n    }\n\n    return train;\n  }(),\n  predict: function () {\n    var _predict = _asyncToGenerator(function* (e) {\n      var {\n        data: t\n      } = e,\n          n = t.model.modelJSON;\n      n.weightData = new Uint32Array(t.model.weights).buffer;\n      var r = yield loadLayersModel$1({\n        load: () => Promise.resolve(n)\n      });\n      var a,\n          s = tensor$1(t.zData);\n      n.modelTopology.config.layers[0].class_name.indexOf(\"2D\") > -1 && (s = s.expandDims(3));\n\n      try {\n        a = yield r.predict(s);\n      } catch (t) {\n        return console.error(\"Error with model.predict during prediction: \", t), _extends({}, e, {\n          data: void 0\n        });\n      }\n\n      var o = yield a.argMax(1).dataSync(),\n          i = yield a.dataSync(),\n          l = [],\n          u = t.zData.length,\n          c = t.model.labels.length;\n\n      for (var _e1126 = 0; _e1126 < u; _e1126++) {\n        var _n449 = {};\n\n        for (var _r444 = 0; _r444 < c; _r444++) {\n          _n449[t.model.labels[_e1126 * c + _r444]] = i[_e1126 * c + _r444];\n        }\n\n        l.push(_n449);\n      }\n\n      return _extends({}, e, {\n        data: {\n          predictAll: l,\n          predictTop: o\n        }\n      });\n    });\n\n    function predict(_x142) {\n      return _predict.apply(this, arguments);\n    }\n\n    return predict;\n  }()\n};\n\nfunction dispatchAsyncMessages(_x143) {\n  return _dispatchAsyncMessages.apply(this, arguments);\n}\n\nfunction _dispatchAsyncMessages() {\n  _dispatchAsyncMessages = _asyncToGenerator(function* (e) {\n    try {\n      var t = handlers[e.type];\n      return yield null == t ? void 0 : t(e);\n    } catch (e) {\n      return void console.error(e);\n    }\n  });\n  return _dispatchAsyncMessages.apply(this, arguments);\n}\n\nfunction handleMessage(_x144) {\n  return _handleMessage.apply(this, arguments);\n}\n\nfunction _handleMessage() {\n  _handleMessage = _asyncToGenerator(function* (e) {\n    var t = e.data,\n        {\n      worker: n\n    } = t;\n    if (_objectWithoutPropertiesLoose(t, _excluded), \"tf\" !== n) return;\n    var r = yield dispatchAsyncMessages(t);\n    self.postMessage(r);\n  });\n  return _handleMessage.apply(this, arguments);\n}\n\nfunction onEpochEnd(e, t) {\n  self.postMessage({\n    type: \"progress\",\n    data: t\n  });\n}\n\nself.addEventListener(\"message\", handleMessage), console.debug(\"jacdac tf: worker registered\");"],"names":[],"sourceRoot":""}